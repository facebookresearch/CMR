{"model_update_steps": 1665, "method_class": "mir", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=0212_debug=reverse_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=True, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=0212_debug=reverse_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=False)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "after_eval": {"predictions": ["The Snowman", "at larger distances", "French 18th-century art and furnishings", "turbine", "the adaptive immune system", "Chinghiz, Chinghis, and Chingiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "the 50 fund", "ned Sherrin", "the Mandate of Heaven", "Tar- Baby", "buddha", "Poseidon", "Washington", "2009", "the direction from which the wind is blowing", "jabs at the soft underbelly of hypocrisy, pomposity and upper class twits", "oxygen", "Georges Seurat", "golf", "the Missouri River", "An elevator with a counterbalance", "137th", "piranha", "the student's transition from the study of preclinical to clinical health sciences", "prague", "Geoffrey Hutchings", "1998", "great train robbery", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "a metaphor for a burden to be carried as penance", "power blackouts across the country"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9104166666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.8]}}, "forgotten_examples": [], "retained_ids": [], "fixed_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-7017"], "unfixed_ids": ["mrqa_squad-validation-7821", "mrqa_triviaqa-validation-1551", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3490", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.84375, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "philanthropy", "Catherine Zeta Jones", "Virginia Wade", "Crystal Gayle and Gary Morris", "to the anterolateral corner of the spinal cord", "1966", "radioisotope thermoelectric generator", "product or policy that is open and honest", "The Stock Market crash in New York", "New York Stadium", "norman Tebbit", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "quran", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "Charlie", "acmthompson", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.125, "QA-F1": 0.20247980505333446}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.23529411764705885, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.07407407407407407, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "after_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "originally designated HU - 1", "philanthropy", "Mariette", "Sue Barker and Joe Durie", "Gary Morris", "usually occurs 1 - 2 spinal nerve segments above the point of entry", "current denomination of U.S. currency", "either small fission systems or radioactive decay for electricity or heat", "ronseal", "The Stock Market crash in New York led people to hoard their money ; as consumption fell, the American economy steadily contracted, 1929 - 32", "rotherham united", "tory MP Andrew Mitchell", "Pangaea or Pangea", "red", "The comptroller ( who is also auditor general and head of the National Audit Office )", "Pedro Rodr\u00edguez", "65 mi", "an obsessed and tormented king", "three years before his death", "islam", "Honda Ballade", "illnesses", "glowed even when turned off", "Sister Anthony, S.C.", "Stroh's", "numb3rs", "for gallantry", "Danny DeVito", "Galileo", "Superman", "2010"], "metric_results": {"EM": 0.875, "QA-F1": 0.9067460317460317}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4684", "before_prediction": "The Stock Market crash in New York", "after_prediction": "The Stock Market crash in New York led people to hoard their money ; as consumption fell, the American economy steadily contracted, 1929 - 32"}, {"id": "mrqa_triviaqa-validation-1924", "before_prediction": "norman Tebbit", "after_prediction": "tory MP Andrew Mitchell"}], "retained_ids": ["mrqa_squad-validation-10015", "mrqa_squad-validation-1516"], "fixed_ids": ["mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "unfixed_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-5406"], "instant_fixing_rate": 0.9285714285714286, "instant_retention_rate": 0.49999999875}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "merioneth and Llantisilly Rail Traction Company Limited", "acetic acid", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac", "sprawl", "a student in the second year", "Bothtec", "Terry Reid", "reports from government agencies and non-governmental organizations", "is not played", "North America", "Andr\u00e9 3000", "rookies", "Akhenaten", "President Theodore Roosevelt", "from 2010 to 2012", "eight", "United States", "1980s", "bizet", "Matt Winer", "1689", "Pacific"], "metric_results": {"EM": 0.15625, "QA-F1": 0.29630300255300257}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.18181818181818182, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.4, 0.8571428571428571, 0.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-683", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113"], "after_eval": {"predictions": ["motor ships", "cricket, rallying, football, rugby union and boxing", "never affiliated with any particular denomination", "*", "2003", "eleven separate regions of the Old and New World", "idris", "polyatomic anion", "Jan Kazimierz", "casket letters", "A55", "oceanic species", "Everywhere", "Many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work", "second", "a cover band", "Spencer Davis Group", "non-peer-reviewed sources", "Enigma Variations", "physiographically a part of the continent of North America", "OutKast", "rookies", "the Aten, a representation of the Egyptian god, Ra", "Roosevelt Corollary", "2010 to 2012", "four", "United States, its NATO allies and others", "in the very late 1980s", "carmen", "Matthew Ward Winer", "1700", "Pacific"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8531746031746033}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3242", "before_prediction": "Bothtec", "after_prediction": "a cover band"}, {"id": "mrqa_naturalquestions-validation-5180", "before_prediction": "United States", "after_prediction": "United States, its NATO allies and others"}, {"id": "mrqa_triviaqa-validation-2722", "before_prediction": "bizet", "after_prediction": "carmen"}], "retained_ids": ["mrqa_squad-validation-4019", "mrqa_squad-validation-4283"], "fixed_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-683", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113"], "unfixed_ids": ["mrqa_squad-validation-7149", "mrqa_hotpotqa-validation-2679"], "instant_fixing_rate": 0.9259259259259259, "instant_retention_rate": 0.3999999992}, {"timecode": 3, "before_eval": {"predictions": ["id", "2016", "180 degrees", "between 27 July and 7 August 2022", "New York", "wet", "2006", "Least of the Great Powers", "lower motor neurons, the efferent nerves that directly innervate muscles", "september", "Glacier Mints", "epson Derby", "coronary thrombosis", "video film", "Overtime", "Sir Henry Cole", "at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "The Young Ones (TV series )", "cement City, Texas", "The plan was overseen by Arlene Foster of the Democratic Unionist Party (DUP, the then-minister for Enterprise, Trade and Investment, who failed to introduce proper cost controls, allowing the plan to spiral out of control", "23 July 1989", "many educational institutions especially within the US, require that prospective teachers pass a background check and psychiatric evaluation to be able to teach in classroom", "the West", "data", "island in the Sun", "callable bonds", "2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM, either 16 or 32 GB of internal storage, and a 2300 mAh battery", "over 10,000 British and 2,000 old master works", "khimar", "proteins carried genetic information", "gallbladder", "venatio Alger"], "metric_results": {"EM": 0.1875, "QA-F1": 0.24847333446123765}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 1.0, 0.0, 0.0, 0.10256410256410256, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.45161290322580644, 0.4, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-6341", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "after_eval": {"predictions": ["period", "qpr", "360", "2022", "Staten Island", "splash", "2005\u20132010", "G20", "efferent nerves", "max Bygraves", "polar bear", "lester piggott", "don't disturb\" sign", "nigeria", "the shootout", "commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843", "it has trouble distinguishing between carbon dioxide and oxygen", "adrian edmondson", "1934", "Northern Ireland Assembly for Fermanagh and South Tyrone", "1989", "US", "the West", "data", "ringo", "callable bonds", "a 2.26 GHz quad - core Snapdragon 800 processor", "10,000", "hijab", "The results of the Avery -- MacLeod -- McCarty experiment", "gallbladder", "meyer"], "metric_results": {"EM": 0.875, "QA-F1": 0.9114583333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-8832", "before_prediction": "at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "after_prediction": "it has trouble distinguishing between carbon dioxide and oxygen"}, {"id": "mrqa_hotpotqa-validation-5662", "before_prediction": "23 July 1989", "after_prediction": "1989"}], "retained_ids": ["mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2385", "mrqa_triviaqa-validation-6800"], "fixed_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-6341", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818"], "unfixed_ids": ["mrqa_squad-validation-1539", "mrqa_triviaqa-validation-2530"], "instant_fixing_rate": 0.9230769230769231, "instant_retention_rate": 0.6666666655555555}, {"timecode": 4, "before_eval": {"predictions": ["Austria", "the Waal distributary of the Rhine", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "month of Juno", "the man chosen to meet God on Sinai and receive the Law on behalf of God\u2019s chosen people", "is a peninsula in the southernmost part of Brooklyn, New York", "is a rare but often fatal disease that affects the central nervous system by causing painful muscular contractions", "bounding the time or space used by the algorithm", "meyer", "Lieutenant Commander Steve McGarrett", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "1934", "the Reverse - Flash", "All Hallows'Day", "A's", "baku", "Catholics", "CeCe Drake", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America, and Quebec", "comprehension and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "rotating discs", "Splodgenessabounds"], "metric_results": {"EM": 0.25, "QA-F1": 0.3750744047619048}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.25, 0.0, 0.6666666666666666, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.75, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840"], "after_eval": {"predictions": ["United Kingdom", "Arnhem", "30 days after the original air date", "discontinued", "Gryphon", "16 April 1898", "june", "aaron", "new york city", "tetanus", "bounding", "facets", "Steve McGarrett", "Edward Joseph Leonski", "Lynwood", "cocaine", "a chain or screw stoking mechanism", "Fleet Street", "Professor Eobard Thawne", "All Saints ( or All Hallows )", "kansas city", "azerbaijan", "new converts", "CeCe Drake", "cricket", "Tania Miller", "8 April 1912", "Quebec", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "rotating discs", "Beverly Callard, Lee Oakes, Hayley Bishop, Alison Mac, Thomas Nelstrop, and Jonathon Dutton"], "metric_results": {"EM": 0.875, "QA-F1": 0.890625}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-6399", "before_prediction": "Austria", "after_prediction": "United Kingdom"}, {"id": "mrqa_naturalquestions-validation-1277", "before_prediction": "1898", "after_prediction": "16 April 1898"}, {"id": "mrqa_squad-validation-3389", "before_prediction": "bunker", "after_prediction": "a chain or screw stoking mechanism"}, {"id": "mrqa_naturalquestions-validation-220", "before_prediction": "Splodgenessabounds", "after_prediction": "Beverly Callard, Lee Oakes, Hayley Bishop, Alison Mac, Thomas Nelstrop, and Jonathon Dutton"}], "retained_ids": ["mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-5168", "mrqa_squad-validation-8700", "mrqa_squad-validation-3467"], "fixed_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.49999999937499995}, {"timecode": 5, "before_eval": {"predictions": ["peter paul", "International Association of Athletics Federations", "a soft wool fabric", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "7 : 25 a.m.", "slide whistle", "peter paul", "to start fires, hunt, and bury their dead", "Columbia", "parietal cells ( also known as oxyntic or delomorphous cells )", "new", "Ready to Die", "a revolver", "imperial rule", "1840", "make a defiant speech, or a speech explaining their actions", "Francis Marion Crawford, Robert Underwood Johnson, Stanford White, Fritz Lowenstein, George Scherff, and Kenneth Swezey", "peter quaife", "a greater tendency to take on debts", "entropy", "averse to wedlock", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "8 December 1980", "Long Island", "structural collapses", "a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1478052584670232}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "after_eval": {"predictions": ["horace walpole", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "paisley", "uninhabited", "cardiac", "Idaho", "6 : 44 p.m. UTC", "swanee or swannee whistle", "mustard", "hunt", "Indian", "gastric glands found in the lining of the fundus and in the body of the stomach", "monotremata", "September 13, 1994", "president garfield", "Ming dynasty", "1787", "defiant speech", "Mark Twain", "sunny afternoon", "the methodology used", "nonconservative forces", "expect the death of a heretic", "The 1700 Cascadia earthquake", "China", "11 November 1869", "near Arenosa Creek and Matagorda Bay", "January 15, 2018", "19408", "Brookhaven", "property damage", "daedalus"], "metric_results": {"EM": 0.875, "QA-F1": 0.8958333333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2197", "before_prediction": "International Association of Athletics Federations", "after_prediction": "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )"}, {"id": "mrqa_naturalquestions-validation-816", "before_prediction": "imperial rule", "after_prediction": "Ming dynasty"}], "retained_ids": ["mrqa_hotpotqa-validation-1718"], "fixed_ids": ["mrqa_triviaqa-validation-4725", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "unfixed_ids": ["mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494"], "instant_fixing_rate": 0.9310344827586207, "instant_retention_rate": 0.3333333322222222}, {"timecode": 6, "before_eval": {"predictions": ["Zinnemann", "fire", "Barack Hussein Obama II", "1996", "Charlotte Douglas International Airport", "Bermuda", "90-60's", "independent schools", "Phillies", "times sign", "BAFTA Television Award", "Juice Newton", "Super Bowl LII", "HTTP Secure ( HTTPS )", "late summer", "chisholm trail", "monatomic", "beach", "kukai", "true or false", "butterfly", "blood", "anterior interventricular branch", "1.1 \u00d7 1011 metric tonnes", "Derwent", "leaf", "Indian club ATK", "land", "Indian Ocean near Grande Comore, Comoros Islands", "lakh", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.125, "QA-F1": 0.18288690476190475}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-validation-5582", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "after_eval": {"predictions": ["film director", "capillary action", "2008", "2010", "north carolina", "minorca", "70", "independent schools", "boston braves", "the symbol \u00d7", "Best Supporting Actress", "Juice Newton", "1960", "Hypertext Transfer Protocol ( HTTP ) for secure communication", "late - September through early January", "wichita", "simplest", "beaches", "japan", "true", "red admiral", "o", "the left coronary artery", "10 percent of the carbon stores in ecosystems", "buttermere", "the leaves", "Republic of Ireland national team", "distinction", "23 November 1996", "lakh", "Norwegian language", "Carbon dioxide"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9602272727272727}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-2190", "before_prediction": "Super Bowl LII", "after_prediction": "1960"}], "retained_ids": ["mrqa_squad-validation-6947", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-6207"], "fixed_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-validation-5582", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "unfixed_ids": ["mrqa_squad-validation-4181"], "instant_fixing_rate": 0.9642857142857143, "instant_retention_rate": 0.7499999981250001}, {"timecode": 7, "before_eval": {"predictions": ["wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few common complex biomolecules", "the ACU", "Kairi", "Lower taxes", "lost weekend", "Armenia", "the last book accepted into the Christian biblical canon", "Beyonc\u00e9", "percent IACS", "gallantry", "16 million", "the late 1950s", "haulage", "1998", "a priest", "most abundant", "18 - season career", "family member", "environmental changes", "8-track cartridge", "the unbalanced centripetal force", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "Abraham Gottlob Werner", "australia", "present-day Charleston", "calling for the destruction of Israel and the establishment of an Islamic state", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven and Andy Cohen", "Panzer"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31238766339869284}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.4, 0.0, 0.25, 0.0, 1.0, 0.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.23529411764705882, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_naturalquestions-validation-824", "mrqa_squad-validation-4318", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902"], "after_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "K'iche '", "a few", "the right", "Kairi", "public high schools lost their accreditation", "lost weekend", "near the Black Sea", "last", "Beyonc\u00e9 and Bruno Mars", "conductivity", "king george vi", "the most popular show at the time", "post\u2013World War II", "as work oxen for haulage", "2011", "a priest", "third", "2001", "family member", "over-fishing and long-term environmental changes", "8-track", "tangential force", "Mike Czerwien, Waynesburg University, 2002 -- 04", "vote", "a maritime signal, indicating that the vessel flying it is about to leave", "James Hutton", "george i", "Charleston Orange district", "quiescent", "Andy Cohen", "Panzer"], "metric_results": {"EM": 0.875, "QA-F1": 0.9270833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3558", "before_prediction": "most abundant", "after_prediction": "third"}], "retained_ids": ["mrqa_squad-validation-8374", "mrqa_triviaqa-validation-7415", "mrqa_hotpotqa-validation-3846", "mrqa_squad-validation-1863", "mrqa_hotpotqa-validation-3146"], "fixed_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_squad-validation-7296", "mrqa_squad-validation-6297", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_squad-validation-358", "mrqa_naturalquestions-validation-824", "mrqa_squad-validation-4318", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902"], "unfixed_ids": ["mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-98"], "instant_fixing_rate": 0.8846153846153846, "instant_retention_rate": 0.8333333319444445}, {"timecode": 8, "before_eval": {"predictions": ["computational complexity theory", "vienna", "46.8", "6.4 nanometers", "eighth and eleventh", "Brad Keselowski", "400", "adrenal glands", "enkuklios paideia or `` education in a circle ''", "Bowland", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "St. Louis County", "1828", "2018", "Sheffield United", "to ensure wide visibility and understanding of cases in a region", "Pottawatomie County", "orangutan", "Newton's Law of Gravitation", "The church tower", "bromley-by-Bow", "Toronto", "foreign", "110 miles", "Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "freud Freud", "Quentin Coldwater", "acidic"], "metric_results": {"EM": 0.28125, "QA-F1": 0.38446180555555554}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true], "QA-F1": [0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032"], "after_eval": {"predictions": ["computability theory", "formula one", "won gold in the half - pipe", "6.4 nanometers", "departing the show to star in CBS's upcoming sci - fi drama Intelligence", "Kyle Busch", "400", "kidneys", "artes liberales", "\"Switzerland of England\"", "Edward IV of England", "Eureka", "1828", "2018", "blades", "to ensure wide visibility and understanding of cases in a region", "Shawnee", "tortoise", "general relativity", "The church tower", "walford east", "Montreal", "welsh word meaning (ironically) \"stranger\" or \"foreigner\"", "110 miles", "Kona coast", "liberal conservative", "gold rushes", "six", "no contest", "freudian psychoanalysis", "New York", "acidic"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9542572463768115}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_squad-validation-8869", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-5401", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-1897", "mrqa_squad-validation-5313", "mrqa_hotpotqa-validation-1021", "mrqa_naturalquestions-validation-7906", "mrqa_hotpotqa-validation-187"], "fixed_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_hotpotqa-validation-5513", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_hotpotqa-validation-1032"], "unfixed_ids": ["mrqa_naturalquestions-validation-856", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-7767"], "instant_fixing_rate": 0.8695652173913043, "instant_retention_rate": 0.9999999988888888}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentinian", "a report", "almond paste", "photosynthesis", "photographs recording the guests at the famous fancy-dress ball held at Devonshire House in 1897 to celebrate Queen Victoria's diamond jubilee", "1600 Pennsylvania Avenue", "The Daily Stormer", "antibonding", "water", "president", "officeholders", "George, Margrave of Brandenburg-Ansbach", "precise notation of a correct African pronunciation", "3D computer-animated comedy film", "Bloomingdale Firehouse", "acting career", "C. W. Grafton", "LED illuminated display", "Americans acting under orders", "iPod Classic", "My Sassy Girl", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "The Edge of Night", "non-combustible substances that corrode, such as iron", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd", "ATP", "organic carbon in all land - living organisms, both alive and dead, as well as carbon stored in soils", "maverick", "medium and heavy-duty diesel trucks", "Reproductive system"], "metric_results": {"EM": 0.375, "QA-F1": 0.50162841191067}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.13333333333333333, 0.4, 0.0, 1.0, 0.923076923076923, 0.8571428571428571, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 0.1290322580645161, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1904761904761905, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449"], "after_eval": {"predictions": ["an instant messaging client", "Argentinian", "a report", "cake", "consumes ATP and oxygen, releases CO2, and produces no sugar", "animals and humans performimg various actions", "White House", "The Daily Stormer", "antibonding", "shortcrust pastry ; however, more recently recipes have recommended a paste consisting of flour and water", "the President of the United States", "citizens", "George, Margrave of Brandenburg-Ansbach", "a very precise notation of a correct African pronunciation", "3D computer-animated comedy", "Worcester Cold Storage and Warehouse Co. fire", "an acting career", "C. W. Grafton", "a liquid crystal on silicon ( LCoS ) ( based on an L CoS chip from Himax ), field - sequential color system, LED illuminated display", "Americans acting under orders", "iPod+HP", "That Bizarre Girl", "removes excess, unnecessary materials from the body fluids of an organism", "The Edge of Night", "phlogiston", "field trips", "vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 )", "ATP", "soil", "1987", "medium and heavy-duty diesel trucks", "Reproductive system"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9974662162162162}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.918918918918919, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_hotpotqa-validation-832", "mrqa_squad-validation-5210", "mrqa_hotpotqa-validation-3428", "mrqa_squad-validation-3442", "mrqa_squad-validation-2582", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-133", "mrqa_naturalquestions-validation-1327", "mrqa_squad-validation-5940", "mrqa_naturalquestions-validation-1682", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "fixed_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449"], "unfixed_ids": ["mrqa_naturalquestions-validation-754"], "instant_fixing_rate": 0.95, "instant_retention_rate": 0.9999999991666666}, {"timecode": 10, "before_eval": {"predictions": ["Pope, Alexander (1688-1744) (DNB00) - wikisource, the free...Pope, Alexander", "Walter Reed Army Hospital in Washington, D.C. from 1942 to 1945", "three legal systems", "Las Vegas", "status code and reason message ( e.g., HTTP / 1.1 200 OK, which indicates that the client's request succeeded )", "globetrotters", "cruiserweight", "a bridge over the Merderet in the fictional town of Ramelle", "violet gibson", "black miserables", "megalithic sites", "Queen Victoria", "acidity or basicity of an aqueous solution", "white- skinned Gods", "the MGM Grand Garden Special Events Center", "valentino garavani", "C. J. Anderson", "formulation of a single all-encompassing definition of the term", "mormon", "60", "Eagle Ridge Mall", "Geoffrey Hurst and Martin Peters just one season before all three went on to star in England's World Cup winning side of 1966", "reduce pressure on the public food supply", "monastir / Tunisia / Africa", "fire", "Barney Fife", "must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Ann Ward", "beat writers", "Jamestown", "Rouen Cathedral", "carbon related emissions"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3754080988455989}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [0.25, 0.0, 0.0, 1.0, 0.1111111111111111, 0.5, 1.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.6666666666666666, 1.0, 0.14285714285714285, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-1553", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639"], "after_eval": {"predictions": ["Alexander Pope", "Hampton's hump and Hampton's line", "English law", "Las Vegas", "A status line", "the best known globetrotters", "cruiserweight", "over the Merderet in the fictional town of Ramelle", "mussolini", "victor hugo", "standing stones", "British Royal Family", "greater than 14", "thor heyerdahl", "Grand Garden Special Events Center", "valentino", "C. J. Anderson", "impossible", "joseph smith", "60", "Winter Haven Mall", "Pel\u00e9", "aiding the war effort", "tunisia", "fire", "Barney Fife", "Typically, no", "Ann", "writer", "Virginia", "Claude Monet", "biomass and subsequent carbon related emissions"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9791666666666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-4506", "before_prediction": "carbon related emissions", "after_prediction": "biomass and subsequent carbon related emissions"}], "retained_ids": ["mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-2016", "mrqa_squad-validation-273", "mrqa_squad-validation-3018", "mrqa_squad-validation-3525", "mrqa_hotpotqa-validation-3976"], "fixed_ids": ["mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-1553", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639"], "unfixed_ids": ["mrqa_triviaqa-validation-1860"], "instant_fixing_rate": 0.96, "instant_retention_rate": 0.8571428559183674}, {"timecode": 11, "before_eval": {"predictions": ["Green Bay Packers", "\"Traumnovelle\"", "a Gender pay gap in favor of males in the labor market", "The TEU", "absolute zero", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "davis", "The world's longest suspension bridges are listed according to the length of their main span", "trams", "handguns", "davis & panegyric", "the Byzantines", "died", "a two-door, four passenger, rear-engine economy car", "arthur d. Roosevelt", "North American Free Trade Agreement ( NAFTA ) and the Maastricht Treaty", "Queen Elizabeth II", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world. An average of 25,000 people ascend the tower every day", "Town House Galleria", "catfish aquaculture", "atomic number 53", "Evermoist", "Iraq", "a co-op of grape growers", "arthur davis", "\"Oberto, Conte di San Bonifacio\"", "July 25, 1951", "the Charlotte Hornets", "the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean Fernel ( 1497 -- 1558 ), a French physician", "on the chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.0625, "QA-F1": 0.16770661067630716}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.3157894736842105, 0.28571428571428575, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.08695652173913043, 0.5, 0.2222222222222222]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "after_eval": {"predictions": ["american football", "Traumnovelle\"", "not taking jobs due to marriage or pregnancy", "Treaty on European Union (TEU)", "100 \u00b0 C", "his brother", "dennis the menace's dog Gnasher", "span", "dublin", "duke of edinburgh", "king Crimson", "Seljuk Turks", "alamo", "ferdinand porsche", "arkansas", "Canada", "britten", "irritation", "visited paid monument", "Galleria Vittorio Emanuele II", "farm - raised catfish", "heaviest of the stable halogens", "Evermoist", "kuw", "An agricultural cooperative", "afghanistan", "giuseppe verdi", "1952", "Charlotte Hornets of the National Basketball Association ( NBA )", "advisory speed signs are classified as warning signs, not regulatory signs", "Jean Fernel", "torso"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_hotpotqa-validation-2852", "mrqa_naturalquestions-validation-3208"], "fixed_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "unfixed_ids": ["mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-792"], "instant_fixing_rate": 0.9333333333333333, "instant_retention_rate": 0.999999995}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay", "Joe Turano", "fencers", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "August", "The stability, security, and predictability of British law and government", "Minos and Kokalos", "1860", "quintero cuban cigars", "tyne tees TV", "Forbes", "Fort Williams (the latter two located on the Oneida Carry between the Mohawk River and Wood Creek at present-day Rome, New York )", "John Carroll Lynch", "afghanistan", "Orwell", "Kingdom of Bohemia", "Gregg Popovich", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "immunological memory", "Mexican drug lord who started out his criminal career under the tutelage of his uncle", "musician", "nodel", "December 1, 1969", "afghanistan", "john buchan", "California State Automobile Association and the Automobile Club of Southern California", "faith", "Cinderella", "delayed the sealing of the hatch", "due to a fear of seeming rude"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3105221461258613}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 0.4, 0.0, 0.23529411764705882, 0.4, 1.0, 0.13333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.631578947368421]}}, "error_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-6924"], "after_eval": {"predictions": ["Coldplay", "Frederick in a duet with Teresa James", "fencing", "Margaret Thatcher", "James Lofton and Mark Malone", "December 2013", "a centre for international trade", "Minoan civilization", "18 November", "cigars", "byker grove", "Forbes in the Central West region of New South Wales, Australia", "garrisons", "Bonnie Lipton ( portrayed by Skyler Samuels )", "ring", "possibly Orwell himself, called upon to shoot an aggressive elephant while working as a police officer in Burma", "Czech Kingdom", "Bob Hill", "secularism", "creative reasons", "adaptive immune system", "uncle", "Originally a musician", "thumbelina", "1973", "maryland", "john buchan", "AAA Auto Clubs", "alone", "Cinderella", "delayed the sealing of the hatch", "lack of understanding of the legal ramifications, or due to a fear of seeming rude"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-6678", "before_prediction": "immunological memory", "after_prediction": "adaptive immune system"}], "retained_ids": ["mrqa_squad-validation-93", "mrqa_hotpotqa-validation-4826", "mrqa_triviaqa-validation-7371", "mrqa_hotpotqa-validation-4165", "mrqa_squad-validation-3935"], "fixed_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-6924"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8333333319444445}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister", "president", "Buddy Pine / Incredi - Boy / Syndrome", "Napoleon", "maryland", "3.7 percent of the entire student population", "negative", "matt Willis and Charlie Quirke", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "jonathan", "paddington", "amyotrophic lateral sclerosis", "is a 1981 American black comedy film directed, produced, and written by John Waters, and starring Divine, Tab Hunter, Edith Massey, and Mink Stole", "Swiss made", "October 17, 1938", "Torah or Bible", "the western coast of Italy", "first and only U.S. born world grand prix champion", "a jazz funeral without a body", "mid November", "Facebook", "Polish Jews", "Miljenko Matijevic", "Issaquah", "King George's War", "cheated on Miley", "punk rock", "Fort Saint Anthony", "daguerreotypes", "infrequent rain"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3434568903318903}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.5, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_triviaqa-validation-6913"], "after_eval": {"predictions": ["The Omega Man", "president", "Dashiell Robert Parr / Dash, the Parrs'second child", "Napoleon's", "sliced bread", "3.7", "negative", "Garth", "Ecumenical Award", "Erastus Utoni", "discipline problems", "nine", "michael hordern", "Lou Gehrig's Disease", "\"Odorama\"", "Swiss made", "October 17, 1938", "religious", "Sicily", "American-born", "Those who follow the band just to enjoy the music", "late November or early December", "Facebook", "bread", "Tim \"Ripper\" Owens", "Issaquah", "King George's War", "Miley finally ends it with him", "alternative rock", "Fort Saint Anthony", "henry fox talbot", "infrequent rain"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9927884615384616}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_hotpotqa-validation-1509", "mrqa_squad-validation-7351", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_triviaqa-validation-2779", "mrqa_hotpotqa-validation-1932", "mrqa_squad-validation-10168", "mrqa_hotpotqa-validation-3669", "mrqa_squad-validation-2656"], "fixed_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_triviaqa-validation-6913"], "unfixed_ids": ["mrqa_naturalquestions-validation-1135"], "instant_fixing_rate": 0.9565217391304348, "instant_retention_rate": 0.9999999988888888}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "Honolulu", "the British military", "harrison Ford as the son of a man anxious to take his family back to a primitive life", "FX option", "electromagnetic waves", "Wahhabi/ Salafi Jihad extremist militant", "gypsy", "Dimensions in Time", "Apollo 12", "the end of January 1981", "estrogen", "baptism", "that priest whose name was Martin Luther", "john Robertson", "slowing the vehicle", "Belle Fourche and Cheyenne", "organisms", "Hanna-barbera, The Jetsons", "Veneto region of Northern Italy", "efficient and effective management of money ( funds )", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "george rincewind", "geoscience australia", "Kur\u00e1nyi", "The public sector ( also called the state sector )", "1940", "poverty", "king", "cornea", "Judge Doom", "James MacArthur"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5690317035905271}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.5714285714285715, 0.0, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.5882352941176471, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_naturalquestions-validation-5944", "mrqa_triviaqa-validation-7153", "mrqa_squad-validation-6023", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-5256"], "after_eval": {"predictions": ["Hong Kong", "Honolulu", "the British military", "dark blood", "a foreign exchange option ( commonly shortened to just FX option or currency option )", "radio and microwave frequencies", "Wahhabi/Salafi extremist militant", "swastika", "Children in Need", "Apollo 12", "1981", "estrogen", "prayer for grace", "the Jewish spokesman who tried to help the Jews of Saxony in 1537", "brian clough", "slowing the vehicle", "belle fourche and cheyenne", "organisms", "Hanna-Barbera", "the Veneto region of Northern Italy", "accomplish the objectives of the organization", "12 mi southeast of Rome", "binky", "tasmania", "Kur\u00e1nyi", "the state sector", "February", "poverty, the lack of access to education and weak government institutions", "king", "vitreous humor", "Judge Doom", "Texas Tower Sniper"], "metric_results": {"EM": 0.875, "QA-F1": 0.9174107142857143}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-2448", "before_prediction": "baptism", "after_prediction": "prayer for grace"}, {"id": "mrqa_squad-validation-2509", "before_prediction": "that priest whose name was Martin Luther", "after_prediction": "the Jewish spokesman who tried to help the Jews of Saxony in 1537"}, {"id": "mrqa_naturalquestions-validation-6019", "before_prediction": "The public sector ( also called the state sector )", "after_prediction": "the state sector"}], "retained_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-727", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-2085", "mrqa_hotpotqa-validation-3877"], "fixed_ids": ["mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_naturalquestions-validation-5944", "mrqa_triviaqa-validation-7153", "mrqa_squad-validation-6023", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-5256"], "unfixed_ids": ["mrqa_squad-validation-9751"], "instant_fixing_rate": 0.9411764705882353, "instant_retention_rate": 0.7999999994666667}, {"timecode": 15, "before_eval": {"predictions": ["comic", "Deathly Hallows", "Brooklyn", "Wichita and the state of Kansas", "france da 5'9", "nola Taylor Redd", "a friend and publicist", "english patient", "masons'marks painted on them", "James Taylor", "Gateshead", "It Ain't Over'til It's Over '' is a song recorded, written, and produced by American musician Lenny Kravitz", "midpiece", "after the Spanish -- American War in the 1898 Treaty of Paris", "professional wrestler", "likely those who encountered the first Europeans", "steal the plans for the Death Star", "casket letters", "postage stamp", "art", "chimpanzees", "March 15, 1945", "absolute temperature", "Jeremy Hammond", "Sam Waterston", "may have more than 1 cusp", "Aegisthus", "November 2015", "tallahassee", "prefabricated housing projects", "the British Press", "WOTV"], "metric_results": {"EM": 0.28125, "QA-F1": 0.32566964285714284}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.5714285714285715, 0.25, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10416", "mrqa_hotpotqa-validation-3456", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_naturalquestions-validation-10439", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_triviaqa-validation-1736", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "after_eval": {"predictions": ["comic", "Harry Potter and the Deathly Hallows", "Flatbush section of Brooklyn, New York City", "the Alamodome", "Royce da 5'9\" (Bad) and Eminem (Evil)", "galilei", "editor of Electrical World magazine", "english patient", "the name of a work gang", "James Taylor", "a roof", "based on a Yogiism, or quotation from Yogi Berra", "midpiece", "1898", "martial artist", "Spanish", "stunt performances", "fred astaire", "postage stamp", "belgium", "chimpanzees", "After World War II", "volume", "Jeremy Hammond", "Sam Waterston", "premolar", "Aegisthus", "3 December", "florida", "an Eastern Bloc city", "fleet river", "KMBC-TV and KQTV"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9895833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_triviaqa-validation-5979", "mrqa_hotpotqa-validation-1048", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-5877", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_naturalquestions-validation-9451"], "fixed_ids": ["mrqa_naturalquestions-validation-10416", "mrqa_hotpotqa-validation-3456", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_squad-validation-1374", "mrqa_naturalquestions-validation-10439", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_triviaqa-validation-1736", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "unfixed_ids": ["mrqa_triviaqa-validation-7512"], "instant_fixing_rate": 0.9565217391304348, "instant_retention_rate": 0.9999999988888888}, {"timecode": 16, "before_eval": {"predictions": ["galileo", "cow's milk cheese", "Benedicto", "the back of the head of the tibia, below the level of the knee joint, and excluded from the formation of this joint", "d\u00f9n Chailleann", "North Sea, through the former Meuse estuary, near Rotterdam", "Namibia, Botswana, and South Africa", "Colin Montgomerie", "October 29, 1985", "Amway", "Mauritius", "Milton Friedman", "President", "the Royal Festival Hall", "zanzibar", "niger", "everglades", "a statue of fame", "Sam's soul is not with him", "Fulham, Greater London, England", "French, English and Spanish", "Tom Baker", "The Double Life of V\u00e9ronique", "What's Up (TV series)", "blood, platelets, and plasma", "flowing water", "switzerland", "matrices", "Snowbell", "the three mystic apes", "sheepskin and Merino Wool products", "the \"second city\" of Oahu"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3823717948717949}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.28571428571428575, 0.7692307692307692, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21428571428571425, 0.33333333333333337, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_triviaqa-validation-866", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250"], "after_eval": {"predictions": ["nightclub", "belgium", "blessed", "leg", "dunkeld", "North Sea", "Botswana", "Colin Montgomerie", "March 30, 1983", "American Way", "secondary school study", "Milton Friedman", "Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "BBC Radio's \"The Show Band Show\"", "tanzania", "Niger", "Florida's Everglades", "top row of windows", "Sam's soul is not with him", "London", "French", "dave lamb", "Beyond the Clouds", "\"The Heirs\" (2013)", "blood, platelets, and plasma", "flowing water", "poland", "matrices", "geena davis", "The three wise monkeys ( Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru, alternatively \u4e09 \u5339 \u306e \u733f sanbiki no saru, literally `` three monkeys", "Sheepskin", "the \"second city\" of Oahu"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8872329059829059}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307692, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-3483", "before_prediction": "President", "after_prediction": "Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha"}, {"id": "mrqa_triviaqa-validation-4055", "before_prediction": "everglades", "after_prediction": "Florida's Everglades"}, {"id": "mrqa_hotpotqa-validation-2445", "before_prediction": "French, English and Spanish", "after_prediction": "French"}], "retained_ids": ["mrqa_squad-validation-8037", "mrqa_triviaqa-validation-6125", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_squad-validation-8223", "mrqa_hotpotqa-validation-2287"], "fixed_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_squad-validation-5407", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_triviaqa-validation-866", "mrqa_triviaqa-validation-298", "mrqa_hotpotqa-validation-1250"], "unfixed_ids": ["mrqa_triviaqa-validation-313", "mrqa_triviaqa-validation-4681", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9087"], "instant_fixing_rate": 0.8260869565217391, "instant_retention_rate": 0.6666666659259258}, {"timecode": 17, "before_eval": {"predictions": ["belle dymock", "Independence Day: Resurgence", "August 6, 1845", "rubidium - 85", "James Zeebo", "sovereign states", "vice president of the United States", "the Discovery Institute's \"Teach the Controversy\" campaign", "Sam's captured parents", "Australian", "36 months for men and 24 months for women", "vary", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Spencer", "\"antiforms\"", "second half of the third season", "Veyyil", "James Weldon Johnson", "Mick Jagger", "n < p < 2n \u2212 2", "Bangor Air National Guard Base", "knowledgeable in that one area", "180th meridian in a 360 \u00b0 - system", "Cartoon Network's late night programming block Adult Swim", "the Presiding Officer on the advice of the parliamentary bureau", "the Miami Heat of the National Basketball Association (NBA)", "33-member", "grapevine", "Annual Conference Cabinet", "field hockey", "the Doctor's third on-screen regeneration"], "metric_results": {"EM": 0.5, "QA-F1": 0.5705257936507937}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333334, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.16, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.33333333333333337, 0.0, 0.2, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_triviaqa-validation-1130"], "after_eval": {"predictions": ["pear", "Independence Day: Resurgence", "August 6, 1845", "rubidium - 85", "Sykes", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "dick cheney", "\"Teach the Controversy\" campaign", "Ravage and the Decepticon Rampage", "Velichko Todorov Tsochev) is a Bulgarian-Canadian", "36 months for men and 24 months for women respectively", "vary", "lower", "575 acres (2.08 km\u00b2)", "Roy Spencer", "\"antiforms\"", "in the second half of the third season", "Veyyil", "James Weldon Johnson", "Rocky Dzidzornu -- congas", "n > 3", "Bangor Air National Guard Base", "knowledgeable", "antimeridian", "Cartoon Network", "Presiding Officer", "the Phoenix Suns", "33-member", "grapevine leaves", "The clergy appointments are made and fixed annually by the resident bishop on the advice of the Annual Conference Cabinet", "olympic bronze medals", "the Doctor's third on-screen regeneration"], "metric_results": {"EM": 0.8125, "QA-F1": 0.9302858758948109}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.9302325581395349, 1.0, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3157894736842105, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-430", "before_prediction": "James Zeebo", "after_prediction": "Sykes"}, {"id": "mrqa_naturalquestions-validation-1090", "before_prediction": "36 months for men and 24 months for women", "after_prediction": "36 months for men and 24 months for women respectively"}, {"id": "mrqa_hotpotqa-validation-5787", "before_prediction": "second half of the third season", "after_prediction": "in the second half of the third season"}, {"id": "mrqa_triviaqa-validation-3071", "before_prediction": "grapevine", "after_prediction": "grapevine leaves"}, {"id": "mrqa_squad-validation-10074", "before_prediction": "Annual Conference Cabinet", "after_prediction": "The clergy appointments are made and fixed annually by the resident bishop on the advice of the Annual Conference Cabinet"}], "retained_ids": ["mrqa_squad-validation-608", "mrqa_hotpotqa-validation-5628", "mrqa_naturalquestions-validation-3724", "mrqa_squad-validation-2053", "mrqa_hotpotqa-validation-1895", "mrqa_squad-validation-5110", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-4463", "mrqa_squad-validation-7664"], "fixed_ids": ["mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_triviaqa-validation-1130"], "unfixed_ids": ["mrqa_naturalquestions-validation-3559"], "instant_fixing_rate": 0.9375, "instant_retention_rate": 0.6874999995703125}, {"timecode": 18, "before_eval": {"predictions": ["The Rwandan genocide, also known as the genocide against the Tutsi", "harmoniously", "500 metres", "Vili Fualaau and Mary Kay Letourneau", "ABC News", "displacement", "five times", "South Kensington Museum", "Edward Longshanks and the Hammer of the Scots", "diego garcia", "dundee", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "8 February 1587", "\"Grindhouse\" fake trailer", "mureen Connolly", "MFSK", "Swiss- Austrian border", "lithium-ion battery", "821", "Sky channels", "pressure difference", "Hyuna", "highest'social efficiency'", "transposed", "King of Cool", "President Wilson and the American delegation from the Paris Peace Conference", "mesogeia", "thirteenth", "violet", "HC Davos", "Michael Patrick Smith", "Qutab Ud - Din - Aibak, founder of the Delhi Sultanate"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6721064814814814}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false], "QA-F1": [0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.07407407407407408, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-2145", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_squad-validation-9841", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_naturalquestions-validation-10490"], "after_eval": {"predictions": ["Rwandan genocide", "harmoniously", "400 metres", "Piper", "ABC News", "displacement", "five", "Museum of Manufactures", "Edward Longshanks and the Hammer of the Scots", "diego garcia", "Discovery Point museum and heritage centre", "the person compelled to pay for reformist programs", "Fotheringhay Castle", "Spy Kids", "venus williams", "MFSK", "Baden-W\u00fcrttemberg", "lithium-ion battery", "821", "basic channels", "pressure", "Hyuna", "highest'social efficiency'", "transposed", "King of Cool", "American delegation from the Paris Peace Conference", "socrates", "thirteenth", "separate tables", "HC Davos", "Robin Hawdon", "Firoz Shah Tughlaq"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8614583333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-523", "before_prediction": "500 metres", "after_prediction": "400 metres"}, {"id": "mrqa_triviaqa-validation-5036", "before_prediction": "dundee", "after_prediction": "Discovery Point museum and heritage centre"}, {"id": "mrqa_triviaqa-validation-7100", "before_prediction": "violet", "after_prediction": "separate tables"}, {"id": "mrqa_hotpotqa-validation-511", "before_prediction": "Michael Patrick Smith", "after_prediction": "Robin Hawdon"}], "retained_ids": ["mrqa_squad-validation-1914", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-2222", "mrqa_hotpotqa-validation-4415", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-1855", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3446"], "fixed_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-2145", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_naturalquestions-validation-5215", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_naturalquestions-validation-10490"], "unfixed_ids": ["mrqa_triviaqa-validation-2683", "mrqa_squad-validation-9841"], "instant_fixing_rate": 0.8666666666666667, "instant_retention_rate": 0.7647058819031142}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "zaragoza", "11.1", "trans-Pacific flight", "Sharman Joshi", "quietly", "Forster I, Forster II, and Forster III", "a recurring decimal", "Ana", "in Cherry Hill", "`` Goodbye Toby ''", "separate tables", "separate tables", "Six Degrees of Separation", "blackstar", "Star Plus", "intellectual virtues", "1889", "Nicki Minaj", "comic opera", "names", "race track", "separate tables", "Teen Titans Go!", "William the Conqueror", "Tel Aviv", "two", "Corinthian and Saronic Gulfs", "taking blood", "youngest TV director ever", "Southern Progress Corporation"], "metric_results": {"EM": 0.5, "QA-F1": 0.5}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-9214", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_naturalquestions-validation-1565", "mrqa_naturalquestions-validation-2064"], "after_eval": {"predictions": ["a Ghanaian boxer", "John Meston", "zaragoza", "7.8", "trans-Pacific flight", "Soha Ali Khan", "quietly", "Forster I, Forster II, and Forster III", "p is not a prime factor of q", "Ana", "in Cherry Hill", "`` Goodbye Toby ''", "call me ishmael", "amelia earhart", "Six Degrees of Separation", "david bowie", "Indian", "fortitude", "1889", "Sir Mix - a-Lot", "slave of duty", "names", "portier", "karl marx", "Teen Titans Go!", "Norman invaders", "Tel Aviv", "two", "peninsula", "phlebotomists' duties and services are usually scheduled", "youngest TV director ever", "Southern Progress Corporation"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-3552", "before_prediction": "taking blood", "after_prediction": "phlebotomists' duties and services are usually scheduled"}], "retained_ids": ["mrqa_triviaqa-validation-6901", "mrqa_hotpotqa-validation-2943", "mrqa_squad-validation-1942", "mrqa_squad-validation-5538", "mrqa_hotpotqa-validation-5710", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4951", "mrqa_squad-validation-3151", "mrqa_hotpotqa-validation-57", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "fixed_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-9214", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_naturalquestions-validation-1565", "mrqa_naturalquestions-validation-2064"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9374999994140625}, {"timecode": 20, "before_eval": {"predictions": ["gas turbines", "David Feldman", "the Sackler Centre for arts education", "Mos Def", "kaleidoscope", "in British Columbia, in the Abbotsford, Vancouver and Langley areas in August 2017, with a mansion in the Aldergrove area of Langely serving as the property at the centre of the story", "the KC135 weightlessness training aircraft", "Ribosomes", "kingfisher", "six-time", "Scott Bakula as Dwayne `` King '' Cassius Pride, NCIS Supervisory Special Agent", "\"Ain't Got Nothin' on Us\"", "Stollen", "south", "Moon's surface", "Sulla", "following the 2017 season", "Golden Globe", "mother tongues", "bring about necessary change", "turkey", "a CMYKOG process", "Qutab - ud - din Aibak", "Reunited Worlds", "Santa Clara", "jellyfish", "lower chamber", "a \"teleforce\" weapon", "Native American", "The Super Bowl 50 Host Committee has vowed to be \"the most giving Super Bowl ever\" and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area", "households were made up of individuals and 7.9% had someone living alone who was 65 years of age or older", "barbarella"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6568200998348057}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.9803921568627451, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_triviaqa-validation-3486", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-2280", "mrqa_squad-validation-393", "mrqa_squad-validation-7272", "mrqa_triviaqa-validation-935"], "after_eval": {"predictions": ["gas turbines", "Robert Smigel, Michael Koman and David Feldman", "(prints, drawings, paintings and photographs)", "Mos Def", "kaleidoscope", "British Columbia, in the Abbotsford, Vancouver and Langley areas in August 2017, with a mansion in the Aldergrove area of Langely serving as the property at the centre of the story", "Mercury astronaut", "Ribosomes", "dacelo", "six-time", "CCH Pounder", "\"Ain't Got Nothin' on Us\"", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "gerry adams", "Moon's surface", "Lucius Cornelius Sulla Felix ( ; c. 138 BC \u2013 78 BC) known commonly as Sulla", "Super Bowl LII", "Golden Globe", "mother tongues", "personal presence and living word", "turkey", "a CMYKOG process", "Qutab - ud - din Aibak", "Reunited Worlds", "Santa Clara", "jellyfish", "lower chamber", "a \"teleforce\" weapon", "Native American", "the most giving Super Bowl ever", "29.7%", "4077th mash"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9729166666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2407", "before_prediction": "Sulla", "after_prediction": "Lucius Cornelius Sulla Felix ( ; c. 138 BC \u2013 78 BC) known commonly as Sulla"}], "retained_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-4627", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2795", "mrqa_squad-validation-8464", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-1521", "mrqa_hotpotqa-validation-2064"], "fixed_ids": ["mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_triviaqa-validation-3486", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-2280", "mrqa_squad-validation-393", "mrqa_squad-validation-7272", "mrqa_triviaqa-validation-935"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9473684205540166}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "blood", "Yazoo", "29 January 1967", "black hole", "(homo enim in hac vita) tired from his daily labour", "cede the former, but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area", "Kris Kristofferson", "ill. (some col.)", "public and private", "a French pirate", "Smith Jerrod", "Charles Dickens", "non-profit, ECOSOC non-governmental organization", "oxygen", "2000", "if there were only finitely many primes then \u03b6(1) would have a finite value", "ITV News at Ten", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1960s", "Tallemaja \"pine tree Mary\"", "western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Orthodox Christians", "007", "4 in", "nijinsky", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "Menace II Society", "backup to Dan Marino as a member of the Miami Dolphins", "Larry Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.125, "QA-F1": 0.21058499408524764}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.1818181818181818, 0.0, 0.0, 0.5, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.41379310344827586, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_hotpotqa-validation-4676"], "after_eval": {"predictions": ["a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "rebecca", "domestic cat", "blood transfusion", "alison moyet", "1926", "stars", "dreams", "value of the Caribbean islands' sugar cane", "Johnny Cash, Waylon Jennings", "auction house", "private", "Scottish", "waiter turned emerging young actor Smith Jerrod", "Beatrix Potter", "Organizations could come together to address global issues", "proteins", "2001", "exceeds any given number", "alastair burnet", "padlocking the gates", "1969", "R\u00e5", "tribes in western portions of the Great Lakes region", "Protestant", "casino royale", "4 in", "oh so sharp", "Western Atlantic ctenophore Mnemiopsis leidyi", "Menace II Society", "backup to Dan Marino as a member of the Miami Dolphins", "trio with his younger brothers Steve and Rudy"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9586975524475525}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-192", "before_prediction": "black hole", "after_prediction": "stars"}], "retained_ids": ["mrqa_naturalquestions-validation-6832", "mrqa_hotpotqa-validation-2642", "mrqa_naturalquestions-validation-2758"], "fixed_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2412", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-1555", "mrqa_hotpotqa-validation-4676"], "unfixed_ids": ["mrqa_squad-validation-10502", "mrqa_squad-validation-4648"], "instant_fixing_rate": 0.9285714285714286, "instant_retention_rate": 0.7499999981250001}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "Thursday", "MSC Crociere S. p.A.", "alison carLSON", "his friends, Humpty Dumpty and Kitty Softpaws", "The Greens", "Royalists", "the presence of correctly oriented P waves", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "assassination to spies", "alison d\u2019\u00c9ducation et de R\u00e9cr\u00e9ation", "Augustus Waters", "1619", "Tony Blair", "\u2018 Often damaging\u2019, along with alcohol, tobacco and gambling", "July 26, 1959", "Kenya", "a chronological collection of critical quotations about William Shakespeare and his works", "alison I", "an active supporter of the League of Nations", "Cargill", "AMC Theatres", "The series", "3 October 1990", "September 21, 2017", "The weak force is due to the exchange of the heavy W and Z bosons", "pig", "Dexter", "Manhattan", "Chronicles of Barsetshire", "vast"], "metric_results": {"EM": 0.3125, "QA-F1": 0.42007598304473304}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.33333333333333337, 1.0, 0.16, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.625, 0.0, 0.1818181818181818, 0.0, 1.0, 0.0, 0.2, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_squad-validation-10459", "mrqa_naturalquestions-validation-1328", "mrqa_squad-validation-2828"], "after_eval": {"predictions": ["Doug Pruzan", "English language patronymic surname", "Thursday", "Mediterranean Shipping Company", "benjamin franklin", "Humpty Dumpty", "Nationals", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "the presence of correctly oriented P waves", "reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "smersh", "jules verne", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "1619", "Tony Blair", "often damaging", "July 26, 1959", "Masai Mara", "chronological collection of critical quotations", "edward I", "long - standing policy of neutrality", "United Healthcare", "AMC Theatres", "It's Always Sunny in Philadelphia", "summer of 1990 and continued until 1992", "September 21, 2017", "weak force", "pig", "Dexter", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.96875, "QA-F1": 0.984375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-7731", "mrqa_hotpotqa-validation-3944", "mrqa_triviaqa-validation-4731", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-803", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_hotpotqa-validation-2448"], "fixed_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_squad-validation-10459", "mrqa_naturalquestions-validation-1328", "mrqa_squad-validation-2828"], "unfixed_ids": ["mrqa_triviaqa-validation-6872"], "instant_fixing_rate": 0.9545454545454546, "instant_retention_rate": 0.9999999989999999}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mononucleosis", "17 nm vs 25 nm", "1937 Austin Seven Ruby Open Top Tourer", "Dirty Dancing", "paintwork where it becomes seriously corroded", "Dreamland", "Best Animated Feature", "European Convention on Human Rights", "American record for the most time in space (381.6 days)", "nine", "NASA", "lesser celandine", "Khrushchev", "Ronald Ralph \"Ronnie\" Schell", "sesquiterpene lactone", "Mumbai", "Leinster", "1940", "the 2017 / 18 Divisional Round game against the New Orleans Saints", "1707", "a golf course designed by William P. Bell", "till September", "numbo", "synovial joint", "moffitt", "Democritus", "Santa Clara Marriott", "beethoven", "political power generated by wealth", "the bound on the complexity of reductions", "Sanders"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5588803557553557}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false], "QA-F1": [0.3076923076923077, 1.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_naturalquestions-validation-1617", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-2741", "mrqa_triviaqa-validation-6107", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_hotpotqa-validation-2340", "mrqa_squad-validation-769"], "after_eval": {"predictions": ["$105 billion", "mononucleosis", "25 nm", "a 1934 Austin seven box saloon", "Dirty Dancing", "red", "Steeplechase Park", "Academy Award for Best Animated Feature", "European Convention on Human Rights", "381.6 days", "nine", "NASA", "yellow", "Khrushchev", "Jack Cassidy", "malaria", "Mumbai", "the east of Ireland", "1940", "2017 / 18", "1707", "Sunnyside", "till September", "live and let die", "Incudomalleolar joint", "robin riggs", "Leucippus", "Santa Clara Marriott", "beethoven", "political power generated by wealth", "the bound on the complexity of reductions", "Ted Ginn Jr"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9345238095238095}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-3420", "before_prediction": "moffitt", "after_prediction": "robin riggs"}], "retained_ids": ["mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3933", "mrqa_squad-validation-4118", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-26", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-2420", "mrqa_naturalquestions-validation-2212", "mrqa_squad-validation-327", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-7481", "mrqa_squad-validation-1750"], "fixed_ids": ["mrqa_squad-validation-7389", "mrqa_squad-validation-8850", "mrqa_naturalquestions-validation-1617", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-2741", "mrqa_triviaqa-validation-6107", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-571", "mrqa_hotpotqa-validation-2340", "mrqa_squad-validation-769"], "unfixed_ids": ["mrqa_triviaqa-validation-6438", "mrqa_naturalquestions-validation-1731"], "instant_fixing_rate": 0.8823529411764706, "instant_retention_rate": 0.9333333327111111}, {"timecode": 24, "before_eval": {"predictions": ["2003", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "alchemy", "WBA", "spice islands", "the first Saturday in May", "Goneril", "US Secretary of State Henry Kissinger had negotiated an Israeli troop withdrawal from parts of the Sinai Peninsula", "1971", "J.R. R. Tolkien", "John Elway", "Selena Gomez", "learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "31", "Eugene", "comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "coupe", "Fa Ze YouTubers", "city of hell", "Mongols", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "Friars Minor (O.F.M)", "CD Castell\u00f3n", "late as 1910, but in an extreme extension the Four Last Songs of Richard Strauss are described stylistically as `` Late Romantic '' and were composed in 1946 -- 48", "12\u20134", "having colloblasts, which are sticky and adhere to prey, although a few ctenophore species lack them", "Anne Fletcher", "Mission Specialist for mission STS-51-L.", "it will retreat to its den and winter will persist for six more weeks", "foreign affairs"], "metric_results": {"EM": 0.3125, "QA-F1": 0.43159604135500734}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.07692307692307693, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.20689655172413793, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7692307692307693, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.4, 0.0, 1.0, 0.23529411764705882, 1.0, 0.33333333333333337, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_hotpotqa-validation-3080", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "after_eval": {"predictions": ["2003", "NADP+", "The British failures in North America, combined with other failures in the European theater, led to the fall from power of Newcastle and his principal military advisor, the Duke of Cumberland", "alchemists", "WBO lightweight title", "spice islands", "Saturday", "Kent", "multilateral negotiations", "1971", "J.R. R. Tolkien", "John Elway", "Instagram", "join a vocational youth/village polytechnic", "31", "Eugene", "comparable to the seven Wonders of the World", "2p + 1 with p prime", "coupe", "FaZe Rug", "dante", "Muslim", "along the coast, the settlements were growing into the interior", "poor clares", "Club Deportivo Castell\u00f3n", "1780 -- 1830", "12\u20134", "having colloblasts", "Anne Fletcher", "STS-51-L", "if a groundhog ( Deitsch : Grundsau, Grunddax, Dax ) emerging from its burrow on this day sees a shadow due to clear weather, it will retreat to its den and winter will", "france"], "metric_results": {"EM": 0.875, "QA-F1": 0.9277956674473068}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9180327868852458, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-10261", "before_prediction": "Pitt", "after_prediction": "The British failures in North America, combined with other failures in the European theater, led to the fall from power of Newcastle and his principal military advisor, the Duke of Cumberland"}], "retained_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-3340", "mrqa_squad-validation-5399", "mrqa_squad-validation-384", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-309", "mrqa_naturalquestions-validation-2164", "mrqa_squad-validation-67", "mrqa_hotpotqa-validation-4542"], "fixed_ids": ["mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-1814", "mrqa_triviaqa-validation-1306"], "unfixed_ids": ["mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3080", "mrqa_naturalquestions-validation-6508"], "instant_fixing_rate": 0.8636363636363636, "instant_retention_rate": 0.8999999991}, {"timecode": 25, "before_eval": {"predictions": ["swimming", "slave", "over 50 million singles", "states'rights to expand slavery", "1923", "Orlando\u2013Kissimmee\u2013 Sanford", "January 19, 1962", "PPA, Pattugliatore Polivalente d'Altura", "lyda", "iteratively", "geese", "The may be possible that another Kuznets' cycle is occurring, specifically the move from the manufacturing sector to the service sector", "north quay", "Peter Davison, Colin Baker and Sylvester McCoy", "August 14, 1848", "lower", "juveniles", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "co-written the book of the musical \"A Chorus Line\"", "2,664 rooms", "iph 6 plus", "a chute beneath his or her feet", "Claims adjuster", "cleaning, catering and security", "The Violin Sonata No. 5 in F major, Opus 24", "gironde", "1603", "status superior to all others in health-related fields such as physicians and acupuncturists", "Nutcracker", "We admitted we were powerless over alcohol -- that our lives had become unmanageable.", "baseball"], "metric_results": {"EM": 0.5, "QA-F1": 0.6381353021978022}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.33333333333333337, 0.4615384615384615, 0.923076923076923, 0.2857142857142857, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_hotpotqa-validation-5762", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_triviaqa-validation-2454", "mrqa_hotpotqa-validation-499"], "after_eval": {"predictions": ["gymnastics", "abram", "40 million", "loyalty to the U.S. Constitution", "1923", "Orlando\u2013Kissimmee\u2013Sanford", "January 19, 1962", "PPA, Pattugliatore Polivalente d'Altura", "crimson tide", "iteratively", "geese", "in effect", "pacific", "Sylvester McCoy", "February 14, 1859", "lower", "In at least some species, juveniles are capable of reproduction before reaching the adult size and shape", "breaded chicken patty", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "musical", "2,664", "bendgate", "through a chute beneath his or her feet", "Claims adjuster", "cleaning, catering and security", "Symphony No. 7", "gironde", "1603", "status superior to all others in health-related fields such as physicians and acupuncturists", "sugar Plum Fairy", "We admitted we were powerless over alcohol -- that our lives had become unmanageable.", "baseball"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9781517094017094}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-7677", "before_prediction": "Peter Davison, Colin Baker and Sylvester McCoy", "after_prediction": "Sylvester McCoy"}, {"id": "mrqa_naturalquestions-validation-4996", "before_prediction": "a chute beneath his or her feet", "after_prediction": "through a chute beneath his or her feet"}], "retained_ids": ["mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-3369", "mrqa_hotpotqa-validation-3977", "mrqa_squad-validation-9532", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7301", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_triviaqa-validation-2098", "mrqa_naturalquestions-validation-6545", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "fixed_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_hotpotqa-validation-5762", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_triviaqa-validation-2454", "mrqa_hotpotqa-validation-499"], "unfixed_ids": ["mrqa_squad-validation-4637"], "instant_fixing_rate": 0.9375, "instant_retention_rate": 0.874999999453125}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences", "south", "investment technique", "kelly", "Honolulu", "1910", "those who refuse vetting", "Catch Me Who Can", "jazz", "tennis", "4,000", "Khagan", "catherine and heathcliff", "abbot of a monastery", "sri lanka", "the Simpson family", "The planner Raymond Unwin", "Riverside", "Shopping Centre", "Albany High School", "charbagh", "Sergeant First Class", "Anakin Skywalker", "jury nullification", "Cee - Lo", "Anglican", "lillian Randolph", "scharnhorst", "hypnosis", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "redistributive", "1757"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6479166666666667}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-2184", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-7435", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-7304", "mrqa_naturalquestions-validation-2214", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "after_eval": {"predictions": ["to Jewish audiences", "north of the Lakes Region and south of the Kancamagus Highway", "outlined by Joel Greenblatt", "kirsty wark", "City and County of Honolulu", "1910\u20131940", "those who refuse vetting", "The Rocket", "jazz", "margaret court", "3,677 seated", "Khagan", "catherine and heathcliff", "birmingham", "sri lanka", "the Simpson family", "The planner Raymond Unwin and the architect Barry Parker", "Riverside", "Shopping Centre", "Albany High School", "charbagh", "Sergeant First Class", "Anakin Skywalker", "jury nullification", "the closing scene of the final episode of the first season", "The Church of England", "hattie mcdaniel", "scharnhorst", "hypnosis", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based", "January 11, 1755 or 1757July 12, 1804"], "metric_results": {"EM": 0.875, "QA-F1": 0.9072916666666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1426", "before_prediction": "south", "after_prediction": "north of the Lakes Region and south of the Kancamagus Highway"}, {"id": "mrqa_triviaqa-validation-6772", "before_prediction": "kelly", "after_prediction": "kirsty wark"}, {"id": "mrqa_squad-validation-3176", "before_prediction": "Catch Me Who Can", "after_prediction": "The Rocket"}], "retained_ids": ["mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_squad-validation-6148", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_naturalquestions-validation-7801"], "fixed_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-2184", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-7435", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-7304", "mrqa_naturalquestions-validation-2214", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_squad-validation-7319"], "unfixed_ids": ["mrqa_hotpotqa-validation-945"], "instant_fixing_rate": 0.9230769230769231, "instant_retention_rate": 0.8421052627146814}, {"timecode": 27, "before_eval": {"predictions": ["l.A. producer Bones Howe", "jools Holland", "blackberry", "horsehead", "Big Mamie", "orinoco river", "Teamsters leader", "clear substances with a light sky-blue color caused by absorption in the red ( in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light", "wat tyler", "2009", "Zaza Pachulia", "inner mitochondria membrane", "renoir", "The Grand Annual Steeplechase at Warrnambool", "Third-party channels", "third", "more fundamental electroweak interaction", "Cost of construction", "gypsum", "A simple iron boar crest", "new universities", "australia", "David", "25 - yard line", "the Swedish astronomer Anders Celsius", "about 7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "faith", "cliff thorburn", "can be produced with constant technology and resources per unit of time", "peter paul rubens", "badgers"], "metric_results": {"EM": 0.6875, "QA-F1": 0.8264914772727272}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.375, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_squad-validation-3539", "mrqa_squad-validation-8739", "mrqa_squad-validation-2966", "mrqa_squad-validation-5337", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_naturalquestions-validation-2893"], "after_eval": {"predictions": ["hair", "2000", "blackberry", "horsehead", "Big Mamie", "orinoco River", "Teamsters leader Jimmy Hoffa", "clear", "the Peasants\u2019 Revolt", "2009", "Zaza Pachulia", "the inner chloroplast membrane", "renoir", "sports tourism", "Third-party channels", "third", "a more fundamental electroweak interaction", "Cost of construction", "hardness", "A simple iron boar crest", "polytechnics became new universities", "australia", "David", "a touchback on kickoffs at the 25 - yard line instead of the previous 20 - yard lines", "named after the Swedish astronomer Anders Celsius", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "faith", "cliff thorburn", "produced with constant technology and resources per unit of time", "Flemish Baroque", "badgers"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8934294871794872}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2428", "before_prediction": "Teamsters leader", "after_prediction": "Teamsters leader Jimmy Hoffa"}, {"id": "mrqa_triviaqa-validation-2980", "before_prediction": "wat tyler", "after_prediction": "the Peasants\u2019 Revolt"}, {"id": "mrqa_triviaqa-validation-5962", "before_prediction": "gypsum", "after_prediction": "hardness"}, {"id": "mrqa_triviaqa-validation-622", "before_prediction": "peter paul rubens", "after_prediction": "Flemish Baroque"}], "retained_ids": ["mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_hotpotqa-validation-994", "mrqa_triviaqa-validation-6331", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_hotpotqa-validation-1226", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_squad-validation-8279", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-1507"], "fixed_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_squad-validation-3539", "mrqa_squad-validation-8739", "mrqa_squad-validation-2966", "mrqa_squad-validation-5337", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_naturalquestions-validation-2893"], "unfixed_ids": ["mrqa_naturalquestions-validation-9105"], "instant_fixing_rate": 0.9, "instant_retention_rate": 0.8181818178099173}, {"timecode": 28, "before_eval": {"predictions": ["Islam", "Anderson Silva", "inform the jury and the public of the political circumstances surrounding the case and their reasons for breaking the law", "Kusha", "Arabic", "Professor Eobard Thawne", "plum", "a US>10 a week raise", "1825", "contributed by member states on a voluntary basis", "oboe", "McKinsey's offices in Silicon Valley and India", "fear of public speaking", "Living Doll", "certain pediatric GI disorders including congenital GI anomalies and necrotizing enterocolitis", "Alceu Ranzi", "Raya Yarbrough", "UMBC", "michael dokes", "Charles L. Hutchinson", "Song of Songs", "ups", "local talent", "North End Football Club", "Tristan Farnon", "mafic", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "abraham lincoln", "1349", "dodo", "the belief that by focusing on positive or positive thoughts people can bring positive or negative experiences into their life", "larky bus driver Stan Butler"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6781126585402901}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6363636363636364, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1, 1.0, 0.0, 1.0, 0.9473684210526315, 0.5714285714285715]}}, "error_ids": ["mrqa_squad-validation-6835", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-6250", "mrqa_squad-validation-4309", "mrqa_naturalquestions-validation-9185", "mrqa_triviaqa-validation-1347", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821", "mrqa_triviaqa-validation-4308"], "after_eval": {"predictions": ["Islam", "Anderson Silva", "plead not guilty", "Kusha", "poet, and writer", "Professor Eobard Thawne", "plum", "US+18 per week salary", "1825", "contributed by member states on a voluntary basis", "oboe", "McKinsey's offices", "fear of public speaking", "lionel bart", "certain pediatric GI disorders including congenital GI anomalies and necrotizing enterocolitis", "Ondemar Dias", "Bear McCreary", "UMBC", "riddick bowe", "Charles L. Hutchinson", "Song of Songs", "ups", "The Mill Volvo Tyne Theatre", "North End Football Club", "peter davison", "canada", "contemporary accounts were exaggerations", "abraham lincoln", "1332", "dodo", "the belief that by focusing on positive or positive thoughts people can bring positive or negative experiences into their life", "stan butler"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9447838345864661}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9473684210526315, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-1276", "before_prediction": "a US>10 a week raise", "after_prediction": "US+18 per week salary"}, {"id": "mrqa_squad-validation-5086", "before_prediction": "local talent", "after_prediction": "The Mill Volvo Tyne Theatre"}], "retained_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-4919", "mrqa_triviaqa-validation-4827", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3760", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-7110", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_triviaqa-validation-7669", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-2953"], "fixed_ids": ["mrqa_squad-validation-6835", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-6250", "mrqa_squad-validation-4309", "mrqa_naturalquestions-validation-9185", "mrqa_triviaqa-validation-1347", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_squad-validation-8190", "mrqa_triviaqa-validation-4308"], "unfixed_ids": ["mrqa_naturalquestions-validation-7821"], "instant_fixing_rate": 0.9230769230769231, "instant_retention_rate": 0.894736841634349}, {"timecode": 29, "before_eval": {"predictions": ["poisonous", "the 1960s", "to finance his own projects", "Formula One", "Xbox 360", "Beijing for the 2022 Winter Olympics", "DeMarcus Ware", "parallelogram", "evolution", "364", "the reactor core", "Don McLean", "cylinder volume", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "NBA Development League (NBA D-League)", "kent", "St. Mary's County", "Emmanuel Sanders", "2,615", "Pyeongchang", "athlete", "Microsoft Windows", "The Man", "husband and wife American designers who made significant historical contributions to the development of modern architecture and furniture", "south korea", "arthur", "the smallest subfield", "heartburn", "-40%", "light reactions"], "metric_results": {"EM": 0.5, "QA-F1": 0.5750233426704014}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.4000000000000001, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.3, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_squad-validation-1249", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_naturalquestions-validation-5826", "mrqa_hotpotqa-validation-2928", "mrqa_naturalquestions-validation-5961", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_hotpotqa-validation-5709", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "after_eval": {"predictions": ["holly", "the 1960s", "His patents", "Formula One", "microsoft", "Tokyo for the 2020 Summer Olympics", "DeMarcus Ware", "parallelogram", "abraham lincoln", "364", "startup neutron source", "van gogh", "the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "National Basketball Development League", "kent", "Washington metropolitan area", "Sanders", "The population was 2,615", "Beijing", "an American football quarterback", "recover many kinds of passwords using methods such as network packet sniffing, cracking various password hashes by using method such as dictionary attacks, brute force and cryptanalysis attacks", "The Man", "husband and wife", "south korea", "arthur", "the smallest subfield of a field F containing both 0 and 1", "heartburn", "53%", "normal grana and thylakoids"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8990064775910365}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9642857142857143, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-3603", "before_prediction": "evolution", "after_prediction": "abraham lincoln"}, {"id": "mrqa_squad-validation-3344", "before_prediction": "cylinder volume", "after_prediction": "the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders"}, {"id": "mrqa_squad-validation-9036", "before_prediction": "the smallest subfield", "after_prediction": "the smallest subfield of a field F containing both 0 and 1"}], "retained_ids": ["mrqa_naturalquestions-validation-866", "mrqa_hotpotqa-validation-3497", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3103", "mrqa_squad-validation-8075", "mrqa_squad-validation-7914", "mrqa_triviaqa-validation-1280", "mrqa_squad-validation-772", "mrqa_naturalquestions-validation-7567", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_triviaqa-validation-814"], "fixed_ids": ["mrqa_triviaqa-validation-4708", "mrqa_squad-validation-1249", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_hotpotqa-validation-2928", "mrqa_naturalquestions-validation-5961", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5709", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "unfixed_ids": ["mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-4572"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.8124999994921874}, {"timecode": 30, "before_eval": {"predictions": ["judges", "Frederick Louis, Prince of Wales, son of King George II", "He has 96 daughters, 1 son named JoJo and a wife named Sally", "john brown", "John Cleese and Connie Booth", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "SyFy", "daniel craig", "demographics and economic ties", "3", "the most recent Super Bowl champion", "a rapid drop in your blood sugar", "alex turner", "imola", "microsoft", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "A computer program", "National Party of Australia", "babylon", "Hekla", "$474 million", "spike", "South Pacific", "7", "Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Netflix", "Scott Bakula", "Tamer Youssef", "state-franchised", "skylab", "catherine of aragon", "facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.5, "QA-F1": 0.525078320802005}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.14285714285714288, 0.2105263157894737, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3157894736842105]}}, "error_ids": ["mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-2577", "mrqa_triviaqa-validation-2750", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_hotpotqa-validation-4604", "mrqa_triviaqa-validation-3134", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "after_eval": {"predictions": ["judges", "Dutch House of Orange-Nassau", "Jesse McCartney as JoJo, the Mayor's son", "John Brown and His Men at Harper's Ferry", "polly", "Martin Ingerman", "SyFy", "daniel craig", "historical political divisions", "3", "the most recent Super Bowl champion", "narcolepsy", "arctic monkeys", "imola", "jurassic park", "all transmissions", "A computer program", "The Greens", "babylon", "hvannadalshnukur", "largest source of foreign direct investment", "national network", "South Pacific", "7", "New Jersey", "Netflix", "Martinja Percy ( Shalita Grant ), a former ATF special agent and LaSalle's partner", "indira gandhi", "state-franchised", "skylab", "spain", "to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8899038461538462}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7692307692307692, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-3920", "before_prediction": "john brown", "after_prediction": "John Brown and His Men at Harper's Ferry"}, {"id": "mrqa_triviaqa-validation-5266", "before_prediction": "alex turner", "after_prediction": "arctic monkeys"}, {"id": "mrqa_naturalquestions-validation-1282", "before_prediction": "Scott Bakula", "after_prediction": "Martinja Percy ( Shalita Grant ), a former ATF special agent and LaSalle's partner"}], "retained_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-6888", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6207", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-3333", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530"], "fixed_ids": ["mrqa_hotpotqa-validation-2559", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-2577", "mrqa_triviaqa-validation-2750", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_hotpotqa-validation-4604", "mrqa_triviaqa-validation-3134", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "unfixed_ids": ["mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-7090"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.8124999994921874}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Thiel", "sarajevo", "sports commentator", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "androids", "height", "shopping mall located in Bloomington, Minnesota, United States ( a suburb of the Twin Cities )", "to avoid responsibility for her actions", "Michael Myers, Eddie Murphy and Cameron Diaz reprising their respective voice roles of Shrek, Donkey, and Fiona from the first film", "waltz king", "his own men", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "the Parthenon", "the RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "reduce growth in relatively poor countries but encourage growth", "Ibrium", "national tour", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "is a song by Cole Porter", "national", "an estimated 390 billion individual trees", "Washington Street between Boylston Street and Kneeland Street", "May 10, 1976", "six total tackles", "Lexy Gold", "his frustration with the atmosphere in the group at that time", "Birmingham", "Paul the Apostle, later cited by John Smith in Jamestown, Virginia, and by Lenin during the Russian Revolution", "the British passenger liner RMS Lusitania", "occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25544836427189366}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.15384615384615385, 0.2222222222222222, 0.0, 1.0, 0.0, 0.37037037037037035, 1.0, 0.07407407407407407, 0.3636363636363636, 1.0, 0.0, 0.0, 0.07407407407407407, 0.5714285714285715, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.33333333333333337, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "after_eval": {"predictions": ["Guadalupe \"Lupe\" Ontiveros", "Andreas", "sarajevo", "football", "Newell Highway", "tenth planet", "4", "shopping", "explaining their actions", "Andrew Adamson", "waltz", "Naimans", "additional warming of the Earth's surface", "the Parthenon", "Britain", "encourage", "Ibbi-Sipish", "strictly Come Dancing", "Polish", "the Falange", "cole porter", "1967", "16,000", "Washington Street", "8 November 1978", "2", "\"Fudge\"", "his frustration with the atmosphere in the group at that time", "barbarella", "John Smith", "lusitania", "worked as weavers"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9583333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-3300", "before_prediction": "waltz king", "after_prediction": "waltz"}, {"id": "mrqa_hotpotqa-validation-1444", "before_prediction": "Ibrium", "after_prediction": "Ibbi-Sipish"}], "retained_ids": ["mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-4148"], "fixed_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.5999999988}, {"timecode": 32, "before_eval": {"predictions": ["A high-gain S-band antenna", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "Munich", "800 m", "Matt Jones", "kalium", "when the top of the cab can be crushed or sliced off as it swings round violently and tries to fold under the trailer", "T cell receptor", "relatively low salaries", "genetically engineered corn", "Heading Out to the Highway", "James Bond", "Wii U", "Michael Oppenheimer", "England national team", "\"degrees of privilege\"", "No Night Today", "Convention", "5,922", "December 5, 1991", "psychological horror", "Philadelphia 76ers", "Sinterklaas", "Stern-Plaza", "WBC/WBA heavyweight champion Joe Frazier", "1991", "Monday", "Dealey Plaza", "Nairobi, Kenya", "the last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.28125, "QA-F1": 0.38930860805860806}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-5683", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_naturalquestions-validation-5960"], "after_eval": {"predictions": ["orbital scientific instrument package", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "september", "south africa", "Thomas Middleditch", "potassium", "If a vehicle towing a trailer skids", "T cell receptor (TCR)", "relatively low salaries", "us", "Point of Entry", "bridge", "u", "Science Magazine's", "Premier League club Manchester United and the England national team", "poverty and were ill treated", "Space is the Place", "France's Legislative Assembly", "The population was 5,922", "June 4, 1931", "is a 2016 science fiction psychological horror", "leg injury", "Sinterklaas", "Stern-Plaza", "Jimmy Ellis", "23 March 1991", "may", "Dallas", "Nairobi, Kenya", "During the last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9553571428571428}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3508", "before_prediction": "5,922", "after_prediction": "The population was 5,922"}, {"id": "mrqa_hotpotqa-validation-2985", "before_prediction": "1991", "after_prediction": "23 March 1991"}], "retained_ids": ["mrqa_squad-validation-6744", "mrqa_hotpotqa-validation-572", "mrqa_squad-validation-2234", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-5557"], "fixed_ids": ["mrqa_squad-validation-3887", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-5683", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_naturalquestions-validation-5960"], "unfixed_ids": ["mrqa_naturalquestions-validation-10311"], "instant_fixing_rate": 0.9565217391304348, "instant_retention_rate": 0.7777777769135802}, {"timecode": 33, "before_eval": {"predictions": ["Napoleon", "Boston Herald", "1967", "the amount charged by a bookmaker", "largest Filipino American community", "Mantle and Maris -- collectively known as the M&M Boys -- are the only teammates to reach the 50 home run club in the same season", "tintin", "enzymatic activation", "higher rates", "J\u0101nis Strazdi\u0146\u0161", "Tzeitel", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "emperor", "oakum", "Spring city", "Mumbai", "Broken Hill and Sydney", "2005", "punishments", "Smith and Jones", "wagon", "ilich ramirez sanchez", "the desire to prevent things that are indisputably bad", "Paris", "niece", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system", "emotional contagion", "pasternak", "power windows", "Bill Clinton", "Buskerud"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7495975378787879}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 0.0, 1.0, 1.0, 0.0, 0.12121212121212123, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-2812", "mrqa_hotpotqa-validation-61"], "after_eval": {"predictions": ["Napoleon", "Boston Herald", "1967", "the amount charged by a bookmaker", "largest Filipino American community", "Roger Maris", "cuthbert calculus", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "higher rates", "J\u0101nis Strazdi\u0146\u0161", "Hodel", "along the St. Lawrence River valley, with some also in Acadia", "japan", "oakum", "Spring city", "London", "Broken Hill and Sydney", "2005", "punishments", "Smith and Jones", "wagon", "ilich ramirez sanchez", "things that are a matter of custom or expectation", "Paris", "niece", "staying with the same group of peers for all classes", "backflow prevention", "emotional contagion", "japan", "power windows", "William Jefferson Clinton", "Buskerud"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9346590909090909}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1410", "before_prediction": "tintin", "after_prediction": "cuthbert calculus"}, {"id": "mrqa_triviaqa-validation-1975", "before_prediction": "emperor", "after_prediction": "japan"}], "retained_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_naturalquestions-validation-6358", "mrqa_hotpotqa-validation-2161", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_squad-validation-1903", "mrqa_squad-validation-2147", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-1211"], "fixed_ids": ["mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-3523", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-2812", "mrqa_hotpotqa-validation-61"], "unfixed_ids": ["mrqa_squad-validation-10180"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.9130434778638941}, {"timecode": 34, "before_eval": {"predictions": ["Chairman", "white", "email", "neither issue made it clear whether Archie was married to Betty or Veronica", "Victorian College of the Arts", "Britain", "onions", "0.2 inhabitants per square kilometre", "separate tables", "France", "Ian Paisley", "bataan", "lTL", "september", "the United States", "the late 1970s", "first published in 1890", "the team's 75th season", "Stanwyck's bedroom window", "Spencer Treat Clark", "revolution or orbital revolution", "Johnny Darrell", "carotid artery disease", "motorcycles or mopeds pulling trailers", "Euler's totient function", "earwax", "the set of all connected graphs", "the third", "africa", "large", "Lauren Oliver", "healing incantation"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5895146520146519}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-7248", "mrqa_hotpotqa-validation-3982", "mrqa_squad-validation-4369", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-9063", "mrqa_squad-validation-1635", "mrqa_triviaqa-validation-7184"], "after_eval": {"predictions": ["Chairman", "norman hartnell", "email", "neither issue made it clear whether Archie was married to Betty or Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "onions", "0.52", "separate tables", "It was held in France from 10 June to 12 July 1998.", "Ian Paisley", "World War II", "euro", "madness", "Taft", "late 1970s", "first published in 1890", "75th", "Manhattan", "Lucius Verus", "revolution or orbital revolution", "Kenny Rogers and The First Edition", "head and neck", "motorcycles or mopeds pulling trailers", "the sum of divisors function", "earwax", "how graphs are encoded as binary strings", "the third", "afghanistan", "large", "Lauren Oliver", "healing incantation"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1220", "before_prediction": "lTL", "after_prediction": "euro"}], "retained_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-951", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-1139", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-4192", "mrqa_squad-validation-4255", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-2751", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "fixed_ids": ["mrqa_triviaqa-validation-7248", "mrqa_hotpotqa-validation-3982", "mrqa_squad-validation-4369", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-9063", "mrqa_squad-validation-1635", "mrqa_triviaqa-validation-7184"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.941176470034602}, {"timecode": 35, "before_eval": {"predictions": ["2, 3, 4, 6, or 12 years", "the interplay of supply and demand", "Ewan McGregor", "the brain", "butterfly", "Washington Redskins", "courtyard", "Howard Ashton", "wyo", "Unemployment", "Broward County", "Song Kang-ho, Lee Byung-hun, and Jung Woo-sung", "changing display or audio settings quickly", "marston Moor", "the spectroscopic notation for the associated atomic orbitals", "declines", "Beauty and the Beast", "South Africa", "Tyler \"Ty\" Mendoza", "alamo", "a seal illegally is broken", "United Methodist", "Brian Liesegang", "Don Hahn", "Port Moresby", "Alvin and the Chipmunks", "NAACP", "1963\u20131989", "Titanic", "John Prescott", "Darrin Stephens", "india"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5201140873015873}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true], "QA-F1": [0.25, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4544", "mrqa_hotpotqa-validation-2971", "mrqa_triviaqa-validation-6423", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969"], "after_eval": {"predictions": ["2% higher", "capital and financial markets", "Dan Stevens", "the brain, muscles, and liver", "tortellini", "New York Yankees, the Atlanta Braves, the Cincinnati Reds and the San Francisco Giants", "courtyard", "William Howard Ashton,", "president harding", "Unemployment", "Miami", "Best Actor prize", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "charles i", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "declines", "Beauty and the Beast", "South Africa", "Scotty Grainger Jr.", "texas state", "seal", "the UMC", "Geno Lenardo", "Rob Minkoff", "Port Moresby, Papua New Guinea", "david seville", "NAACP", "1963\u20131989", "Titanic", "margaret beckett", "elizabeth montgomery", "indiana"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8295833333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4799999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-7704", "before_prediction": "the brain", "after_prediction": "the brain, muscles, and liver"}, {"id": "mrqa_triviaqa-validation-4829", "before_prediction": "butterfly", "after_prediction": "tortellini"}, {"id": "mrqa_naturalquestions-validation-585", "before_prediction": "the spectroscopic notation for the associated atomic orbitals", "after_prediction": "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet"}, {"id": "mrqa_hotpotqa-validation-1475", "before_prediction": "Tyler \"Ty\" Mendoza", "after_prediction": "Scotty Grainger Jr."}, {"id": "mrqa_hotpotqa-validation-1136", "before_prediction": "Don Hahn", "after_prediction": "Rob Minkoff"}, {"id": "mrqa_triviaqa-validation-1812", "before_prediction": "india", "after_prediction": "indiana"}], "retained_ids": ["mrqa_squad-validation-9400", "mrqa_squad-validation-7352", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-5690", "mrqa_triviaqa-validation-6450", "mrqa_hotpotqa-validation-1720", "mrqa_squad-validation-7610", "mrqa_triviaqa-validation-3860"], "fixed_ids": ["mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4544", "mrqa_hotpotqa-validation-2971", "mrqa_triviaqa-validation-6423", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969"], "unfixed_ids": ["mrqa_squad-validation-9123"], "instant_fixing_rate": 0.9444444444444444, "instant_retention_rate": 0.5714285710204081}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Amber Laura Heard", "william herschel", "rudolph", "Cobham\u2013Edmonds thesis", "to remind the Doctor of his \"moral duty\"", "II", "March 2012", "big easy", "Raymond Patterson", "Coldplay", "Menorca", "plead guilty to one misdemeanor count and receive no jail time", "emperors", "2%", "March 28, 1979", "Easter egg", "decision problem", "conducting", "the right side of the heart", "Miasma theory", "imperial fluid ounces", "the \"Crown of the Continent Ecosystem\"", "white", "California and South Carolina", "perennial", "$12", "20 %", "Best Of The Pops", "It for use in the ARPANET", "west through the Netherlands and extended to the southwest, through the English Channel and finally, to the Atlantic Ocean", "Chad"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6870659722222222}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.125, 0.5]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_squad-validation-4877", "mrqa_hotpotqa-validation-1118", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "after_eval": {"predictions": ["National Broadcasting Company", "McG", "either its axial inclination is a bit more than 90 degrees and the rotation is direct", "rudolph", "the Cobham\u2013Edmonds thesis", "to remind the Doctor of his \"moral duty\"", "II", "April", "city of new orleans", "Raymond Patterson", "Coldplay", "Menorca", "plead guilty to one misdemeanor count and receive no jail time", "emperors", "2%", "1979", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "The formal language associated with this decision problem", "conducting", "the right side of the heart", "That the plague was caused by bad air", "imperial fluid ounces", "mountain ranges", "white", "other states", "nettle", "$12", "20 %", "love is all around", "use in the ARPANET", "west", "Republic of Chad"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-4824", "before_prediction": "william herschel", "after_prediction": "either its axial inclination is a bit more than 90 degrees and the rotation is direct"}], "retained_ids": ["mrqa_hotpotqa-validation-152", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-1758", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_squad-validation-3060", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_squad-validation-3635", "mrqa_naturalquestions-validation-2323"], "fixed_ids": ["mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_squad-validation-4877", "mrqa_hotpotqa-validation-1118", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9473684205540166}, {"timecode": 37, "before_eval": {"predictions": ["San Joaquin Valley Railroad", "three of his ribs were broken", "2007", "Post Alley under Pike Place Market", "important early Nepalese bronze sculptures, repouss\u00e9 work and embroidery", "February 20, 1978", "sheep", "George H.W. Bush", "96", "the Roman Empire", "World War I", "white, blue, pink, rainbow neon and glittering", "Mesopotamia", "around 11 miles (18 km) south of San Jose", "Spotty Dog", "Henry", "shared", "the desert does sustain many types of lizard including the vulnerable great desert skink ( Egernia kintorei ) and a number of small marsupials including the endangered sandhill dunnart", "events and festivals", "kabinett", "2010", "the Na'vi", "7 January 1936", "lifetime protection", "It contains twenty-three episodes", "Carl Sagan", "problems with funding education, sanitation, and traffic control within the city limits", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Pierre Nlend Wom\u00e9", "mistreatment", "whey", "Boston, Massachusetts"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7078067765567766}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.23076923076923078, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-1698", "mrqa_triviaqa-validation-3876", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375"], "after_eval": {"predictions": ["San Joaquin Valley Railroad", "broken", "2007", "San Luis Obispo, California", "mother-of-pearl", "February 20, 1978", "sheep", "Walter Mondale", "96", "the Roman Empire", "two", "white, blue, pink, rainbow neon and glittering", "Mesopotamia", "37\u00b0 9' 58.23\"", "woodentops", "Henry", "shared", "sandhill dunnart", "many events and festivals", "kabinett", "2010", "avatar", "7 January 1936", "lifetime protection", "It contains twenty-three episodes", "Carl Sagan", "Much of the city's tax base dissipated", "Republicans", "Pierre Nlend Wom\u00e9", "mistreatment", "whey", "Boston, Massachusetts"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9642857142857143}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4664", "before_prediction": "George H.W. Bush", "after_prediction": "Walter Mondale"}, {"id": "mrqa_squad-validation-677", "before_prediction": "events and festivals", "after_prediction": "many events and festivals"}], "retained_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-513", "mrqa_triviaqa-validation-1015", "mrqa_hotpotqa-validation-4154", "mrqa_naturalquestions-validation-4212", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_hotpotqa-validation-85", "mrqa_naturalquestions-validation-969", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524", "mrqa_hotpotqa-validation-5371"], "fixed_ids": ["mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-1698", "mrqa_triviaqa-validation-3876", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9090909086776859}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "1958", "Bart Cummings", "dragon", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "a children's story published by John Newbery in London in 1765", "224.7 Earth days", "backers from anywhere in the world and to creators from the US, UK, Canada, Australia, New Zealand, The Netherlands, Denmark, Ireland, Norway, Sweden, Italy, Belgium, Luxembourg, Switzerland and Mexico", "Thorgan Hazard", "when commissioned", "Jeff Meldrum", "a week", "phil archer", "French", "The Chipettes", "suez canal", "60", "journalist", "the fact that there is no revising chamber", "1960s", "ramification", "those were newly accessioned into the collection, probably don't show up in the computer system", "fiat money", "strychnine", "Iowa", "approximately in the early 16th century", "Lord's", "today", "Hathi Jr.", "in sequence with each heartbeat"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6574105122818358}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.09523809523809523, 0.0, 0.0, 1.0, 0.05128205128205129, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2555"], "after_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen,", "1958", "Etienne de Mestre", "dragon", "slavery", "the colonies of British America", "The History of Little Goody Two - Shoes", "224.7 Earth days", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment", "Thorgan Ganael Francis Hazard", "when commissioned", "Don Jeffrey \"Jeff\" Meldrum (born May 24, 1958) is a Professor of Anatomy and Anthropology", "a week", "phil archer", "French and English", "The Chipettes", "suez", "60 by West All - Stars ( 2017 )", "journalist", "take evidence from witnesses, conduct inquiries and scrutinise legislation", "beehive", "ramification", "newly accessioned", "fiat money", "strychnine", "Iowa", "approximately in the early 16th century", "Lord's", "today", "Hathi Jr.", "the closing of the atrioventricular valves and semilunar valves, respectively"], "metric_results": {"EM": 0.875, "QA-F1": 0.9238505747126436}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.896551724137931, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-2499", "before_prediction": "Bart Cummings", "after_prediction": "Etienne de Mestre"}, {"id": "mrqa_triviaqa-validation-3118", "before_prediction": "suez canal", "after_prediction": "suez"}], "retained_ids": ["mrqa_squad-validation-1862", "mrqa_naturalquestions-validation-3893", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4067", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_naturalquestions-validation-3707", "mrqa_squad-validation-1660", "mrqa_squad-validation-9032", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-3320", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142"], "fixed_ids": ["mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-2555"], "unfixed_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-2100"], "instant_fixing_rate": 0.8571428571428571, "instant_retention_rate": 0.8888888883950616}, {"timecode": 39, "before_eval": {"predictions": ["Taiwan", "Dan Conner", "east and west berlin", "president japan", "Jean Parker (born Lois Mae Green, August 11, 1915 \u2013 November 30, 2005) was an American film and stage actress", "anti-Semitism", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "kuskusi", "1977", "John M. Grunsfeld", "detroit", "your song", "2003", "mule, deer, white-tailed deer, moose, elk and caribou", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "an equality", "agatha Christie", "Porto", "August 10, 1933", "one - mile - wide", "February 23, 2014", "those who already hold wealth", "B. Traven", "It is themed to the Disney-Pixar film \" Finding Nemo\" and named after Crush, a green sea turtle character from the film", "Fortean", "inflation", "squirrel", "247,597", "Princeton", "German service cartridge", "DC electricity"], "metric_results": {"EM": 0.625, "QA-F1": 0.6870847902097902}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9230769230769231, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_squad-validation-2493", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-4298"], "after_eval": {"predictions": ["taipei", "Dan Conner", "east and west berlin", "warren commission", "Katharine Hepburn, Joan Bennett, Frances Dee", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "couscous", "1980s", "John M. Grunsfeld, geneticist James Watson", "Detroit", "your song", "2000", "every year", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "an equality", "agatha Christie", "Porto", "August 10, 1933", "one - mile - wide", "vancouver", "those who already hold wealth", "bilingual German author B. Traven, whose identity remains unknown", "Finding Nemo", "Fortean", "inflation", "squirrel", "247,597", "The Institute for Advanced Study", "German service cartridge", "DC electricity"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9559294871794872}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-8070", "before_prediction": "John M. Grunsfeld", "after_prediction": "John M. Grunsfeld, geneticist James Watson"}, {"id": "mrqa_naturalquestions-validation-3698", "before_prediction": "2003", "after_prediction": "2000"}], "retained_ids": ["mrqa_hotpotqa-validation-2243", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_hotpotqa-validation-5233", "mrqa_naturalquestions-validation-6839", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_hotpotqa-validation-2332", "mrqa_naturalquestions-validation-3108", "mrqa_squad-validation-7547", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "fixed_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_squad-validation-2493", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-4298"], "unfixed_ids": ["mrqa_naturalquestions-validation-9227"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.89999999955}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "corgis", "14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "placing them on prophetic faith", "Bacon", "Charlton Heston", "anti-inflammatory molecules, such as cortisol and catecholamines", "vUHMOaD/JVI aiAQBAJA jaMOJOD", "Kevin Kolb", "money", "officers and agencies of the U.S. federal government", "night", "Uighurs surrendered peacefully without violently resisting", "Sochi, Russia", "right", "central Saskatchewan", "immediately", "margaret", "shorthand typist", "30 Major League Baseball teams", "MI6", "neurons", "julius caesar", "photolysis", "4 - inch screen size", "Queen City", "qui tam"], "metric_results": {"EM": 0.59375, "QA-F1": 0.625380608974359}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.375, 1.0, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-3280"], "after_eval": {"predictions": ["50 fund", "Cetshwayo", "on the road back to Samarkand", "sarajevo", "Sacajawea, Sojourner Truth, Eleanor of Aquitaine, Empress Theodora of Byzantium, Virginia Woolf, Susan B. Anthony, and Georgia O'Keeffe", "corgi", "14th to 17th centuries", "seven relief pitchers", "placing them on prophetic faith", "Bacon", "Yul Brynner", "cytotoxic natural killer cells and CTLs", "tartan", "philadelphia eagles", "a tradeable entity used to avoid the inconveniences of a pure barter system", "express or implied Acts of Congress that delegate to the President some degree of discretionary power ( delegated legislation )", "sound and light", "Uighurs surrendered peacefully without violently resisting", "Sochi, Russia", "sweden", "central Saskatchewan", "immediately", "new zealand", "shorthand typist", "30 Major League Baseball teams", "MI6", "neurons", "julius caesar", "photolysis", "4 - inch screen size", "Queen City", "qui tam"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1453", "before_prediction": "Isabella (Belle) Baumfree", "after_prediction": "Sacajawea, Sojourner Truth, Eleanor of Aquitaine, Empress Theodora of Byzantium, Virginia Woolf, Susan B. Anthony, and Georgia O'Keeffe"}], "retained_ids": ["mrqa_squad-validation-395", "mrqa_triviaqa-validation-1571", "mrqa_squad-validation-4953", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_squad-validation-8247", "mrqa_hotpotqa-validation-4076", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_hotpotqa-validation-178", "mrqa_naturalquestions-validation-993"], "fixed_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-3280"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9473684205540166}, {"timecode": 41, "before_eval": {"predictions": ["all war", "Hindi", "Gaelic", "Three-card brag", "Idisi", "lion", "The cinema of Russia", "sediment load", "FedExField in Landover, Maryland", "the scission of newly formed vesicles from the membrane of one cellular compartment", "User State Migration Tool", "Ordos City", "flying disc", "Boise State University", "leicester", "Section 30 of the Teaching Council Act 2001", "Paul Lynde as Templeton, a care - free, egotistical rat who lives on a web in a corner of Homer's barn above Wilbur's pig pen", "October 1986", "Huge-LQG", "Northeast Monsoon or Retreating Monsoon", "switzerland", "george i", "5AA", "Q", "Philippians", "what builds nations'wealth", "Gerard Marenghi (born January 24, 1920) known as Jerry Maren", "bobby brown", "Nebula Award", "margaret thatcher", "jonathan", "Luigi Creatore"], "metric_results": {"EM": 0.6875, "QA-F1": 0.796413768900182}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.17391304347826084, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 0.7499999999999999, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1391", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_triviaqa-validation-2384", "mrqa_triviaqa-validation-2181", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-434"], "after_eval": {"predictions": ["all war", "Hindi", "Gaelic", "In Crash, there is no betting, as in Brag", "Idisi", "lion", "The cinema of Russia", "sediment load", "FedExField in Landover, Maryland", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment", "Windows Easy Transfer", "Ordos City", "flying disc", "Boise State University", "leicester", "Section 30", "Paul Lynde", "October 1986", "4 billion", "Northeast Monsoon or Retreating Monsoon", "switzerland", "king george iii", "5AA", "Q", "Philippians", "what builds nations'wealth", "Gerard Marenghi (born January 24, 1920)", "bobby brown", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "margaret thatcher", "jonathan", "Hugo Peretti, Luigi Creatore, and George David Weiss"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9647375030629748}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7547169811320755, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-9763", "before_prediction": "Luigi Creatore", "after_prediction": "Hugo Peretti, Luigi Creatore, and George David Weiss"}], "retained_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_squad-validation-9355", "mrqa_naturalquestions-validation-10565", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320"], "fixed_ids": ["mrqa_hotpotqa-validation-1391", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_triviaqa-validation-2384", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-434"], "unfixed_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8338"], "instant_fixing_rate": 0.8, "instant_retention_rate": 0.9545454541115702}, {"timecode": 42, "before_eval": {"predictions": ["david carradine", "cavatelli, acini di pepe, pastina, orzo, etc.", "ballet", "derision", "various causes", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "Chartered", "statement suggested a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Danish", "a midfielder", "rommel", "the side - chain of the amino acid N - terminal", "disaccharide sucrose", "their unusual behavior", "Gainsborough Trinity Football Club", "portrait", "By functions ; Introverted Sensing ( Si ), Extroverted Thinking ( Te ), Introverted Feeling ( Fi ) and Extrovert Intuition ( Ne )", "Thursday", "4", "drug choice, dose, route, frequency, and duration", "4", "feats of exploration", "piston", "rob lowe", "The Private Education Student Financial Assistance", "pernambuco wood", "rebuild St. Peter's Basilica", "colonies", "two", "Bills", "Qualcomm", "DTIME(n2)"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5820618872549019}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true], "QA-F1": [0.0, 0.9333333333333333, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.4, 1.0, 0.0, 0.0, 0.4666666666666667, 0.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8750000000000001, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_squad-validation-1429", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-4842", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-9792"], "after_eval": {"predictions": ["kill bill", "usually cavatelli, acini di pepe, pastina, orzo, etc.", "ballet", "derision", "damage to the skin was not caused by the Roentgen rays, but by the ozone generated in contact with the skin", "the American Civil War", "Chartered or CPA", "statement suggested a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Danish", "centre-back", "egypt", "where the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position ) is a large hydrophobic amino acid", "F fructose", "their unusual behavior", "Gainsborough Trinity Football Club", "portrait", "Extroverted Thinking ( Te )", "May", "white", "drug choice, dose, route, frequency, and duration of therapy", "mars", "feats of exploration", "piston", "rob lowe", "The Private Education Student Financial Assistance", "bow", "rebuild St. Peter's Basilica", "Algeria", "two forces, one pointing north, and one pointing east", "Bills", "Qualcomm, a San Diego-based telecommunications equipment company and the stadium was known as Qualcomm Stadium", "DTIME(n2)"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8848214285714286}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.9047619047619047, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.14285714285714288, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-1841", "before_prediction": "Chartered", "after_prediction": "Chartered or CPA"}, {"id": "mrqa_squad-validation-10395", "before_prediction": "two", "after_prediction": "two forces, one pointing north, and one pointing east"}, {"id": "mrqa_hotpotqa-validation-5522", "before_prediction": "Qualcomm", "after_prediction": "Qualcomm, a San Diego-based telecommunications equipment company and the stadium was known as Qualcomm Stadium"}], "retained_ids": ["mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_squad-validation-2150", "mrqa_squad-validation-9452", "mrqa_squad-validation-1808"], "fixed_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_squad-validation-1429", "mrqa_naturalquestions-validation-9093", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-9792"], "unfixed_ids": ["mrqa_squad-validation-6735", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-4842"], "instant_fixing_rate": 0.7647058823529411, "instant_retention_rate": 0.7999999994666667}, {"timecode": 43, "before_eval": {"predictions": ["niagara falls", "15", "19th state", "temple square", "Italian", "a sailor coming home from a round trip", "top - level domain", "recurring and life-threatening infections", "meyerbeer", "natural-ingredients-only personal care products", "horse", "Sparafucile", "largest country", "third", "ikea", "169", "mexico", "George Frampton", "Plies", "the Outfield", "tennis", "Edward Furlong", "almost universal for marine engines after 1880", "richard burton", "when the Moon's ecliptic longitude and the Sun's Ecliptica longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "Sergeant-major Bullimore", "Glenn T. Seaborg", "Kentucky, Virginia, and Tennessee", "avionics, telecommunications, and computers", "mitochondrial eve", "237", "Matthew 2:11"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8557539682539682}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.22222222222222224, 1.0, 0.8095238095238095, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-5822", "mrqa_squad-validation-3330", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-305"], "after_eval": {"predictions": ["niagara falls", "15", "19th state", "salt lake city", "Italian", "sailor", "top - level domain", "recurring and life-threatening infections", "meyerbeer", "natural-ingredients-only personal care products", "horse", "Osmin in Mozart's \"Abduction from the Seraglio\"", "largest country", "third", "ikea", "169", "mexico", "George Frampton", "Plies", "English rock band the Outfield", "tennis", "Edward Furlong", "road engines", "richard burton", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "snudge", "Glenn T. Seaborg", "Kentucky", "NASA's current human spaceflight capability, and funded construction of its Johnson Space Center and Kennedy Space Center", "mitochondrial eve", "237", "matthew"], "metric_results": {"EM": 0.90625, "QA-F1": 0.944047619047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8095238095238095, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-4101", "before_prediction": "Sparafucile", "after_prediction": "Osmin in Mozart's \"Abduction from the Seraglio\""}, {"id": "mrqa_hotpotqa-validation-4624", "before_prediction": "Kentucky, Virginia, and Tennessee", "after_prediction": "Kentucky"}], "retained_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_hotpotqa-validation-2403", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_triviaqa-validation-7482", "mrqa_naturalquestions-validation-7641", "mrqa_triviaqa-validation-2803", "mrqa_squad-validation-8054", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541"], "fixed_ids": ["mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-5822", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-305"], "unfixed_ids": ["mrqa_naturalquestions-validation-5968"], "instant_fixing_rate": 0.8571428571428571, "instant_retention_rate": 0.919999999632}, {"timecode": 44, "before_eval": {"predictions": ["Wakanda", "high test scores", "2003", "cricket", "sweden", "campaign setting", "the `` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867 feet", "the Hebrew name Immanu'el ( \u05e2\u05b4\u05de\u05b8\u05bc\u05e0\u05d5\u05bc\u05d0\u05b5\u05dc \u202c, which means `` God with us. ''", "Shape of You", "Christopher Lee as Count Dooku / Darth Tyranus", "second most commonly", "sprightly folktale", "all health care settings", "pharmacists are expected to become more integral within the health care system", "treble clef", "Gabriel Alberto Azucena (born September 23, 1988)", "average speed 112 km / h", "Rome", "May 18, 2010", "Estelle Sylvia Pankhurst", "schengen", "James Spedding", "belfast", "Indian government", "Irish", "the original Regia and House of the Vestal Virgins", "bront\u00eb", "energy", "Hubble", "a genuine love of God with heart, soul, mind, and strength", "Christ lag"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5337391774891774}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.19999999999999998, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-389", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "after_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "more experience and higher education", "in 2003 for the inter-county competition in England and Wales", "paris", "football", "published campaign settings", "the `` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\u00f7 (pronounced \"divide\") is the third studio album by English singer-songwriter Ed Sheeran.", "Yoda", "second most commonly", "well", "all health care settings", "pharmacists are expected to become more integral within the health care system", "music", "Lecrae Devaughn Moore", "average speed 112 km / h", "Rome", "May 18, 2010", "Sylvia Pankhurst", "schengen", "Lord Chancellor of England", "belfast", "Indian government", "Irish", "Vesta", "branwell", "energy-storage molecules ATP and NADPH", "space telescope", "Christian Perfection", "chorale cantatas"], "metric_results": {"EM": 0.90625, "QA-F1": 0.93125}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-6579", "before_prediction": "sweden", "after_prediction": "football"}, {"id": "mrqa_squad-validation-8625", "before_prediction": "energy", "after_prediction": "energy-storage molecules ATP and NADPH"}, {"id": "mrqa_triviaqa-validation-1504", "before_prediction": "Hubble", "after_prediction": "space telescope"}], "retained_ids": ["mrqa_naturalquestions-validation-10619", "mrqa_squad-validation-7067", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696"], "fixed_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-389", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.7857142851530612}, {"timecode": 45, "before_eval": {"predictions": ["mary Ann spillane", "Detroit Lions", "perique", "his left leg was cut off close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "death penalty", "stout man with a \" double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "gulf stream", "Grail", "Mangal Pandey of the 34th BNI", "Colonia Agrippina", "Cartwright clan", "Kentucky ( the law creating Kentucky names it the `` State of Kentucky '' but it was originally part of the land grant of the Colony of Virginia ), Massachusetts, Pennsylvania, and Virginia", "paris", "the eighth series", "the main highway entrance at California State Route 1,", "St. Louis", "Canadian", "Gareth", "LOVE Radio", "the Miami Marlins", "the court", "tony manero", "Donald Henkel", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "\" Cashin' In\"", "the most recent Super Bowl champions", "Command/Service Module", "Santa Clara", "the tsar's Moscow residence", "Operation Neptune", "Mediterranean Sea"], "metric_results": {"EM": 0.5, "QA-F1": 0.5485477224388515}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.06451612903225806, 1.0, 0.0, 1.0, 1.0, 0.7692307692307693, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.07142857142857142, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "after_eval": {"predictions": ["mike hammer", "Detroit Lions", "smoke it", "under `` the immortal Hawke ''", "death penalty", "frail", "gulf stream", "Holy Grail", "29 - year - old Mangal Pandey of the 34th BNI", "oppidum Ubiorum", "lorne greene", "Kentucky ( the law creating Kentucky names it the `` State of Kentucky '' but it was originally part of the land grant of the Colony of Virginia ), Massachusetts, Pennsylvania, and Virginia", "1998", "They were the first group to win the competition", "the main highway entrance at California State Route 1, and entrances in Carmel and Pacific Grove", "St. Louis", "Canadian", "Gareth", "LOVE Radio", "Colorado Rockies", "the court", "tony manero", "David Dobkin", "worked to radicalize the Islamist movement", "People! and The Carnabeats", "Eric Bolling", "the most recent Super Bowl champions", "Command/Service Module", "Santa Clara", "the tsar's Moscow residence", "Neptune", "peninsular"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9065834527791049}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-3363", "before_prediction": "the main highway entrance at California State Route 1,", "after_prediction": "the main highway entrance at California State Route 1, and entrances in Carmel and Pacific Grove"}, {"id": "mrqa_hotpotqa-validation-3509", "before_prediction": "\" Cashin' In\"", "after_prediction": "Eric Bolling"}, {"id": "mrqa_hotpotqa-validation-712", "before_prediction": "Operation Neptune", "after_prediction": "Neptune"}], "retained_ids": ["mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-3989", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_squad-validation-5852", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13"], "fixed_ids": ["mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_naturalquestions-validation-2067"], "unfixed_ids": ["mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4905"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.8124999994921874}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "take that \u201cborrows\u201d from Rick Astley", "being one of the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure", "the development of safety lamps, Stephenson's Rocket, Lord Armstrong's artillery, Be-Ro flour, Joseph Swan's electric light bulbs, and Charles Parsons' invention of the steam turbine", "used obscure languages as a means of secret communication during wartime", "Sir Isaac Newton", "electromagnetic theory", "Premier League club Swansea City", "pre-Raphaelite", "Elizabeth Weber", "an earlier Funcom game, \"The Secret World\"", "hundreds of television and radio channels", "\"Waiting for Guffman\"", "2003", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "apple", "partial funding", "a 5% abv draught beer", "unattainable", "Chu'Tsai", "Shaun, a man attempting to get some kind of focus in his life as he deals with his girlfriend, his mother and stepfather", "least onerous", "como", "Grissom, White, and Chaffee", "multinational retail corporation", "passion fruit", "Natya Shastra", "the sand grains cause a scrubbing noise as they rub against each other when walked on", "ryder cup", "incorrectly", "vienna"], "metric_results": {"EM": 0.46875, "QA-F1": 0.554606398550364}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.5, 0.4827586206896552, 0.08, 0.14814814814814817, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.1818181818181818, 0.2857142857142857, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0689655172413793, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_squad-validation-5157", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_squad-validation-5464", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-5226", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-2808"], "after_eval": {"predictions": ["cuba", "continents `` ploughed '' through the sea.", "take that", "youngest person to become a national transgender figure", "electric lighting", "improved the speed of encryption of communications at both ends in front line operations during World War II", "Einstein", "quantum electrodynamics", "Swansea City", "millais", "Joel", "massively multiplayer online role-playing video game", "hundreds", "\"Waiting for Guffman\"", "2003", "The Watermark business park", "apple", "partial funding", "pale lager", "unattainable", "Chu'Tsai", "Liz", "least onerous", "italy", "Grissom, White, and Chaffee", "multinational retail corporation", "passiflora edulis", "The Natya Shastra", "sedimentary rock and other material with a high iron concentration which oxidizes upon exposure to the air", "ryder cup", "incorrectly", "vienna"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9652777777777778}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-10489", "before_prediction": "electromagnetic theory", "after_prediction": "quantum electrodynamics"}], "retained_ids": ["mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-7792", "mrqa_naturalquestions-validation-2890", "mrqa_squad-validation-6271", "mrqa_squad-validation-4064", "mrqa_squad-validation-3913", "mrqa_naturalquestions-validation-3616", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "fixed_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_squad-validation-5157", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_squad-validation-5464", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-5226", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-2808"], "unfixed_ids": ["mrqa_naturalquestions-validation-8204"], "instant_fixing_rate": 0.9411764705882353, "instant_retention_rate": 0.9333333327111111}, {"timecode": 47, "before_eval": {"predictions": ["florida", "horseracing", "New Zealand national team", "Kumbum Monastery or Ta'er Shi near Xining", "Styal Mill", "William Jennings Bryan", "Goldie & Bear", "during initial entry training", "moral tale", "They announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled ( 2004 )", "leeds", "A tree - topper or treetopper is a decorative ornament placed on the top ( or `` crown '' ) of a Christmas tree or Hanukkah bush.", "260", "piccadilly", "Neighbourhood", "The Washington Post", "tentacles", "insects", "specific catechism questions", "dye", "about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "2001", "George Whitefield", "Science and Discovery", "if 1 were considered a prime", "Campbellsville University", "james dean", "Jude", "morgan spurlock", "Maria works in a bridal shop with Anita", "XXXTentacion", "relatively stagnant wages for the working class amidst rising levels of property income for the capitalist class"], "metric_results": {"EM": 0.875, "QA-F1": 0.9023048642533936}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1]}}, "error_ids": ["mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-3685", "mrqa_squad-validation-7182"], "after_eval": {"predictions": ["egypt", "racehorses", "Burnley and the New Zealand national team", "Kumbum Monastery or Ta'er Shi near Xining", "Quarry Bank Mill", "William Jennings Bryan", "Goldie & Bear", "during initial entry training", "moral tale", "Franklin quit after five months, leaving the group as a trio", "leeds", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "260", "piccadilly", "Neighbourhood", "The Washington Post", "tentacles", "insects", "specific catechism questions", "a dye", "about 50% oxygen composition at standard pressure", "2001", "George Whitefield", "Science and Discovery", "if 1 were considered a prime", "Campbellsville University", "james dean", "Jude", "morgan spurlock", "Maria works in a bridal shop with Anita", "XXXTentacion", "less workers are required"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9437500000000001}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2770", "before_prediction": "florida", "after_prediction": "egypt"}, {"id": "mrqa_hotpotqa-validation-1924", "before_prediction": "New Zealand national team", "after_prediction": "Burnley and the New Zealand national team"}, {"id": "mrqa_hotpotqa-validation-5788", "before_prediction": "Styal Mill", "after_prediction": "Quarry Bank Mill"}], "retained_ids": ["mrqa_triviaqa-validation-945", "mrqa_squad-validation-6287", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-1256", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-1609", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-2092"], "fixed_ids": ["mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-3685", "mrqa_squad-validation-7182"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8928571425382653}, {"timecode": 48, "before_eval": {"predictions": ["James Stenbeck", "Section.80", "yosemite", "interventive", "3", "Bishop Lloyd Christ Wicke", "Ray Charles", "During his epic battle with Frieza", "the director's own approved edit", "Shirley Williams", "unesco", "Thon Maker", "loop", "painting, mathematics, calligraphy, poetry, and theater", "at the 1964 Republican National Convention in San Francisco, California", "sin", "whether they wish to collect a jackpot prize in cash or annuity", "Vader's daughter", "Buffalo Bill", "justice", "France", "neutrality", "coffee", "halal", "Arthur Russell", "people", "Wylie Draper", "political", "the university's off- campuses rental policies", "hockey greats Bobby Hull and Dennis Hull", "Pittsburgh Steelers", "conditions such as war, famine, and weather"], "metric_results": {"EM": 0.75, "QA-F1": 0.831060606060606}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.6666666666666665, 0.26666666666666666, 1.0, 0.7272727272727273]}}, "error_ids": ["mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-2582", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "after_eval": {"predictions": ["James Stenbeck", "Section.80", "yosemite", "interventive", "3", "Bishop Lloyd Christ Wicke", "georgia", "During his epic battle with Frieza", "the director's own approved edit", "Shirley Williams", "unesco", "Thon Maker", "loop", "painting, mathematics, calligraphy, poetry, and theater", "part of a pre-recorded television program, Rendezvous with Destiny", "a sin", "whether they wish to collect a jackpot prize in cash or annuity", "Vader's daughter", "Buffalo Bill", "justice", "France", "neutral rights, which included allowing private corporations and banks to sell or loan money to either side", "coffee", "permissible", "Arthur Russell (born Charles Arthur Russell, Jr. May 21, 1951 \u2013 April 4, 1992) was an American cellist, composer, producer, singer, and musician whose work spanned a disparate range of styles.", "people", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "political", "the university's off-campus rental policies", "Dennis Hull, as well as painter Manley MacDonald.", "the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock", "war, famine, and weather"], "metric_results": {"EM": 0.875, "QA-F1": 0.8998655913978495}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.12903225806451613, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-1649", "before_prediction": "neutrality", "after_prediction": "neutral rights, which included allowing private corporations and banks to sell or loan money to either side"}, {"id": "mrqa_hotpotqa-validation-962", "before_prediction": "Arthur Russell", "after_prediction": "Arthur Russell (born Charles Arthur Russell, Jr. May 21, 1951 \u2013 April 4, 1992) was an American cellist, composer, producer, singer, and musician whose work spanned a disparate range of styles."}, {"id": "mrqa_squad-validation-264", "before_prediction": "Pittsburgh Steelers", "after_prediction": "the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock"}], "retained_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_naturalquestions-validation-49", "mrqa_squad-validation-9860", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-2445", "mrqa_squad-validation-8248", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_triviaqa-validation-1197", "mrqa_squad-validation-2337", "mrqa_squad-validation-9518"], "fixed_ids": ["mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-2582", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "unfixed_ids": ["mrqa_naturalquestions-validation-3342"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.8749999996354166}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "Sesame Street", "Timur", "R.E.M.", "from Tamil Nadu in the south to West Bengal in the north through Andhra Pradesh and Odisha", "Ravenna (], also locally ] ; Romagnol: \"Rav\u00e8na\" ) is the capital city of the Province of Ravenna", "12", "georgia", "1895", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "he had assigned them to the company in lieu of stock", "a Marxist and a Leninist", "Gregor Mendel", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax.", "18,000", "State Street", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "110", "georgia", "sun", "Lawton Mainor Chiles Jr.", "`` Kobol's Last Gleaming ''", "Meyer v. Nebraska, 262 U.S. 390 (1923)", "imperialism", "chicken, sour milk, meat, fish or any other stew is generally eaten by much of the population for lunch or supper", "adenosine triphosphate, or ATP", "georgia", "Ruth Elizabeth \"Bette\" Davis", "cobalt", "29 September 2014"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5877109225899548}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true], "QA-F1": [0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 0.888888888888889, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.5161290322580645, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-2032", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_squad-validation-9489", "mrqa_naturalquestions-validation-9013", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876"], "after_eval": {"predictions": ["light utility vehicles", "Sesame Street", "Timur", "\"Losing My Religion\"", "from Tamil Nadu in the south to West Bengal in the north through Andhra Pradesh and Odisha", "Ravenna (], also locally ] ; Romagnol: \"Rav\u00e8na\" ) is the capital city of the Province of Ravenna", "Bocelli became completely blind at the age of 12", "meat", "1937", "improved markedly", "VTOL aircraft", "assigned them to the company", "communist", "Gregor Mendel", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax", "18,000 regulars, militia and Native American allies", "State Street", "the Saudi Arab kingdom", "110", "my fair lady", "falcon", "Lawton Chiles", "`` Kobol's Last Gleaming ''", "McCrary", "imperialism", "Ugali with vegetables, sour milk, meat, fish or any other stew", "adenosine triphosphate", "stonehenge", "Ruth Elizabeth \"Bette\" Davis", "cobalt", "29 September 2014"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9275793650793651}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-987", "before_prediction": "1895", "after_prediction": "1937"}], "retained_ids": ["mrqa_hotpotqa-validation-5452", "mrqa_squad-validation-6257", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_naturalquestions-validation-3663", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-4530", "mrqa_squad-validation-9807", "mrqa_hotpotqa-validation-1772", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "fixed_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-2032", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-9013", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_squad-validation-7049", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876"], "unfixed_ids": ["mrqa_squad-validation-9489", "mrqa_naturalquestions-validation-5283"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.9285714279081632}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.4175, "QA-F1": 0.5035843685865131}, "overall_error_number": 932, "overall_instant_fixing_rate": 0.9245394142265184, "final_instream_test": {"EM": 0.85125, "QA-F1": 0.8931910814879888}, "final_upstream_test": {"EM": 0.699, "QA-F1": 0.7629163727527107}}}