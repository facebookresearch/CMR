{"model_update_steps": 1960, "method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=2021_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=2021_ckpts/', replay_candidate_size=8, replay_frequency=1, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion", "john Leslie", "Queen Elizabeth II", "The Dallas Lovers' Song", "the anterolateral system", "1966", "for scientific observation", "john Cameron", "The Stock Market crash in New York", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs", "the great heroism or of the most conspicuous courage in circumstances of extreme danger", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.125, "QA-F1": 0.18157436433298502}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "retrieved_ids": ["mrqa_naturalquestions-train-52344", "mrqa_naturalquestions-train-33594", "mrqa_naturalquestions-train-37120", "mrqa_naturalquestions-train-65641", "mrqa_naturalquestions-train-9917", "mrqa_naturalquestions-train-57495", "mrqa_naturalquestions-train-82809", "mrqa_naturalquestions-train-31108", "mrqa_naturalquestions-train-22937", "mrqa_naturalquestions-train-79184", "mrqa_naturalquestions-train-46915", "mrqa_naturalquestions-train-71074", "mrqa_naturalquestions-train-27053", "mrqa_naturalquestions-train-69738", "mrqa_naturalquestions-train-18259", "mrqa_naturalquestions-train-84997", "mrqa_naturalquestions-train-65607", "mrqa_naturalquestions-train-29335", "mrqa_naturalquestions-train-48313", "mrqa_naturalquestions-train-58848", "mrqa_naturalquestions-train-50507", "mrqa_naturalquestions-train-27691", "mrqa_naturalquestions-train-30469", "mrqa_naturalquestions-train-85949", "mrqa_naturalquestions-train-8713", "mrqa_naturalquestions-train-8238", "mrqa_naturalquestions-train-1867", "mrqa_naturalquestions-train-4890", "mrqa_naturalquestions-train-59008", "mrqa_naturalquestions-train-31709", "mrqa_naturalquestions-train-43913", "mrqa_naturalquestions-train-19696"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "merioneth and Llantisilly Rail Traction Company Limited", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "technological advances", "a student in the second year at a high school, college, or university", "Bothtec", "Terry Reid", "non- peer- reviewed sources", "is an English composer who has something to say", "North America", "OutKast", "rookies", "Akhenaten", "President Theodore Roosevelt", "the fourth season", "four", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "Georges Bizet", "Matt Winer", "1689", "Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2471011627261627}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.0, 0.18181818181818182, 1.0, 0.0, 0.3333333333333333, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.3333333333333333, 0.6666666666666666, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "retrieved_ids": ["mrqa_naturalquestions-train-26719", "mrqa_naturalquestions-train-28888", "mrqa_naturalquestions-train-47028", "mrqa_naturalquestions-train-27257", "mrqa_naturalquestions-train-62092", "mrqa_naturalquestions-train-75509", "mrqa_naturalquestions-train-27379", "mrqa_naturalquestions-train-77003", "mrqa_naturalquestions-train-1294", "mrqa_naturalquestions-train-28015", "mrqa_naturalquestions-train-86002", "mrqa_naturalquestions-train-70892", "mrqa_naturalquestions-train-16429", "mrqa_naturalquestions-train-33115", "mrqa_naturalquestions-train-10993", "mrqa_naturalquestions-train-41241", "mrqa_naturalquestions-train-77199", "mrqa_naturalquestions-train-12196", "mrqa_naturalquestions-train-20219", "mrqa_naturalquestions-train-22568", "mrqa_naturalquestions-train-36398", "mrqa_naturalquestions-train-2117", "mrqa_naturalquestions-train-31412", "mrqa_naturalquestions-train-13458", "mrqa_naturalquestions-train-25524", "mrqa_naturalquestions-train-82361", "mrqa_naturalquestions-train-84539", "mrqa_naturalquestions-train-75111", "mrqa_naturalquestions-train-68283", "mrqa_naturalquestions-train-54251", "mrqa_naturalquestions-train-37898", "mrqa_naturalquestions-train-69320"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "is not involved in this venture", "id", "27 July and 7 August 2022", "New York", "is getting a remake", "2006 British Academy Television Award for Best Drama Series", "Least of the Great Powers", "the lower motor neurons, the efferent nerves that directly innervate muscles", "is a British television game show based on the American game show Family Feud", "Fox's Glacier Mints", "Lester Piggott", "death mask", "video film", "Overtime", "Sir Henry Cole", "trouble distinguishing between carbon dioxide and oxygen", "The Young Ones", "Clyde Barrow", "the Democratic Unionist Party", "23 July 1989", "many educational institutions especially within the US", "gurus", "control purposes", "is officially out on March 31", "Callability", "2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM, either 16 or 32 GB of internal storage, and a 2300 mAh battery", "over 10,000", "al - khimar", "proteins", "laparoscopic cholecystectomy", "berenice Abbott"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1497481561307962}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, 0.10526315789473685, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.45161290322580644, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-6341", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-9493", "mrqa_naturalquestions-train-70582", "mrqa_naturalquestions-train-50800", "mrqa_naturalquestions-train-80268", "mrqa_naturalquestions-train-11831", "mrqa_naturalquestions-train-10879", "mrqa_naturalquestions-train-56908", "mrqa_naturalquestions-train-54477", "mrqa_naturalquestions-train-7569", "mrqa_naturalquestions-train-5890", "mrqa_naturalquestions-train-35508", "mrqa_naturalquestions-train-51316", "mrqa_naturalquestions-train-66160", "mrqa_naturalquestions-train-70223", "mrqa_naturalquestions-train-59662", "mrqa_naturalquestions-train-41935", "mrqa_naturalquestions-train-42276", "mrqa_naturalquestions-train-68260", "mrqa_naturalquestions-train-9234", "mrqa_naturalquestions-train-22759", "mrqa_naturalquestions-train-26667", "mrqa_naturalquestions-train-7307", "mrqa_naturalquestions-train-87063", "mrqa_naturalquestions-train-60554", "mrqa_naturalquestions-train-49559", "mrqa_naturalquestions-train-33124", "mrqa_naturalquestions-train-61444", "mrqa_naturalquestions-train-36440", "mrqa_naturalquestions-train-64128", "mrqa_naturalquestions-train-27083", "mrqa_naturalquestions-train-13614", "mrqa_naturalquestions-train-78521"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "Arnhem", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "January", "a shepherd", "at elevation 2 meters above sea level", "tetanus disease", "bounding the time or space used by the algorithm", "aditally modified image by Al Gilbertson", "Alex O'Loughlin", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "1934", "the Reverse - Flash", "All Hallows'Day", "1934", "Baku", "new converts", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America", "a person's speech or language must be significantly impaired in one ( or several ) of the four communication modalities following acquired brain injury or have significant decline over a short time period", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "rotating in one direction", "Splodgenessabounds"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3145275297619048}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0625, 1.0, 0.3333333333333333, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "retrieved_ids": ["mrqa_naturalquestions-train-44139", "mrqa_naturalquestions-train-2643", "mrqa_naturalquestions-train-80708", "mrqa_naturalquestions-train-1319", "mrqa_naturalquestions-train-78684", "mrqa_naturalquestions-train-22238", "mrqa_naturalquestions-train-48955", "mrqa_naturalquestions-train-16362", "mrqa_naturalquestions-train-38514", "mrqa_naturalquestions-train-17363", "mrqa_naturalquestions-train-17811", "mrqa_naturalquestions-train-3465", "mrqa_naturalquestions-train-34340", "mrqa_naturalquestions-train-87846", "mrqa_naturalquestions-train-84481", "mrqa_naturalquestions-train-20847", "mrqa_naturalquestions-train-53674", "mrqa_naturalquestions-train-30612", "mrqa_naturalquestions-train-81727", "mrqa_naturalquestions-train-52728", "mrqa_naturalquestions-train-74012", "mrqa_naturalquestions-train-65460", "mrqa_naturalquestions-train-28428", "mrqa_naturalquestions-train-35659", "mrqa_naturalquestions-train-60931", "mrqa_naturalquestions-train-7022", "mrqa_naturalquestions-train-79099", "mrqa_naturalquestions-train-79325", "mrqa_naturalquestions-train-66924", "mrqa_naturalquestions-train-14629", "mrqa_naturalquestions-train-52170", "mrqa_naturalquestions-train-70931"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["aaron", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "about 6 : 44 p.m. UTC", "a swanee or swannee whistle", "a genetically engineered plant", "to start fires, hunt, and bury their dead", "a mission specialist and primary robotic arm operator", "lining of the fundus", "a distant ancestor of the \"superior\" placental mammals", "Ready to Die", "a young Alexander Graham Bell", "imperial rule", "February 1840", "a defiant speech, or a speech explaining their actions", "George Sylvester Viereck", "a sunny afternoon", "a greater tendency to take on debts", "random conditions as entropy increases", "averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "People's Republic of China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "as a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.0625, "QA-F1": 0.18003205944382417}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.6153846153846154, 0.0, 0.0, 0.5454545454545454, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "retrieved_ids": ["mrqa_naturalquestions-train-42768", "mrqa_naturalquestions-train-37272", "mrqa_naturalquestions-train-85447", "mrqa_naturalquestions-train-39061", "mrqa_naturalquestions-train-16274", "mrqa_naturalquestions-train-53634", "mrqa_naturalquestions-train-77257", "mrqa_naturalquestions-train-18726", "mrqa_naturalquestions-train-28069", "mrqa_naturalquestions-train-79817", "mrqa_naturalquestions-train-20042", "mrqa_naturalquestions-train-68339", "mrqa_naturalquestions-train-86700", "mrqa_naturalquestions-train-14269", "mrqa_naturalquestions-train-17622", "mrqa_naturalquestions-train-32842", "mrqa_naturalquestions-train-61361", "mrqa_naturalquestions-train-86575", "mrqa_naturalquestions-train-8345", "mrqa_naturalquestions-train-60971", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-54884", "mrqa_naturalquestions-train-46567", "mrqa_naturalquestions-train-46547", "mrqa_naturalquestions-train-45410", "mrqa_naturalquestions-train-73718", "mrqa_naturalquestions-train-23433", "mrqa_naturalquestions-train-31412", "mrqa_naturalquestions-train-29205", "mrqa_naturalquestions-train-49264", "mrqa_naturalquestions-train-69230", "mrqa_naturalquestions-train-78994"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out and enhances a thermally induced draft", "Barack Hussein Obama II", "1998", "$30,955", "Bermuda", "90-60's", "an aided or an unaided school", "dolph Camilli", "times sign or the dimension sign", "BAFTA Television Award for Best Actor", "Juice Newton", "1960", "HTTP Secure ( HTTPS )", "late summer", "chisholm trail", "monatomic", "for its popular beaches, and the desert city of Palm Springs is popular for its resort feel and nearby open spaces", "kobo Daishi", "invertebrates", "horkney", "blood type", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "clangers", "in most plants, chloroplasts are concentrated in the leaves", "Indian club ATK", "land that a nation has conquered and expanded", "near Grande Comore, Comoros Islands", "`` \u20b9 \u200d5L '' ( for `` rupees 5 lakhs '' )", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1391684704184704}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.4, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-31607", "mrqa_naturalquestions-train-14978", "mrqa_naturalquestions-train-21946", "mrqa_naturalquestions-train-28903", "mrqa_naturalquestions-train-15774", "mrqa_naturalquestions-train-75416", "mrqa_naturalquestions-train-46068", "mrqa_naturalquestions-train-2139", "mrqa_naturalquestions-train-52201", "mrqa_naturalquestions-train-32584", "mrqa_naturalquestions-train-49438", "mrqa_naturalquestions-train-67538", "mrqa_naturalquestions-train-1943", "mrqa_naturalquestions-train-51833", "mrqa_naturalquestions-train-79719", "mrqa_naturalquestions-train-69814", "mrqa_naturalquestions-train-16707", "mrqa_naturalquestions-train-84147", "mrqa_naturalquestions-train-57451", "mrqa_naturalquestions-train-86422", "mrqa_naturalquestions-train-47736", "mrqa_naturalquestions-train-80333", "mrqa_naturalquestions-train-63395", "mrqa_naturalquestions-train-85771", "mrqa_naturalquestions-train-7498", "mrqa_naturalquestions-train-51417", "mrqa_naturalquestions-train-54625", "mrqa_naturalquestions-train-18740", "mrqa_naturalquestions-train-19566", "mrqa_naturalquestions-train-18684", "mrqa_naturalquestions-train-3152", "mrqa_naturalquestions-train-63615"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["wisdom and prudence of certain decisions of procurement", "Maya group", "a few common complex biomolecules, such as squalene and the carotenes", "The U.S. Army Chaplain insignia", "Kairi", "from both inner city blacks, who wanted more involvement in government, and whites in the suburbs, who want more services and more control over the central city", "film", "Armenia", "Revelation was the last book accepted into the Christian biblical canon, and to the present day some `` Nestorian '' churches such as the Church of the East reject it", "Bruno Mars", "buttermens", "for gallantry", "16 million", "post\u2013World War II", "work oxen for haulage", "1998", "a priest", "third most abundant chemical element in the universe", "18 - season", "family member", "many factors, such as over-fishing and long-term environmental changes", "William Powell Lear", "the radial (centripetal ) force", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "Abraham Gottlob Werner", "marlborough", "present-day Charleston", "a \"quiescent\" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's \"indulgence\" to build up a network of mosques and charitable organizations", "Scott Dunlop", "XIX Corps"], "metric_results": {"EM": 0.125, "QA-F1": 0.2463671398046398}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.19999999999999998, 0.0, 0.25, 0.0, 0.0, 0.0, 0.16, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.7142857142857143, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.07692307692307693, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_squad-validation-4318", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "retrieved_ids": ["mrqa_naturalquestions-train-47068", "mrqa_naturalquestions-train-58035", "mrqa_naturalquestions-train-27429", "mrqa_naturalquestions-train-34719", "mrqa_naturalquestions-train-30517", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-train-39956", "mrqa_naturalquestions-train-42131", "mrqa_naturalquestions-train-24311", "mrqa_naturalquestions-train-2660", "mrqa_naturalquestions-train-73170", "mrqa_naturalquestions-train-88101", "mrqa_naturalquestions-train-52858", "mrqa_naturalquestions-train-33473", "mrqa_naturalquestions-train-11996", "mrqa_naturalquestions-train-47721", "mrqa_naturalquestions-train-59051", "mrqa_naturalquestions-train-70043", "mrqa_naturalquestions-train-14480", "mrqa_naturalquestions-train-23908", "mrqa_naturalquestions-train-19208", "mrqa_naturalquestions-train-62706", "mrqa_naturalquestions-train-66540", "mrqa_naturalquestions-train-33548", "mrqa_naturalquestions-train-85230", "mrqa_naturalquestions-train-71385", "mrqa_naturalquestions-train-50280", "mrqa_naturalquestions-train-75844", "mrqa_naturalquestions-train-8325", "mrqa_naturalquestions-train-3717", "mrqa_naturalquestions-train-34229", "mrqa_naturalquestions-train-40166"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computability theory", "car manufacturing industry was located", "37.7", "6.4 nanometers apart", "will be departing the show to star in CBS's upcoming sci - fi drama Intelligence", "Carl Edwards", "400", "adrenal glands", "artes liberales", "Forest of Bowland", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "Eureka", "1868", "2018", "wolverhampton Wanderers", "law firm", "Pottawatomie County", "orangutan", "Newton's Law of Gravitation", "The church tower", "east end", "Toronto", "wales", "110 miles (177 km )", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "psychotherapeutic", "Quentin Coldwater", "acidic bogs"], "metric_results": {"EM": 0.21875, "QA-F1": 0.39383680555555556}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.8, 0.8, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.2666666666666667, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_triviaqa-validation-7506", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "retrieved_ids": ["mrqa_naturalquestions-train-13278", "mrqa_naturalquestions-train-59549", "mrqa_naturalquestions-train-66204", "mrqa_naturalquestions-train-18219", "mrqa_naturalquestions-train-4107", "mrqa_naturalquestions-train-83424", "mrqa_naturalquestions-train-72231", "mrqa_naturalquestions-train-49934", "mrqa_naturalquestions-train-48935", "mrqa_naturalquestions-train-35223", "mrqa_naturalquestions-train-3746", "mrqa_naturalquestions-train-83744", "mrqa_naturalquestions-train-71270", "mrqa_naturalquestions-train-30829", "mrqa_naturalquestions-train-40171", "mrqa_naturalquestions-train-55204", "mrqa_naturalquestions-train-31645", "mrqa_naturalquestions-train-20331", "mrqa_naturalquestions-train-76463", "mrqa_naturalquestions-train-37393", "mrqa_naturalquestions-train-34756", "mrqa_naturalquestions-train-52771", "mrqa_naturalquestions-train-84209", "mrqa_naturalquestions-train-39773", "mrqa_naturalquestions-train-20432", "mrqa_naturalquestions-train-33811", "mrqa_naturalquestions-train-19182", "mrqa_naturalquestions-train-10513", "mrqa_naturalquestions-train-74661", "mrqa_naturalquestions-train-52866", "mrqa_naturalquestions-train-16618", "mrqa_naturalquestions-train-36666"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["the English phrase `` I Seek You ''", "Argentinian", "Ear Institute at the University College London", "to taste sweetened sheeps\u2019 milk ricotta", "the oxygen concentration is too high", "to celebrate Queen Victoria's diamond jubilee", "1600 Pennsylvania Avenue", "The Daily Stormer", "antibonding", "water", "president", "citizens", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "American 3D computer-animated comedy film", "Worcester Cold Storage and Warehouse Co.", "Los Angeles", "C. W. Grafton", "a liquid crystal on silicon ( LCoS )", "Americans", "IPod Classic", "My Sassy Girl", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "The Edge of Night", "non-combustible substances that corrode, such as iron", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd ) book of prayers", "the root respiration", "soils", "drug dealer", "medium and heavy- Duty diesel trucks", "testes"], "metric_results": {"EM": 0.21875, "QA-F1": 0.38581195934150003}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.8, 1.0, 1.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.18181818181818182, 0.7499999999999999, 0.923076923076923, 0.0, 1.0, 0.4347826086956522, 0.4, 0.0, 0.3333333333333333, 0.1290322580645161, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-27218", "mrqa_naturalquestions-train-10041", "mrqa_naturalquestions-train-27596", "mrqa_naturalquestions-train-1606", "mrqa_naturalquestions-train-49953", "mrqa_naturalquestions-train-33428", "mrqa_naturalquestions-train-63870", "mrqa_naturalquestions-train-4104", "mrqa_naturalquestions-train-26191", "mrqa_naturalquestions-train-31922", "mrqa_naturalquestions-train-78326", "mrqa_naturalquestions-train-68896", "mrqa_naturalquestions-train-5454", "mrqa_naturalquestions-train-872", "mrqa_naturalquestions-train-28574", "mrqa_naturalquestions-train-27742", "mrqa_naturalquestions-train-73699", "mrqa_naturalquestions-train-45549", "mrqa_naturalquestions-train-6113", "mrqa_naturalquestions-validation-1489", "mrqa_naturalquestions-train-78814", "mrqa_naturalquestions-train-67907", "mrqa_naturalquestions-train-3005", "mrqa_naturalquestions-train-32711", "mrqa_naturalquestions-train-7251", "mrqa_naturalquestions-train-14278", "mrqa_naturalquestions-train-18726", "mrqa_naturalquestions-train-23390", "mrqa_naturalquestions-train-10251", "mrqa_naturalquestions-train-31214", "mrqa_naturalquestions-train-42599", "mrqa_naturalquestions-train-78994"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["1688", "yellow fever", "three legal systems", "Las Vegas", "An empty line", "globetrotters", "Anthony Bellew", "a bridge over the Merderet in the fictional town of Ramelle", "a Dubliner tried to kill Benito Mussolini", "no plan", "menhirs", "Queen Victoria", "pH 7 ( 25 \u00b0 C )", "a115 Day Journey", "the MGM Grand Garden Special Events Center", "a virtual museum dedicated to Italian fashion designer Valentino", "Ronnie Hillman", "a single all-encompassing definition of the term", "\"The Quincy Miracle: How One Town Saved Thousands of Mormon Refugees\"", "60", "Eagle Ridge Mall", "Pel\u00e9", "reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "the classical element fire", "Gomer Pyle", "at least 18 or 21 years old ( or have a legal guardian present )", "Bacha", "an American novelist and poet", "Jamestown", "Monet began the paintings in January or early February 1892", "tree growth stages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.305145202020202}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 0.9333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.2222222222222222, 0.0, 0.16666666666666666, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "retrieved_ids": ["mrqa_naturalquestions-train-80747", "mrqa_naturalquestions-train-32961", "mrqa_naturalquestions-train-25077", "mrqa_naturalquestions-train-78420", "mrqa_naturalquestions-train-65801", "mrqa_naturalquestions-train-63309", "mrqa_naturalquestions-train-68896", "mrqa_naturalquestions-train-63337", "mrqa_naturalquestions-train-55200", "mrqa_naturalquestions-train-51904", "mrqa_naturalquestions-train-74165", "mrqa_naturalquestions-train-6129", "mrqa_naturalquestions-train-52932", "mrqa_naturalquestions-train-16910", "mrqa_naturalquestions-train-80808", "mrqa_naturalquestions-train-12547", "mrqa_naturalquestions-train-36288", "mrqa_naturalquestions-train-80770", "mrqa_naturalquestions-train-15054", "mrqa_naturalquestions-train-42251", "mrqa_naturalquestions-train-73654", "mrqa_naturalquestions-train-57155", "mrqa_naturalquestions-train-12335", "mrqa_naturalquestions-train-6065", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-55239", "mrqa_naturalquestions-train-42329", "mrqa_naturalquestions-train-41461", "mrqa_naturalquestions-train-74958", "mrqa_naturalquestions-train-58123", "mrqa_naturalquestions-train-3699", "mrqa_naturalquestions-train-5302"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "\"Traumnovelle\"", "a Gender pay gap in favor of males in the labor market", "Treaty on the Functioning of the European Union", "ice melting at 100 degrees", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "joseph smith", "The world's longest suspension bridges are listed according to the length of their main span", "trams", "tunisia", "joseph smith", "the Pechenegs, the Bulgars, and especially the Seljuk Turks", "died in battle", "Volkswagen Beetle", "joseph smith", "Maastricht Treaty", "jubilee year", "infection, irritation, or allergies", "the most - visited paid monument in the world", "Vittorio Emanuele II", "catfish aquaculture", "at standard conditions that sublimes readily to form a violet gas", "James and D.J. Looney as Young Sparrow and DJ Dragon Nutz", "Iraq", "a co-op of grape growers", "victor willsmeron", "valenti", "July 25, 1951", "the Charlotte Hornets", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the back of the head"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23240028966131906}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.7272727272727272, 0.25, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.8571428571428571, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.09090909090909091, 0.22222222222222224, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769"], "retrieved_ids": ["mrqa_naturalquestions-train-25095", "mrqa_naturalquestions-train-66484", "mrqa_naturalquestions-train-3010", "mrqa_naturalquestions-train-43132", "mrqa_naturalquestions-train-9240", "mrqa_naturalquestions-train-50611", "mrqa_naturalquestions-train-4188", "mrqa_naturalquestions-train-70252", "mrqa_naturalquestions-train-78141", "mrqa_naturalquestions-train-7667", "mrqa_naturalquestions-train-87571", "mrqa_naturalquestions-train-83023", "mrqa_naturalquestions-train-25251", "mrqa_naturalquestions-train-5169", "mrqa_naturalquestions-train-63337", "mrqa_naturalquestions-train-69565", "mrqa_naturalquestions-train-10586", "mrqa_naturalquestions-train-44632", "mrqa_squad-validation-3021", "mrqa_naturalquestions-train-30360", "mrqa_naturalquestions-train-58209", "mrqa_naturalquestions-train-67839", "mrqa_naturalquestions-train-53059", "mrqa_naturalquestions-train-19485", "mrqa_naturalquestions-train-45725", "mrqa_naturalquestions-train-21738", "mrqa_naturalquestions-train-3821", "mrqa_naturalquestions-train-61726", "mrqa_naturalquestions-train-44552", "mrqa_naturalquestions-train-17228", "mrqa_naturalquestions-train-57249", "mrqa_naturalquestions-train-60098"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Frederick in a duet with Teresa James", "norway", "Margaret Thatcher", "Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "2014", "The stability, security, and predictability of British law and government", "Minos and Kokalos", "1860", "norway", "norway", "Forbes", "The French", "John Carroll Lynch", "norway", "Orwell", "Kingdom of Bohemia", "Gregg Popovich", "not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive immune system", "Mexican", "a musician", "norway", "December 1, 1969", "american", "jK Rowling", "California State Automobile Association", "\"alone\"", "Cinderella", "delayed the sealing of the hatch", "due to a lack of understanding of the legal ramifications"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2685004623859113}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [0.19999999999999998, 1.0, 0.0, 1.0, 0.5263157894736842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 0.4, 0.0, 0.2424242424242424, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-16923", "mrqa_naturalquestions-train-71820", "mrqa_naturalquestions-train-32645", "mrqa_naturalquestions-train-11392", "mrqa_naturalquestions-train-68685", "mrqa_naturalquestions-train-13875", "mrqa_naturalquestions-train-83114", "mrqa_naturalquestions-train-7202", "mrqa_naturalquestions-train-76744", "mrqa_naturalquestions-train-68867", "mrqa_naturalquestions-train-37649", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-train-3911", "mrqa_naturalquestions-train-4154", "mrqa_naturalquestions-train-38208", "mrqa_naturalquestions-train-59079", "mrqa_naturalquestions-train-30102", "mrqa_naturalquestions-train-23483", "mrqa_naturalquestions-train-43185", "mrqa_naturalquestions-train-81528", "mrqa_naturalquestions-train-19578", "mrqa_naturalquestions-train-66189", "mrqa_naturalquestions-train-36117", "mrqa_naturalquestions-train-10687", "mrqa_naturalquestions-train-47646", "mrqa_naturalquestions-train-26304", "mrqa_naturalquestions-train-14634", "mrqa_naturalquestions-train-53107", "mrqa_naturalquestions-train-77952", "mrqa_naturalquestions-train-79743", "mrqa_naturalquestions-train-42005", "mrqa_naturalquestions-train-32607"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister ( 1982 film)", "president of Guggenheim Partners", "Jason Lee", "Napoleon's army", "maryland baking", "3.7 percent of the entire student population", "a waste of resources, but also because it generates redistributive pressures and subsequent distortions, drives people to poverty, constrains liquidity limiting labor mobility, and erodes self-esteem promoting social dislocation, unrest and conflict", "tracey Stubbs", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni", "discipline problems with the Flight Director's orders during their flight", "Many for each severed", "paddington Audiobook", "amyotrophic lateral sclerosis", "\"Odorama\"", "a pioneer in watch design, manufacturing and distribution", "October 17, 1938", "Torah or Bible", "the western coast of Italy", "the first and only U.S. born world grand prix champion", "brass band parades", "mid November", "winklevoss twins", "mohnbeugel", "the real-life story of Tim \"Ripper\" Owens, singer in a Judas Priest tribute band who was chosen to replace singer Rob Halford when he left the band", "Issaquah, Washington (a suburb of Seattle)", "The War of the Austrian Succession ( whose North American theater is known as King George's War) formally ended in 1748 with the signing of the Treaty of Aix-la-Chapelle", "pretends to be Rico's father for two - thousand dollars so he can get money to see Siena modeling in Peru", "Punk", "Fort Saint Anthony", "daguerreotypes", "a Mediterranean climate"], "metric_results": {"EM": 0.09375, "QA-F1": 0.20018037518037518}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.4444444444444445, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.28571428571428575, 0.0, 0.0, 0.2222222222222222, 0.33333333333333337, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1178", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2779", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_squad-validation-10168", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_triviaqa-validation-6913", "mrqa_squad-validation-2656"], "retrieved_ids": ["mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-21130", "mrqa_naturalquestions-train-86039", "mrqa_naturalquestions-train-79858", "mrqa_naturalquestions-train-32358", "mrqa_naturalquestions-train-41811", "mrqa_naturalquestions-train-32850", "mrqa_naturalquestions-train-68795", "mrqa_naturalquestions-train-31734", "mrqa_naturalquestions-train-45811", "mrqa_naturalquestions-train-26304", "mrqa_naturalquestions-train-29581", "mrqa_naturalquestions-train-65121", "mrqa_naturalquestions-train-74261", "mrqa_naturalquestions-train-70249", "mrqa_squad-validation-10322", "mrqa_naturalquestions-train-44294", "mrqa_naturalquestions-train-56376", "mrqa_naturalquestions-train-991", "mrqa_naturalquestions-train-9798", "mrqa_hotpotqa-validation-3870", "mrqa_naturalquestions-train-47979", "mrqa_naturalquestions-train-42403", "mrqa_naturalquestions-train-25636", "mrqa_naturalquestions-train-53482", "mrqa_naturalquestions-train-9878", "mrqa_naturalquestions-train-7589", "mrqa_naturalquestions-train-46196", "mrqa_naturalquestions-train-71565", "mrqa_naturalquestions-train-14365", "mrqa_naturalquestions-train-33810", "mrqa_naturalquestions-train-9779"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Daniel Hale Williams Preparatory School of Medicine", "maryland", "FX option", "electromagnetic waves", "Wahhabi/ Salafi", "to be good", "Dimensions in Time", "Surveyor 3", "the end of January 1981", "luteinizing hormone", "baptism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "trevor john Francis", "slowing the vehicle", "Cheyenne", "the appearance of fossils in sedimentary rocks", "Hanna-barbera", "Cortina d'Ampezzo", "to accomplish the objectives of the organization", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "maryjewski", "maryland", "Timo Hildebrand", "The public sector ( also called the state sector )", "1940", "lack of access to education", "The name Moloch results from a dysphemic vocalisation in the Second Temple period of a theonym based on the root mlk `` king ''", "sclera", "Uncle Fester", "homicidal sniper Bobby Thompson"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2851978291316527}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.888888888888889, 0.5882352941176471, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.1111111111111111, 0.0, 0.0, 0.28571428571428575]}}, "error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "retrieved_ids": ["mrqa_naturalquestions-train-6769", "mrqa_naturalquestions-train-63439", "mrqa_naturalquestions-train-1451", "mrqa_naturalquestions-train-68063", "mrqa_naturalquestions-train-34903", "mrqa_naturalquestions-train-84481", "mrqa_naturalquestions-train-42231", "mrqa_naturalquestions-train-68425", "mrqa_naturalquestions-train-49563", "mrqa_naturalquestions-train-6169", "mrqa_naturalquestions-train-14906", "mrqa_naturalquestions-train-57430", "mrqa_naturalquestions-train-3960", "mrqa_naturalquestions-train-43066", "mrqa_naturalquestions-train-19983", "mrqa_naturalquestions-train-87790", "mrqa_naturalquestions-train-9490", "mrqa_naturalquestions-train-84055", "mrqa_naturalquestions-train-25637", "mrqa_naturalquestions-train-36043", "mrqa_naturalquestions-train-2852", "mrqa_naturalquestions-train-48004", "mrqa_naturalquestions-train-30330", "mrqa_naturalquestions-train-44895", "mrqa_naturalquestions-train-56084", "mrqa_naturalquestions-train-45965", "mrqa_naturalquestions-train-84328", "mrqa_naturalquestions-train-72231", "mrqa_naturalquestions-train-36235", "mrqa_naturalquestions-train-67691", "mrqa_naturalquestions-train-43728", "mrqa_naturalquestions-train-70717"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["radioactive/genetically-altered spiders", "Part 2", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Proof", "Galileo Galilei", "a friend and publicist", "hana", "Nelson's Chamber", "1981", "Old Town Hall, Gateshead", "Motown, Philly soul, and Earth, Wind & Fire", "the female cervix, uterus and uterine tubes", "1898", "Heavyweight", "Payaya Indians", "steal the plans for the Death Star", "vito Corleone", "Curtiss JN-4", "antwerp", "chimpanzees", "March 15, 1945", "absolute temperature", "whistlebl-blowing", "Julius Robert Oppenheimer", "bicuspid", "Aegisthus", "25 November 2015", "tallahassee", "prefabricated housing projects", "the British Press", "WOTV"], "metric_results": {"EM": 0.15625, "QA-F1": 0.15625}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-39995", "mrqa_naturalquestions-train-80054", "mrqa_naturalquestions-train-64852", "mrqa_naturalquestions-train-18690", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-train-85620", "mrqa_naturalquestions-train-33257", "mrqa_naturalquestions-train-29354", "mrqa_naturalquestions-train-63972", "mrqa_naturalquestions-train-16910", "mrqa_naturalquestions-train-25841", "mrqa_squad-validation-3971", "mrqa_naturalquestions-train-18981", "mrqa_naturalquestions-train-61337", "mrqa_naturalquestions-train-10733", "mrqa_naturalquestions-train-83787", "mrqa_naturalquestions-train-28978", "mrqa_naturalquestions-train-54068", "mrqa_naturalquestions-train-43273", "mrqa_naturalquestions-train-32765", "mrqa_naturalquestions-train-10038", "mrqa_triviaqa-validation-2376", "mrqa_naturalquestions-train-2235", "mrqa_naturalquestions-train-75274", "mrqa_naturalquestions-train-39670", "mrqa_naturalquestions-train-21115", "mrqa_naturalquestions-train-41449", "mrqa_naturalquestions-train-55306", "mrqa_naturalquestions-train-86520", "mrqa_naturalquestions-train-41292", "mrqa_naturalquestions-train-66332", "mrqa_naturalquestions-train-67644"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["belle fierstein", "cheeses made with milk collected from a group of farms that are located within close proximity to where the cheese is produced", "Benedict", "on the lateral side of the tibia, with which it is connected above and below", "fergus Mor of Dalriada", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "faldo", "October 29, 1985", "Amway", "secondary school study", "Thomas Sowell", "the Speaker", "the Golden Globe Award for Best Supporting Actor", "Tanzania", "Chad", "florida", "an open work crown surmounted by a statue of fame", "to help him find Purgatory, the afterlife of monsters, and that Samuel has been working for him", "Fulham, Greater London, England", "French, English and Spanish", "the BBC", "U.S. Marshals", "What's Up (TV series)", "supply chain management", "Mars rover", "Stanislaw August Poniatowski", "polynomial algebra", "freda Davis", "The three wise monkeys", "sheepskin and Merino Wool products", "Honolulu"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18396739130434783}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.16666666666666666, 0.08695652173913043, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.21428571428571425, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-2287"], "retrieved_ids": ["mrqa_naturalquestions-train-37154", "mrqa_naturalquestions-train-41033", "mrqa_naturalquestions-train-37443", "mrqa_naturalquestions-train-17512", "mrqa_hotpotqa-validation-5682", "mrqa_naturalquestions-train-71223", "mrqa_naturalquestions-train-69155", "mrqa_naturalquestions-train-81958", "mrqa_naturalquestions-train-28922", "mrqa_naturalquestions-train-35017", "mrqa_naturalquestions-train-53274", "mrqa_naturalquestions-train-50526", "mrqa_naturalquestions-train-33865", "mrqa_naturalquestions-train-36046", "mrqa_naturalquestions-train-51847", "mrqa_naturalquestions-train-33783", "mrqa_naturalquestions-train-60585", "mrqa_naturalquestions-train-60968", "mrqa_naturalquestions-train-17475", "mrqa_naturalquestions-train-86333", "mrqa_naturalquestions-train-62044", "mrqa_naturalquestions-train-25702", "mrqa_naturalquestions-train-74173", "mrqa_naturalquestions-train-74048", "mrqa_naturalquestions-train-30427", "mrqa_naturalquestions-train-11571", "mrqa_naturalquestions-train-55282", "mrqa_naturalquestions-train-19384", "mrqa_naturalquestions-train-26005", "mrqa_naturalquestions-train-1291", "mrqa_naturalquestions-train-23688", "mrqa_naturalquestions-train-86341"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["dymock", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "stable, non-radioactive rubidium - 85", "Lula", "sovereign states", "Vice President of the United States", "\"Teach the Controversy\"", "Sam", "Australian", "36 months for men and 24 months for women", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Frank Wentz", "rock layers are dragged along, forming drag folds as slip occurs along the fault", "June 9, 2015", "V. Prakash Kumar", "Grace Nail Johnson", "Mick Jagger", "at least one prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "knowledgeable in that one area", "at which longitude is defined to be 0 \u00b0", "Cartoon Network", "Presiding Officer", "Phoenix Suns", "33", "vitifoliae", "District Superintendents of the Districts of the Annual Conference", "field hockey player Hannah Macleod", "William Hartnell's poor health"], "metric_results": {"EM": 0.15625, "QA-F1": 0.28629528985507247}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.08333333333333334, 0.0, 0.8, 0.0, 0.0, 1.0, 0.19999999999999998, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4347826086956522, 0.25, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-1895", "mrqa_squad-validation-5110", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_squad-validation-10074", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "retrieved_ids": ["mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-train-44523", "mrqa_naturalquestions-train-65166", "mrqa_triviaqa-validation-6800", "mrqa_naturalquestions-train-38366", "mrqa_naturalquestions-train-74104", "mrqa_naturalquestions-train-73355", "mrqa_naturalquestions-train-48292", "mrqa_naturalquestions-train-73399", "mrqa_naturalquestions-train-16692", "mrqa_naturalquestions-train-27042", "mrqa_naturalquestions-train-5590", "mrqa_naturalquestions-train-26869", "mrqa_naturalquestions-train-14024", "mrqa_naturalquestions-train-24304", "mrqa_naturalquestions-train-36039", "mrqa_naturalquestions-train-7397", "mrqa_naturalquestions-train-36733", "mrqa_naturalquestions-train-82748", "mrqa_naturalquestions-train-70252", "mrqa_naturalquestions-train-22388", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-train-32171", "mrqa_naturalquestions-train-26122", "mrqa_naturalquestions-train-84923", "mrqa_naturalquestions-train-58605", "mrqa_naturalquestions-train-60680", "mrqa_naturalquestions-train-36043", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-train-77088", "mrqa_naturalquestions-train-11898", "mrqa_naturalquestions-train-14866"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwandaandan genocide", "Co-teachers work in sync with one another to create a climate of learning", "500 metres", "Diondre Cole", "the entertainment division", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "12", "South Kensington Museum", "Edward I", "naval support", "olympic city of dundee", "the person compelled to pay for reformist programs", "Linlithgow Palace in Scotland", "\"Grindhouse\"", "olympic tennis", "digital transmission", "the Swiss cantons of Thurgau and St. Gallen", "lithium-ion battery factory", "821", "Sky channels", "gas", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposition", "the \"Queen of Cool\"", "President Wilson", "araspas", "the fifth season", "dors", "Hockey Club Davos", "Michael Crawford", "Aibak's successor and son - in - law Iltutmish"], "metric_results": {"EM": 0.09375, "QA-F1": 0.26413690476190477}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.25, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.6666666666666665, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.25]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-5036", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-4415", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-29454", "mrqa_naturalquestions-train-57622", "mrqa_naturalquestions-train-8115", "mrqa_naturalquestions-train-43368", "mrqa_naturalquestions-train-39520", "mrqa_naturalquestions-train-61185", "mrqa_naturalquestions-train-50525", "mrqa_naturalquestions-train-66458", "mrqa_naturalquestions-train-43342", "mrqa_naturalquestions-train-73424", "mrqa_naturalquestions-train-62062", "mrqa_naturalquestions-train-30910", "mrqa_naturalquestions-train-44530", "mrqa_naturalquestions-train-22032", "mrqa_naturalquestions-train-3690", "mrqa_naturalquestions-train-55828", "mrqa_naturalquestions-train-51733", "mrqa_naturalquestions-train-15479", "mrqa_naturalquestions-train-49041", "mrqa_naturalquestions-train-80574", "mrqa_naturalquestions-train-4045", "mrqa_naturalquestions-train-7812", "mrqa_naturalquestions-train-13920", "mrqa_naturalquestions-train-80951", "mrqa_naturalquestions-train-75814", "mrqa_naturalquestions-train-34585", "mrqa_naturalquestions-train-9943", "mrqa_naturalquestions-train-25995", "mrqa_naturalquestions-train-82763", "mrqa_naturalquestions-train-43139", "mrqa_naturalquestions-train-16413", "mrqa_naturalquestions-train-52270"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "John Meston", "aragon", "11.1", "trans-Pacific flight", "Sharman Joshi", "sit in a classroom and do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "Fermat's little theorem", "Ana", "Cherry Hill", "Happy Hour", "venus", "venus", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name.", "venus davis", "India", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "In 1889", "Nicki Minaj", "venzance", "French Huguenot ancestry", "Monaco Grand Prix", "venus", "Drawn Together", "William the Conqueror", "Ben Gurion International Airport", "two", "Corinthian and Saronic Gulfs", "taking blood samples from patients and correctly cataloging them for lab analysis", "Guinness World Records", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23388122294372293}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.08333333333333334, 0.0, 0.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "retrieved_ids": ["mrqa_naturalquestions-train-74515", "mrqa_naturalquestions-train-38205", "mrqa_naturalquestions-train-23214", "mrqa_naturalquestions-train-43133", "mrqa_naturalquestions-train-55672", "mrqa_naturalquestions-train-33241", "mrqa_naturalquestions-train-16265", "mrqa_naturalquestions-train-67301", "mrqa_naturalquestions-train-83446", "mrqa_naturalquestions-train-34964", "mrqa_naturalquestions-train-25702", "mrqa_naturalquestions-train-51631", "mrqa_naturalquestions-train-29733", "mrqa_naturalquestions-train-55416", "mrqa_naturalquestions-train-36373", "mrqa_naturalquestions-train-41450", "mrqa_hotpotqa-validation-1968", "mrqa_naturalquestions-train-30716", "mrqa_naturalquestions-train-38005", "mrqa_naturalquestions-train-13278", "mrqa_naturalquestions-train-73836", "mrqa_naturalquestions-train-18697", "mrqa_naturalquestions-train-55238", "mrqa_squad-validation-10042", "mrqa_naturalquestions-train-18507", "mrqa_naturalquestions-train-41859", "mrqa_naturalquestions-train-25005", "mrqa_triviaqa-validation-6125", "mrqa_naturalquestions-train-14989", "mrqa_naturalquestions-train-61948", "mrqa_naturalquestions-train-61066", "mrqa_naturalquestions-train-44678"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating Diesel engines", "David Feldman", "the Sackler Centre for arts education", "dante terrell Smith", "David Brewster", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "AS-204", "ribosomal RNA (rRNA) molecules", "kookaburra", "six-time Silver Slugger Award winner", "Shalita Grant as Sonja Percy, ATF Agent / NCIS Special Agent ( seasons 2 -- 4 ; recurring previously )", "'Friends\"", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) )", "mary marx", "a lunar flyby", "Lucius Cornelius Sulla Felix", "Super Bowl LII, following the 2017 season", "Golden Globe nomination", "Kenyan English", "the primacy of core Christian values such as love, patience, charity, and freedom, and reminded the citizens to trust God's word rather than violence to bring about necessary change", "karl marx", "Pantone Matching System (PMS)", "Firoz Shah Tughlaq", "My Love from the Star", "San Jose", "sea wasp", "the Hawaii Senate is the upper chamber of the Hawaii State Legislature.", "a \"teleforce\" weapon", "Thunderbird of Native American tradition", "the most giving Super Bowl ever", "29.7", "b.J. Hunnicutt"], "metric_results": {"EM": 0.125, "QA-F1": 0.2775793650793651}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.48484848484848486, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 0.0, 0.0, 0.4, 0.6666666666666666, 0.8, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_hotpotqa-validation-2795", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-935"], "retrieved_ids": ["mrqa_naturalquestions-train-54715", "mrqa_naturalquestions-train-81286", "mrqa_naturalquestions-train-12128", "mrqa_naturalquestions-train-45547", "mrqa_naturalquestions-train-16428", "mrqa_naturalquestions-train-62734", "mrqa_naturalquestions-train-66704", "mrqa_naturalquestions-train-87362", "mrqa_naturalquestions-train-55816", "mrqa_naturalquestions-train-1481", "mrqa_naturalquestions-train-36754", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-70302", "mrqa_naturalquestions-train-79325", "mrqa_naturalquestions-train-23010", "mrqa_naturalquestions-train-21258", "mrqa_naturalquestions-train-40122", "mrqa_squad-validation-9841", "mrqa_naturalquestions-train-24102", "mrqa_naturalquestions-train-61316", "mrqa_naturalquestions-train-2949", "mrqa_naturalquestions-train-27788", "mrqa_naturalquestions-train-5636", "mrqa_naturalquestions-train-57055", "mrqa_naturalquestions-train-16152", "mrqa_naturalquestions-train-13614", "mrqa_naturalquestions-train-74012", "mrqa_naturalquestions-train-39467", "mrqa_naturalquestions-train-35602", "mrqa_naturalquestions-train-10476", "mrqa_naturalquestions-train-85949", "mrqa_naturalquestions-train-12551"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["Czech word, robota, meaning `` forced labor ''", "dust jacket", "various registries", "17th Century", "Yazoo", "1894", "stars exceeding about eight times the mass of the sun", "(homo enim in hac vita)", "as defence of their North American colonies would no longer be an issue", "Willie Nelson", "ill. (chiefly col.)", "5 University of California campuses", "French", "Lewis", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "carbohydrates", "2001", "(i.e. exceeds any given number)", "30-minute", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "the 1950s", "Tallemaja \"pine tree Mary\"", "in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Orthodox Christians", "first edition", "640 \u00d7 1136 at 326 ppi", "fillies", "the Western Atlantic ctenophore Mnemiopsis leidyi", "\"Menace II Society\"", "backup quarterback", "trio"], "metric_results": {"EM": 0.0625, "QA-F1": 0.17206045816605997}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4210526315789474, 0.0, 1.0, 0.888888888888889, 0.0, 0.23529411764705882, 0.0, 0.0, 0.43750000000000006, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.22222222222222224, 0.2222222222222222]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-83141", "mrqa_naturalquestions-train-20270", "mrqa_triviaqa-validation-6119", "mrqa_naturalquestions-train-34914", "mrqa_naturalquestions-train-79790", "mrqa_naturalquestions-train-19028", "mrqa_naturalquestions-train-52224", "mrqa_naturalquestions-train-21068", "mrqa_naturalquestions-train-78287", "mrqa_hotpotqa-validation-3146", "mrqa_naturalquestions-train-20736", "mrqa_naturalquestions-train-4665", "mrqa_naturalquestions-train-40531", "mrqa_naturalquestions-train-33392", "mrqa_naturalquestions-train-43337", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-train-64601", "mrqa_naturalquestions-train-80058", "mrqa_naturalquestions-train-22338", "mrqa_naturalquestions-train-59045", "mrqa_naturalquestions-train-17725", "mrqa_naturalquestions-train-72486", "mrqa_naturalquestions-train-6288", "mrqa_squad-validation-3478", "mrqa_naturalquestions-train-11390", "mrqa_naturalquestions-train-84237", "mrqa_naturalquestions-train-85705", "mrqa_naturalquestions-train-23214", "mrqa_naturalquestions-train-37045", "mrqa_naturalquestions-train-41952", "mrqa_naturalquestions-train-25311", "mrqa_naturalquestions-train-29195"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "Mediterranean Shipping Company S.A.", "casino cheney", "Humpty Dumpty", "Liberals", "supporters of King Charles I against the supporters of the Long Parliament", "depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "rebecca seydoux", "jules verne", "Augustus Waters", "1619", "Hutchinson", "casino", "June 11, 1973", "Kenya", "a chronological collection of critical quotations about William Shakespeare and his works", "reptonbury", "an active supporter of the League of Nations", "Cargill", "AMC Entertainment Holdings, Inc.", "\"The Gang\", a group of debauched self- centered friends who run the Irish bar Paddy's Pub in South Philadelphia", "3 October 1990", "March 1, 2018", "heavy", "jeeves", "Martin Luther King III", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.1875, "QA-F1": 0.29544575216450214}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.16, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.1818181818181818, 0.0, 0.3333333333333333, 0.19047619047619047, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_naturalquestions-validation-3859", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929"], "retrieved_ids": ["mrqa_naturalquestions-train-56652", "mrqa_hotpotqa-validation-1566", "mrqa_naturalquestions-train-62757", "mrqa_naturalquestions-train-30913", "mrqa_naturalquestions-train-2603", "mrqa_naturalquestions-train-544", "mrqa_naturalquestions-train-33358", "mrqa_naturalquestions-train-25650", "mrqa_naturalquestions-train-39009", "mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-train-65913", "mrqa_naturalquestions-train-56733", "mrqa_naturalquestions-train-67404", "mrqa_naturalquestions-train-6285", "mrqa_naturalquestions-train-80908", "mrqa_naturalquestions-train-49391", "mrqa_naturalquestions-train-87141", "mrqa_naturalquestions-train-30738", "mrqa_naturalquestions-train-54960", "mrqa_naturalquestions-train-58790", "mrqa_naturalquestions-train-80194", "mrqa_naturalquestions-train-57621", "mrqa_naturalquestions-train-60016", "mrqa_naturalquestions-train-55244", "mrqa_naturalquestions-train-70645", "mrqa_naturalquestions-train-74949", "mrqa_naturalquestions-train-27718", "mrqa_naturalquestions-train-55361", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-train-63740", "mrqa_naturalquestions-train-10353", "mrqa_naturalquestions-train-64005"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "reigo luna", "red lead primer", "Dreamland", "Best Animated Feature", "European Union institutions", "(381.6 days)", "nine", "CAL IPSO", "benjamin franklin", "Ulbricht", "Ronald Ralph \"Ronnie\" Schell", "benjamin franklin", "Tata Consultancy Services Limited (TCS) is an Indian multinational information technology (IT) service, consulting and business solutions company Headquartered in Mumbai, Maharashtra.", "the east", "1939", "2017 / 18", "possibly 1707", "a golf course designed by William P. Bell", "southern Hemisphere", "Jane Seymour", "Incudomalleolar joint", "bobby riggs", "Democritus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Ted Ginn Jr."], "metric_results": {"EM": 0.1875, "QA-F1": 0.3174910159285159}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.4444444444444445, 0.0, 0.5, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0909090909090909, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_naturalquestions-validation-1617", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750"], "retrieved_ids": ["mrqa_naturalquestions-train-81959", "mrqa_naturalquestions-train-40166", "mrqa_naturalquestions-train-85273", "mrqa_naturalquestions-train-80166", "mrqa_naturalquestions-train-70143", "mrqa_naturalquestions-train-348", "mrqa_naturalquestions-train-30454", "mrqa_naturalquestions-train-6205", "mrqa_naturalquestions-train-48312", "mrqa_naturalquestions-train-87031", "mrqa_naturalquestions-train-46587", "mrqa_naturalquestions-train-66866", "mrqa_naturalquestions-train-69230", "mrqa_naturalquestions-train-44544", "mrqa_naturalquestions-train-55882", "mrqa_naturalquestions-train-44080", "mrqa_naturalquestions-train-84680", "mrqa_naturalquestions-train-33609", "mrqa_naturalquestions-train-52937", "mrqa_naturalquestions-train-68775", "mrqa_naturalquestions-train-18283", "mrqa_naturalquestions-train-37815", "mrqa_naturalquestions-train-23688", "mrqa_naturalquestions-train-4589", "mrqa_naturalquestions-train-612", "mrqa_naturalquestions-train-8329", "mrqa_naturalquestions-train-689", "mrqa_naturalquestions-train-63488", "mrqa_naturalquestions-train-45965", "mrqa_naturalquestions-train-14673", "mrqa_naturalquestions-train-12564", "mrqa_naturalquestions-train-82662"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "the reenergized electrons are taken by NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Abercrombie", "alchemy", "WBA title", "moluccas", "the first Saturday in May", "Albany ( in the Quarto version )", "multilateral negotiations", "1971", "the book \"The Lord of the Rings: The Return of the King\"", "Peyton Manning", "Selena Gomez", "learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "is the world's first multi-purpose, domed sports stadium", "primes of the form 2p + 1 with p prime", "letter series", "Fa Ze YouTubers", "the nine circles of Hell", "two Mongols and a Muslim", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "poor Clares", "CD Castell\u00f3n", "roughly between 1770 and 1848", "12\u20134", "having colloblasts, which are sticky and adhere to prey", "Patrick Wachsberger and Erik Feig of Summit Entertainment produced with Adam Shankman and Jennifer Gibgot of Offspring Entertainment", "STS-51-C.", "it will retreat to its den and winter will persist for six more weeks", "mitterr"], "metric_results": {"EM": 0.125, "QA-F1": 0.25047969265539616}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.06451612903225806, 0.0, 0.0, 0.4, 0.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0, 0.4, 0.2222222222222222, 1.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_squad-validation-10261", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-80171", "mrqa_naturalquestions-train-23688", "mrqa_hotpotqa-validation-5513", "mrqa_squad-validation-2659", "mrqa_naturalquestions-train-10698", "mrqa_naturalquestions-train-35118", "mrqa_naturalquestions-train-39799", "mrqa_naturalquestions-train-1624", "mrqa_naturalquestions-train-65664", "mrqa_naturalquestions-train-27894", "mrqa_naturalquestions-train-86251", "mrqa_naturalquestions-train-66904", "mrqa_naturalquestions-train-71145", "mrqa_naturalquestions-train-68335", "mrqa_naturalquestions-train-30187", "mrqa_naturalquestions-train-61196", "mrqa_naturalquestions-train-446", "mrqa_naturalquestions-train-1983", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-train-45488", "mrqa_naturalquestions-train-17753", "mrqa_naturalquestions-train-75212", "mrqa_naturalquestions-train-14055", "mrqa_naturalquestions-train-65050", "mrqa_naturalquestions-train-87059", "mrqa_naturalquestions-train-7091", "mrqa_naturalquestions-train-12209", "mrqa_naturalquestions-train-44365", "mrqa_naturalquestions-train-28896", "mrqa_naturalquestions-train-47889", "mrqa_naturalquestions-train-16445", "mrqa_naturalquestions-train-10895"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["olympic sports", "a slave", "over 50 million singles", "states'rights to expand slavery", "between 1923 and 1925", "Metropolitan Statistical Area", "January 19, 1962", "Frigate", "I Never Sang for My Father", "d'Hondt method", "American Buff geese", "the move from the manufacturing sector to the service sector", "Brisbane River Valley between Moreton Bay and the Great Dividing Range", "Peter Davison, Colin Baker and Sylvester McCoy", "February 14, 1859", "lower rates of health and social problems", "juveniles", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "A Chorus Line", "2,664 rooms", "a specific weak point on the inside of the chassis right beneath the volume buttons that allows it to bend very easily with pressure added in the right place", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning services, support services, property services, catering services, security services and facility management services", "Symphony No. 7 in F major, Opus 24", "dordogne", "1603", "ranked above the two personal physicians of the Emperor", "dance of the Sugar Plum Fairy", "Sought through prayer and meditation to improve our conscious contact with God as we understood Him, praying only for knowledge of His will for us and the power to carry that out", "wrigley field"], "metric_results": {"EM": 0.125, "QA-F1": 0.3395557567432567}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.25, 0.33333333333333337, 0.4615384615384615, 0.923076923076923, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.3636363636363636, 0.4444444444444445, 0.5454545454545454, 0.0, 1.0, 0.22222222222222224, 0.7499999999999999, 0.13636363636363635, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-499", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "retrieved_ids": ["mrqa_naturalquestions-train-45195", "mrqa_naturalquestions-train-78860", "mrqa_naturalquestions-train-38329", "mrqa_naturalquestions-train-5479", "mrqa_naturalquestions-train-8325", "mrqa_naturalquestions-train-32032", "mrqa_naturalquestions-train-7211", "mrqa_naturalquestions-train-39242", "mrqa_naturalquestions-train-39148", "mrqa_naturalquestions-train-34195", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-37323", "mrqa_naturalquestions-train-67303", "mrqa_naturalquestions-train-48185", "mrqa_naturalquestions-train-83229", "mrqa_naturalquestions-train-29474", "mrqa_naturalquestions-train-47092", "mrqa_naturalquestions-train-27718", "mrqa_naturalquestions-train-8374", "mrqa_naturalquestions-train-31881", "mrqa_naturalquestions-train-58272", "mrqa_naturalquestions-train-63615", "mrqa_naturalquestions-train-48067", "mrqa_naturalquestions-train-3456", "mrqa_naturalquestions-train-55917", "mrqa_naturalquestions-train-43434", "mrqa_naturalquestions-train-48647", "mrqa_naturalquestions-train-85736", "mrqa_naturalquestions-train-68779", "mrqa_naturalquestions-train-68711", "mrqa_naturalquestions-train-86204", "mrqa_naturalquestions-train-16239"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences", "north of the Lakes Region", "Magic formula investing", "true history of the Kelly Gang", "Waialua District of the island of O\u02bb ahu, City and County of Honolulu", "1910\u20131940", "non-teaching posts", "Salamanca", "jazz saxophonist", "tennis", "4,000", "Khagan", "wuthering Heights", "by river", "spice", "The Simpsons Spin-Off Showcase", "Raymond Unwin", "San Bernardino", "The city has an extensive neoclassical centre referred to as Tyneside Classical largely developed in the 1830s by Richard Grainger and John Dobson, and recently extensively restored.", "Albany High School for Educating People of Color", "charbagh", "a non-commissioned officer in the United States Army's premier special operations unit, the 1st Special Forces Operational Detachment- Delta (1SFOD-D) or \" Delta Force\"", "Shaw", "seek jury nullification", "Cee - Lo", "local taxes were funneled through the local parish to handle the needs of local government, such as roads and poor relief, in addition to the salary of the minister", "mammy two Shoes", "warship", "magnetism", "The Niger Delta is the delta of the Niger River sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria. It is typically considered to be located within nine coastal southern Nigerian states", "The economic impact of the former type of entrepreneurialism tends to be redistributive while the latter is expected to foster technological progress and thus have a more positive impact on economic growth", "January 11, 1755 or 1757"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2703217330055565}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.8, 0.0, 0.0, 0.33333333333333337, 0.5882352941176471, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.07692307692307693, 0.5454545454545454, 1.0, 0.0, 0.0, 0.8, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.5238095238095238, 0.0, 0.8000000000000002]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_squad-validation-3176", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_naturalquestions-validation-7801", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "retrieved_ids": ["mrqa_naturalquestions-train-85636", "mrqa_naturalquestions-train-1093", "mrqa_naturalquestions-train-12357", "mrqa_naturalquestions-train-16112", "mrqa_naturalquestions-train-1172", "mrqa_naturalquestions-train-71923", "mrqa_naturalquestions-train-71270", "mrqa_naturalquestions-train-37396", "mrqa_naturalquestions-train-26894", "mrqa_naturalquestions-train-88029", "mrqa_naturalquestions-train-46587", "mrqa_naturalquestions-train-77536", "mrqa_naturalquestions-train-29429", "mrqa_naturalquestions-train-66904", "mrqa_naturalquestions-train-13590", "mrqa_naturalquestions-train-73906", "mrqa_naturalquestions-train-28938", "mrqa_naturalquestions-train-77585", "mrqa_naturalquestions-train-70483", "mrqa_naturalquestions-train-2856", "mrqa_naturalquestions-train-3078", "mrqa_naturalquestions-train-24597", "mrqa_naturalquestions-train-42076", "mrqa_naturalquestions-train-52565", "mrqa_naturalquestions-train-38208", "mrqa_naturalquestions-train-1629", "mrqa_naturalquestions-train-43875", "mrqa_naturalquestions-train-62758", "mrqa_naturalquestions-train-22577", "mrqa_naturalquestions-train-67590", "mrqa_squad-validation-2053", "mrqa_naturalquestions-train-59962"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A.", "geldof", "blackberry and dewberry", "o Orion", "\" Big Mamie\"", "joseph smith", "the \"eternal outsider, the sardonic drifter\" someone who rebels against the social structure over the years", "a light sky-blue color caused by absorption in the red ( in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light)", "the peasants had to work for free on church land", "1963", "won his first NBA Championship", "the internal thylakoid system", "h Chatou", "Grand Annual Steeplechase at Warrnambool and the Australian International Airshow at Geelong", "has no veto", "the third season", "the weak and electromagnetic forces are expressions of a more fundamental electro weak interaction", "the availability of skilled tradespeople", "the ability of a material to resist wear and tear", "A simple iron boar crest adorns the top of this helmet", "the University of Northumbria at Newcastle in 1992", "curtin", "Lofton", "award a touchback on kickoffs at the 25 - yard line", "the Latin centum, which means 100", "7,000 out of 20,000 inhabitants", "lion, leopard, buffalo, rhinoceros, and elephant", "the Bible speaks", "ludwig smith", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "antwerp", "group"], "metric_results": {"EM": 0.0625, "QA-F1": 0.21298415126540127}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.26666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.375, 0.28571428571428575, 0.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.6666666666666667, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.6060606060606061, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-43966", "mrqa_naturalquestions-train-83637", "mrqa_squad-validation-2629", "mrqa_naturalquestions-train-33085", "mrqa_naturalquestions-train-6133", "mrqa_naturalquestions-train-80053", "mrqa_naturalquestions-train-78094", "mrqa_naturalquestions-train-34711", "mrqa_naturalquestions-train-38903", "mrqa_naturalquestions-validation-1294", "mrqa_naturalquestions-train-68593", "mrqa_naturalquestions-train-65467", "mrqa_naturalquestions-train-41557", "mrqa_squad-validation-3558", "mrqa_naturalquestions-train-86689", "mrqa_naturalquestions-train-31108", "mrqa_naturalquestions-train-81116", "mrqa_hotpotqa-validation-3080", "mrqa_naturalquestions-train-68341", "mrqa_naturalquestions-train-61173", "mrqa_naturalquestions-train-629", "mrqa_naturalquestions-train-44294", "mrqa_naturalquestions-train-11921", "mrqa_naturalquestions-train-55108", "mrqa_naturalquestions-train-24696", "mrqa_naturalquestions-train-36733", "mrqa_squad-validation-2884", "mrqa_naturalquestions-train-51726", "mrqa_naturalquestions-train-55802", "mrqa_naturalquestions-train-53795", "mrqa_naturalquestions-train-29371", "mrqa_naturalquestions-train-52519"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["The Turk", "Chris Weidman", "nullification", "Harishchandra", "poet", "Professor Eobard Thawne", "plum brandy", "a US$10 a week raise", "1875", "member states", "clarinets", "McKinsey's offices in Silicon Valley and India", "gypsia", "2000", "Crohn's disease or ulcerative colitis", "Ondemar Dias", "Raya Yarbrough", "No. 1 seed Virginia and No. 4 seed Arizona", "cruiserweight", "John D. Rockefeller", "Old Testament", "australia", "local talent", "Football League", "renoir", "mafic to felsic", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations", "john surrattsville", "1349", "australia", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships.", "Stan Butler"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2637581613105012}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 0.0, 0.4444444444444445, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.2758620689655173, 0.0, 0.0, 0.0, 0.2040816326530612, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_triviaqa-validation-7669", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_triviaqa-validation-2953", "mrqa_naturalquestions-validation-7821"], "retrieved_ids": ["mrqa_naturalquestions-train-75360", "mrqa_naturalquestions-train-70893", "mrqa_naturalquestions-train-41838", "mrqa_naturalquestions-train-9404", "mrqa_naturalquestions-train-84136", "mrqa_naturalquestions-train-39460", "mrqa_naturalquestions-train-13731", "mrqa_naturalquestions-train-46993", "mrqa_naturalquestions-train-77702", "mrqa_naturalquestions-train-42769", "mrqa_naturalquestions-train-77999", "mrqa_naturalquestions-train-43066", "mrqa_naturalquestions-train-51965", "mrqa_naturalquestions-train-559", "mrqa_naturalquestions-train-56700", "mrqa_naturalquestions-train-17510", "mrqa_naturalquestions-train-23419", "mrqa_naturalquestions-train-31446", "mrqa_naturalquestions-train-87808", "mrqa_naturalquestions-train-39073", "mrqa_naturalquestions-train-21622", "mrqa_naturalquestions-train-29917", "mrqa_naturalquestions-train-5859", "mrqa_naturalquestions-train-82704", "mrqa_naturalquestions-train-33644", "mrqa_naturalquestions-train-25288", "mrqa_naturalquestions-train-75579", "mrqa_naturalquestions-train-73658", "mrqa_naturalquestions-train-21180", "mrqa_naturalquestions-train-30589", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-20253"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["poisonous", "886 AD", "used to finance his own projects", "24 Hours of Le Mans", "Kinect", "Tokyo", "safety Darian Stewart", "parallelogram rule of vector addition", "lionel raulston", "pheasants", "startup neutron source", "canada", "the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "The Justified Ancients of Mu Mu", "National Basketball Development League", "gillingham", "St. Mary's County", "Emmanuel Sanders", "2,615 at the 2010 census", "Pyeongchang", "athlete", "a password recovery tool for Microsoft Windows", "Captain John Guidry", "Charles and Ray Eames", "Brazil", "Steve Redgrave", "the smallest subfield of a field F", "heartburn", "53", "photosynthesis"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23715423669467786}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.25, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.47058823529411764, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_squad-validation-7914", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_squad-validation-8873"], "retrieved_ids": ["mrqa_naturalquestions-train-76754", "mrqa_naturalquestions-train-22543", "mrqa_naturalquestions-train-19020", "mrqa_naturalquestions-train-27259", "mrqa_naturalquestions-train-28205", "mrqa_naturalquestions-train-55274", "mrqa_naturalquestions-train-1147", "mrqa_naturalquestions-train-34866", "mrqa_naturalquestions-train-64295", "mrqa_naturalquestions-train-21691", "mrqa_naturalquestions-train-39020", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-30968", "mrqa_naturalquestions-train-9779", "mrqa_naturalquestions-train-72312", "mrqa_naturalquestions-train-79317", "mrqa_naturalquestions-train-18085", "mrqa_naturalquestions-train-44892", "mrqa_naturalquestions-train-2983", "mrqa_naturalquestions-train-64762", "mrqa_naturalquestions-train-51674", "mrqa_naturalquestions-train-81014", "mrqa_naturalquestions-train-53324", "mrqa_naturalquestions-train-12687", "mrqa_naturalquestions-train-37133", "mrqa_naturalquestions-train-39182", "mrqa_naturalquestions-train-61924", "mrqa_naturalquestions-train-68260", "mrqa_naturalquestions-train-88165", "mrqa_naturalquestions-train-42086", "mrqa_naturalquestions-train-6818", "mrqa_naturalquestions-train-13920"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "a wife named Sally", "arpers Ferry", "Basil Fawlty", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "BBC UKTV", "mikael blomkvist", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "monza", "arthur", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "instructions", "rural-based National Party of Australia, and the environmentalist Australian Greens", "king nebuchadnezzar", "arctic fox", "reached $474 million, representing Kenya's largest source of foreign direct investment, and... bilateral trade", "spike", "off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Easy", "Sebastian Lund ( Rob Kerkovich ), a criminalist turned forensics agent and the team's newest member", "Indira Gandhi", "National Lottery", "Apollo", "katherine swynford", "in order to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1522287608225108}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.12121212121212122, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.9090909090909091]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-9602", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-train-14876", "mrqa_naturalquestions-train-24977", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-train-39648", "mrqa_naturalquestions-train-12593", "mrqa_naturalquestions-train-35951", "mrqa_naturalquestions-train-45", "mrqa_naturalquestions-train-11451", "mrqa_naturalquestions-train-37769", "mrqa_naturalquestions-train-75860", "mrqa_naturalquestions-train-19841", "mrqa_naturalquestions-train-78404", "mrqa_naturalquestions-train-21442", "mrqa_naturalquestions-validation-3677", "mrqa_triviaqa-validation-1130", "mrqa_naturalquestions-train-20900", "mrqa_naturalquestions-train-55444", "mrqa_naturalquestions-train-2066", "mrqa_naturalquestions-train-29474", "mrqa_naturalquestions-train-19157", "mrqa_naturalquestions-train-67684", "mrqa_naturalquestions-train-72458", "mrqa_naturalquestions-train-3371", "mrqa_naturalquestions-train-63943", "mrqa_naturalquestions-train-18272", "mrqa_naturalquestions-train-21906", "mrqa_naturalquestions-train-13557", "mrqa_naturalquestions-train-14118", "mrqa_naturalquestions-train-70645", "mrqa_naturalquestions-train-45833"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Peter", "spain", "polly", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid- Western Highway between Sydney and Adelaide", "androids", "16", "shopping mall", "to avoid responsibility for her actions", "DreamWorks Animation", "Ronald Reagan", "his own men", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "spain", "the RAF", "reduce growth in relatively poor countries but encourage growth", "He was the son of his predecessor, Ibrium", "surt daniel", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "polly", "surtree", "an estimated 390 billion individual trees divided into 16,000 species", "Washington Street between Boylston Street and Kneeland Street", "8 November 1978", "six", "polly", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "polly", "Paul the Apostle, later cited by John Smith in Jamestown, Virginia, and by Lenin during the Russian Revolution", "surtania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.03125, "QA-F1": 0.14807449494949493}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.0, 0.3636363636363636, 0.25, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.3636363636363636, 0.4, 1.0, 0.0, 0.0, 0.45000000000000007, 0.0, 0.2222222222222222, 0.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_hotpotqa-validation-1444", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "retrieved_ids": ["mrqa_naturalquestions-train-72924", "mrqa_naturalquestions-train-85086", "mrqa_naturalquestions-train-16813", "mrqa_naturalquestions-train-11916", "mrqa_naturalquestions-train-21686", "mrqa_naturalquestions-train-59988", "mrqa_naturalquestions-train-6578", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-8970", "mrqa_naturalquestions-train-72399", "mrqa_naturalquestions-train-75058", "mrqa_triviaqa-validation-5231", "mrqa_naturalquestions-train-83811", "mrqa_naturalquestions-train-19449", "mrqa_naturalquestions-train-86789", "mrqa_naturalquestions-train-52320", "mrqa_naturalquestions-train-25749", "mrqa_naturalquestions-train-77137", "mrqa_naturalquestions-train-37747", "mrqa_naturalquestions-train-34559", "mrqa_naturalquestions-train-42262", "mrqa_squad-validation-108", "mrqa_naturalquestions-train-3980", "mrqa_naturalquestions-train-12854", "mrqa_naturalquestions-train-34737", "mrqa_triviaqa-validation-7767", "mrqa_naturalquestions-train-42550", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-train-54960", "mrqa_naturalquestions-train-18047", "mrqa_naturalquestions-train-53810"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A high-gain S-band antenna", "the Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "Bavarian beer", "South Africa's northern Limpopo province", "Tom Bower", "kalium", "in order to halt it following brake failure", "T cell receptor", "generally paid on graduated scales", "non-GMO", "Heading Out to the Highway", "Moonraker", "Wii U", "Michael Oppenheimer", "England national team", "entitled institutionally and legally", "Space is the Place", "Convention", "5,922", "December 5, 1991", "\"Valerian and the City of a Thousand Planets\"", "Philadelphia 76ers", "the historical Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "Saturday", "Dealey Plaza", "Nairobi, Kenya", "the chalk ridge line west of the Needles breached", "Anno 2053"], "metric_results": {"EM": 0.21875, "QA-F1": 0.31765873015873014}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_naturalquestions-validation-5960"], "retrieved_ids": ["mrqa_naturalquestions-train-58177", "mrqa_naturalquestions-train-58310", "mrqa_naturalquestions-train-42274", "mrqa_naturalquestions-train-80348", "mrqa_naturalquestions-train-62717", "mrqa_naturalquestions-train-57349", "mrqa_naturalquestions-train-53174", "mrqa_naturalquestions-train-65664", "mrqa_naturalquestions-train-67404", "mrqa_naturalquestions-train-13752", "mrqa_naturalquestions-train-2319", "mrqa_naturalquestions-train-78075", "mrqa_naturalquestions-train-9747", "mrqa_naturalquestions-train-36736", "mrqa_squad-validation-9319", "mrqa_hotpotqa-validation-2957", "mrqa_naturalquestions-train-23", "mrqa_naturalquestions-train-76306", "mrqa_naturalquestions-train-52866", "mrqa_naturalquestions-train-46188", "mrqa_naturalquestions-train-30671", "mrqa_naturalquestions-train-31449", "mrqa_naturalquestions-train-31758", "mrqa_naturalquestions-train-43368", "mrqa_naturalquestions-train-44716", "mrqa_naturalquestions-train-35311", "mrqa_naturalquestions-train-80162", "mrqa_naturalquestions-train-47223", "mrqa_naturalquestions-train-9938", "mrqa_naturalquestions-train-66204", "mrqa_naturalquestions-train-3346", "mrqa_naturalquestions-train-20665"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "\"Boston Herald\" Rumor Clinic", "June 1967", "legprints in the Sand", "the twelfth most populous city in the United States", "115", "bridge", "is involved in oncogenesis, either by gene mutation, or chromosome translocation, or simply by over-expression. In every case, the result is a hyper - active kinase", "lower", "Bass", "Tevye", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "japan", "bridge", "Kunming", "Mumbai", "Broken Hill and Sydney", "2005", "buyers from all punishments and granted them salvation", "\"Smith and Jones\"", "may", "bridge", "the desire to prevent things that are indisputably bad", "1879", "She was the daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia.", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm", "bridge", "Datsun 810", "Bill Clinton", "Buskerud and Telemark"], "metric_results": {"EM": 0.125, "QA-F1": 0.23561962242060924}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16216216216216214, 0.0, 0.0, 0.0, 0.12121212121212123, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.375, 0.0, 0.10526315789473684, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-8907", "mrqa_naturalquestions-train-84647", "mrqa_naturalquestions-train-85006", "mrqa_naturalquestions-train-69289", "mrqa_naturalquestions-train-33039", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-train-22235", "mrqa_naturalquestions-train-68087", "mrqa_naturalquestions-train-30204", "mrqa_naturalquestions-train-25865", "mrqa_naturalquestions-train-77534", "mrqa_naturalquestions-train-64006", "mrqa_naturalquestions-train-13785", "mrqa_naturalquestions-train-83663", "mrqa_naturalquestions-train-44541", "mrqa_naturalquestions-train-53592", "mrqa_naturalquestions-train-65964", "mrqa_naturalquestions-train-45009", "mrqa_naturalquestions-train-80489", "mrqa_naturalquestions-train-87038", "mrqa_naturalquestions-train-32810", "mrqa_naturalquestions-train-56636", "mrqa_naturalquestions-train-54637", "mrqa_naturalquestions-train-44223", "mrqa_naturalquestions-train-87834", "mrqa_hotpotqa-validation-2673", "mrqa_naturalquestions-train-23561", "mrqa_naturalquestions-train-50459", "mrqa_naturalquestions-train-64716", "mrqa_naturalquestions-train-7908", "mrqa_naturalquestions-train-2323", "mrqa_naturalquestions-train-56108"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Speaker", "tinticelli", "Threatening government officials", "Veronica", "Victorian College of the Arts", "Germany", "red or pimento peppers", "0.2 inhabitants per square kilometre", "peter carrington v.C. (1955)", "France", "Ian Richard Kyle Paisley", "Bataan Death March", "euro", "september", "the United States", "1974", "1890", "Sam Bradford", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Maximus Decimus Meridius", "a revolution or orbital revolution", "Johnny Darrell", "a waxy substance called plaque builds up inside the carotid arteries", "margarine", "the relationship of the number to its corresponding value of Euler's totient function", "ear wax", "graphs are encoded as binary strings", "the second-busiest airport in the United States by passenger volume", "red", "Toyota Corona", "Kurt Vonnegut", "tinted hair"], "metric_results": {"EM": 0.09375, "QA-F1": 0.19137667887667886}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.14285714285714288, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_naturalquestions-validation-951", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1139", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "retrieved_ids": ["mrqa_naturalquestions-train-70273", "mrqa_naturalquestions-train-78075", "mrqa_naturalquestions-train-57277", "mrqa_naturalquestions-train-78304", "mrqa_naturalquestions-train-87228", "mrqa_naturalquestions-train-31976", "mrqa_naturalquestions-train-72600", "mrqa_naturalquestions-train-25890", "mrqa_naturalquestions-train-5416", "mrqa_naturalquestions-train-33010", "mrqa_naturalquestions-train-18386", "mrqa_naturalquestions-train-33599", "mrqa_naturalquestions-train-20312", "mrqa_naturalquestions-train-51273", "mrqa_naturalquestions-train-40188", "mrqa_naturalquestions-train-24947", "mrqa_naturalquestions-train-61057", "mrqa_naturalquestions-train-5136", "mrqa_naturalquestions-train-17277", "mrqa_naturalquestions-train-46077", "mrqa_naturalquestions-train-28467", "mrqa_naturalquestions-train-22788", "mrqa_naturalquestions-train-13731", "mrqa_naturalquestions-train-3776", "mrqa_naturalquestions-train-72700", "mrqa_naturalquestions-train-77563", "mrqa_naturalquestions-train-396", "mrqa_hotpotqa-validation-3497", "mrqa_naturalquestions-train-66035", "mrqa_naturalquestions-train-25904", "mrqa_naturalquestions-train-3369", "mrqa_naturalquestions-train-15206"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "supply and demand", "Dan Stevens", "the brain, muscles, and liver", "afghanistan", "Washington Redskins", "General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "William Howard Ashton", "national security, big oil companies and bribery and corruption at the highest levels of the government of the United States", "high and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "West Palm Beach", "Song Kang-ho", "changing display or audio settings quickly", "first Civil War", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "if the income share of the top 20 percent (the rich) increases, then GDP growth actually declines over the medium term, suggesting that the benefits don't trickle down", "Beautyy and the Beast", "South Africa", "Tyler \" Ty\" Mendoza on \"Switched at Birth\" and Simon Waverly on \"Satisfaction\"", "Alamo", "a seal illegally is broken", "the UMC", "Brian Liesegang", "Roger Allers and Rob Minkoff", "Papua New Guinea", "Alvin Simon Theodore Ross Bagdasarian David Seville", "National Association for the Advancement of Colored People", "1963\u20131989", "Titanic", "headmaster", "Darrin Stephens", "6500 - 1500 BC"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3493064309056956}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false], "QA-F1": [0.5, 0.28571428571428575, 1.0, 0.4, 0.0, 0.0, 0.25, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.4, 0.0, 0.4799999999999999, 0.08333333333333333, 0.6666666666666666, 1.0, 0.4, 0.0, 0.4, 1.0, 0.0, 0.0, 0.7499999999999999, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-5690", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "retrieved_ids": ["mrqa_naturalquestions-train-85827", "mrqa_triviaqa-validation-6901", "mrqa_naturalquestions-train-20421", "mrqa_triviaqa-validation-3760", "mrqa_naturalquestions-train-75408", "mrqa_naturalquestions-train-5475", "mrqa_naturalquestions-train-58010", "mrqa_naturalquestions-validation-8877", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-train-71962", "mrqa_naturalquestions-train-27936", "mrqa_naturalquestions-train-13561", "mrqa_naturalquestions-train-14055", "mrqa_naturalquestions-train-9265", "mrqa_naturalquestions-train-60166", "mrqa_naturalquestions-train-86753", "mrqa_naturalquestions-train-40065", "mrqa_naturalquestions-train-25554", "mrqa_naturalquestions-train-35387", "mrqa_naturalquestions-train-39376", "mrqa_naturalquestions-train-33865", "mrqa_triviaqa-validation-6259", "mrqa_naturalquestions-train-78463", "mrqa_naturalquestions-train-18148", "mrqa_naturalquestions-validation-10416", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-train-53565", "mrqa_naturalquestions-train-62337", "mrqa_naturalquestions-train-10681", "mrqa_naturalquestions-train-33775", "mrqa_naturalquestions-train-23560", "mrqa_naturalquestions-train-19886"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Kevin Costner", "planet Uranus", "president and Mrs. Ford's genealogy", "Cobham\u2013Edmonds thesis", "to remind the Doctor of his \"moral duty\"", "Best Male Pop Vocal Performance", "March 2012", "jazz", "Muhammad Ali", "Beyonc\u00e9 and Bruno Mars", "Menorca", "to civil disobedients", "Julius Caesar", "2", "1979", "a virtual reality simulator", "decision problem", "elizabeth lecouvreur", "the lungs", "Miasma theory", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges", "butterfly", "other states", "butterflyweed", "$12", "20 %", "Best Of The Pops", "to build a nationwide network in the UK", "roughly west", "Sudan"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3800770308123249}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.35294117647058826, 0.4444444444444445, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-4069", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-train-56376", "mrqa_naturalquestions-train-20436", "mrqa_naturalquestions-train-72473", "mrqa_naturalquestions-train-44547", "mrqa_naturalquestions-train-28642", "mrqa_naturalquestions-train-43139", "mrqa_naturalquestions-train-27417", "mrqa_naturalquestions-train-24912", "mrqa_naturalquestions-train-144", "mrqa_naturalquestions-train-33251", "mrqa_naturalquestions-train-18256", "mrqa_naturalquestions-train-59943", "mrqa_naturalquestions-train-27234", "mrqa_naturalquestions-train-25206", "mrqa_naturalquestions-train-51904", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-105", "mrqa_naturalquestions-train-57147", "mrqa_naturalquestions-train-80378", "mrqa_naturalquestions-train-4860", "mrqa_naturalquestions-train-51928", "mrqa_naturalquestions-train-47904", "mrqa_naturalquestions-train-21697", "mrqa_naturalquestions-train-63352", "mrqa_naturalquestions-train-19551", "mrqa_naturalquestions-train-36986", "mrqa_naturalquestions-train-81862", "mrqa_naturalquestions-train-68305", "mrqa_naturalquestions-train-53387", "mrqa_naturalquestions-train-62261", "mrqa_naturalquestions-train-1893"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["Southern Pacific", "back was severely wrenched and three of his ribs were broken", "2007", "San Luis Obispo", "mother-of-pearl", "February 20, 1978", "stomach", "Walter Mondale", "96", "in the basic curriculum -- the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "korea", "white, blue, pink, rainbow neon and glittering dotted lines", "on the alluvial plain", "at exactly 37 \u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "willy", "Rumplestiltskin", "Carlos Tevez", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "musical venues, including the Teatr Wielki, the Polish National Opera, the Chamber opera, the National Philharmonic Hall and the National Theatre", "riper grapes", "1991", "william herschelus", "7 January 1936", "lifetime protection, for themselves, spouses, and children under 16", "twenty- three", "Carl Sagan", "tax base dissipated, leading to problems with funding education, sanitation, and traffic control within the city limits. In addition, residents in unincorporated suburbs had difficulty obtaining municipal services, such as sewage and building code enforcement. In", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Poulsen", "allocution", "sour cream", "Boston, Massachusetts"], "metric_results": {"EM": 0.21875, "QA-F1": 0.31945014399300387}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.16666666666666669, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8750000000000001, 0.0, 0.21052631578947367, 0.0, 0.0, 0.0, 0.35294117647058826, 0.10526315789473684, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.15789473684210528, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-validation-969", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "retrieved_ids": ["mrqa_naturalquestions-train-33527", "mrqa_naturalquestions-train-58010", "mrqa_naturalquestions-train-34415", "mrqa_naturalquestions-train-41535", "mrqa_naturalquestions-train-30477", "mrqa_naturalquestions-train-37649", "mrqa_naturalquestions-train-55159", "mrqa_naturalquestions-validation-5184", "mrqa_naturalquestions-train-49601", "mrqa_naturalquestions-train-1467", "mrqa_naturalquestions-train-83040", "mrqa_naturalquestions-train-22466", "mrqa_hotpotqa-validation-5699", "mrqa_naturalquestions-train-16618", "mrqa_naturalquestions-train-69992", "mrqa_naturalquestions-train-73550", "mrqa_naturalquestions-train-68992", "mrqa_naturalquestions-train-10104", "mrqa_naturalquestions-train-20817", "mrqa_naturalquestions-train-19600", "mrqa_naturalquestions-train-4540", "mrqa_naturalquestions-train-67146", "mrqa_naturalquestions-train-51272", "mrqa_naturalquestions-train-27805", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-18576", "mrqa_naturalquestions-train-85823", "mrqa_naturalquestions-train-67181", "mrqa_naturalquestions-train-42705", "mrqa_naturalquestions-train-44389", "mrqa_naturalquestions-train-48955", "mrqa_naturalquestions-train-16881"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Emma Watson", "New England", "Bart Cummings", "dragon", "The primary catalyst for secession was slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "The History of Little Goody Two - Shoes", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "gathering money from the public", "Eden", "commissioned to purchase their required uniform items", "Don Jeffrey \" Jeff\" Meldrum", "937 total weeks", "Phil Archer", "He spoke French and English, and learned German and Spanish during his six years in Europe from 1823 to 1829", "The Paris Sisters", "the Suez Canal", "60", "journalist", "the fact that there is no revising chamber", "avatar", "the points of algebro-geometric objects, via the notion of the spectrum of a ring", "most of the items in the collection, unless those were newly accessioned into the collection", "does not satisfy the criteria for a medium of exchange", "strychnine", "Texas", "The early modern period began approximately in the early 16th century ; notable historical milestones included the European Renaissance, the Age of Discovery, and the Protestant Reformation.", "Lord's", "eddy Shah", "Den Favreau", "in sequence with each heartbeat"], "metric_results": {"EM": 0.25, "QA-F1": 0.35698844793314105}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.07692307692307693, 0.0, 1.0, 0.0606060606060606, 0.47058823529411764, 0.0, 0.22222222222222224, 0.4444444444444445, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.2857142857142857, 1.0, 0.6, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.3846153846153846, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "retrieved_ids": ["mrqa_naturalquestions-train-57320", "mrqa_naturalquestions-train-38634", "mrqa_naturalquestions-train-44661", "mrqa_hotpotqa-validation-5117", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-train-21289", "mrqa_naturalquestions-train-49648", "mrqa_naturalquestions-train-10399", "mrqa_naturalquestions-train-80053", "mrqa_naturalquestions-train-39827", "mrqa_naturalquestions-train-2397", "mrqa_naturalquestions-train-56639", "mrqa_naturalquestions-train-45965", "mrqa_naturalquestions-train-42399", "mrqa_naturalquestions-train-31928", "mrqa_naturalquestions-train-9244", "mrqa_naturalquestions-train-33266", "mrqa_naturalquestions-train-5543", "mrqa_naturalquestions-train-44117", "mrqa_naturalquestions-train-22286", "mrqa_naturalquestions-train-9618", "mrqa_naturalquestions-train-82406", "mrqa_naturalquestions-train-48803", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-train-57055", "mrqa_naturalquestions-train-33677", "mrqa_naturalquestions-train-45478", "mrqa_naturalquestions-train-79359", "mrqa_naturalquestions-train-37625", "mrqa_naturalquestions-train-41488", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-train-82921"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Republic of China", "Dan Conner", "West Berlin", "President John F. Kennedy", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "kuskus", "the 1980s", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA", "New York", "a man who makes potions in a traveling show", "2003", "mule, deer, white-tailed deer, moose, elk and caribou", "NTV", "the second Sunday of March", "relative units of force and mass then are fixed", "woman", "two", "August 10, 1933", "the Golden Gate, the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "Sochi, Russia", "already-wealthy individuals or entities", "B. Traven", "Finding Nemo", "unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "debt", "bumblebee", "264,152", "Princeton, New Jersey", "the German Empire", "high pressure or an electric current"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3549973719091366}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.9230769230769231, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.3636363636363636, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_squad-validation-8070", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_naturalquestions-validation-3108", "mrqa_triviaqa-validation-4024", "mrqa_squad-validation-7547", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-78716", "mrqa_naturalquestions-train-21061", "mrqa_naturalquestions-train-66189", "mrqa_naturalquestions-train-17000", "mrqa_naturalquestions-train-43494", "mrqa_naturalquestions-train-28113", "mrqa_naturalquestions-train-37787", "mrqa_naturalquestions-train-2444", "mrqa_naturalquestions-train-1773", "mrqa_hotpotqa-validation-4463", "mrqa_naturalquestions-train-6209", "mrqa_naturalquestions-train-50539", "mrqa_naturalquestions-train-38028", "mrqa_naturalquestions-train-29325", "mrqa_naturalquestions-train-35644", "mrqa_naturalquestions-train-20821", "mrqa_naturalquestions-train-15232", "mrqa_naturalquestions-train-85317", "mrqa_naturalquestions-train-30083", "mrqa_naturalquestions-train-59497", "mrqa_naturalquestions-train-12597", "mrqa_naturalquestions-train-63152", "mrqa_naturalquestions-train-57056", "mrqa_naturalquestions-train-37579", "mrqa_naturalquestions-train-47902", "mrqa_naturalquestions-train-48486", "mrqa_naturalquestions-train-38221", "mrqa_naturalquestions-train-81131", "mrqa_naturalquestions-train-63266", "mrqa_naturalquestions-train-13683", "mrqa_naturalquestions-train-60285", "mrqa_naturalquestions-train-53230"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "on the road back to Samarkand", "warrens", "Isabella (Belle) Baumfree", "warren palace", "14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "he broadened the foundations of the Reformation placing them on prophetic faith", "Bacon", "Charlton Heston", "anti-inflammatory molecules, such as cortisol and catecholamines", "warren", "Eagles' starting quarterback Donovan McNabb", "one of the uses of money", "may have the force of law, if based on the authority derived from statute or the Constitution itself", "son et lumi\u00e8re", "when the Mongols placed the Uighurs of the Kingdom of Qocho over the Koreans at the court the Korean King objected", "Sochi, Russia", "right and left", "from the Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "How soon after the cabin fire incident", "new Zealand", "shorthand typist", "30", "the Secret Intelligence Service", "100 billion", "kai su, teknon", "photolysis", "4.7 / 5.5 - inch", "Queen City", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"], "metric_results": {"EM": 0.3125, "QA-F1": 0.39961747491638794}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.375, 0.6666666666666666, 1.0, 0.0, 0.15384615384615383, 0.0, 0.28571428571428575, 0.13333333333333333, 0.125, 0.0, 0.09523809523809522, 1.0, 0.0, 0.17391304347826084, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.16]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_triviaqa-validation-1571", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "retrieved_ids": ["mrqa_naturalquestions-train-11236", "mrqa_naturalquestions-train-78177", "mrqa_naturalquestions-train-42262", "mrqa_naturalquestions-train-44158", "mrqa_naturalquestions-train-38221", "mrqa_naturalquestions-train-30265", "mrqa_naturalquestions-train-53209", "mrqa_naturalquestions-train-49780", "mrqa_naturalquestions-train-15087", "mrqa_naturalquestions-train-60382", "mrqa_naturalquestions-train-78133", "mrqa_naturalquestions-train-5519", "mrqa_naturalquestions-train-65964", "mrqa_naturalquestions-train-45569", "mrqa_naturalquestions-train-73044", "mrqa_naturalquestions-train-61658", "mrqa_naturalquestions-train-52095", "mrqa_naturalquestions-train-87808", "mrqa_naturalquestions-train-47106", "mrqa_naturalquestions-train-25433", "mrqa_naturalquestions-train-70701", "mrqa_naturalquestions-train-26143", "mrqa_naturalquestions-train-81426", "mrqa_naturalquestions-train-74439", "mrqa_naturalquestions-train-87911", "mrqa_naturalquestions-train-39419", "mrqa_naturalquestions-train-10083", "mrqa_naturalquestions-train-38308", "mrqa_naturalquestions-train-85648", "mrqa_naturalquestions-train-84384", "mrqa_naturalquestions-train-17917", "mrqa_naturalquestions-train-83023"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["all war", "Kishore Kumar", "Gaels", "Crash", "d\u00edsir", "cave lion", "Russian film industry", "flooding and sedimentation", "in the Washington metropolitan area", "newly formed vesicles", "User State Migration Tool", "Ordos City China Science Flying Universe Science and Technology Co.", "frisbee", "PPG Paints Arena, Pittsburgh, Pennsylvania", "Jewry Wall Museum", "Section 30", "Ron Messick as William B. White as Henry Fussy, a boy of about Fern's age", "mid-1988", "quasars", "Monsoon", "Romansh", "Tudor king", "5AA", "Q Branch (or later Q Division)", "the Philippians", "what builds nations'wealth", "Margaret Pellegrini", "Whitney Houston", "Nebula Award", "conservative", "David", "Elvis Presley"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26783685064935064}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.0, 0.0, 0.25, 0.16666666666666669, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.4, 1.0, 0.2857142857142857, 1.0, 0.5714285714285715, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_squad-validation-9355", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "retrieved_ids": ["mrqa_naturalquestions-train-2939", "mrqa_naturalquestions-train-50602", "mrqa_naturalquestions-train-1307", "mrqa_naturalquestions-train-42530", "mrqa_naturalquestions-train-16818", "mrqa_naturalquestions-train-70911", "mrqa_naturalquestions-train-56680", "mrqa_naturalquestions-train-64934", "mrqa_naturalquestions-train-41264", "mrqa_naturalquestions-train-21683", "mrqa_naturalquestions-train-21483", "mrqa_naturalquestions-train-26076", "mrqa_naturalquestions-train-66915", "mrqa_naturalquestions-train-48926", "mrqa_naturalquestions-train-62566", "mrqa_naturalquestions-train-68703", "mrqa_naturalquestions-train-5136", "mrqa_naturalquestions-train-31209", "mrqa_naturalquestions-train-73196", "mrqa_naturalquestions-train-74279", "mrqa_naturalquestions-train-24991", "mrqa_naturalquestions-train-18344", "mrqa_naturalquestions-train-75406", "mrqa_naturalquestions-train-17355", "mrqa_naturalquestions-train-25838", "mrqa_naturalquestions-train-52915", "mrqa_naturalquestions-train-22514", "mrqa_naturalquestions-train-51562", "mrqa_naturalquestions-train-40103", "mrqa_naturalquestions-train-7540", "mrqa_naturalquestions-train-16730", "mrqa_naturalquestions-train-53107"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["Daryl Hannah", "pasta ( usually cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "bmingham", "independence from the Duke of Savoy through an alliance between the city-state of Geneva and the Swiss Confederation", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "physicians, lawyers, engineers, and accountants", "a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "a large Danish shipping company that operates passenger and freight services across northern Europe", "centre-back", "jonathan", "in the duodenum", "often bonded to glucose to form the disaccharide sucrose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "portrait artist", "Extverted Feeling ( Fi ) and Extroverted Intuition ( Ne )", "Thursday", "yellow", "medication choice, dose, route, frequency, and duration of therapy", "jupiter", "navigator and expedition leader", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "george kirby keger", "The Private Education Student Financial Assistance", "bow", "to raise money to rebuild St. Peter's Basilica in Rome", "to its colonies", "two forces, one pointing north, and one pointing east", "Bills", "Qualcomm Stadium", "hierarchy theorems"], "metric_results": {"EM": 0.15625, "QA-F1": 0.33985860442661914}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.7272727272727273, 0.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.2857142857142857, 0.14285714285714288, 1.0, 0.0, 0.0, 0.0, 0.375, 0.5, 0.6666666666666666, 0.6, 0.0, 0.0, 0.8888888888888888, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-40261", "mrqa_naturalquestions-train-73886", "mrqa_naturalquestions-train-70201", "mrqa_naturalquestions-train-85311", "mrqa_naturalquestions-train-76856", "mrqa_naturalquestions-train-82997", "mrqa_naturalquestions-train-31482", "mrqa_naturalquestions-train-28167", "mrqa_naturalquestions-train-9234", "mrqa_naturalquestions-train-20647", "mrqa_naturalquestions-train-19065", "mrqa_naturalquestions-train-81601", "mrqa_naturalquestions-train-30389", "mrqa_naturalquestions-train-74050", "mrqa_naturalquestions-train-84879", "mrqa_naturalquestions-train-22506", "mrqa_naturalquestions-train-26781", "mrqa_naturalquestions-train-12075", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-train-69565", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-train-36608", "mrqa_naturalquestions-train-5160", "mrqa_naturalquestions-train-29462", "mrqa_naturalquestions-train-30095", "mrqa_naturalquestions-train-83023", "mrqa_naturalquestions-train-14215", "mrqa_squad-validation-1688", "mrqa_naturalquestions-train-61935", "mrqa_naturalquestions-train-21738", "mrqa_naturalquestions-train-56528", "mrqa_naturalquestions-validation-8444"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["4 million cubic feet (110,000 m3)", "4 Points - F, H, V, W and Y", "switzerland", "joseph smith", "French", "a \"homeward bounder\"", "labels", "immunodeficiency can either be the result of a genetic disease such as severe combined immunomeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication.", "opera", "natural-ing Ingredients- only personal care products", "spider", "Rigoletto", "Russia is the largest country on Earth by land area, distances within Russia can be very long, and air travel is frequently needed for the President to travel across the country as well as internationally.", "third-most abundant element in the universe", "furniture", "216", "brazil", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers, Sir Henry Cheere", "Algernod Lanier Washington", "the Outfield", "Croatia", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong", "railway locomotives", "eddie", "third quarter ( also known as last quarter )", "army game", "chemists Glenn T. Seaborg", "Kentucky, Virginia, and Tennessee", "many areas of technology incidental to rocketry and manned spaceflight", "eve", "237 square miles", "magi"], "metric_results": {"EM": 0.03125, "QA-F1": 0.16632841294412504}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.6, 0.0, 0.0, 0.11764705882352941, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.4210526315789474, 0.6666666666666666, 0.5, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_naturalquestions-validation-7641", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "retrieved_ids": ["mrqa_naturalquestions-train-85795", "mrqa_naturalquestions-train-60702", "mrqa_naturalquestions-train-25132", "mrqa_naturalquestions-train-72312", "mrqa_naturalquestions-train-31942", "mrqa_naturalquestions-train-8633", "mrqa_naturalquestions-train-37732", "mrqa_naturalquestions-train-86581", "mrqa_naturalquestions-train-67106", "mrqa_naturalquestions-train-6209", "mrqa_naturalquestions-train-62660", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-train-11898", "mrqa_naturalquestions-train-11314", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-train-36679", "mrqa_naturalquestions-train-3737", "mrqa_naturalquestions-train-52754", "mrqa_naturalquestions-train-13388", "mrqa_naturalquestions-train-50337", "mrqa_naturalquestions-train-45881", "mrqa_naturalquestions-train-11403", "mrqa_naturalquestions-train-9346", "mrqa_naturalquestions-train-65346", "mrqa_naturalquestions-train-41668", "mrqa_naturalquestions-train-83485", "mrqa_naturalquestions-train-53255", "mrqa_naturalquestions-train-69688", "mrqa_naturalquestions-train-33243", "mrqa_naturalquestions-train-38744", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-train-26459"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["only in Wakanda and the Savage Land", "income depending on experience. Teachers with more experience and higher education earn more than those with a standard bachelor's degree and certificate. Salaries vary greatly depending on state, relative cost of living, and grade taught.", "2003", "gymnastics", "sweden", "campaign setting", "2003", "867 feet", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\u00f7", "Ian McDiarmid as Chancellor Palpatine / Darth Sidious : A former senator of Naboo and secretly a Sith Lord who has recently been elected Supreme Chancellor of the Galactic Republic", "second most commonly named \"dream college\"", "ikki tikki tembo-no sa rembo- chari bari ruchi-pip peri pembo", "physicians and other healthcare professionals", "increased patient health outcomes and decreased costs to the health care system", "treble clef", "Gabriel Alberto Azucena (born September 23, 1988)", "12534 New Delhi - Kanpur Shatabdi Express - maximum speed is between 120 and 130 km / h", "between the Piazza di Spagna at the base and Piazzo Trinit\u00e0 dei Monti", "May 18, 2010", "an English campaigner for the suffragette movement, a prominent left communist and, later, an activist in the cause of anti-fascism", "euro Union", "philosophical advocate and practitioner of the scientific method during the scientific revolution", "meyer", "Ministry of Corporate Affairs", "Irish", "Numa Pompilius is believed to have built this temple along with the original Regia and House of the Vestal Virgins in its original form", "richard burton", "energy-storage molecules ATP and NADPH", "astrology", "Sanctifying Grace", "Bach included several verses as chorales in his Cantatas and based chorale cantatas entirely on them, namely Christ lag in Todes Banden, BWV 4, as early as possibly 1707,"], "metric_results": {"EM": 0.125, "QA-F1": 0.2455627462544871}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.16666666666666666, 0.25641025641025644, 0.19999999999999998, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.2758620689655173, 0.6666666666666666, 0.0, 0.0, 0.39999999999999997, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12903225806451613]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "retrieved_ids": ["mrqa_naturalquestions-train-62127", "mrqa_naturalquestions-train-49261", "mrqa_naturalquestions-train-19346", "mrqa_naturalquestions-train-39376", "mrqa_naturalquestions-train-22926", "mrqa_squad-validation-5178", "mrqa_naturalquestions-train-4123", "mrqa_naturalquestions-train-35470", "mrqa_squad-validation-3887", "mrqa_naturalquestions-train-5734", "mrqa_naturalquestions-train-66062", "mrqa_naturalquestions-train-13512", "mrqa_naturalquestions-train-32880", "mrqa_naturalquestions-train-29474", "mrqa_naturalquestions-train-79224", "mrqa_naturalquestions-train-87031", "mrqa_squad-validation-2751", "mrqa_hotpotqa-validation-1414", "mrqa_naturalquestions-train-56208", "mrqa_triviaqa-validation-7269", "mrqa_naturalquestions-train-84113", "mrqa_hotpotqa-validation-2993", "mrqa_naturalquestions-train-52181", "mrqa_naturalquestions-train-13555", "mrqa_naturalquestions-train-85388", "mrqa_naturalquestions-train-57619", "mrqa_naturalquestions-train-41043", "mrqa_triviaqa-validation-1473", "mrqa_naturalquestions-train-63308", "mrqa_hotpotqa-validation-5802", "mrqa_naturalquestions-train-61066", "mrqa_naturalquestions-train-38551"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["well-boiled", "Cleveland Browns", "perique", "cut off close by the hip", "death penalty", "a stout man with a \" double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "agulhas", "rich Fisher king is the son of the king", "Mangal Pandey of the 34th BNI", "Colonia Agrippina", "Cartwright", "four of the 50 states", "curling", "first group to win the competition", "Pebble Beach", "It is an American football coach who last served as offensive line coach for the Los Angeles Rams of the National Football League (NFL)", "French", "Gareth", "\"LOVE Radio\" which featured a limited selection of music genres", "Boston Red Sox", "the court", "Tony Manero", "David Dobkin", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Cashin' In", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Operation Neptune", "peninsular mainland"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3362205502830503}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.07142857142857142, 0.0, 1.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.07142857142857142, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_squad-validation-5852", "mrqa_hotpotqa-validation-5149", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-63281", "mrqa_naturalquestions-train-76175", "mrqa_naturalquestions-train-15366", "mrqa_naturalquestions-train-76764", "mrqa_naturalquestions-train-22390", "mrqa_naturalquestions-train-33056", "mrqa_naturalquestions-train-45457", "mrqa_naturalquestions-train-33296", "mrqa_naturalquestions-train-11458", "mrqa_naturalquestions-train-81419", "mrqa_naturalquestions-train-31348", "mrqa_naturalquestions-train-8685", "mrqa_naturalquestions-train-66471", "mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-train-10043", "mrqa_naturalquestions-train-59713", "mrqa_hotpotqa-validation-1116", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-73728", "mrqa_naturalquestions-train-4869", "mrqa_naturalquestions-train-84982", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-train-7375", "mrqa_naturalquestions-train-61876", "mrqa_naturalquestions-train-2603", "mrqa_naturalquestions-train-81033", "mrqa_naturalquestions-train-1310", "mrqa_naturalquestions-train-72040", "mrqa_naturalquestions-train-61324", "mrqa_naturalquestions-train-7510", "mrqa_naturalquestions-train-7812", "mrqa_triviaqa-validation-3074"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["bat-and-ball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "m.E.N. Arena", "youngest person to become a national transgender figure", "the development of safety lamps, Stephenson's Rocket, Lord Armstrong's artillery, Be-Ro flour, Joseph Swan's electric light bulbs, and Charles Parsons' invention of the steam turbine", "used their knowledge of Native American languages as a basis to transmit coded messages", "Einstein", "the absenceistence of the ultraviolet catastrophe", "Hong Kong First Division League", "Isabella", "Elizabeth Weber", "an earlier Funcom game, \"The Secret World\"", "hundreds of television and radio channels", "Waiting for Guffman", "1978", "on The Watermark business park next to the MetroCentre in Gateshead", "apple", "partial funding", "pale lager", "inefficient", "Chu'Tsai", "Liz", "least onerous", "como", "White", "Walmart de M\u00e9xico y Centroam\u00e9rica in Mexico and Central America", "passion fruit is one of the few antioxidant fruits that not only improves your physical health, but also your mental health.", "Natya Shastra", "the sand grains cause a scrubbing noise as they rub against each other when walked on", "men's golf", "parts of the air in the vessel were converted into the classical element fire", "Vienna"], "metric_results": {"EM": 0.34375, "QA-F1": 0.39890571353933424}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.08, 0.06666666666666667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1818181818181818, 0.2857142857142857, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.18181818181818182, 1.0, 0.0689655172413793, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_squad-validation-5157", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10489", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_squad-validation-3913", "mrqa_naturalquestions-validation-3616", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523"], "retrieved_ids": ["mrqa_naturalquestions-train-2331", "mrqa_naturalquestions-train-46166", "mrqa_naturalquestions-train-33428", "mrqa_naturalquestions-train-14164", "mrqa_naturalquestions-train-29474", "mrqa_naturalquestions-train-68513", "mrqa_naturalquestions-train-40335", "mrqa_naturalquestions-train-54518", "mrqa_naturalquestions-train-47601", "mrqa_naturalquestions-train-54891", "mrqa_naturalquestions-train-84384", "mrqa_naturalquestions-train-33911", "mrqa_naturalquestions-train-80808", "mrqa_naturalquestions-train-44151", "mrqa_naturalquestions-train-71057", "mrqa_naturalquestions-train-28710", "mrqa_naturalquestions-train-16982", "mrqa_naturalquestions-train-85086", "mrqa_naturalquestions-train-5988", "mrqa_naturalquestions-train-75687", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-train-56722", "mrqa_naturalquestions-train-8424", "mrqa_naturalquestions-train-65613", "mrqa_naturalquestions-train-33405", "mrqa_naturalquestions-train-27206", "mrqa_naturalquestions-train-58765", "mrqa_naturalquestions-train-56859", "mrqa_naturalquestions-train-63699", "mrqa_naturalquestions-train-5737", "mrqa_naturalquestions-train-63929"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["George Bush", "horse racing", "Burnley and the New Zealand national team", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "big - name lawyers", "Milk Barn Animation", "when they enter the army during initial entry training", "one of The Canterbury Tales by Geoffrey Chaucer", "quit", "leeds", "tree - topper or treetopper", "around 74 per cent", "Heathrow Express", "Neighbourhoods are often social communities with considerable face-to-face interaction among members.", "Sylvia F. Porter", "monophyletic", "insecticide toxicology", "a liturgical setting of the Lord's Prayer and as a means of examining candidates on specific catechism questions", "a pH indicator, a color marker, and a dye", "20 kPa", "63,182,000", "John Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic (mentioned above) would not hold as stated", "Campbellsville University", "daredevil", "Yaz in the musical romance drama film \" Across the Universe\" (2007)", "vegan", "work in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "XXXTentacion", "downward pressure on wages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2704861111111111}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.19999999999999998, 1.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5555555555555556, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_squad-validation-7182"], "retrieved_ids": ["mrqa_naturalquestions-train-27369", "mrqa_naturalquestions-train-74415", "mrqa_naturalquestions-train-73056", "mrqa_naturalquestions-train-45471", "mrqa_hotpotqa-validation-2340", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-train-42830", "mrqa_naturalquestions-train-18206", "mrqa_naturalquestions-train-49333", "mrqa_naturalquestions-train-38117", "mrqa_naturalquestions-train-17458", "mrqa_naturalquestions-train-79688", "mrqa_triviaqa-validation-3603", "mrqa_naturalquestions-train-26798", "mrqa_naturalquestions-train-87823", "mrqa_naturalquestions-train-53850", "mrqa_naturalquestions-train-40156", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-train-64933", "mrqa_naturalquestions-validation-2501", "mrqa_naturalquestions-train-13106", "mrqa_naturalquestions-train-51674", "mrqa_naturalquestions-train-902", "mrqa_naturalquestions-train-74979", "mrqa_naturalquestions-train-17192", "mrqa_naturalquestions-train-32850", "mrqa_naturalquestions-train-53824", "mrqa_naturalquestions-train-74708", "mrqa_naturalquestions-train-61743", "mrqa_naturalquestions-train-13602", "mrqa_naturalquestions-train-21764", "mrqa_naturalquestions-train-36951"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["Barbara Ryan Coleman ( formerly Stenbeck, St. Clair, Dixon, Munson and Montgomery) is a fictional character from the American CBS soap opera \"As the World Turns\"", "Good Kid, M.A.D City", "el Capitan", "Interventive treatment", "3", "Bishop Lloyd Christ Wicke", "james char Charles", "Goku becomes the first Saiyan in a thousand years to transform into a fabled Super Saiyan", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "education, Scientific and Cultural Organization", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "1964 Republican National Convention in San Francisco, California", "every good work designed to attract God's favor is a sin. All humans are sinners by nature, he explained, and God's grace ( which Cannot be earned) alone can make them just.", "annuity", "Anakin Skywalker", "Frank Theodore `` Ted '' Levine", "We will commit sins while we are here, for this life is not a place where justice resides", "a semi-independent State of Vietnam, within the French Union", "war with the United States", "half steamed milk and half foam", "morgan", "Hecuba", "to be memorised by the people themselves", "Wylie Draper", "a political role for Islam", "the university's off-campus rental policies", "Manley MacDonald", "New England Patriots", "several existing conditions such as war, famine, and weather"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26663380302493206}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.08333333333333334, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.06451612903225806, 0.16666666666666669, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.2666666666666667, 0.4, 1.0, 0.4, 0.0, 0.6153846153846153]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-5996", "mrqa_naturalquestions-train-20436", "mrqa_naturalquestions-train-56792", "mrqa_naturalquestions-train-66130", "mrqa_hotpotqa-validation-3911", "mrqa_naturalquestions-train-33893", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-train-5043", "mrqa_naturalquestions-train-3077", "mrqa_naturalquestions-train-11351", "mrqa_squad-validation-2751", "mrqa_naturalquestions-train-42655", "mrqa_naturalquestions-train-7385", "mrqa_naturalquestions-train-5662", "mrqa_naturalquestions-train-43356", "mrqa_naturalquestions-train-78786", "mrqa_naturalquestions-train-71635", "mrqa_naturalquestions-train-27879", "mrqa_naturalquestions-train-82514", "mrqa_naturalquestions-train-51844", "mrqa_naturalquestions-train-7951", "mrqa_naturalquestions-train-84486", "mrqa_naturalquestions-train-36465", "mrqa_naturalquestions-train-31194", "mrqa_naturalquestions-train-68221", "mrqa_naturalquestions-train-24079", "mrqa_naturalquestions-train-40122", "mrqa_naturalquestions-train-52472", "mrqa_naturalquestions-train-35744", "mrqa_naturalquestions-train-66530", "mrqa_naturalquestions-train-82480", "mrqa_naturalquestions-train-84423"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["suitable for use on rough terrain", "AOL", "Genghis Khan", "\"Losing My Religion\" is a song by the American alternative rock band R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "It then served as the capital of the Ostrogothic Kingdom until it was re-conquered in 540 by the Eastern Roman (Byzantine) Empire.", "12", "fear of public speaking", "1937", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock. He had to work at various electrical repair jobs and even as a ditch digger for $2 per day.", "Marxist and a Leninist", "Gregor Mendel, who is known as the `` father of modern genetics ''", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "3,600", "Central Avenue", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "approximately 44 hectares", "georgia", "horakhty", "Lawton Mainor Chiles Jr.", "In the episode `` Kobol's Last Gleaming ''", "Wisconsin v. Yoder", "imperialism", "bread, chapati, mahamri, boiled sweet potatoes or yams", "energy", "georgia", "Ruth Elizabeth \"Bette\" Davis", "uranium", "7 December 2004"], "metric_results": {"EM": 0.0625, "QA-F1": 0.19049468801950903}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.42857142857142855, 0.18181818181818182, 0.12903225806451615, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 0.34782608695652173, 0.0, 0.33333333333333337, 0.1111111111111111, 0.09523809523809525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 1.0, 0.10526315789473685, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_squad-validation-6257", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1023", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "retrieved_ids": ["mrqa_naturalquestions-train-30692", "mrqa_naturalquestions-train-6799", "mrqa_naturalquestions-train-61443", "mrqa_naturalquestions-train-62322", "mrqa_triviaqa-validation-751", "mrqa_naturalquestions-train-13294", "mrqa_naturalquestions-train-62717", "mrqa_naturalquestions-train-56236", "mrqa_naturalquestions-train-13830", "mrqa_naturalquestions-train-38716", "mrqa_naturalquestions-train-29289", "mrqa_naturalquestions-train-65417", "mrqa_naturalquestions-train-81843", "mrqa_naturalquestions-train-75548", "mrqa_naturalquestions-train-78270", "mrqa_naturalquestions-train-86314", "mrqa_naturalquestions-train-16029", "mrqa_naturalquestions-train-64016", "mrqa_naturalquestions-train-75030", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-train-81122", "mrqa_naturalquestions-train-26395", "mrqa_naturalquestions-train-30515", "mrqa_naturalquestions-train-24847", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-train-27879", "mrqa_naturalquestions-train-45499", "mrqa_naturalquestions-train-8706", "mrqa_naturalquestions-train-56363", "mrqa_naturalquestions-train-66004", "mrqa_naturalquestions-train-3439"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.145, "QA-F1": 0.2611989562781557}, "overall_error_number": 1368, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.70125, "QA-F1": 0.7588869606522904}, "final_upstream_test": {"EM": 0.741, "QA-F1": 0.8456201293360103}}}