{"model_update_steps": 1945, "method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=0212_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=0212_ckpts/', replay_candidate_size=8, replay_frequency=1, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "philanthropy", "The Darling Buds of May", "Virginia Wade", "The Dallas Lovers' Song", "to the anterolateral corner of the spinal cord", "1966", "for scientific observation", "product or policy that is open and honest", "The Stock Market crash in New York", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "naba", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs", "acmthompson", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21802213309566248}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.23529411764705885, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "retrieved_ids": ["mrqa_naturalquestions-train-83867", "mrqa_naturalquestions-train-68220", "mrqa_naturalquestions-train-15967", "mrqa_naturalquestions-train-23912", "mrqa_naturalquestions-train-73282", "mrqa_naturalquestions-train-23560", "mrqa_naturalquestions-train-28130", "mrqa_naturalquestions-train-1342", "mrqa_naturalquestions-train-75750", "mrqa_naturalquestions-train-48923", "mrqa_naturalquestions-train-21346", "mrqa_naturalquestions-train-30761", "mrqa_naturalquestions-train-44295", "mrqa_naturalquestions-train-48692", "mrqa_naturalquestions-train-55786", "mrqa_naturalquestions-train-83787", "mrqa_naturalquestions-train-68183", "mrqa_naturalquestions-train-10932", "mrqa_naturalquestions-train-18148", "mrqa_naturalquestions-train-57937", "mrqa_naturalquestions-train-42959", "mrqa_naturalquestions-train-5035", "mrqa_naturalquestions-train-30217", "mrqa_naturalquestions-train-38668", "mrqa_naturalquestions-train-32959", "mrqa_naturalquestions-train-6174", "mrqa_naturalquestions-train-21157", "mrqa_naturalquestions-train-87210", "mrqa_naturalquestions-train-57622", "mrqa_naturalquestions-train-4041", "mrqa_naturalquestions-train-75255", "mrqa_naturalquestions-train-50733"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "Wales", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "technological advances", "a student in the second year", "Masaharu Iwata", "Terry Reid", "non- peer- reviewed sources", "Elgar", "North America", "Andr\u00e9 3000", "Commander", "Akhenaten", "President Theodore Roosevelt", "the fourth season", "Denver Broncos", "the Western Bloc ( the United States, its NATO allies and others )", "the 1970s", "Georges Bizet", "Matt Winer", "1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.0, "QA-F1": 0.1622526778776779}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.0, 0.4, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.4, 0.0, 0.0, 0.3636363636363636, 0.0, 0.6666666666666666, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_hotpotqa-validation-3242", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "retrieved_ids": ["mrqa_naturalquestions-train-20665", "mrqa_naturalquestions-train-46712", "mrqa_naturalquestions-train-56996", "mrqa_naturalquestions-train-46980", "mrqa_naturalquestions-train-22618", "mrqa_naturalquestions-train-43362", "mrqa_naturalquestions-train-29596", "mrqa_naturalquestions-train-40394", "mrqa_naturalquestions-train-68806", "mrqa_naturalquestions-train-72644", "mrqa_naturalquestions-train-8880", "mrqa_naturalquestions-train-4999", "mrqa_naturalquestions-train-21351", "mrqa_naturalquestions-train-54614", "mrqa_naturalquestions-train-83787", "mrqa_naturalquestions-train-36700", "mrqa_naturalquestions-train-28136", "mrqa_naturalquestions-train-41648", "mrqa_naturalquestions-train-16311", "mrqa_squad-validation-5622", "mrqa_naturalquestions-train-83495", "mrqa_naturalquestions-train-59850", "mrqa_naturalquestions-train-8706", "mrqa_naturalquestions-train-62491", "mrqa_naturalquestions-train-21686", "mrqa_naturalquestions-train-1467", "mrqa_naturalquestions-train-12593", "mrqa_naturalquestions-train-23177", "mrqa_naturalquestions-train-7944", "mrqa_naturalquestions-train-58522", "mrqa_naturalquestions-train-24496", "mrqa_naturalquestions-train-27062"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "baijan", "parallelogram", "27 July and 7 August 2022", "New York", "is getting a remake", "2006", "Least of the Great Powers", "the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "Glacier Mints", "baze", "death mask", "video film", "Overtime", "Sir Henry Cole", "at high oxygen concentrations", "The Young Ones", "Clyde Barrow", "the Democratic Unionist Party", "23 July 1989", "many educational institutions especially within the US", "gurus often exercising a great deal of control over the lives of their disciples", "control purposes", "bamboula", "Callability", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "hijab, or al - khimar", "proteins", "cholecystectomy", "berenice Abbott"], "metric_results": {"EM": 0.0625, "QA-F1": 0.14172077922077922}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.5, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.4, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-4310", "mrqa_naturalquestions-train-87063", "mrqa_naturalquestions-train-7666", "mrqa_naturalquestions-train-78504", "mrqa_naturalquestions-train-37489", "mrqa_naturalquestions-train-10104", "mrqa_naturalquestions-train-75160", "mrqa_naturalquestions-train-7341", "mrqa_naturalquestions-train-26211", "mrqa_naturalquestions-train-49589", "mrqa_naturalquestions-train-51631", "mrqa_naturalquestions-train-20514", "mrqa_naturalquestions-train-38716", "mrqa_naturalquestions-train-52701", "mrqa_naturalquestions-train-77011", "mrqa_naturalquestions-train-79042", "mrqa_naturalquestions-train-1470", "mrqa_naturalquestions-train-80210", "mrqa_naturalquestions-train-35605", "mrqa_naturalquestions-train-34136", "mrqa_naturalquestions-train-60790", "mrqa_naturalquestions-train-55672", "mrqa_naturalquestions-train-22546", "mrqa_naturalquestions-train-34071", "mrqa_naturalquestions-train-68279", "mrqa_naturalquestions-train-7728", "mrqa_naturalquestions-train-7167", "mrqa_naturalquestions-train-43071", "mrqa_naturalquestions-train-58312", "mrqa_naturalquestions-train-47871", "mrqa_naturalquestions-train-19536", "mrqa_naturalquestions-train-12427"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Arnhem", "December 9, 2016", "NASA discontinued the manned Block I program", "British progressive folk-rock band Gryphon", "1898", "month", "Moses", "at elevation 2 meters above sea level", "Tetanus", "bounding the time or space used by the algorithm", "splash", "Lieutenant Commander Steve McGarrett", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "1934", "the Reverse - Flash", "All Souls'Day", "nigeria", "baku", "Catholics", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001", "the Dutch Cape Colony in South Africa", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "steam turbine", "Splodgenessabounds"], "metric_results": {"EM": 0.21875, "QA-F1": 0.31941964285714286}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_squad-validation-3467"], "retrieved_ids": ["mrqa_naturalquestions-train-36730", "mrqa_naturalquestions-train-43426", "mrqa_naturalquestions-train-84896", "mrqa_naturalquestions-train-43342", "mrqa_naturalquestions-train-35810", "mrqa_naturalquestions-train-8979", "mrqa_naturalquestions-train-80606", "mrqa_naturalquestions-train-67990", "mrqa_naturalquestions-train-29731", "mrqa_naturalquestions-train-57687", "mrqa_naturalquestions-train-38366", "mrqa_naturalquestions-train-14215", "mrqa_naturalquestions-train-48733", "mrqa_naturalquestions-train-66379", "mrqa_naturalquestions-train-3178", "mrqa_naturalquestions-train-38156", "mrqa_naturalquestions-train-22301", "mrqa_naturalquestions-train-32880", "mrqa_naturalquestions-train-43056", "mrqa_naturalquestions-train-39809", "mrqa_naturalquestions-train-24688", "mrqa_naturalquestions-train-17210", "mrqa_naturalquestions-train-52874", "mrqa_naturalquestions-train-43090", "mrqa_naturalquestions-train-78734", "mrqa_naturalquestions-train-11402", "mrqa_naturalquestions-train-61924", "mrqa_naturalquestions-train-35644", "mrqa_naturalquestions-train-36054", "mrqa_naturalquestions-train-51147", "mrqa_naturalquestions-train-51474", "mrqa_naturalquestions-train-629"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["otranto", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric with a colorful swirled pattern of curved shapes", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "7 : 25 a.m. HST", "swanee", "rapeseed", "to start fires, hunt, and bury their dead", "Indian astronaut Kalpana Chawla", "Parietal cells", "placental", "Ready to Die", "june", "imperial rule", "1840", "make a defiant speech, or a speech explaining their actions", "George Sylvester Viereck", "kinks", "by using net wealth (adding up assets and subtracting debts )", "entropy increases", "my mind is averse to wedlock", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.0625, "QA-F1": 0.13322192513368986}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "retrieved_ids": ["mrqa_naturalquestions-train-45778", "mrqa_naturalquestions-train-5508", "mrqa_naturalquestions-train-8950", "mrqa_naturalquestions-train-130", "mrqa_naturalquestions-train-45764", "mrqa_naturalquestions-train-19752", "mrqa_naturalquestions-train-86116", "mrqa_naturalquestions-train-7757", "mrqa_naturalquestions-train-15748", "mrqa_naturalquestions-train-26014", "mrqa_naturalquestions-train-21331", "mrqa_naturalquestions-train-43247", "mrqa_naturalquestions-train-56827", "mrqa_naturalquestions-train-45773", "mrqa_naturalquestions-train-12207", "mrqa_naturalquestions-train-23005", "mrqa_naturalquestions-train-16152", "mrqa_naturalquestions-train-24119", "mrqa_naturalquestions-train-52639", "mrqa_naturalquestions-train-87863", "mrqa_naturalquestions-train-83124", "mrqa_naturalquestions-train-39349", "mrqa_naturalquestions-train-27343", "mrqa_naturalquestions-train-86619", "mrqa_naturalquestions-train-66215", "mrqa_naturalquestions-train-63175", "mrqa_naturalquestions-train-56276", "mrqa_naturalquestions-train-49413", "mrqa_naturalquestions-train-47000", "mrqa_naturalquestions-train-25749", "mrqa_naturalquestions-train-5976", "mrqa_naturalquestions-train-5887"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "The glass chimney needs a `` throat '', or slight constriction, to create the proper draft for complete combustion of the fuel", "Barack Hussein Obama II", "1998", "n Carolina", "the east coast of the island of Menorca", "90-60's", "independent schools", "dolph Camilli", "times sign", "Best Supporting Actress", "Emily Blunt", "1960", "HTTP Secure", "late summer", "d. Eisenhower National Airport", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "june", "dads bury themselves in mud", "Butterfly Conservation", "universal", "The anterior interventricular branch of left coronary artery", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf", "Indian club ATK", "land that a nation has conquered and expanded", "near Grande Comore, Comoros Islands", "\u20b9 \u200d5L '' ( for `` rupees 5 lakhs '' )", "Norwegian", "the human respiratory system"], "metric_results": {"EM": 0.0625, "QA-F1": 0.16117311507936508}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.28571428571428575, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-validation-5582", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-23912", "mrqa_naturalquestions-train-81528", "mrqa_naturalquestions-train-44530", "mrqa_naturalquestions-train-7855", "mrqa_naturalquestions-train-65761", "mrqa_naturalquestions-train-48900", "mrqa_naturalquestions-train-9210", "mrqa_naturalquestions-train-20331", "mrqa_naturalquestions-train-33877", "mrqa_naturalquestions-train-12824", "mrqa_naturalquestions-train-40725", "mrqa_naturalquestions-train-27782", "mrqa_naturalquestions-train-31356", "mrqa_naturalquestions-train-80855", "mrqa_naturalquestions-train-52076", "mrqa_naturalquestions-train-79743", "mrqa_naturalquestions-train-57392", "mrqa_naturalquestions-train-80803", "mrqa_naturalquestions-train-87252", "mrqa_naturalquestions-train-36269", "mrqa_naturalquestions-train-36054", "mrqa_naturalquestions-train-5572", "mrqa_naturalquestions-train-51456", "mrqa_naturalquestions-train-63337", "mrqa_naturalquestions-train-65340", "mrqa_naturalquestions-train-19048", "mrqa_naturalquestions-train-4931", "mrqa_naturalquestions-train-22241", "mrqa_naturalquestions-train-72320", "mrqa_naturalquestions-train-41872", "mrqa_naturalquestions-train-46890", "mrqa_naturalquestions-train-65218"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few", "The U.S. Army Chaplain insignia", "Kairi", "the suburbs, who wanted more services and more control over the central city", "Ray Milland", "near the Black Sea", "the last book accepted into the Christian biblical canon", "Bruno Mars", "buttermens per meter", "for \u201cacts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger.\u201d", "the most popular show at the time", "post\u2013World War II", "work oxen for haulage", "1998", "a priest", "third most abundant chemical element in the universe", "2001", "a family member", "long-term environmental changes", "William Powell Lear", "the unbalanced centripetal force felt by any object", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "Abraham Gottlob Werner", "braves", "present-day Charleston", "a \"quiescent\" stance", "Adam Karpel", "Heinz Guderian"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3958085317460317}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "retrieved_ids": ["mrqa_naturalquestions-train-79385", "mrqa_naturalquestions-train-11314", "mrqa_naturalquestions-train-44640", "mrqa_naturalquestions-train-69879", "mrqa_naturalquestions-train-57704", "mrqa_naturalquestions-train-17802", "mrqa_naturalquestions-train-3080", "mrqa_naturalquestions-train-79470", "mrqa_naturalquestions-train-67317", "mrqa_naturalquestions-train-64934", "mrqa_naturalquestions-train-11177", "mrqa_naturalquestions-train-24746", "mrqa_naturalquestions-train-46476", "mrqa_naturalquestions-train-57056", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-train-36346", "mrqa_naturalquestions-train-19358", "mrqa_naturalquestions-train-13039", "mrqa_naturalquestions-train-33141", "mrqa_naturalquestions-train-3621", "mrqa_naturalquestions-train-984", "mrqa_naturalquestions-train-53073", "mrqa_naturalquestions-train-18047", "mrqa_naturalquestions-train-54350", "mrqa_naturalquestions-train-6355", "mrqa_triviaqa-validation-2722", "mrqa_naturalquestions-train-75241", "mrqa_naturalquestions-train-37667", "mrqa_naturalquestions-train-6446", "mrqa_naturalquestions-train-33594", "mrqa_naturalquestions-train-5428", "mrqa_naturalquestions-train-87803"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computational complexity theory", "Giuseppe Antonio 'Nino' Farina", "46", "6.4 nanometers apart", "the eighth and eleventh episodes of the season", "Carl Michael Edwards II", "over 400", "endocrine", "enkuklios paideia or `` education in a circle ''", "the Bowland Fells", "Richard, Duke of Gloucester", "St. Louis County", "1868", "2018", "george warnock", "law firm", "Pottawatomie County", "orangutan", "Newton's Law of Gravitation", "The church tower", "bromley- by-Bow", "Toronto", "wales", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to the south", "the Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "psychoanalytic", "Quentin Coldwater", "acidic bogs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23234126984126985}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.4, 0.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.2666666666666667, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "retrieved_ids": ["mrqa_naturalquestions-train-9929", "mrqa_naturalquestions-train-56000", "mrqa_naturalquestions-train-43221", "mrqa_naturalquestions-train-16689", "mrqa_naturalquestions-train-35789", "mrqa_naturalquestions-train-33890", "mrqa_naturalquestions-train-22585", "mrqa_naturalquestions-train-36146", "mrqa_naturalquestions-train-24456", "mrqa_naturalquestions-train-57931", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-54251", "mrqa_naturalquestions-train-35082", "mrqa_naturalquestions-train-84389", "mrqa_naturalquestions-train-64832", "mrqa_naturalquestions-train-36696", "mrqa_naturalquestions-train-40860", "mrqa_naturalquestions-train-6834", "mrqa_naturalquestions-train-53906", "mrqa_naturalquestions-train-19252", "mrqa_naturalquestions-train-87874", "mrqa_naturalquestions-train-14252", "mrqa_naturalquestions-train-29987", "mrqa_naturalquestions-train-12315", "mrqa_naturalquestions-train-8238", "mrqa_naturalquestions-train-21451", "mrqa_naturalquestions-train-38395", "mrqa_naturalquestions-train-52378", "mrqa_naturalquestions-train-32458", "mrqa_naturalquestions-train-11626", "mrqa_naturalquestions-train-55656", "mrqa_naturalquestions-validation-10680"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentinian", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "the right pan", "photosynthesis", "a wide range of society figures of the period", "ward Hotel", "The Daily Stormer", "triplet", "water", "The president", "the citizens", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy film", "Brown Square Station", "acting", "C. W. Grafton", "liquid crystal", "British", "The Apple iPod+HP", "My Sassy Girl", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "General Hospital", "phlogiston", "pedagogy", "vaskania", "ATP", "soils", "drug dealer", "medium and heavy- Duty diesel trucks", "testes"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3808416465926546}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.1, 0.0, 0.0, 0.125, 0.0, 1.0, 0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.18181818181818182, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.3333333333333333, 0.1290322580645161, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-5940", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-44080", "mrqa_naturalquestions-train-21014", "mrqa_naturalquestions-train-79634", "mrqa_naturalquestions-train-49961", "mrqa_naturalquestions-train-13602", "mrqa_naturalquestions-train-6358", "mrqa_naturalquestions-train-38005", "mrqa_naturalquestions-train-23035", "mrqa_naturalquestions-train-87973", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-43142", "mrqa_naturalquestions-train-41867", "mrqa_naturalquestions-train-39013", "mrqa_naturalquestions-train-10949", "mrqa_naturalquestions-train-48449", "mrqa_naturalquestions-train-87911", "mrqa_naturalquestions-train-53051", "mrqa_naturalquestions-train-87077", "mrqa_naturalquestions-train-71706", "mrqa_naturalquestions-train-67441", "mrqa_naturalquestions-train-23908", "mrqa_naturalquestions-train-68058", "mrqa_naturalquestions-train-32742", "mrqa_naturalquestions-train-44617", "mrqa_naturalquestions-train-78357", "mrqa_naturalquestions-train-18061", "mrqa_naturalquestions-train-72831", "mrqa_naturalquestions-train-77715", "mrqa_naturalquestions-train-70025", "mrqa_naturalquestions-train-42067", "mrqa_naturalquestions-train-43621", "mrqa_naturalquestions-train-29654"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["Alexander Pope", "Walter Reed Army Hospital in Washington, D.C. from 1942 to 1945", "three", "Summerlin, Clark County, Nevada", "status line", "Lutz Pfannenstiel", "Anthony Bellew", "a bridge over the Merderet in the fictional town of Ramelle", "a Dubliner tried to kill Benito Mussolini", "cake", "menhirs", "Victoria", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "1947 Kon-Tiki expedition", "Britney Spears Live from Las Vegas", "a virtual museum dedicated to Valentino, in New York, December 7, 2011", "Ronnie Hillman", "all-encompassing definition of the term", "\" When Body Snatchers Targeted Mormons and the Miraculous Dream That Saved a Slain\"", "more than 60 %", "Eagle Ridge Mall", "Pel\u00e9", "Victory Garden", "Monastir / Tunisia / Africa", "the classical element fire", "James Thurston Nabors", "at least 18 or 21 years old ( or have a legal guardian present )", "Ann Ward", "a Belgian\u2013 French explorer, spiritualist, Buddhist, anarchist and writer", "Jamestown", "Claude Monet", "tree growth stages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30220959595959596}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.18181818181818182, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_squad-validation-4506"], "retrieved_ids": ["mrqa_naturalquestions-train-75274", "mrqa_naturalquestions-train-54595", "mrqa_naturalquestions-train-56427", "mrqa_naturalquestions-train-62981", "mrqa_naturalquestions-train-44067", "mrqa_naturalquestions-train-40881", "mrqa_naturalquestions-train-40166", "mrqa_naturalquestions-train-22400", "mrqa_naturalquestions-train-39641", "mrqa_naturalquestions-train-4886", "mrqa_naturalquestions-train-37239", "mrqa_naturalquestions-train-46247", "mrqa_naturalquestions-train-15691", "mrqa_naturalquestions-train-25327", "mrqa_naturalquestions-train-23412", "mrqa_naturalquestions-train-84669", "mrqa_naturalquestions-train-81637", "mrqa_naturalquestions-train-2797", "mrqa_naturalquestions-train-45527", "mrqa_naturalquestions-train-65394", "mrqa_naturalquestions-train-64974", "mrqa_naturalquestions-train-36718", "mrqa_naturalquestions-train-54264", "mrqa_naturalquestions-train-23115", "mrqa_naturalquestions-train-28107", "mrqa_naturalquestions-train-67181", "mrqa_naturalquestions-train-6501", "mrqa_naturalquestions-train-4155", "mrqa_naturalquestions-train-88026", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-train-16669", "mrqa_naturalquestions-train-83473"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "Arthur Schnitzler's 1926 novella \"Traumnovelle\"", "women not taking jobs due to marriage or pregnancy", "The TEU", "a specially purified water", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "valour", "The world's longest suspension bridges are listed according to the length of their main span", "alexis", "cricket", "joseidon", "the Bulgars, and especially the Seljuk Turks", "killed in battle", "Volkswagen Beetle", "joseph", "The North American Free Trade Agreement ( NAFTA )", "Elizabeth I", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world", "the Vittorio Emanuele II Gallery and Piazza della Scala", "catfish aquaculture", "atomic number 53", "James and D.J. Looney as Young Sparrow and DJ Dragon Nutz, respectively", "Iraq", "a co-op of grape growers", "The Heroes of Telemark", "joseppe Verdi", "1952", "the Charlotte Hornets", "the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "on the chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.0625, "QA-F1": 0.232558608094094}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.33333333333333337, 1.0, 0.33333333333333337, 0.0, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5454545454545454, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.08695652173913043, 0.22222222222222224, 0.2222222222222222]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_hotpotqa-validation-2852", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "retrieved_ids": ["mrqa_naturalquestions-train-61273", "mrqa_naturalquestions-train-17271", "mrqa_naturalquestions-train-18107", "mrqa_naturalquestions-train-15795", "mrqa_naturalquestions-train-11322", "mrqa_naturalquestions-train-33348", "mrqa_naturalquestions-train-68221", "mrqa_naturalquestions-train-15514", "mrqa_naturalquestions-train-15675", "mrqa_naturalquestions-train-50097", "mrqa_naturalquestions-train-14030", "mrqa_naturalquestions-train-10170", "mrqa_naturalquestions-train-3236", "mrqa_naturalquestions-train-24708", "mrqa_naturalquestions-train-67304", "mrqa_naturalquestions-train-68035", "mrqa_naturalquestions-train-22952", "mrqa_naturalquestions-train-87815", "mrqa_naturalquestions-train-83090", "mrqa_naturalquestions-train-8861", "mrqa_naturalquestions-train-43368", "mrqa_naturalquestions-train-29720", "mrqa_naturalquestions-train-6703", "mrqa_naturalquestions-train-52458", "mrqa_naturalquestions-train-21077", "mrqa_naturalquestions-train-75864", "mrqa_naturalquestions-train-25132", "mrqa_naturalquestions-train-5558", "mrqa_naturalquestions-train-62377", "mrqa_naturalquestions-train-13733", "mrqa_naturalquestions-train-2693", "mrqa_naturalquestions-train-31071"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay", "Joe Turano", "fencers", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "August 31, 2014", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Minoan or Mykenaian age", "Ignacy Jan Paderewski", "ferrey", "norway", "Forbes", "Fort Williams (the latter two located on the Oneida Carry between the Mohawk River and Wood Creek at present-day Rome, New York", "gabourey Sidibe", "ferciatelli", "Orwell", "Kvapil", "Gregg Popovich", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive", "under the tutelage of his uncle", "a musician", "noran Andersen", "December 1, 1969", "american", "norway", "California State Automobile Association", "\"alone\"", "Cinderella", "The astronauts were asphyxiated before the hatch could be opened", "due to a fear of seeming rude"], "metric_results": {"EM": 0.125, "QA-F1": 0.2432667113432526}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.34782608695652173, 0.3333333333333333, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.23529411764705882, 0.4, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.631578947368421]}}, "error_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-68829", "mrqa_naturalquestions-train-80803", "mrqa_naturalquestions-train-51378", "mrqa_naturalquestions-train-54801", "mrqa_naturalquestions-train-16174", "mrqa_naturalquestions-train-56787", "mrqa_naturalquestions-train-24397", "mrqa_naturalquestions-train-25659", "mrqa_naturalquestions-train-7892", "mrqa_naturalquestions-train-71783", "mrqa_naturalquestions-train-84113", "mrqa_naturalquestions-train-52283", "mrqa_naturalquestions-train-9056", "mrqa_naturalquestions-train-59740", "mrqa_naturalquestions-train-44643", "mrqa_naturalquestions-train-31896", "mrqa_naturalquestions-train-80837", "mrqa_naturalquestions-train-20418", "mrqa_naturalquestions-train-42550", "mrqa_naturalquestions-train-17375", "mrqa_naturalquestions-train-34860", "mrqa_naturalquestions-train-62240", "mrqa_naturalquestions-train-46285", "mrqa_naturalquestions-train-55567", "mrqa_naturalquestions-train-55439", "mrqa_naturalquestions-train-21794", "mrqa_naturalquestions-train-52871", "mrqa_naturalquestions-train-57746", "mrqa_naturalquestions-train-84423", "mrqa_naturalquestions-train-80730", "mrqa_naturalquestions-train-60125", "mrqa_naturalquestions-train-49135"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister ( 1982 film)", "president of Guggenheim Partners", "Jason Lee", "Napoleon's army", "maryland", "3.7 percent of the entire student population", "high and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Tracey Stubbs", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni", "discipline problems with the Flight Director's orders during their flight", "maryland", "paddington", "amyotrophic lateral sclerosis (ALS)", "a gimmick called \"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "a pioneer in watch design, manufacturing and distribution", "the Evel Knievel craze of the mid 1970s", "Torah or Bible", "the western coast of Italy", "first and only U.S. born world grand prix champion", "brass band parades", "mid November", "thumblevoss twins", "bajgiel", "Tim \"Ripper\" Owens, singer in a Judas Priest tribute band who was chosen to replace singer Rob Halford when he left the band", "Issaquah, Washington (a suburb of Seattle)", "King George's War", "he cheated on Miley", "alternative rock", "Fort Snelling, Minnesota", "a sliding wooden box camera", "infrequent rain"], "metric_results": {"EM": 0.09375, "QA-F1": 0.22118731962481963}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.2222222222222222, 0.0, 0.0, 0.4444444444444445, 0.3636363636363636, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.28571428571428575, 0.0, 0.0, 0.25, 0.33333333333333337, 1.0, 0.2, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2779", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "retrieved_ids": ["mrqa_naturalquestions-train-31190", "mrqa_naturalquestions-train-36750", "mrqa_naturalquestions-train-76583", "mrqa_hotpotqa-validation-14", "mrqa_naturalquestions-train-6073", "mrqa_naturalquestions-train-61948", "mrqa_naturalquestions-train-1246", "mrqa_naturalquestions-train-64316", "mrqa_naturalquestions-train-13334", "mrqa_naturalquestions-train-81244", "mrqa_naturalquestions-train-21973", "mrqa_naturalquestions-train-36736", "mrqa_naturalquestions-train-43141", "mrqa_naturalquestions-train-15363", "mrqa_naturalquestions-train-68672", "mrqa_naturalquestions-train-70990", "mrqa_naturalquestions-train-75157", "mrqa_naturalquestions-train-63815", "mrqa_naturalquestions-train-50166", "mrqa_naturalquestions-train-3517", "mrqa_naturalquestions-train-71648", "mrqa_naturalquestions-train-74996", "mrqa_naturalquestions-train-43366", "mrqa_naturalquestions-train-71668", "mrqa_naturalquestions-train-22243", "mrqa_naturalquestions-train-71432", "mrqa_naturalquestions-train-45", "mrqa_naturalquestions-train-46007", "mrqa_naturalquestions-train-60098", "mrqa_naturalquestions-train-82200", "mrqa_naturalquestions-train-18735", "mrqa_naturalquestions-train-27259"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["2002 Hong Kong comedy film", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "duSable High School", "River Phoenix", "FX option or currency option", "electromagnetic waves", "a Wahhabi/ Salafi", "foxraskelion (Swastika)", "Dimensions in Time", "the Surveyor 3 unmanned lunar probe", "the end of January 1981", "gonadotropin - releasing hormone ( GnRH )", "baptism in the Small Catechism", "a Lutheran pastor in Hochfelden", "Trevor Francis", "It opens exhaust valves in the cylinder after the compression cycle, releasing the compressed air trapped in the cylinders, and slowing the vehicle", "Cheyenne rivers", "fossils in sedimentary rocks", "Hanna- Barbera, The Jetsons", "Cortina d'Ampezzo", "the efficient and effective management of money ( funds )", "Alba Longa", "joseph pryjewski", "Australian islands", "goalkeeper Timo Hildebrand", "the state sector", "2 February 1940", "poverty", "a god of the Ammonites", "retina", "Uncle Fester", "Charles Whitman"], "metric_results": {"EM": 0.0, "QA-F1": 0.1569781321619557}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.5714285714285715, 0.16666666666666669, 0.0, 0.0, 0.5882352941176471, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.4, 0.0, 0.4, 0.0, 0.0, 0.19999999999999998, 0.3333333333333333, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-validation-727", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "retrieved_ids": ["mrqa_naturalquestions-train-71635", "mrqa_naturalquestions-train-38876", "mrqa_naturalquestions-train-75818", "mrqa_naturalquestions-train-60187", "mrqa_naturalquestions-train-64526", "mrqa_naturalquestions-train-87883", "mrqa_naturalquestions-train-21739", "mrqa_naturalquestions-train-18855", "mrqa_naturalquestions-train-53017", "mrqa_naturalquestions-train-44653", "mrqa_naturalquestions-train-22514", "mrqa_naturalquestions-train-18197", "mrqa_naturalquestions-train-47504", "mrqa_naturalquestions-train-22488", "mrqa_naturalquestions-train-40245", "mrqa_naturalquestions-train-18340", "mrqa_naturalquestions-train-84106", "mrqa_naturalquestions-train-75817", "mrqa_squad-validation-4506", "mrqa_naturalquestions-train-11616", "mrqa_naturalquestions-train-14876", "mrqa_naturalquestions-train-52981", "mrqa_naturalquestions-train-66357", "mrqa_naturalquestions-train-3908", "mrqa_naturalquestions-train-57993", "mrqa_naturalquestions-train-25838", "mrqa_naturalquestions-train-76411", "mrqa_naturalquestions-train-47840", "mrqa_naturalquestions-train-36288", "mrqa_naturalquestions-train-78116", "mrqa_naturalquestions-train-40318", "mrqa_naturalquestions-train-42633"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["being bitten by radioactive/genetically-altered spiders", "Part 2", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Taio Cruz", "Jupiter (now known as the Galilean moons)", "a friend and publicist", "michael ondaatje", "masons'marks", "Theodore Haynes", "the Old Town Hall, Gateshead", "Motown, Philly soul, and Earth, Wind & Fire ( particularly `` That's the Way of the World '' )", "The neck", "after the Spanish -- American War in the 1898 Treaty of Paris", "professional wrestler", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's superweapon", "joseph diahann Carroll in \"Claudine\"", "Curtiss JN-4 airplane", "tibility for impressions, and an inclination to be touched by emotions,  as may be termed sorrowful, suffering and tender", "gorillas", "March 15, 1945", "absolute temperature", "whistlebl-blowing", "J. Robert Oppenheimer", "bicuspid", "his brother, Menelaus", "3 December", "tallahassee", "prefabricated housing projects", "brian brian", "WWSB and WOTV"], "metric_results": {"EM": 0.0625, "QA-F1": 0.07916666666666666}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-464", "mrqa_naturalquestions-train-78898", "mrqa_naturalquestions-validation-1489", "mrqa_naturalquestions-train-58277", "mrqa_naturalquestions-train-34208", "mrqa_naturalquestions-train-86573", "mrqa_naturalquestions-train-22478", "mrqa_naturalquestions-train-60999", "mrqa_naturalquestions-train-33039", "mrqa_naturalquestions-train-41344", "mrqa_naturalquestions-train-17964", "mrqa_naturalquestions-train-58779", "mrqa_naturalquestions-train-23762", "mrqa_naturalquestions-train-85866", "mrqa_naturalquestions-train-81644", "mrqa_hotpotqa-validation-1142", "mrqa_naturalquestions-train-79385", "mrqa_naturalquestions-train-41292", "mrqa_naturalquestions-train-73843", "mrqa_naturalquestions-train-4720", "mrqa_naturalquestions-train-52728", "mrqa_naturalquestions-train-4863", "mrqa_naturalquestions-train-75298", "mrqa_naturalquestions-train-68867", "mrqa_naturalquestions-train-75702", "mrqa_naturalquestions-train-72794", "mrqa_naturalquestions-train-80019", "mrqa_naturalquestions-train-70062", "mrqa_naturalquestions-train-17355", "mrqa_naturalquestions-train-66350", "mrqa_naturalquestions-train-24246", "mrqa_naturalquestions-validation-5146"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["belle belle", "cow's milk cheese", "bel belle", "on the lateral side of the tibia", "fergus Mor of Dalriada", "the North Sea", "bantu", "faldo", "October 29, 1985", "Amway", "secondary school study", "Thomas Sowell", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in His absence, the Deputy - Chairman of the Rajya Sabha", "fred Globe Award", "Tanzania", "bordered by Libya, Chad to the East, Nigeria in the southwest, Mali in the west and Algeria in the Northwest", "60-mile-wide river", "an open work crown", "allowing a child to go through a torturous treatment to gain information", "Fulham", "French", "belle belle", "U.S. Marshals", "What's Up (TV series)", "supply chain management", "belle belle", "Poland", "polynomial algebra", "belle belle", "The three wise monkeys", "sheepskin", "Honolulu"], "metric_results": {"EM": 0.15625, "QA-F1": 0.184375}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.21428571428571425, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-2445", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-2287"], "retrieved_ids": ["mrqa_naturalquestions-train-77185", "mrqa_naturalquestions-train-56033", "mrqa_naturalquestions-train-75249", "mrqa_naturalquestions-train-45617", "mrqa_naturalquestions-train-22705", "mrqa_naturalquestions-train-2652", "mrqa_naturalquestions-train-51396", "mrqa_naturalquestions-train-32959", "mrqa_naturalquestions-train-23165", "mrqa_naturalquestions-train-6769", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-71584", "mrqa_naturalquestions-train-14030", "mrqa_naturalquestions-train-69875", "mrqa_naturalquestions-train-42290", "mrqa_naturalquestions-train-83787", "mrqa_naturalquestions-train-55838", "mrqa_naturalquestions-train-82639", "mrqa_naturalquestions-train-25983", "mrqa_naturalquestions-train-65679", "mrqa_naturalquestions-train-14699", "mrqa_naturalquestions-train-45309", "mrqa_naturalquestions-train-41348", "mrqa_naturalquestions-train-62232", "mrqa_naturalquestions-train-85418", "mrqa_naturalquestions-train-27894", "mrqa_naturalquestions-train-13385", "mrqa_naturalquestions-train-25241", "mrqa_naturalquestions-train-51153", "mrqa_naturalquestions-train-43186", "mrqa_naturalquestions-train-15363", "mrqa_naturalquestions-train-21245"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["milk soft cheese", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "decay energy of 687 keV", "James Zeebo", "sovereign states", "Vice President of the United States (VPOTUS)", "\"Teach the Controversy\" campaign", "Bumblebee", "Australian", "18 months", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Frank Wentz", "rock units, other than muds, don't significantly change in volume", "June 9, 2015", "\"Veyyil\"", "Grace Nail Johnson", "Keith Richards", "at least one prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "students learn from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "latitude ( a line of longitude ) in a geographical coordinate system at which longitude is defined to be 0 \u00b0", "Cartoon Network", "the Presiding Officer on the advice of the parliamentary bureau", "Miami Heat of the National Basketball Association", "33", "vitifolia", "Annual Conference Cabinet", "field hockey player Hannah Macleod", "William Hartnell's poor health"], "metric_results": {"EM": 0.1875, "QA-F1": 0.27068081174059433}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 1.0, 0.08333333333333334, 0.0, 1.0, 0.0, 0.0, 0.1818181818181818, 0.19999999999999998, 0.16, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.4347826086956522, 0.25, 0.07407407407407407, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-1895", "mrqa_squad-validation-5110", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "retrieved_ids": ["mrqa_naturalquestions-train-70828", "mrqa_naturalquestions-train-52771", "mrqa_naturalquestions-train-19531", "mrqa_naturalquestions-train-65515", "mrqa_naturalquestions-train-8912", "mrqa_naturalquestions-train-37815", "mrqa_naturalquestions-train-65814", "mrqa_naturalquestions-train-70177", "mrqa_hotpotqa-validation-5802", "mrqa_naturalquestions-train-84390", "mrqa_naturalquestions-train-68697", "mrqa_naturalquestions-train-67438", "mrqa_naturalquestions-train-59565", "mrqa_naturalquestions-train-85477", "mrqa_naturalquestions-train-48067", "mrqa_naturalquestions-train-3754", "mrqa_naturalquestions-train-55023", "mrqa_naturalquestions-train-37019", "mrqa_naturalquestions-train-62378", "mrqa_naturalquestions-train-22527", "mrqa_naturalquestions-train-51475", "mrqa_naturalquestions-train-68815", "mrqa_naturalquestions-train-24717", "mrqa_naturalquestions-train-29933", "mrqa_naturalquestions-train-11567", "mrqa_naturalquestions-train-77881", "mrqa_naturalquestions-train-59549", "mrqa_hotpotqa-validation-4263", "mrqa_naturalquestions-train-57642", "mrqa_naturalquestions-train-85554", "mrqa_naturalquestions-train-41702", "mrqa_naturalquestions-train-78723"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwandaandan genocide, also known as the genocide against the Tutsi", "create a climate of learning", "500 metres", "Vili Fualaau and Mary Kay Letourneau", "the entertainment division", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "12", "the Great Exhibition of 1851", "King Edward I to Henry VIII", "rubric", "dundee", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "darnley", "\"Grindhouse\" fake trailer", "olympic", "digital transmission modes", "Baden-W\u00fcrttemberg", "Tesla Gigafactory 1", "821", "Video On Demand content which was not previously carried by cable", "liquid", "Kim Hyun-ah", "the races which can do this work best", "transposition", "the \"King of Cool\"", "President Woodrow Wilson", "antileogeia", "the fifth season", "dick cheney", "Hockey Club Davos", "Michael Crawford", "Qutab Ud - Din - Aibak, founder of the Delhi Sultanate"], "metric_results": {"EM": 0.15625, "QA-F1": 0.22999338624338622}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 0.07407407407407408, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.6666666666666666]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_hotpotqa-validation-4415", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-38269", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-29987", "mrqa_naturalquestions-train-79592", "mrqa_naturalquestions-train-27730", "mrqa_naturalquestions-train-73367", "mrqa_naturalquestions-train-42395", "mrqa_naturalquestions-train-72757", "mrqa_naturalquestions-train-88239", "mrqa_naturalquestions-train-70630", "mrqa_naturalquestions-train-3516", "mrqa_naturalquestions-train-71357", "mrqa_naturalquestions-train-39614", "mrqa_naturalquestions-train-75658", "mrqa_naturalquestions-train-61444", "mrqa_naturalquestions-train-6746", "mrqa_naturalquestions-train-17258", "mrqa_naturalquestions-train-46275", "mrqa_naturalquestions-train-87227", "mrqa_naturalquestions-train-62450", "mrqa_naturalquestions-train-17268", "mrqa_naturalquestions-train-50539", "mrqa_naturalquestions-train-85421", "mrqa_naturalquestions-train-53456", "mrqa_squad-validation-2812", "mrqa_naturalquestions-train-49961", "mrqa_naturalquestions-train-83744", "mrqa_naturalquestions-train-16995", "mrqa_naturalquestions-train-10601", "mrqa_naturalquestions-train-12106", "mrqa_hotpotqa-validation-4734", "mrqa_naturalquestions-train-50435"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "director Norman Macdonnell", "aragon", "11.1%", "trans-Pacific flight", "Sharman Joshi", "in a classroom", "Forster I, Forster II, and Forster III", "Giuga's conjecture", "Ana", "in New Brunswick", "`` Happy Hour ''", "f. o. Matthiessen", "fredys o'Donnell", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "blackstar", "India", "fear of the Lord", "1974", "Nicki Minaj", "comic opera", "Huguenot ancestry", "f1", "friedrich", "\"Rugrats\" \"The Powerpuff Girls\"\"The Fairly OddParents\" \"Drawn Together\"", "William the Conqueror", "Ben Gurion International Airport", "two degrees of freedom", "Mainland Greece", "blood borne diseases", "youngest TV director ever", "Southern Progress Corporation"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2098958333333333}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.08333333333333334, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.4, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552"], "retrieved_ids": ["mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-train-32874", "mrqa_naturalquestions-train-65451", "mrqa_naturalquestions-train-76321", "mrqa_naturalquestions-train-75460", "mrqa_naturalquestions-train-60968", "mrqa_naturalquestions-train-70644", "mrqa_naturalquestions-train-5976", "mrqa_naturalquestions-train-35670", "mrqa_naturalquestions-train-26608", "mrqa_naturalquestions-train-87250", "mrqa_naturalquestions-train-59812", "mrqa_naturalquestions-train-20473", "mrqa_naturalquestions-train-42218", "mrqa_naturalquestions-train-61138", "mrqa_naturalquestions-train-79457", "mrqa_naturalquestions-train-28428", "mrqa_naturalquestions-train-56048", "mrqa_naturalquestions-train-14926", "mrqa_naturalquestions-train-8424", "mrqa_naturalquestions-train-32838", "mrqa_squad-validation-1003", "mrqa_naturalquestions-train-79259", "mrqa_naturalquestions-train-1240", "mrqa_naturalquestions-train-52562", "mrqa_naturalquestions-train-9617", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-60125", "mrqa_naturalquestions-train-16789", "mrqa_naturalquestions-train-69599", "mrqa_naturalquestions-train-12508", "mrqa_naturalquestions-train-72473"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating", "David Feldman", "the Sackler Centre for arts education", "is an American hip hop recording artist, actor and activist", "kaleidoscope", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "Apollo", "ribosomes", "kookaburra", "six", "Zoe McLellan as Meredith Brody, NCIS Special Agent ( seasons 1 -- 2 )", "Friends", "Cozonac", "Belfast", "a heliocentric orbit", "Lucius Cornelius Sulla Felix", "following the 2017 season", "a Golden Globe", "English and Swahili", "the primacy of core Christian values such as love, patience, charity, and freedom", "karl marx", "Hexachrome", "Qutab - ud - din Aibak, first ruler of the Delhi Sultanate", "\"Cinderella and Four Knights\"", "San Jose", "sea wasp", "Suzanne N.J.'Susie' Chun Oakland", "a \"teleforce\" weapon", "Thunderbird", "giving Super Bowl", "29.7%", "karl marx"], "metric_results": {"EM": 0.21875, "QA-F1": 0.29212621528798}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.48484848484848486, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_hotpotqa-validation-3547", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-935"], "retrieved_ids": ["mrqa_naturalquestions-train-20604", "mrqa_naturalquestions-train-75593", "mrqa_naturalquestions-train-66079", "mrqa_naturalquestions-train-84653", "mrqa_naturalquestions-train-27433", "mrqa_naturalquestions-train-63507", "mrqa_triviaqa-validation-5526", "mrqa_squad-validation-5622", "mrqa_naturalquestions-train-14722", "mrqa_naturalquestions-train-86873", "mrqa_naturalquestions-train-81280", "mrqa_naturalquestions-train-19051", "mrqa_naturalquestions-train-75814", "mrqa_naturalquestions-train-43728", "mrqa_naturalquestions-train-55469", "mrqa_naturalquestions-train-9126", "mrqa_naturalquestions-train-88117", "mrqa_naturalquestions-train-22546", "mrqa_naturalquestions-train-71122", "mrqa_naturalquestions-train-41844", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-train-24847", "mrqa_hotpotqa-validation-2852", "mrqa_naturalquestions-train-71962", "mrqa_naturalquestions-train-62518", "mrqa_squad-validation-7149", "mrqa_naturalquestions-train-35768", "mrqa_naturalquestions-train-25617", "mrqa_naturalquestions-train-81958", "mrqa_naturalquestions-train-75224", "mrqa_naturalquestions-train-78380", "mrqa_naturalquestions-train-78287"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "blood transfusion", "Yazoo", "22 April 1894", "black hole", "soul does not sleep (anima non sic dormit) but wakes (sed vigilat) and experiences visions", "was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area", "Willie Nelson and Kris Kristofferson", "karl marx", "12 California State University campuses (Bakersfield, Channel Islands, Dominguez Hills, Fullerton, Los Angeles, San Bernardino, San Diego, San Marcos, and San Luis Obispo)", "a French pirate active in the Caribbean and off the coast of Africa", "Lewis", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "proteins", "2001", "there must be infinitely many primes", "ITV News at Ten", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "hulder", "tribes in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British", "Orthodox Christians", "James Bond", "640 \u00d7 1136 at 326 ppi", "fillies", "the Western Atlantic ctenophore Mnemiopsis leidyi", "\"Menace II Society\"", "quarterback", "Larry Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.1875, "QA-F1": 0.27446210568013124}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.19512195121951217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.25, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-2006", "mrqa_naturalquestions-train-21245", "mrqa_naturalquestions-train-65641", "mrqa_naturalquestions-train-67785", "mrqa_naturalquestions-train-85607", "mrqa_naturalquestions-train-46447", "mrqa_naturalquestions-train-13757", "mrqa_naturalquestions-train-40747", "mrqa_naturalquestions-train-83443", "mrqa_naturalquestions-train-70215", "mrqa_naturalquestions-train-37664", "mrqa_naturalquestions-train-29455", "mrqa_squad-validation-4060", "mrqa_naturalquestions-train-77697", "mrqa_naturalquestions-train-67521", "mrqa_naturalquestions-train-62765", "mrqa_naturalquestions-train-34656", "mrqa_naturalquestions-train-47764", "mrqa_naturalquestions-train-62480", "mrqa_naturalquestions-train-1997", "mrqa_naturalquestions-train-56633", "mrqa_naturalquestions-train-31504", "mrqa_naturalquestions-train-34338", "mrqa_naturalquestions-train-85828", "mrqa_naturalquestions-train-36145", "mrqa_naturalquestions-train-30797", "mrqa_naturalquestions-train-47530", "mrqa_naturalquestions-train-16362", "mrqa_naturalquestions-train-15289", "mrqa_naturalquestions-train-84636", "mrqa_naturalquestions-train-75685", "mrqa_naturalquestions-train-38547"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "MSC Crociere S. p.A.", "alison marx", "his friends, Humpty Dumpty and Kitty Softpaws", "The centre-right Australian Labor Party", "Royalists", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "They are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "al\u00e9a seydoux", "alibris", "Augustus Waters", "1619", "Tony Blair", "gambling", "June 11, 1973", "Kenya", "critical quotations", "boudicca", "an active supporter of the League of Nations", "Cargill", "AMC Cinemas", "\"The Gang\"", "3 October 1990", "September 21, 2017", "weak force", "fred royale", "Bernice", "Development of Substitute Materials", "fictional county of Barsetshire", "vast areas"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31773114548849846}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.33333333333333337, 0.1111111111111111, 0.15686274509803924, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.1818181818181818, 0.0, 0.5, 0.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_hotpotqa-validation-2448"], "retrieved_ids": ["mrqa_naturalquestions-train-77410", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-train-30455", "mrqa_naturalquestions-train-32377", "mrqa_naturalquestions-train-79226", "mrqa_naturalquestions-train-78116", "mrqa_naturalquestions-train-71683", "mrqa_naturalquestions-train-6533", "mrqa_naturalquestions-train-29462", "mrqa_naturalquestions-train-43963", "mrqa_naturalquestions-train-21550", "mrqa_naturalquestions-train-15570", "mrqa_naturalquestions-train-18357", "mrqa_naturalquestions-train-78716", "mrqa_naturalquestions-train-26608", "mrqa_naturalquestions-train-12394", "mrqa_naturalquestions-train-6113", "mrqa_naturalquestions-train-75713", "mrqa_naturalquestions-train-57144", "mrqa_naturalquestions-train-11436", "mrqa_naturalquestions-train-71660", "mrqa_naturalquestions-train-78133", "mrqa_naturalquestions-train-34613", "mrqa_squad-validation-874", "mrqa_naturalquestions-train-39617", "mrqa_naturalquestions-train-1984", "mrqa_naturalquestions-train-19806", "mrqa_naturalquestions-train-42379", "mrqa_naturalquestions-train-68142", "mrqa_naturalquestions-train-86749", "mrqa_naturalquestions-train-8343", "mrqa_naturalquestions-train-87724"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "javier (Luna)", "red lead primer and a lead - based topcoat", "Dreamland", "animated film", "European Union institutions", "flew on one Space Shuttle mission, STS-134, and 46", "nine other contenders from across the United States", "CAL IPSO", "celandine flowers", "US President John F. Kennedy", "Ronnie Schell", "artemisinin- Based therapy", "Mumbai, Maharashtra", "east", "1939", "2017 / 18", "1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "April, heavy showers coming from pre-monsoonal convective clouds mainly in the form of squall lines also known as the north easterlies formed mainly as a result of the interactions of the two dominant airmasses", "benanga", "Incudomalleolar joint", "bobby riggs", "Leucippus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Ted Ginn Jr."], "metric_results": {"EM": 0.1875, "QA-F1": 0.3034149877899878}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.4444444444444445, 0.0, 0.25, 0.0, 0.25, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.05714285714285715, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_naturalquestions-validation-1617", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750"], "retrieved_ids": ["mrqa_naturalquestions-train-20085", "mrqa_naturalquestions-train-72994", "mrqa_naturalquestions-train-37024", "mrqa_naturalquestions-train-56827", "mrqa_triviaqa-validation-1494", "mrqa_naturalquestions-train-38540", "mrqa_naturalquestions-train-2937", "mrqa_naturalquestions-train-61433", "mrqa_naturalquestions-train-41983", "mrqa_naturalquestions-train-70489", "mrqa_naturalquestions-train-13980", "mrqa_naturalquestions-train-19241", "mrqa_naturalquestions-train-78075", "mrqa_naturalquestions-train-52987", "mrqa_naturalquestions-train-10784", "mrqa_naturalquestions-train-79569", "mrqa_naturalquestions-train-41557", "mrqa_naturalquestions-train-72572", "mrqa_naturalquestions-train-40949", "mrqa_naturalquestions-train-45457", "mrqa_naturalquestions-train-46381", "mrqa_naturalquestions-train-30547", "mrqa_naturalquestions-train-87769", "mrqa_naturalquestions-train-64993", "mrqa_naturalquestions-train-61037", "mrqa_triviaqa-validation-5429", "mrqa_naturalquestions-train-54745", "mrqa_naturalquestions-train-22550", "mrqa_naturalquestions-train-31769", "mrqa_naturalquestions-train-31864", "mrqa_naturalquestions-train-10396", "mrqa_naturalquestions-train-36667"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "photosystem I", "Pitt", "alchemy", "WBC and lineal titles", "moluccas", "the first Saturday in May", "Albany", "multilateral negotiations", "1990", "J.R. R. Tolkien", "Peyton Manning", "Instagram's own account", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "letter series", "Fa Ze Clan", "the nine circles of Hell", "two Mongols and a Muslim", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "Friars Minor (O.F.M)", "CD Castell\u00f3n", "roughly between 1770 and 1848", "12\u20134", "having colloblasts, which are sticky and adhere to prey", "Jon M. Chu", "STS-51-L.", "it will retreat to its den and winter will persist for six more weeks, and if it does not see its shadow because of cloudiness, spring will arrive early", "minister of external trade and tourism"], "metric_results": {"EM": 0.1875, "QA-F1": 0.338236052248149}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.6486486486486487, 0.0, 0.0, 0.5, 0.7692307692307693, 0.0, 0.0, 0.0, 0.4, 0.2222222222222222, 0.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.5483870967741935, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-28218", "mrqa_naturalquestions-train-51868", "mrqa_naturalquestions-train-6321", "mrqa_naturalquestions-train-47068", "mrqa_naturalquestions-train-240", "mrqa_naturalquestions-train-84415", "mrqa_naturalquestions-train-1696", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-train-47978", "mrqa_naturalquestions-train-12357", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-train-78095", "mrqa_naturalquestions-train-87063", "mrqa_naturalquestions-train-30898", "mrqa_naturalquestions-train-25168", "mrqa_naturalquestions-train-74979", "mrqa_naturalquestions-train-61733", "mrqa_naturalquestions-train-2336", "mrqa_naturalquestions-train-76401", "mrqa_naturalquestions-train-21881", "mrqa_naturalquestions-train-54019", "mrqa_naturalquestions-train-70671", "mrqa_naturalquestions-train-87588", "mrqa_naturalquestions-train-66424", "mrqa_naturalquestions-train-6765", "mrqa_naturalquestions-train-38470", "mrqa_naturalquestions-train-43478", "mrqa_naturalquestions-train-81051", "mrqa_naturalquestions-train-81958", "mrqa_naturalquestions-train-47601", "mrqa_naturalquestions-train-80708", "mrqa_naturalquestions-train-14391"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["olympic sports", "france", "40 million", "states'rights to expand slavery", "between 1923 and 1925", "Metropolitan Statistical Area", "January 19, 1962", "Frigate", "france", "until all available list seats are allocated", "geese", "effect at any given time", "france", "Sylvester McCoy", "August 14, 1848", "lower rates of health and social problems", "juveniles are capable of reproduction before reaching the adult size and shape", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "by their fellow students because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Chorus Line", "2,664", "iPhone 6 Plus", "through a chute beneath his or her feet", "Claims adjuster", "facility management services", "Symphony No. 7", "dordogne", "1603", "ranked above the two personal physicians of the Emperor", "france", "We admitted we were powerless over alcohol -- that our lives had become unmanageable.   Came to believe that a Power greater than ourselves could restore us to sanity.", "Cubs"], "metric_results": {"EM": 0.25, "QA-F1": 0.43373798076923076}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.25, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.4444444444444445, 0.3333333333333333, 0.25, 0.7200000000000001, 0.4615384615384615, 0.8750000000000001, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_naturalquestions-validation-4996", "mrqa_hotpotqa-validation-1414", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "retrieved_ids": ["mrqa_naturalquestions-train-46077", "mrqa_naturalquestions-train-48930", "mrqa_naturalquestions-train-14139", "mrqa_naturalquestions-train-82978", "mrqa_naturalquestions-train-73644", "mrqa_naturalquestions-train-9926", "mrqa_naturalquestions-train-27429", "mrqa_naturalquestions-train-40582", "mrqa_naturalquestions-train-71934", "mrqa_naturalquestions-train-52986", "mrqa_naturalquestions-train-21715", "mrqa_naturalquestions-train-54002", "mrqa_naturalquestions-train-86755", "mrqa_naturalquestions-train-10968", "mrqa_naturalquestions-train-75411", "mrqa_naturalquestions-train-16335", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-train-47764", "mrqa_naturalquestions-train-3478", "mrqa_naturalquestions-train-52455", "mrqa_naturalquestions-train-75500", "mrqa_squad-validation-3126", "mrqa_naturalquestions-train-72549", "mrqa_naturalquestions-train-2022", "mrqa_naturalquestions-train-8329", "mrqa_naturalquestions-train-52894", "mrqa_naturalquestions-train-54891", "mrqa_naturalquestions-train-38514", "mrqa_triviaqa-validation-4725", "mrqa_naturalquestions-train-32966", "mrqa_naturalquestions-train-52237", "mrqa_naturalquestions-train-55521"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["to answer a theological problem, namely how the Messiah of the Jews came to have an overwhelmingly non-Jewish church", "north of the Lakes Region and south of the Kancamagus Highway", "Magic formula investing", "true history of the Kelly Gang", "Waialua District of the island of O\u02bb ahu, City and County of Honolulu", "1910\u20131940", "non-teaching posts", "Catch Me Who Can", "jazz", "tennis", "4,000", "Khagan", "wuthering Heights", "canal", "spice", "The Simpsons Spin-Off Showcase", "Raymond Unwin", "San Bernardino", "an extensive neoclassical centre referred to as Tyneside Classical largely developed in the 1830s by Richard Grainger and John Dobson, and recently extensively restored", "Albany High School for Educating People of Color", "Lalbagh Fort at Dhaka", "Sergeant First Class", "Shaw", "seek jury nullification", "Cee - Lo", "Anglican", "pacunia", "warcruiser Renown, the cruiser Kenya", "magnetism", "located within nine coastal southern Nigerian states", "redistributive", "January 11, 1755 or 1757"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27830975193107543}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.1111111111111111, 0.19999999999999998, 0.0, 0.33333333333333337, 0.5882352941176471, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.08333333333333334, 0.5454545454545454, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8000000000000002]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_naturalquestions-validation-7801", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "retrieved_ids": ["mrqa_naturalquestions-train-42179", "mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-train-10655", "mrqa_naturalquestions-train-6822", "mrqa_naturalquestions-train-35605", "mrqa_naturalquestions-train-23500", "mrqa_naturalquestions-train-72808", "mrqa_naturalquestions-train-6049", "mrqa_naturalquestions-train-66471", "mrqa_naturalquestions-train-9244", "mrqa_naturalquestions-train-70253", "mrqa_naturalquestions-train-69622", "mrqa_naturalquestions-train-70592", "mrqa_naturalquestions-train-1389", "mrqa_naturalquestions-train-14699", "mrqa_naturalquestions-train-22959", "mrqa_naturalquestions-train-2930", "mrqa_naturalquestions-train-49897", "mrqa_naturalquestions-train-75509", "mrqa_naturalquestions-train-23419", "mrqa_naturalquestions-train-54644", "mrqa_naturalquestions-train-60682", "mrqa_naturalquestions-train-55006", "mrqa_naturalquestions-train-58559", "mrqa_naturalquestions-train-3316", "mrqa_naturalquestions-train-40504", "mrqa_naturalquestions-train-59470", "mrqa_naturalquestions-train-8746", "mrqa_naturalquestions-train-46113", "mrqa_naturalquestions-train-80232", "mrqa_naturalquestions-train-43901", "mrqa_naturalquestions-train-63878"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A. producer Bones Howe", "kelly smith", "kelly kelly", "dark dust and gases", "\" Big Mamie\"", "kelly", "the \"eternal outsider, the sardonic drifter\" someone who rebels against the social structure", "a light sky-blue color caused by absorption in the red", "the peasants had to work for free on church land", "1963", "February 10, 1984", "inner mitochondria membrane", "charigot", "The Grand Annual Steeplechase at Warrnambool and the Australian International Airshow at Geelong", "B SkyB has no veto", "the fourth season", "a more fundamental electroweak interaction", "the availability of skilled tradespeople", "diamond", "A simple iron boar crest adorns the top of this helmet", "the Newcastle Polytechnic, established in 1969 and became the University of Northumbria at Newcastle", "kelly kelly", "David Lofton", "on kickoffs at the 25 - yard line", "after the Swedish astronomer Anders Celsius ( 1701 -- 1744 ), who developed a similar temperature scale", "about 7,000 out of 20,000 inhabitants", "lion, leopard, buffalo, rhinoceros, and elephant", "by faith", "margaret smith", "can be produced with constant technology and resources per unit of time", "daniel van dyck", "group"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2907273966895677}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.28571428571428575, 0.0, 0.6153846153846153, 0.125, 0.0, 0.6666666666666666, 0.631578947368421, 0.5555555555555556, 0.2857142857142857, 1.0, 0.8, 0.0, 0.9090909090909091, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-61893", "mrqa_naturalquestions-train-26497", "mrqa_naturalquestions-train-35199", "mrqa_naturalquestions-train-39529", "mrqa_naturalquestions-train-58035", "mrqa_naturalquestions-train-75460", "mrqa_naturalquestions-train-56787", "mrqa_naturalquestions-train-48333", "mrqa_naturalquestions-train-32125", "mrqa_naturalquestions-train-35092", "mrqa_naturalquestions-train-75010", "mrqa_naturalquestions-train-62092", "mrqa_naturalquestions-train-72954", "mrqa_naturalquestions-train-23742", "mrqa_naturalquestions-train-41534", "mrqa_naturalquestions-train-52519", "mrqa_naturalquestions-train-36346", "mrqa_naturalquestions-train-3005", "mrqa_naturalquestions-train-65218", "mrqa_naturalquestions-train-25891", "mrqa_naturalquestions-train-3516", "mrqa_naturalquestions-train-42355", "mrqa_naturalquestions-train-33768", "mrqa_naturalquestions-train-24496", "mrqa_naturalquestions-train-80542", "mrqa_naturalquestions-train-9599", "mrqa_naturalquestions-train-28081", "mrqa_naturalquestions-train-58357", "mrqa_naturalquestions-train-61173", "mrqa_naturalquestions-train-19209", "mrqa_triviaqa-validation-681", "mrqa_naturalquestions-train-8533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["the Muslim faith as a tool of the devil, he was indifferent to its practice", "Chris Weidman", "jury nullification", "Rama", "studied Arabic grammar", "Professor Eobard Thawne", "slivovitz", "a US$10 a week raise", "October 25, 1825", "member states on a voluntary basis", "oboe", "McKinsey's offices in Silicon Valley and India", "gypsia", "living Doll", "Crohn's disease", "Ondemar Dias", "Raya Yarbrough", "Arizona", "cruiserweight", "John D. Rockefeller", "Old Testament", "australian-based United Parcel Service", "local talent", "Football League", "tristan Farnon", "mafic", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "birrattsville", "1349", "dodo bird", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "Stan Butler"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3135700113378685}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.8333333333333333, 1.0, 0.4444444444444445, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.1, 0.0, 0.0, 1.0, 0.2040816326530612, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_triviaqa-validation-7669", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821"], "retrieved_ids": ["mrqa_naturalquestions-train-73404", "mrqa_naturalquestions-train-19393", "mrqa_naturalquestions-train-17260", "mrqa_naturalquestions-train-34182", "mrqa_naturalquestions-train-23189", "mrqa_naturalquestions-train-18271", "mrqa_naturalquestions-train-41954", "mrqa_naturalquestions-train-8016", "mrqa_naturalquestions-train-33810", "mrqa_naturalquestions-train-67755", "mrqa_naturalquestions-train-34253", "mrqa_naturalquestions-train-36951", "mrqa_naturalquestions-train-36809", "mrqa_naturalquestions-train-33085", "mrqa_naturalquestions-train-46092", "mrqa_naturalquestions-train-51866", "mrqa_naturalquestions-train-85123", "mrqa_naturalquestions-train-47978", "mrqa_naturalquestions-train-24557", "mrqa_naturalquestions-train-58738", "mrqa_naturalquestions-train-86880", "mrqa_naturalquestions-train-84390", "mrqa_naturalquestions-train-70963", "mrqa_naturalquestions-train-26313", "mrqa_naturalquestions-train-26186", "mrqa_naturalquestions-train-84682", "mrqa_naturalquestions-train-10299", "mrqa_triviaqa-validation-4383", "mrqa_naturalquestions-train-46380", "mrqa_naturalquestions-train-75030", "mrqa_naturalquestions-train-13346", "mrqa_naturalquestions-train-65547"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["abraham lincoln", "886 AD", "to finance his own projects with varying degrees of success", "24 Hours of Le Mans", "Xbox 360", "Tokyo", "safety Darian Stewart", "parallelogram rule of vector addition", "abraham lincoln", "abraham lincoln", "a neutron source used for stable and reliable initiation of nuclear chain reaction in nuclear reactors, when they are loaded with fresh nuclear fuel, whose neutron flux from spontaneous fission is insufficient for a reliable startup, or after prolonged shutdown periods", "abraham lincoln", "the bore, and often the stroke", "performs six major functions ; support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "Doctorin' the Tardis", "National Basketball Development League", "gillingham", "St. Mary's County", "Graham Gano", "2,615 at the 2010 census", "Pyeongchang", "athlete", "a password recovery tool for Microsoft Windows", "Captain John Guidry", "Charles and Ray Eames", "Brazil", "abraham lincoln", "either Q or the finite field with p elements, whence the name", "pamper an irritated digestive tract", "53% in Botswana to -40% in Bahrain", "the light reactions"], "metric_results": {"EM": 0.0625, "QA-F1": 0.15815388655462187}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.38095238095238093, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 0.8235294117647058, 0.0, 0.25, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_triviaqa-validation-814", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "retrieved_ids": ["mrqa_naturalquestions-train-16348", "mrqa_naturalquestions-train-48556", "mrqa_naturalquestions-train-55656", "mrqa_naturalquestions-train-52138", "mrqa_naturalquestions-train-7143", "mrqa_naturalquestions-train-3601", "mrqa_naturalquestions-train-32769", "mrqa_naturalquestions-train-33686", "mrqa_naturalquestions-train-70206", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-train-48940", "mrqa_naturalquestions-train-1960", "mrqa_naturalquestions-train-20198", "mrqa_naturalquestions-train-70562", "mrqa_naturalquestions-train-23625", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-train-6132", "mrqa_naturalquestions-train-34331", "mrqa_triviaqa-validation-5487", "mrqa_naturalquestions-train-81468", "mrqa_naturalquestions-train-53688", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-6013", "mrqa_naturalquestions-train-38075", "mrqa_naturalquestions-train-62430", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-train-78867", "mrqa_naturalquestions-train-55033", "mrqa_naturalquestions-train-75317", "mrqa_naturalquestions-train-5260", "mrqa_naturalquestions-train-57937", "mrqa_naturalquestions-train-24688"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "an outgoing, eccentic, big - hearted, loving, sweet, and thoughtful elephant and teacher", "abraham lincoln", "arthur", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "BBC UKTV", "arthur", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "f1", "arthur", "usernames, passwords, commands and data", "A computer program", "The Greens, who won their first lower house seats in 2014, are strongest in inner Melbourne", "marduk", "hvannadalshnukur", "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "arthur", "the South Pacific off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Easy", "Scott Bakula", "India's first and, to date, only female Prime Minister", "National Lottery", "arthur", "katherine swynford", "to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1475287413609782}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.125, 0.0, 0.0, 0.10526315789473684, 0.0, 0.4444444444444445, 0.4, 0.12121212121212122, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4000000000000001]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-42553", "mrqa_naturalquestions-train-10931", "mrqa_naturalquestions-train-39500", "mrqa_naturalquestions-train-24300", "mrqa_naturalquestions-train-36763", "mrqa_naturalquestions-train-33358", "mrqa_naturalquestions-train-24727", "mrqa_naturalquestions-train-44223", "mrqa_naturalquestions-train-71500", "mrqa_naturalquestions-train-26378", "mrqa_naturalquestions-train-68090", "mrqa_naturalquestions-train-80651", "mrqa_naturalquestions-train-83344", "mrqa_naturalquestions-train-65748", "mrqa_naturalquestions-train-12929", "mrqa_naturalquestions-train-41130", "mrqa_naturalquestions-train-72924", "mrqa_naturalquestions-train-15888", "mrqa_naturalquestions-train-70948", "mrqa_naturalquestions-train-57053", "mrqa_naturalquestions-train-23244", "mrqa_naturalquestions-train-84465", "mrqa_naturalquestions-train-33817", "mrqa_naturalquestions-train-44790", "mrqa_naturalquestions-train-59608", "mrqa_triviaqa-validation-316", "mrqa_naturalquestions-train-70151", "mrqa_naturalquestions-train-36561", "mrqa_naturalquestions-train-61127", "mrqa_naturalquestions-train-35820", "mrqa_naturalquestions-train-40572", "mrqa_hotpotqa-validation-1001"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Thiel", "spain", "polly", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid- Western Highway between Sydney and Adelaide", "androids", "spain", "shopping mall located in Bloomington, Minnesota, United States ( a suburb of the Twin Cities )", "to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "DreamWorks Animation", "waltz", "his own men", "emissions resulting from human activities", "polly", "the RAF", "encourage growth", "Ibrium", "peterborough", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "polly", "surtam", "390 billion", "Washington Street between Boylston Street and Kneeland Street", "May 10, 1976", "six", "polly", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "polly", "Lenin", "surtania", "Weavers, a half-timbered house by the river"], "metric_results": {"EM": 0.0625, "QA-F1": 0.13545175548852018}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.15384615384615385, 0.11764705882352941, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.45000000000000007, 0.0, 0.0, 0.0, 0.25]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "retrieved_ids": ["mrqa_naturalquestions-train-66546", "mrqa_naturalquestions-train-85327", "mrqa_naturalquestions-train-1219", "mrqa_naturalquestions-train-50841", "mrqa_naturalquestions-train-79701", "mrqa_naturalquestions-train-1916", "mrqa_naturalquestions-train-79470", "mrqa_naturalquestions-train-57622", "mrqa_naturalquestions-train-46748", "mrqa_naturalquestions-train-56841", "mrqa_naturalquestions-train-83679", "mrqa_naturalquestions-train-66183", "mrqa_naturalquestions-train-3943", "mrqa_naturalquestions-train-24834", "mrqa_naturalquestions-train-62685", "mrqa_naturalquestions-train-79858", "mrqa_naturalquestions-train-38196", "mrqa_naturalquestions-train-81207", "mrqa_naturalquestions-train-24157", "mrqa_naturalquestions-train-56352", "mrqa_naturalquestions-train-33621", "mrqa_naturalquestions-train-2949", "mrqa_naturalquestions-train-7404", "mrqa_naturalquestions-train-66564", "mrqa_naturalquestions-train-25841", "mrqa_naturalquestions-train-45531", "mrqa_naturalquestions-train-63639", "mrqa_naturalquestions-train-25005", "mrqa_naturalquestions-train-77648", "mrqa_naturalquestions-train-3097", "mrqa_naturalquestions-train-56971", "mrqa_naturalquestions-train-77013"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A cylindrical Service Module (SM)", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "beer festival", "gender test", "Thomas Middleditch", "potassium hydroxide", "If a vehicle towing a trailer skids, the trailer can push the towing vehicle from behind until it spins the vehicle around and faces backwards", "T cell receptor", "generally paid on graduated scales, with income depending on experience", "fruit, vegetables and tomatoes", "Heading Out to the Highway", "James Bond, Moonraker Reboot", "$12.99", "Michael Oppenheimer", "England national team", "class of people within the four-class system was not an indication of their actual social power and wealth, but just entailed \"degrees of privilege\" to which they were entitled institutionally and legally", "Space is the Place", "Convention", "5,922", "June 4, 1931", "a 2016 science fiction psychological horror", "76ers", "the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "the Astrodome in Houston", "23 March 1991", "lily-of-the-valley", "Dallas", "Nairobi", "the chalk ridge line west of the Needles breached", "Anno 2053"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4358495670995671}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1142857142857143, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "retrieved_ids": ["mrqa_naturalquestions-train-27447", "mrqa_naturalquestions-train-1997", "mrqa_naturalquestions-train-57842", "mrqa_naturalquestions-train-49045", "mrqa_naturalquestions-train-1234", "mrqa_naturalquestions-train-16386", "mrqa_naturalquestions-train-84013", "mrqa_naturalquestions-train-28504", "mrqa_naturalquestions-train-66737", "mrqa_naturalquestions-train-62232", "mrqa_naturalquestions-train-87842", "mrqa_naturalquestions-train-77279", "mrqa_naturalquestions-train-42615", "mrqa_naturalquestions-train-15479", "mrqa_naturalquestions-train-3371", "mrqa_naturalquestions-train-28976", "mrqa_naturalquestions-train-78716", "mrqa_naturalquestions-train-4191", "mrqa_squad-validation-9827", "mrqa_naturalquestions-train-6794", "mrqa_naturalquestions-train-86700", "mrqa_naturalquestions-train-8016", "mrqa_naturalquestions-train-78633", "mrqa_naturalquestions-train-17749", "mrqa_naturalquestions-train-18600", "mrqa_naturalquestions-train-54884", "mrqa_naturalquestions-train-8443", "mrqa_naturalquestions-train-38564", "mrqa_naturalquestions-train-41744", "mrqa_naturalquestions-train-67956", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-train-26679"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "\" Boston Herald\" Rumor Clinic", "1967", "legprints in the Sand", "the twelfth most populous city in the United States", "115", "bridge", "is an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "bass", "Tevye", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "korea", "bridge", "Yunnan- Fu", "Mumbai, India", "Broken Hill and Sydney", "2005", "forgiveness was God's alone to grant", "\"The Doctor's Daughter\"", "bridge", "bridge", "The project must adhere to zoning and building code requirements. Constructing a project that fails to adhere to codes does not benefit the owner", "1877", "paternal great-grandmother", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm and energy", "pasternak", "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking", "Arkansas", "Telemark"], "metric_results": {"EM": 0.125, "QA-F1": 0.18813055120167188}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.08, 0.0, 0.0, 0.12121212121212123, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-69923", "mrqa_naturalquestions-train-40825", "mrqa_naturalquestions-train-30790", "mrqa_naturalquestions-train-69324", "mrqa_naturalquestions-train-22", "mrqa_squad-validation-2509", "mrqa_naturalquestions-train-74971", "mrqa_naturalquestions-train-33614", "mrqa_naturalquestions-train-31176", "mrqa_naturalquestions-train-56239", "mrqa_naturalquestions-train-21835", "mrqa_naturalquestions-train-81760", "mrqa_hotpotqa-validation-1168", "mrqa_naturalquestions-train-9941", "mrqa_triviaqa-validation-1085", "mrqa_naturalquestions-train-36373", "mrqa_naturalquestions-train-53137", "mrqa_hotpotqa-validation-4430", "mrqa_naturalquestions-train-22543", "mrqa_squad-validation-6835", "mrqa_naturalquestions-train-70684", "mrqa_naturalquestions-train-63786", "mrqa_naturalquestions-train-34336", "mrqa_naturalquestions-train-39546", "mrqa_naturalquestions-train-48787", "mrqa_naturalquestions-train-70203", "mrqa_naturalquestions-train-42290", "mrqa_naturalquestions-train-83840", "mrqa_squad-validation-7301", "mrqa_naturalquestions-train-21786", "mrqa_naturalquestions-train-3439", "mrqa_naturalquestions-train-35493"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Speaker", "wartime", "Threatening government officials", "Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "boulangere", "0.2 inhabitants per square kilometre", "tintment", "France", "Ian Paisley", "bataan", "euro", "us", "the United States", "in the stratosphere, the sun's ultraviolet radiation is strong enough to cause the homolytic cleavage of the C - Cl bond", "in The Nursery Rhymes of England ( London and New York, c. 1886 ), by James Halliwell - Phillipps", "St. Louis Rams", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "juba", "pole", "Johnny Darrell", "carotid artery disease", "a Belgian law requiring all margarine to be in cube shaped packages", "Euler's totient function", "ear canal", "the set of all connected graphs", "Busiest airports in the United States by international passenger traffic", "red", "Subaru DL", "Kurt Vonnegut", "Rapunzel"], "metric_results": {"EM": 0.09375, "QA-F1": 0.14748208662682347}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526315789473685, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.5, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "retrieved_ids": ["mrqa_naturalquestions-train-56738", "mrqa_naturalquestions-train-5310", "mrqa_naturalquestions-train-79272", "mrqa_naturalquestions-train-79224", "mrqa_naturalquestions-train-43685", "mrqa_naturalquestions-train-42329", "mrqa_naturalquestions-train-33877", "mrqa_naturalquestions-train-24619", "mrqa_triviaqa-validation-7669", "mrqa_naturalquestions-train-34757", "mrqa_naturalquestions-train-44640", "mrqa_naturalquestions-train-54155", "mrqa_naturalquestions-train-83537", "mrqa_naturalquestions-train-50101", "mrqa_naturalquestions-train-26675", "mrqa_naturalquestions-train-26143", "mrqa_naturalquestions-train-57327", "mrqa_naturalquestions-train-5662", "mrqa_naturalquestions-train-78372", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-train-14311", "mrqa_naturalquestions-train-32728", "mrqa_naturalquestions-train-37756", "mrqa_naturalquestions-train-84177", "mrqa_naturalquestions-train-59351", "mrqa_naturalquestions-train-54770", "mrqa_naturalquestions-train-59936", "mrqa_naturalquestions-train-38390", "mrqa_squad-validation-4648", "mrqa_naturalquestions-train-44365", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-train-33519"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher", "supply and demand", "Emma Watson", "brain", "butterfly", "Washington Redskins", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "William Howard Ashton", "wyo history", "high and persistent unemployment", "Miami metropolitan area", "Lee Byung-hun", "to combine keys which are usually kept separate", "fought 1642-1651", "from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "income share of the top 20 percent (the rich) increases", "Beauty and the Beast", "South Africa", "Scotty Grainger Jr.", "El Alamo", "a seal illegally is broken", "UMC", "Brian Liesegang", "Don Hahn", "Port Moresby", "afghanistan", "National Association for the Advancement of Colored People", "1963\u20131989", "5 star hotel", "norman hartan", "beginning in season three", "6500 - 1500 BC"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3728494162087912}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.4, 0.5, 0.0, 0.0625, 0.0, 0.4615384615384615, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "retrieved_ids": ["mrqa_naturalquestions-train-51273", "mrqa_naturalquestions-train-56639", "mrqa_naturalquestions-train-44809", "mrqa_naturalquestions-train-28978", "mrqa_naturalquestions-train-5205", "mrqa_naturalquestions-train-63826", "mrqa_naturalquestions-train-41947", "mrqa_naturalquestions-train-42966", "mrqa_naturalquestions-train-16637", "mrqa_naturalquestions-train-62103", "mrqa_naturalquestions-train-72269", "mrqa_naturalquestions-train-82577", "mrqa_naturalquestions-train-18271", "mrqa_triviaqa-validation-1085", "mrqa_naturalquestions-train-40181", "mrqa_naturalquestions-train-37649", "mrqa_naturalquestions-train-80808", "mrqa_naturalquestions-train-13607", "mrqa_naturalquestions-train-41461", "mrqa_naturalquestions-train-22759", "mrqa_naturalquestions-train-22844", "mrqa_naturalquestions-train-37820", "mrqa_naturalquestions-train-55656", "mrqa_triviaqa-validation-1588", "mrqa_naturalquestions-train-43061", "mrqa_naturalquestions-train-23227", "mrqa_naturalquestions-train-83020", "mrqa_naturalquestions-train-39682", "mrqa_naturalquestions-train-83568", "mrqa_naturalquestions-train-36066", "mrqa_naturalquestions-train-55431", "mrqa_naturalquestions-train-37894"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["NBC", "Amber Laura Heard", "Uranus", "president rudolf", "the Cobham\u2013Edmonds thesis", "human, or humanoid aliens", "Best Male Pop Vocal Performance", "March 2012", "jazz", "Muhammad Ali", "Beyonc\u00e9 and Bruno Mars", "Gibraltar", "plead guilty to one misdemeanor count and receive no jail time", "Julius Caesar", "2% of its population. Most are concentrated in Alsace in northeast France and the C\u00e9vennes mountain region in the south, who still regard themselves as Huguenots to this day", "1898", "James Halliday", "decision problem", "elizabeth lecouvreur", "the lungs", "Miasma theory", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges (sub-ranges of the Rocky Mountains)", "red", "The U.S. state of Georgia", "norman dioica", "$12", "a flat rate of 20 % as of April 2015", "love is all around", "a member of Davies' team ( Roger Scantlebury) met Lawrence Roberts at the 1967 ACM Symposium on Operating System Principles and suggested it for use in the ARPANET", "roughly west", "Sudan"], "metric_results": {"EM": 0.125, "QA-F1": 0.21438492063492065}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 0.4, 0.0, 1.0, 0.2222222222222222, 1.0, 0.2222222222222222, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-152", "mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-4115", "mrqa_squad-validation-3060", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_hotpotqa-validation-1118", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_naturalquestions-validation-2323", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-74033", "mrqa_naturalquestions-train-6562", "mrqa_naturalquestions-train-2987", "mrqa_naturalquestions-train-8053", "mrqa_naturalquestions-train-64161", "mrqa_naturalquestions-train-66924", "mrqa_naturalquestions-train-43426", "mrqa_naturalquestions-train-80038", "mrqa_naturalquestions-train-1761", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-train-62757", "mrqa_naturalquestions-train-44987", "mrqa_naturalquestions-train-40399", "mrqa_naturalquestions-train-86254", "mrqa_naturalquestions-train-49001", "mrqa_naturalquestions-train-43959", "mrqa_naturalquestions-train-47955", "mrqa_naturalquestions-train-50299", "mrqa_naturalquestions-train-33080", "mrqa_naturalquestions-train-9667", "mrqa_naturalquestions-train-52409", "mrqa_naturalquestions-train-61866", "mrqa_naturalquestions-train-21550", "mrqa_naturalquestions-train-1696", "mrqa_naturalquestions-train-13271", "mrqa_naturalquestions-train-22026", "mrqa_naturalquestions-train-43169", "mrqa_naturalquestions-train-17267", "mrqa_naturalquestions-train-54845", "mrqa_naturalquestions-train-87685", "mrqa_naturalquestions-train-51737", "mrqa_naturalquestions-train-6993"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["Southern Pacific", "three of his ribs were broken", "7 December 2000", "Post Alley under Pike Place Market", "mother-of-pearl", "February 20, 1978", "haggis", "Walter Mondale", "96", "De Inventione by Marcus Tullius Cicero", "japan", "a black background representing the circle with glossy gold letters", "Jericho in the Levant region", "around 11 miles (18 km) south of San Jose", "Spotty Dog", "Rumplestiltskin", "Harry Kane", "small marsupials", "the Congress Hall in the Palace of Culture and Science", "riper grapes", "1991", "india", "7 January 1936", "lifetime protection", "twenty- three", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA", "education, sanitation, and traffic control", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Christian Poulsen", "mistreatment from government officials", "william herschel", "Boston, Massachusetts"], "metric_results": {"EM": 0.21875, "QA-F1": 0.25002705627705624}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_triviaqa-validation-2524"], "retrieved_ids": ["mrqa_naturalquestions-train-38175", "mrqa_naturalquestions-train-25259", "mrqa_naturalquestions-train-56545", "mrqa_naturalquestions-train-14066", "mrqa_naturalquestions-train-64300", "mrqa_naturalquestions-train-53824", "mrqa_naturalquestions-train-23412", "mrqa_hotpotqa-validation-4164", "mrqa_naturalquestions-train-15206", "mrqa_naturalquestions-train-77503", "mrqa_naturalquestions-train-9261", "mrqa_naturalquestions-train-74174", "mrqa_naturalquestions-train-71549", "mrqa_naturalquestions-train-6576", "mrqa_naturalquestions-train-72123", "mrqa_naturalquestions-train-81906", "mrqa_naturalquestions-train-57281", "mrqa_naturalquestions-train-39172", "mrqa_naturalquestions-train-21534", "mrqa_naturalquestions-train-79039", "mrqa_naturalquestions-train-20807", "mrqa_naturalquestions-train-34876", "mrqa_naturalquestions-train-86423", "mrqa_naturalquestions-train-86165", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-train-36809", "mrqa_naturalquestions-train-29557", "mrqa_naturalquestions-train-82495", "mrqa_naturalquestions-train-20604", "mrqa_naturalquestions-train-50721", "mrqa_naturalquestions-train-78270", "mrqa_naturalquestions-train-13537"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the decision problem in Presburger arithmetic", "Dan Stevens", "New England", "Etienne de Mestre", "dragon", "The primary catalyst for secession was slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "an excessively virtuous person, a do - gooder", "the longest rotation period ( 243 days )", "gathering money from the public", "Ethan", "when commissioned to purchase their required uniform items", "Jeff Meldrum", "741 weeks", "Phil Archer", "Shoshone, his mother tongue, and other western American Indian languages", "The Paris Sisters", "Suez Canal", "60", "his laboratory", "the fact that there is no revising chamber", "women", "the points of algebro-geometric objects", "most of the items in the collection, unless those were newly accessioned into the collection", "money is free floating and depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "strychnine", "Texas", "The early modern period began approximately in the early 16th century ; notable historical milestones included the European Renaissance, the Age of Discovery, and the Protestant Reformation.", "13 June 2003", "two", "Darleen Carr", "with each heartbeat"], "metric_results": {"EM": 0.125, "QA-F1": 0.24288007743890097}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.22222222222222224, 0.11764705882352941, 0.0, 0.0, 1.0, 0.07692307692307693, 0.0, 0.0, 0.25, 0.47058823529411764, 0.0, 0.4, 0.25, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6, 0.0, 0.0, 0.5, 0.06060606060606061, 1.0, 0.0, 0.3846153846153846, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1862", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-1660", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "retrieved_ids": ["mrqa_naturalquestions-train-87863", "mrqa_naturalquestions-train-62860", "mrqa_squad-validation-4417", "mrqa_naturalquestions-train-57392", "mrqa_naturalquestions-train-38199", "mrqa_naturalquestions-train-36537", "mrqa_naturalquestions-train-56778", "mrqa_naturalquestions-train-3960", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-train-60751", "mrqa_naturalquestions-train-9637", "mrqa_naturalquestions-train-63996", "mrqa_naturalquestions-train-53255", "mrqa_naturalquestions-train-82739", "mrqa_naturalquestions-train-57343", "mrqa_naturalquestions-train-5918", "mrqa_naturalquestions-train-5497", "mrqa_squad-validation-8190", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-train-432", "mrqa_naturalquestions-train-73852", "mrqa_naturalquestions-train-84916", "mrqa_naturalquestions-train-77257", "mrqa_naturalquestions-train-88014", "mrqa_naturalquestions-train-29462", "mrqa_naturalquestions-train-13834", "mrqa_naturalquestions-train-66530", "mrqa_naturalquestions-train-20514", "mrqa_naturalquestions-train-26005", "mrqa_naturalquestions-train-20821", "mrqa_naturalquestions-train-61847", "mrqa_naturalquestions-train-82361"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Republic of Taiwan", "Dan Conner", "Berlin", "Lee Harvey Oswald", "Lois Mae Green", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "keskes", "during the 1980s", "John M. Grunsfeld", "New Orleans", "a man who makes potions in a traveling show", "2000", "a couple months of age", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "relative units of force and mass", "woman", "two", "August 10, 1933", "spanning the Golden Gate, the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "Sochi, Russia", "those who already hold wealth", "bilingual German author B. Traven", "Finding Nemo", "the subject of unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "oil prices", "beavers", "264,152", "Princeton, New Jersey", "the German Empire", "high pressure or an electric current"], "metric_results": {"EM": 0.25, "QA-F1": 0.3809921438210912}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 0.9230769230769231, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3157894736842105, 0.0, 1.0, 0.7142857142857143, 1.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_naturalquestions-validation-3108", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-30173", "mrqa_naturalquestions-train-47736", "mrqa_naturalquestions-train-45753", "mrqa_naturalquestions-train-6688", "mrqa_naturalquestions-train-61742", "mrqa_naturalquestions-train-32711", "mrqa_naturalquestions-train-16417", "mrqa_naturalquestions-train-51701", "mrqa_naturalquestions-train-84818", "mrqa_naturalquestions-train-15774", "mrqa_naturalquestions-train-72679", "mrqa_naturalquestions-train-14852", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-43117", "mrqa_naturalquestions-train-6647", "mrqa_naturalquestions-train-72581", "mrqa_naturalquestions-train-68048", "mrqa_naturalquestions-train-56194", "mrqa_naturalquestions-train-25404", "mrqa_naturalquestions-train-57524", "mrqa_naturalquestions-train-73752", "mrqa_naturalquestions-train-63952", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-train-1701", "mrqa_naturalquestions-train-86203", "mrqa_naturalquestions-train-4728", "mrqa_naturalquestions-train-72268", "mrqa_naturalquestions-train-59745", "mrqa_naturalquestions-train-26820", "mrqa_naturalquestions-train-46020", "mrqa_naturalquestions-train-84899", "mrqa_naturalquestions-train-32620"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "ice dancing", "Isabella", "corgis", "14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "The antichrist of 2 Thessalonians 2", "Bacon", "Charlton Heston", "anti-inflammatory molecules", "vUHMOaD/JVI", "Jeff Garcia", "a tradeable entity used to avoid the inconveniences of a pure barter system", "may have the force of law, if based on the authority derived from statute or the Constitution itself", "son et lumi\u00e8re", "because the Uighurs surrendered to the Mongols first", "Sochi, Russia", "right", "from the Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "After the determination of responsibility for the accident was complex, the review board concluded that \"deficiencies existed in Command Module design, workmanship and quality control\"", "women\u2019s vote", "shorthand typist", "30", "the Secret Intelligence Service", "100 billion", "your son", "photolysis", "4 - inch", "Queen City", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"], "metric_results": {"EM": 0.25, "QA-F1": 0.3458514492753623}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.375, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.125, 0.0, 0.3333333333333333, 1.0, 0.0, 0.17391304347826084, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.16]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-1571", "mrqa_hotpotqa-validation-1453", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "retrieved_ids": ["mrqa_naturalquestions-train-87879", "mrqa_naturalquestions-train-42076", "mrqa_naturalquestions-train-82613", "mrqa_naturalquestions-train-27936", "mrqa_naturalquestions-train-44616", "mrqa_naturalquestions-train-10895", "mrqa_naturalquestions-train-21346", "mrqa_naturalquestions-train-41659", "mrqa_naturalquestions-train-57105", "mrqa_naturalquestions-train-36750", "mrqa_naturalquestions-train-75010", "mrqa_naturalquestions-train-6387", "mrqa_naturalquestions-train-85773", "mrqa_naturalquestions-train-23881", "mrqa_naturalquestions-train-70361", "mrqa_naturalquestions-train-23512", "mrqa_naturalquestions-train-5859", "mrqa_naturalquestions-train-37010", "mrqa_naturalquestions-train-50049", "mrqa_naturalquestions-train-1538", "mrqa_naturalquestions-train-77105", "mrqa_naturalquestions-train-25965", "mrqa_squad-validation-1249", "mrqa_naturalquestions-train-87100", "mrqa_naturalquestions-train-29325", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-train-3152", "mrqa_naturalquestions-train-80763", "mrqa_naturalquestions-train-79498", "mrqa_naturalquestions-train-83227", "mrqa_naturalquestions-train-62062", "mrqa_naturalquestions-train-14240"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["charged particle beam weapons", "Kishore Kumar", "Gaels", "Three-card brag", "d\u00edsabl\u00f3t", "cave lion", "Russian film industry", "delta growth", "Washington metropolitan area", "newly formed vesicles", "User State Migration Tool ( USMT )", "Ordos City China Science Flying Universe Science and Technology Co.", "ferdinand manscioni", "PPG Paints Arena, Pittsburgh, Pennsylvania", "city", "Section 30 of the Teaching Council Act 2001", "agnes Moorehead as the Goose", "In 1984, Congress passed the National Minimum Drinking Age Act, which required states to raise their ages for purchase and public possession to 21 by October 1986 or lose 10 % of their federal highway funds", "huge-LQG", "Monsoon", "Romansh", "elisabeth II", "Perth's number one rating radio station, MIX 94.5.", "Q Branch (or later Q Division) the fictional research and development division of the British Secret Service", "Paul and Timothy", "An Inquiry into the Nature and Causes of the Wealth of Nations", "Margaret Pellegrini (nee Williams)", "Bobbi Kristina Brown", "Hugo Award", "conservative", "a shepherd", "Elvis Presley"], "metric_results": {"EM": 0.0, "QA-F1": 0.10730320027195027}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.1818181818181818, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.1111111111111111, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.125, 0.0, 0.30769230769230765, 0.0, 0.4, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_squad-validation-9355", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "retrieved_ids": ["mrqa_naturalquestions-train-9804", "mrqa_naturalquestions-train-16807", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-train-14248", "mrqa_naturalquestions-train-81030", "mrqa_naturalquestions-train-45846", "mrqa_naturalquestions-train-44073", "mrqa_naturalquestions-train-11599", "mrqa_naturalquestions-train-81278", "mrqa_naturalquestions-train-5926", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-train-81711", "mrqa_naturalquestions-train-84997", "mrqa_naturalquestions-train-5737", "mrqa_naturalquestions-train-48270", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-4019", "mrqa_naturalquestions-train-75385", "mrqa_naturalquestions-train-69378", "mrqa_naturalquestions-train-53108", "mrqa_naturalquestions-train-23459", "mrqa_naturalquestions-train-28973", "mrqa_naturalquestions-train-25311", "mrqa_naturalquestions-train-6321", "mrqa_naturalquestions-train-35634", "mrqa_naturalquestions-train-67468", "mrqa_naturalquestions-train-84177", "mrqa_naturalquestions-train-4660", "mrqa_naturalquestions-train-55936", "mrqa_triviaqa-validation-6944", "mrqa_naturalquestions-train-35789", "mrqa_naturalquestions-train-9637"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["david carradine", "sometimes contains pasta ( usually cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "king edris stannus", "independence from the Duke of Savoy", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "paid professionals", "a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "a large Danish shipping company that operates passenger and freight services across northern Europe", "a midfielder", "jonathan", "where the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position )", "carbohydrate monosaccharides, along with glucose and galactose, that are absorbed directly into blood during digestion", "the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "public and private collections in the United States and abroad", "by functions ; Introverted Sensing ( Si ), Extroverted Thinking ( Te ), Introverted Feeling ( Fi ) and Extrovert Intuition ( Ne )", "The date of the poll may be varied by up to one month either way by the Monarch on the proposal of the Presiding Officer", "green", "the appropriateness of the drug choice, dose, route, frequency, and duration", "jupiter", "navigator and expedition leader", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "bobby jules", "The Education Service Contracting scheme of the government", "Pernambuco wood", "The benefits of good works could be obtained by donating money to the church", "colonies", "two forces, one pointing north, and one pointing east", "a committee of the Parliament can present a bill in one of the areas under its remit", "Jack Murphy Stadium", "the time and space hierarchy theorems"], "metric_results": {"EM": 0.03125, "QA-F1": 0.16569542948219418}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.2857142857142857, 0.14285714285714288, 0.0, 0.0, 0.7567567567567568, 0.0, 0.0, 0.5, 0.0, 0.33333333333333337, 0.09523809523809523, 0.0, 0.8888888888888888, 0.0, 0.0, 0.13333333333333333, 0.0, 0.1818181818181818, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_squad-validation-9452", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-64615", "mrqa_naturalquestions-train-8187", "mrqa_naturalquestions-train-42553", "mrqa_naturalquestions-train-36626", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-train-34418", "mrqa_naturalquestions-train-42003", "mrqa_squad-validation-9036", "mrqa_naturalquestions-train-50040", "mrqa_naturalquestions-train-83246", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-28007", "mrqa_naturalquestions-train-72779", "mrqa_naturalquestions-train-52986", "mrqa_naturalquestions-train-18874", "mrqa_naturalquestions-train-76321", "mrqa_naturalquestions-train-87801", "mrqa_naturalquestions-train-79459", "mrqa_naturalquestions-train-69388", "mrqa_naturalquestions-train-33446", "mrqa_squad-validation-8924", "mrqa_naturalquestions-train-30851", "mrqa_naturalquestions-train-16915", "mrqa_naturalquestions-train-67805", "mrqa_naturalquestions-train-33526", "mrqa_naturalquestions-train-66039", "mrqa_naturalquestions-train-34312", "mrqa_naturalquestions-train-39175", "mrqa_naturalquestions-train-37109", "mrqa_naturalquestions-train-40718", "mrqa_naturalquestions-train-64538", "mrqa_naturalquestions-train-28169"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["Niagara Falls", "letters", "Indiana", "temple building", "French", "a sailor coming home from a round trip", "domain names www.example.com and example.com", "when the immune system is less active than normal", "baldi", "personal care products", "bird", "Rigoletto", "land area", "third-most abundant element in the universe", "furniture chain", "216 countries and territories around the world", "football", "Nicholas Stone", "Algernod Lanier Washington", "the Outfield", "Croatia", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong", "railway locomotives", "eddie turkey", "third quarter ( also known as last quarter )", "australia", "James Henry Breasted, mathematician Alberto Calder\u00f3n, Nobel prize winning economist and classical liberalism defender Friedrich Hayek, meteorologist Ted Fujita, chemists Glenn T. Seaborg, the developer of the actinide concept", "Tennessee", "many areas of technology incidental to rocketry and manned spaceflight", "eve", "about 615 square kilometers or 237 square miles", "magi"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1969624582289056}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.25, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.4, 0.4210526315789474, 0.6666666666666666, 0.2222222222222222, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_naturalquestions-validation-7641", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "retrieved_ids": ["mrqa_naturalquestions-train-34174", "mrqa_naturalquestions-train-56651", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-29909", "mrqa_naturalquestions-train-57687", "mrqa_naturalquestions-train-44386", "mrqa_naturalquestions-train-75241", "mrqa_naturalquestions-train-31090", "mrqa_naturalquestions-train-68523", "mrqa_naturalquestions-train-56048", "mrqa_hotpotqa-validation-2484", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-train-47294", "mrqa_naturalquestions-train-17660", "mrqa_naturalquestions-train-25091", "mrqa_naturalquestions-train-12128", "mrqa_naturalquestions-train-85584", "mrqa_naturalquestions-train-83169", "mrqa_naturalquestions-train-85774", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-train-36118", "mrqa_naturalquestions-train-44378", "mrqa_naturalquestions-train-7947", "mrqa_naturalquestions-train-42843", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-train-63860", "mrqa_naturalquestions-train-56194", "mrqa_naturalquestions-train-68447", "mrqa_naturalquestions-train-71714", "mrqa_naturalquestions-train-23387", "mrqa_naturalquestions-train-67341", "mrqa_naturalquestions-train-985"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["Wakanda", "great classroom evaluations, high test scores and for high success at their overall school", "2003", "cricket", "mexico", "campaign setting", "2003", "867 feet", "the Hebrew name Immanu'el", "\u00f7", "Christopher Lee as Count Dooku / Darth Tyranus", "14th", "chari bari ruchi-pip peri pembo", "all health care settings", "increased patient health outcomes and decreased costs to the health care system", "Every Good Boy Deserves Favour", "Gabriel Alberto Azucena", "12951 / 52 Mumbai Rajdhani Express considering average speed including halts 12049 / 50 : Agra Cantonment - H. Nizamuddin Gatimaan Express - maximum speed 160 km / h", "Italian : Scalinata di Trinit\u00e0 dei Monti", "May 18, 2010", "Estelle Sylvia Pankhurst", "euro Union", "philosopher, jurist, orator, and author", "city status", "Ministry of Corporate Affairs", "Irish", "ancient cult activity", "Penguin Classics", "oxygen", "1569 km", "a genuine love of our neighbors as ourselves", "chorale cantatas"], "metric_results": {"EM": 0.125, "QA-F1": 0.2529970577667946}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.2857142857142857, 0.10526315789473682, 0.19999999999999998, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 0.7272727272727273, 0.0, 0.0, 1.0, 0.39999999999999997, 0.0, 0.0, 0.3571428571428571, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951"], "retrieved_ids": ["mrqa_naturalquestions-train-35181", "mrqa_naturalquestions-train-70252", "mrqa_naturalquestions-train-80855", "mrqa_naturalquestions-train-81557", "mrqa_naturalquestions-train-86571", "mrqa_naturalquestions-train-18806", "mrqa_hotpotqa-validation-1118", "mrqa_naturalquestions-train-33124", "mrqa_naturalquestions-train-1310", "mrqa_naturalquestions-train-24187", "mrqa_naturalquestions-train-27018", "mrqa_squad-validation-3708", "mrqa_squad-validation-2987", "mrqa_naturalquestions-train-24217", "mrqa_naturalquestions-train-31442", "mrqa_naturalquestions-train-42262", "mrqa_naturalquestions-train-52320", "mrqa_naturalquestions-train-36718", "mrqa_naturalquestions-train-10860", "mrqa_naturalquestions-train-67676", "mrqa_naturalquestions-train-56815", "mrqa_naturalquestions-train-82514", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-train-80177", "mrqa_naturalquestions-train-1669", "mrqa_naturalquestions-train-22013", "mrqa_naturalquestions-train-32924", "mrqa_naturalquestions-train-77220", "mrqa_naturalquestions-train-8986", "mrqa_naturalquestions-train-16354", "mrqa_naturalquestions-train-76888", "mrqa_squad-validation-5345"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["spillane", "the 2009 St. Louis Rams", "perique", "under `` the immortal Hawke ''", "death penalty", "stout man with a \" Double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "belfast", "rich Fisher king", "Mangal Pandey of the 34th BNI", "oppidum Ubiorum", "Hoss", "four of the 50 states of the United States", "sweden", "the eighth series", "Pebble Beach", "Los Angeles", "French", "Gareth", "LOVE Radio", "Boston Red Sox", "the state supreme court", "richard burman", "Dan Fogelman", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "People", "Cashin' In", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Operation Neptune", "Isthmus of Corinth"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3092319139194139}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.07142857142857142, 0.5, 1.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.07142857142857142, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-14811", "mrqa_naturalquestions-train-51475", "mrqa_naturalquestions-train-67193", "mrqa_naturalquestions-train-87124", "mrqa_naturalquestions-train-267", "mrqa_naturalquestions-train-63728", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-train-41811", "mrqa_naturalquestions-train-51466", "mrqa_naturalquestions-train-86780", "mrqa_naturalquestions-train-9653", "mrqa_naturalquestions-train-45823", "mrqa_squad-validation-6961", "mrqa_naturalquestions-train-63273", "mrqa_squad-validation-5360", "mrqa_naturalquestions-train-81843", "mrqa_triviaqa-validation-3647", "mrqa_naturalquestions-train-84923", "mrqa_naturalquestions-train-30761", "mrqa_naturalquestions-train-9917", "mrqa_naturalquestions-train-8225", "mrqa_hotpotqa-validation-2399", "mrqa_naturalquestions-train-1219", "mrqa_naturalquestions-train-79525", "mrqa_naturalquestions-validation-6844", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-train-14238", "mrqa_naturalquestions-train-42478", "mrqa_naturalquestions-train-76208", "mrqa_naturalquestions-train-15621", "mrqa_naturalquestions-train-42599"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "January 16", "transgender", "electric lighting", "used their knowledge of Native American languages as a basis to transmit coded messages", "Sir Isaac Newton", "electromagnetic theory", "Hong Kong First Division League", "pre-Raphaelite", "Elizabeth Weber", "an earlier Funcom game, \"The Secret World\"", "hundreds", "Waiting for Guffman", "1979", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "partial funding", "a 5% abv draught beer", "production of more of at least one good without sacrificing the production of any other good", "Chu'Tsai", "Liz", "least onerous", "lake como", "Grissom, White, and Chaffee", "multinational retail corporation", "passion fruit", "Bharata Muni", "sand grains cause a scrubbing noise as they rub against each other when walked on", "golf tournament", "parts of the air in the vessel were converted into the classical element fire", "emperor of Austria"], "metric_results": {"EM": 0.34375, "QA-F1": 0.37487568107180175}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 1.0, 0.06666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0689655172413793, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "retrieved_ids": ["mrqa_naturalquestions-train-25258", "mrqa_squad-validation-2531", "mrqa_naturalquestions-train-82254", "mrqa_naturalquestions-train-71434", "mrqa_naturalquestions-train-68513", "mrqa_naturalquestions-train-33919", "mrqa_naturalquestions-train-76897", "mrqa_naturalquestions-train-86685", "mrqa_squad-validation-7711", "mrqa_naturalquestions-train-18794", "mrqa_triviaqa-validation-6107", "mrqa_squad-validation-5249", "mrqa_naturalquestions-train-55663", "mrqa_naturalquestions-train-76742", "mrqa_naturalquestions-train-35448", "mrqa_naturalquestions-train-85118", "mrqa_naturalquestions-train-88000", "mrqa_naturalquestions-train-66000", "mrqa_naturalquestions-train-56163", "mrqa_naturalquestions-train-27771", "mrqa_naturalquestions-train-13655", "mrqa_naturalquestions-train-13557", "mrqa_naturalquestions-train-85064", "mrqa_naturalquestions-train-13537", "mrqa_naturalquestions-train-65748", "mrqa_naturalquestions-train-15645", "mrqa_naturalquestions-train-51094", "mrqa_naturalquestions-train-16265", "mrqa_naturalquestions-train-13547", "mrqa_naturalquestions-train-15426", "mrqa_naturalquestions-train-45167", "mrqa_naturalquestions-train-69936"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["gerry", "horse racing", "Burnley", "1939", "Styal Mill", "William Jennings Bryan", "Milk Barn Animation", "when they enter the army during initial entry training", "moral tale", "They announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled", "l Leeds", "a star", "260", "cuba", "often social communities with considerable face-to-face interaction among members", "Landon Jones", "monophyletic", "take the Sokanu Career Test", "a liturgical setting of the Lord's Prayer", "a pH indicator, a color marker, and a dye", "about 50% oxygen composition at standard pressure", "63,182,000", "John Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Western Kentucky University", "cuba", "appearing as Jude in the musical romance drama film \" Across the Universe\" (2007)", "cuba", "her arranged marriage to Chino, a friend of Bernardo's", "Jocelyn Flores", "downward pressure on wages"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2818014705882353}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-5305", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-2092", "mrqa_squad-validation-7182"], "retrieved_ids": ["mrqa_naturalquestions-train-43133", "mrqa_naturalquestions-train-22308", "mrqa_naturalquestions-train-79359", "mrqa_naturalquestions-train-87923", "mrqa_squad-validation-6442", "mrqa_naturalquestions-train-33619", "mrqa_naturalquestions-train-15102", "mrqa_naturalquestions-train-73216", "mrqa_naturalquestions-train-27199", "mrqa_naturalquestions-train-15386", "mrqa_naturalquestions-train-6132", "mrqa_naturalquestions-train-44895", "mrqa_squad-validation-2987", "mrqa_naturalquestions-train-67993", "mrqa_naturalquestions-train-35663", "mrqa_naturalquestions-train-60098", "mrqa_naturalquestions-train-72503", "mrqa_naturalquestions-train-5211", "mrqa_naturalquestions-train-40618", "mrqa_naturalquestions-train-64524", "mrqa_naturalquestions-train-73170", "mrqa_naturalquestions-train-77503", "mrqa_hotpotqa-validation-5637", "mrqa_naturalquestions-train-26144", "mrqa_naturalquestions-train-26395", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-train-75813", "mrqa_naturalquestions-train-17322", "mrqa_naturalquestions-train-75513", "mrqa_naturalquestions-train-84756", "mrqa_naturalquestions-train-7685", "mrqa_naturalquestions-train-63368"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["Anthony John Herrera", "Good Kid, M. a.A.D City", "morgan park", "\"interventive\" conservation", "3", "The Methodist Church", "Ray Charles", "a thousand years", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "social Democratic", "morgan", "Anfernee Simons", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "the 1964 Republican National Convention in San Francisco, California", "every good work designed to attract God's favor is a sin", "annuity", "twin sister", "Frank Theodore `` Ted '' Levine", "every good work designed to attract God's favor is a sin", "France", "1918", "espresso", "halal meat", "Arthur Russell (musician)", "to be memorised by the people themselves", "Wylie Draper", "political role for Islam", "the university's off-campus rental policies", "hockey greats Bobby Hull and Dennis Hull", "New England Patriots", "famine, and weather"], "metric_results": {"EM": 0.125, "QA-F1": 0.27386138167388163}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.22222222222222218, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.18181818181818182, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.2857142857142857, 0.2666666666666667, 0.4, 1.0, 0.26666666666666666, 0.0, 0.8571428571428571]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-15074", "mrqa_naturalquestions-train-6458", "mrqa_naturalquestions-train-22873", "mrqa_naturalquestions-train-12030", "mrqa_naturalquestions-train-31969", "mrqa_naturalquestions-train-47238", "mrqa_naturalquestions-train-73662", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-8660", "mrqa_naturalquestions-train-36077", "mrqa_naturalquestions-train-29671", "mrqa_naturalquestions-train-49973", "mrqa_naturalquestions-train-35698", "mrqa_naturalquestions-train-77752", "mrqa_naturalquestions-train-44841", "mrqa_naturalquestions-train-26439", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-train-31146", "mrqa_naturalquestions-train-3776", "mrqa_naturalquestions-train-87641", "mrqa_naturalquestions-train-32401", "mrqa_naturalquestions-train-27288", "mrqa_naturalquestions-train-6205", "mrqa_naturalquestions-train-34220", "mrqa_naturalquestions-train-48976", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-train-55917", "mrqa_naturalquestions-train-38376", "mrqa_naturalquestions-train-68523", "mrqa_naturalquestions-train-81496", "mrqa_naturalquestions-train-17900", "mrqa_naturalquestions-train-18600"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "AOL", "Timur", "R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "georgia", "1937", "improved markedly", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock. He had to work at various electrical repair jobs and even as a ditch digger for $2 per day.", "Marxist", "inspired by both his professors at the Palack\u00fd University, Olomouc ( Friedrich Franz and Johann Karl Nestler ), and his colleagues at the monastery ( such as Franz Diebl ) to study variation in plants", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades.", "3,600", "State Street", "Abdulaziz Al Saud", "44 hectares", "georgia cukor", "horakhty", "Lawton Mainor Chiles Jr.", "In the episode `` Kobol's Last Gleaming ''", "Wisconsin", "uneven trade agreements", "tea or porridge with bread, chapati, mahamri, boiled sweet potatoes or yams. Ugali with vegetables, sour milk, meat, fish or any other stew is generally eaten by much of the population", "energy", "georgia", "Ruth Elizabeth \"Bette\" Davis", "georgia", "7 December 2004 to 29 September 2014"], "metric_results": {"EM": 0.125, "QA-F1": 0.245522750785211}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.13333333333333333, 0.0, 1.0, 0.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 1.0, 0.3636363636363636, 0.34782608695652173, 0.0, 0.0, 0.1111111111111111, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.5365853658536585, 0.0, 0.0, 1.0, 0.0, 0.6]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "retrieved_ids": ["mrqa_naturalquestions-train-6134", "mrqa_naturalquestions-train-29742", "mrqa_naturalquestions-train-7910", "mrqa_triviaqa-validation-3967", "mrqa_naturalquestions-train-37244", "mrqa_naturalquestions-train-25838", "mrqa_naturalquestions-train-48542", "mrqa_naturalquestions-train-62544", "mrqa_naturalquestions-train-10336", "mrqa_naturalquestions-train-33202", "mrqa_naturalquestions-train-43580", "mrqa_naturalquestions-train-74376", "mrqa_naturalquestions-train-12807", "mrqa_naturalquestions-train-74104", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-train-26550", "mrqa_naturalquestions-train-43221", "mrqa_naturalquestions-train-54120", "mrqa_naturalquestions-train-19886", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-train-71208", "mrqa_naturalquestions-train-33603", "mrqa_naturalquestions-train-53108", "mrqa_naturalquestions-train-33141", "mrqa_naturalquestions-train-26963", "mrqa_naturalquestions-train-45531", "mrqa_naturalquestions-train-47913", "mrqa_naturalquestions-train-47814", "mrqa_naturalquestions-train-15254", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-train-62574", "mrqa_naturalquestions-train-85324"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.14375, "QA-F1": 0.24853069275590253}, "overall_error_number": 1370, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.72875, "QA-F1": 0.7890064097153712}, "final_upstream_test": {"EM": 0.746, "QA-F1": 0.8478301692240184}}}