{"model_update_steps": 1960, "method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=123_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=123_ckpts/', replay_candidate_size=8, replay_frequency=1, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion", "john Leslie", "Virginia Wade", "The Dallas Lovers' Song", "the anterolateral system", "1966", "radioisotope thermoelectric generator", "the tin", "The Stock Market crash in New York led people to hoard their money ; as consumption fell, the American economy steadily contracted, 1929 - 32", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "computer researcher", "the most conspicuous courage in circumstances of extreme danger", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1347489675075882}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_naturalquestions-validation-4684", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "retrieved_ids": ["mrqa_naturalquestions-train-24991", "mrqa_naturalquestions-train-44616", "mrqa_naturalquestions-train-78116", "mrqa_naturalquestions-train-39052", "mrqa_naturalquestions-train-49122", "mrqa_naturalquestions-train-79419", "mrqa_naturalquestions-train-77303", "mrqa_naturalquestions-train-14852", "mrqa_naturalquestions-train-24281", "mrqa_naturalquestions-train-83262", "mrqa_naturalquestions-train-82530", "mrqa_naturalquestions-train-16330", "mrqa_naturalquestions-train-53515", "mrqa_naturalquestions-train-70465", "mrqa_naturalquestions-train-21150", "mrqa_naturalquestions-train-78234", "mrqa_naturalquestions-train-55313", "mrqa_naturalquestions-train-11290", "mrqa_naturalquestions-train-35128", "mrqa_naturalquestions-train-83424", "mrqa_naturalquestions-train-23124", "mrqa_naturalquestions-train-57794", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-train-42395", "mrqa_naturalquestions-train-32716", "mrqa_naturalquestions-train-28875", "mrqa_naturalquestions-train-26741", "mrqa_naturalquestions-train-74434", "mrqa_naturalquestions-train-76964", "mrqa_naturalquestions-train-79454", "mrqa_naturalquestions-train-21283", "mrqa_naturalquestions-train-39792"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2003", "diverse range of taxa", "marioneth and Llantisilly Rail Traction Company Limited", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit", "a student in the second year at a high school, college, or university", "Bothtec", "Terry Reid", "available information about climate change based on published sources", "Elgar", "North America", "Andr\u00e9 3000", "rookies", "Akhenaten", "President Theodore Roosevelt", "the fourth season", "four", "the Western Bloc ( the United States, its NATO allies and others )", "1970s", "CarmenCarmen", "Matt Winer", "1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.125, "QA-F1": 0.2834532297767592}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.9019607843137255, 0.18181818181818182, 1.0, 0.0, 0.1818181818181818, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "retrieved_ids": ["mrqa_naturalquestions-train-9735", "mrqa_naturalquestions-train-51421", "mrqa_naturalquestions-train-52754", "mrqa_naturalquestions-train-14120", "mrqa_naturalquestions-train-48486", "mrqa_naturalquestions-train-45042", "mrqa_naturalquestions-train-52433", "mrqa_naturalquestions-train-70064", "mrqa_naturalquestions-train-28685", "mrqa_naturalquestions-train-47814", "mrqa_naturalquestions-train-20698", "mrqa_naturalquestions-train-71230", "mrqa_naturalquestions-train-10396", "mrqa_naturalquestions-train-33599", "mrqa_naturalquestions-train-79548", "mrqa_naturalquestions-train-48976", "mrqa_naturalquestions-train-47958", "mrqa_naturalquestions-train-10736", "mrqa_naturalquestions-train-85388", "mrqa_naturalquestions-train-26161", "mrqa_naturalquestions-train-86251", "mrqa_naturalquestions-train-82810", "mrqa_naturalquestions-train-63452", "mrqa_naturalquestions-train-69020", "mrqa_naturalquestions-train-73085", "mrqa_naturalquestions-train-65989", "mrqa_naturalquestions-train-25091", "mrqa_naturalquestions-train-36813", "mrqa_naturalquestions-train-53108", "mrqa_naturalquestions-train-59417", "mrqa_naturalquestions-train-82497", "mrqa_naturalquestions-train-32874"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "is not involved", "c + angle d = 180 degrees", "between 27 July and 7 August 2022", "New York", "casket letters", "2006 British Academy Television Award for Best Drama Series", "Least of the Great Powers", "the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "boggling array of products that makes it almost impossible to finalise a choice", "baze", "death mask", "bollywood", "Overtime", "Sir Henry Cole", "trouble distinguishing between carbon dioxide and oxygen", "is a British sitcom, broadcast in the United Kingdom from 1982 to 1984", "cement City, Texas", "Arlene Foster of the Democratic Unionist Party", "23 July 1989", "many educational institutions especially within the US", "gurus often exercising a great deal of control over the lives of their disciples", "control purposes", "idubbed percussion", "Callability", "a 2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM, either 16 or 32 GB of internal storage, and a 2300 mAh battery", "over 10,000 British and 2,000 old master works", "Wearing hijab, or al - khimar", "proteins", "bile duct", "berenice Abbott"], "metric_results": {"EM": 0.03125, "QA-F1": 0.10904875136889144}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.10526315789473685, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.45161290322580644, 0.4, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-6341", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-56569", "mrqa_naturalquestions-train-47290", "mrqa_naturalquestions-train-19449", "mrqa_naturalquestions-train-23029", "mrqa_naturalquestions-train-70644", "mrqa_naturalquestions-train-3078", "mrqa_naturalquestions-train-86039", "mrqa_naturalquestions-train-59983", "mrqa_naturalquestions-train-63679", "mrqa_naturalquestions-train-44459", "mrqa_naturalquestions-train-73454", "mrqa_naturalquestions-train-24557", "mrqa_naturalquestions-train-57327", "mrqa_naturalquestions-train-84958", "mrqa_naturalquestions-train-15289", "mrqa_naturalquestions-train-74601", "mrqa_naturalquestions-train-44483", "mrqa_naturalquestions-train-28761", "mrqa_naturalquestions-train-59327", "mrqa_naturalquestions-train-17467", "mrqa_naturalquestions-train-14231", "mrqa_naturalquestions-train-72924", "mrqa_naturalquestions-train-16323", "mrqa_naturalquestions-train-52397", "mrqa_naturalquestions-train-64615", "mrqa_naturalquestions-train-83787", "mrqa_naturalquestions-train-9941", "mrqa_naturalquestions-train-78037", "mrqa_naturalquestions-train-1634", "mrqa_naturalquestions-train-24157", "mrqa_naturalquestions-train-42206", "mrqa_naturalquestions-train-80017"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "Nijmegen", "December 9, 2016", "NASA discontinued the manned Block I program", "British progressive folk-rock band Gryphon", "1898", "month of Juno", "a shepherd", "at elevation 2 meters above sea level", "is a rare but often fatal disease that affects the central nervous system by causing painful muscular contractions", "bounding the time or space used by the algorithm", "museum", "Alex O'Loughlin", "Eddie Leonski", "Jack Ridley", "a mixture of phencyclidine and cocaine", "bunker", "never was a barber on Fleet Street", "Reverse - Flash", "All Hallows'Day", "1968", "Baku", "new converts", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "steam turbine", "Splodgenessabounds"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2933779761904762}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.2857142857142857, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.5, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_squad-validation-3467"], "retrieved_ids": ["mrqa_naturalquestions-train-87318", "mrqa_naturalquestions-train-7211", "mrqa_naturalquestions-train-57842", "mrqa_naturalquestions-train-47646", "mrqa_naturalquestions-train-44666", "mrqa_naturalquestions-train-53978", "mrqa_naturalquestions-train-21534", "mrqa_naturalquestions-train-87574", "mrqa_naturalquestions-train-24010", "mrqa_naturalquestions-train-63693", "mrqa_naturalquestions-train-85317", "mrqa_naturalquestions-train-15758", "mrqa_naturalquestions-train-16445", "mrqa_naturalquestions-train-48627", "mrqa_naturalquestions-train-47092", "mrqa_naturalquestions-train-60372", "mrqa_naturalquestions-train-72017", "mrqa_naturalquestions-train-65766", "mrqa_naturalquestions-train-79380", "mrqa_naturalquestions-train-51681", "mrqa_naturalquestions-train-58522", "mrqa_naturalquestions-train-23357", "mrqa_naturalquestions-train-28221", "mrqa_naturalquestions-train-31209", "mrqa_naturalquestions-train-27603", "mrqa_naturalquestions-train-34042", "mrqa_naturalquestions-train-18115", "mrqa_naturalquestions-train-79995", "mrqa_naturalquestions-train-68734", "mrqa_naturalquestions-train-4328", "mrqa_naturalquestions-train-38869", "mrqa_naturalquestions-train-46045"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["aaron walpole", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale", "a city in the Strathclyde region, in SW Scotland, W of Glasgow", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "about 6 : 44 p.m. UTC", "a swanee or swannee whistle", "sunflowers", "to start fires, hunt, and bury their dead", "the Department of Physics, Cochin University of science and Technology", "Parietal cells", "placental", "September 13, 1994", "a revolver", "Ming dynasty", "1840", "a defiant speech, or a speech explaining their actions", "George Sylvester Viereck", "kinks", "on the basis of the methodology used: by using net wealth (adding up assets and subtracting debts )", "nonconservative forces", "my mind is averse to wedlock because I daily expect the death of a heretic", "1700 Cascadia earthquake", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.125, "QA-F1": 0.2431636936764646}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.6153846153846154, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.4210526315789474, 1.0, 0.47058823529411764, 1.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_naturalquestions-validation-816", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-2757", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "retrieved_ids": ["mrqa_naturalquestions-train-44541", "mrqa_naturalquestions-train-82801", "mrqa_naturalquestions-train-70948", "mrqa_naturalquestions-train-85315", "mrqa_naturalquestions-train-16316", "mrqa_naturalquestions-train-76442", "mrqa_naturalquestions-train-6187", "mrqa_naturalquestions-train-86755", "mrqa_naturalquestions-train-45994", "mrqa_naturalquestions-train-6745", "mrqa_naturalquestions-train-66216", "mrqa_naturalquestions-train-62627", "mrqa_naturalquestions-train-12205", "mrqa_naturalquestions-train-23341", "mrqa_naturalquestions-train-17000", "mrqa_naturalquestions-train-6525", "mrqa_naturalquestions-train-24795", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-72538", "mrqa_naturalquestions-train-46285", "mrqa_naturalquestions-train-61948", "mrqa_naturalquestions-train-78285", "mrqa_naturalquestions-train-3989", "mrqa_naturalquestions-train-79169", "mrqa_naturalquestions-train-24249", "mrqa_naturalquestions-train-34182", "mrqa_naturalquestions-train-79006", "mrqa_naturalquestions-train-11574", "mrqa_naturalquestions-train-6958", "mrqa_naturalquestions-train-58897", "mrqa_naturalquestions-train-15907", "mrqa_naturalquestions-train-78089"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["Zinnemann", "to prevent the flame from being blown out", "Barack Hussein Obama II", "2010", "n Carolina", "islands capital is Mahon", "90-60's", "independent schools", "dolph Camilli", "times sign", "BAFTA Television Award", "Emily Blunt", "1960", "HTTP Secure ( HTTPS )", "late - September through early January", "cessna", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "kukai", "invertebrates", "butterflies", "blood", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "casket letters", "the leaves", "Indian club ATK", "land that a nation has conquered and expanded", "near Grande Comore, Comoros Islands", "rupees 10 lakhs", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21692708333333333}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-22025", "mrqa_naturalquestions-train-55245", "mrqa_naturalquestions-train-67", "mrqa_naturalquestions-train-55302", "mrqa_naturalquestions-train-79470", "mrqa_naturalquestions-train-46210", "mrqa_naturalquestions-train-46634", "mrqa_naturalquestions-train-5098", "mrqa_naturalquestions-train-38504", "mrqa_naturalquestions-train-60248", "mrqa_naturalquestions-train-21077", "mrqa_naturalquestions-train-50042", "mrqa_naturalquestions-train-45965", "mrqa_naturalquestions-train-42966", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-86643", "mrqa_naturalquestions-train-71074", "mrqa_naturalquestions-train-74009", "mrqa_naturalquestions-train-16920", "mrqa_naturalquestions-train-47444", "mrqa_naturalquestions-train-56651", "mrqa_naturalquestions-train-49927", "mrqa_naturalquestions-train-35199", "mrqa_naturalquestions-train-7236", "mrqa_naturalquestions-train-1653", "mrqa_naturalquestions-train-49267", "mrqa_naturalquestions-train-70154", "mrqa_naturalquestions-train-16130", "mrqa_naturalquestions-train-77566", "mrqa_naturalquestions-train-37240", "mrqa_naturalquestions-train-43426", "mrqa_naturalquestions-train-70836"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few common complex biomolecules, such as squalene and the carotenes", "The U.S. Army Chaplain insignia", "Kairi", "both inner city blacks, who wanted more involvement in government, and whites in the suburbs, who want more services and more control over the central city", "Ray Milland", "Armenia", "the Christian biblical canon", "Beyonc\u00e9", "buttermens per meter, but conductivity values are often reported as percent IACS", "gallantry", "16 million", "1950s", "cattle are slaughtered for meat before the age of three years, except where they are needed ( castrated ) as work oxen for haulage", "1998", "a priest", "23.1", "18 - season", "family member", "long-term environmental changes", "William Powell Lear", "the tangential force, which accelerates the object by either slowing it down or speeding it up", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "The `` Blue Peter '' is used as a maritime signal, indicating that the vessel flying it is about to leave, and Reed chose the name to represent'a voyage of adventure'on which the programme would set out.", "Abraham Gottlob Werner", "casket letters", "present-day Charleston", "focusing on preaching, education and social services, and benefiting from Israel's \"indulgence\" to build up a network of mosques and charitable organizations", "Adam Karpel", "Panzer"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23251348499061913}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.19999999999999998, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.4, 0.15384615384615385, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.5365853658536585, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902"], "retrieved_ids": ["mrqa_naturalquestions-train-40881", "mrqa_naturalquestions-train-78861", "mrqa_naturalquestions-train-40188", "mrqa_naturalquestions-train-83861", "mrqa_naturalquestions-train-16088", "mrqa_naturalquestions-train-60585", "mrqa_squad-validation-2659", "mrqa_naturalquestions-train-13165", "mrqa_naturalquestions-train-40588", "mrqa_naturalquestions-train-47576", "mrqa_naturalquestions-train-76208", "mrqa_naturalquestions-train-3685", "mrqa_naturalquestions-train-42134", "mrqa_naturalquestions-train-54415", "mrqa_naturalquestions-train-67644", "mrqa_naturalquestions-train-38850", "mrqa_naturalquestions-train-62077", "mrqa_naturalquestions-train-31356", "mrqa_naturalquestions-train-61694", "mrqa_naturalquestions-train-19710", "mrqa_naturalquestions-train-80038", "mrqa_naturalquestions-train-59983", "mrqa_naturalquestions-train-35311", "mrqa_naturalquestions-train-69230", "mrqa_naturalquestions-train-1307", "mrqa_naturalquestions-train-61948", "mrqa_naturalquestions-train-82254", "mrqa_naturalquestions-train-80618", "mrqa_naturalquestions-train-9352", "mrqa_naturalquestions-train-6504", "mrqa_naturalquestions-train-62182", "mrqa_naturalquestions-train-17622"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computability theory", "georgeppe Antonio 'Nino'Farina", "a score of 46.8", "about 5 nanometers across, arranged in rows 6.4 nanometers apart", "the eighth and eleventh episodes", "Kyle Busch", "400", "adrenal Glands", "the enkuklios paideia or `` education in a circle ''", "the Bowland Fells", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "St. Louis County", "1868", "2018", "w Warnock", "law firm", "Pottawatomie County", "The tuatara, a lizard - like reptile", "Newton's Law of Gravitation", "The Parish Church of St Andrew", "bromley- By-Bow", "Toronto", "wales", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to Block Island Sound", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "aphasic children had no apparent organic cause for their symptoms", "Quentin Coldwater", "acidic bogs"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2592418262594893}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.28571428571428575, 0.0, 0.0, 0.0, 0.17391304347826084, 0.2666666666666667, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_squad-validation-5313", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "retrieved_ids": ["mrqa_naturalquestions-train-60865", "mrqa_naturalquestions-train-39255", "mrqa_naturalquestions-train-9103", "mrqa_naturalquestions-train-41264", "mrqa_naturalquestions-train-9382", "mrqa_naturalquestions-train-79454", "mrqa_naturalquestions-train-18256", "mrqa_naturalquestions-train-26006", "mrqa_naturalquestions-train-26144", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-5671", "mrqa_naturalquestions-train-60598", "mrqa_naturalquestions-train-22113", "mrqa_naturalquestions-train-85636", "mrqa_naturalquestions-train-85131", "mrqa_naturalquestions-train-20169", "mrqa_naturalquestions-train-1505", "mrqa_naturalquestions-train-39792", "mrqa_naturalquestions-train-60759", "mrqa_naturalquestions-train-7256", "mrqa_naturalquestions-train-80906", "mrqa_naturalquestions-train-61410", "mrqa_naturalquestions-train-68945", "mrqa_naturalquestions-train-68591", "mrqa_naturalquestions-train-83249", "mrqa_naturalquestions-train-66540", "mrqa_naturalquestions-train-22018", "mrqa_naturalquestions-train-4649", "mrqa_naturalquestions-train-36327", "mrqa_naturalquestions-train-4681", "mrqa_naturalquestions-train-5935", "mrqa_naturalquestions-train-7372"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["the English phrase `` I Seek You ''", "Argentinian", "a report", "but not actually find the right pan", "photosynthesis", "to celebrate Queen Victoria's diamond jubilee", "White House", "The Rebel Media", "triplet", "water", "President of the United States", "almost all officeholders", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy film", "Worcester Cold Storage and Warehouse Co. fire", "acting", "C. W. Grafton", "a liquid crystal on silicon ( L CoS ) ( based on an LCoS chip from Himax ), field - sequential color system, LED illuminated display", "Americans", "IPod Classic", "her role as The Girl in the romantic comedy \" My Sassy Girl\"", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "Procter & Gamble", "Highly combustible materials", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd", "the root respiration", "land - living organisms, both alive and dead, as well as carbon stored in soils", "drug dealer", "medium and heavy-duty diesel trucks", "testes"], "metric_results": {"EM": 0.25, "QA-F1": 0.3648766402324277}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.18181818181818182, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.918918918918919, 0.4, 0.0, 0.15384615384615383, 0.1290322580645161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_hotpotqa-validation-3428", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-5940", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-64615", "mrqa_naturalquestions-train-49795", "mrqa_naturalquestions-train-19450", "mrqa_naturalquestions-train-50063", "mrqa_naturalquestions-train-82150", "mrqa_naturalquestions-train-59665", "mrqa_naturalquestions-train-64975", "mrqa_naturalquestions-train-38269", "mrqa_naturalquestions-train-87710", "mrqa_naturalquestions-train-13167", "mrqa_naturalquestions-train-39460", "mrqa_naturalquestions-train-39052", "mrqa_naturalquestions-train-58860", "mrqa_naturalquestions-train-65467", "mrqa_naturalquestions-train-76468", "mrqa_naturalquestions-train-57630", "mrqa_naturalquestions-train-72269", "mrqa_naturalquestions-train-28491", "mrqa_naturalquestions-train-11260", "mrqa_naturalquestions-train-57277", "mrqa_naturalquestions-train-74270", "mrqa_naturalquestions-train-31214", "mrqa_naturalquestions-train-58934", "mrqa_naturalquestions-train-6798", "mrqa_naturalquestions-train-83903", "mrqa_naturalquestions-train-15042", "mrqa_hotpotqa-validation-409", "mrqa_naturalquestions-train-13902", "mrqa_naturalquestions-train-79157", "mrqa_naturalquestions-train-23570", "mrqa_naturalquestions-train-48291", "mrqa_naturalquestions-train-63503"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["Pope, Alexander (1688-1744)", "yellow fever", "three legal systems", "Las Vegas, Nevada in unincorporated Clark County", "status line", "globetrotters", "Anthony Bellew", "Manoir de la Fi\u00e8re and Chef - du - Pont", "1987", "1987", "cromlech", "Queen Victoria", "the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "a new era of social history and kitchen-sink realism in living museums", "the MGM Grand Garden Special Events Center", "digital fashion gallery", "C. J. Anderson", "a single all-encompassing definition of the term", "\" When Body Snatchers Targeted Mormons and the Miraculous vote\"", "more than 60 percent", "Eagle Ridge Mall", "Pel\u00e9", "Rationing Stamps and Cards to reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "the classical element fire", "\" The Andy Griffith Show\"", "at least 18 or 21 years old ( or have a legal guardian present )", "Ward", "an American novelist and poet", "Jamestown", "Rouen Cathedral", "tree growth stages"], "metric_results": {"EM": 0.125, "QA-F1": 0.2422979797979798}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 0.4444444444444445, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.16666666666666666, 0.0, 0.4, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "retrieved_ids": ["mrqa_naturalquestions-train-19242", "mrqa_naturalquestions-train-76302", "mrqa_naturalquestions-train-67527", "mrqa_naturalquestions-train-52322", "mrqa_naturalquestions-train-5734", "mrqa_naturalquestions-train-57687", "mrqa_naturalquestions-train-28016", "mrqa_naturalquestions-train-53017", "mrqa_naturalquestions-train-43357", "mrqa_naturalquestions-train-37881", "mrqa_naturalquestions-train-3257", "mrqa_naturalquestions-train-70497", "mrqa_naturalquestions-train-87351", "mrqa_naturalquestions-train-48977", "mrqa_naturalquestions-train-14083", "mrqa_naturalquestions-train-12822", "mrqa_naturalquestions-train-30733", "mrqa_naturalquestions-train-2798", "mrqa_naturalquestions-train-74279", "mrqa_naturalquestions-train-68456", "mrqa_naturalquestions-train-22022", "mrqa_naturalquestions-train-37618", "mrqa_naturalquestions-train-27853", "mrqa_naturalquestions-train-57229", "mrqa_naturalquestions-train-15087", "mrqa_naturalquestions-train-58901", "mrqa_naturalquestions-train-36540", "mrqa_naturalquestions-train-49362", "mrqa_naturalquestions-train-9250", "mrqa_naturalquestions-train-38196", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-84075"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["vincent Thomas \"Vince\" Lombardi", "Traumnovelle", "a Gender pay gap in favor of males in the labor market", "The TEU", "thermodynamic", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "joseph smith", "length of their main span ( i.e. the length of suspended roadway between the bridge's towers )", "trams", "handguns", "cadence & Cascade", "the Bulgars", "coket", "Beetle", "casket letters", "North American Technate", "casket letters", "infection, irritation, or allergies", "the most - visited paid monument in the world", "the Vittorio Emanuele II Gallery", "catfish aquaculture", "atomic number 53", "Evermoist", "Iraq", "a co-op of grape growers", "mann on a mission", "casket letters", "1952", "Los Angeles Lakers", "The United States uses a small yellow sign under the main warning sign, as well as a standalone variation on the standard speed limit sign", "Jean F kernel ( 1497 -- 1558 ), a French physician", "chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21498862986054634}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.75, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.19354838709677416, 0.22222222222222224, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "retrieved_ids": ["mrqa_naturalquestions-train-51881", "mrqa_naturalquestions-train-22528", "mrqa_naturalquestions-train-55798", "mrqa_naturalquestions-train-5261", "mrqa_naturalquestions-train-78286", "mrqa_naturalquestions-train-41998", "mrqa_naturalquestions-train-7176", "mrqa_naturalquestions-train-42601", "mrqa_naturalquestions-train-26017", "mrqa_naturalquestions-train-28875", "mrqa_naturalquestions-train-28754", "mrqa_naturalquestions-train-50054", "mrqa_naturalquestions-train-62651", "mrqa_naturalquestions-train-46045", "mrqa_naturalquestions-train-7564", "mrqa_naturalquestions-train-75548", "mrqa_naturalquestions-train-38491", "mrqa_naturalquestions-train-39560", "mrqa_naturalquestions-train-16789", "mrqa_naturalquestions-train-82151", "mrqa_naturalquestions-train-86741", "mrqa_naturalquestions-train-47570", "mrqa_naturalquestions-train-45832", "mrqa_naturalquestions-train-48160", "mrqa_naturalquestions-train-65370", "mrqa_naturalquestions-train-21249", "mrqa_naturalquestions-train-53755", "mrqa_naturalquestions-train-29421", "mrqa_naturalquestions-train-79698", "mrqa_naturalquestions-train-1629", "mrqa_naturalquestions-train-58276", "mrqa_naturalquestions-train-59175"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Joe Turano", "fencers", "John Major", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "August 31, 2014", "The stability, security, and predictability of British law and government", "Minos and Kokalos", "Ignacy Jan Paderewski", "quintero", "norway", "New South Wales", "Fort Bull", "patti LaBelle", "eatele", "an English narrator, possibly Orwell himself, called upon to shoot an aggressive elephant while working as a police officer in Burma", "the Czech Kingdom", "Gregg Popovich", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive immune system", "under the tutelage of his uncle Juan Nepomuceno Guerra", "a musician", "ferdinand", "December 1, 1969", "american", "norway", "California State Automobile Association", "\" Faith\"", "Cinderella", "delayed the sealing of the hatch", "a fear of seeming rude"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21774551701022288}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.19999999999999998, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.9411764705882353, 1.0, 0.0, 0.23529411764705882, 0.4, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.47058823529411764]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_hotpotqa-validation-4826", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-40245", "mrqa_naturalquestions-train-68857", "mrqa_naturalquestions-train-23189", "mrqa_naturalquestions-train-20085", "mrqa_naturalquestions-train-23346", "mrqa_naturalquestions-train-7088", "mrqa_naturalquestions-train-16983", "mrqa_naturalquestions-train-50796", "mrqa_naturalquestions-train-49644", "mrqa_naturalquestions-train-84923", "mrqa_naturalquestions-train-13575", "mrqa_naturalquestions-train-48221", "mrqa_naturalquestions-train-24660", "mrqa_naturalquestions-train-80186", "mrqa_naturalquestions-train-61047", "mrqa_naturalquestions-train-61880", "mrqa_naturalquestions-train-74301", "mrqa_naturalquestions-train-19230", "mrqa_naturalquestions-train-51833", "mrqa_naturalquestions-train-5911", "mrqa_naturalquestions-train-22759", "mrqa_naturalquestions-train-70828", "mrqa_naturalquestions-train-2983", "mrqa_naturalquestions-train-87059", "mrqa_naturalquestions-train-10511", "mrqa_naturalquestions-train-2165", "mrqa_naturalquestions-train-73993", "mrqa_naturalquestions-train-12315", "mrqa_naturalquestions-train-68512", "mrqa_naturalquestions-train-19685", "mrqa_naturalquestions-train-44505", "mrqa_naturalquestions-train-80908"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Diahann Carroll", "president of Guggenheim Partners", "Jason Lee", "Napoleon's army", "baking", "3.7% of the entire student population", "High and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Tracey Stubbs", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "maryland", "paddington", "amyotrophic lateral sclerosis", "a gimmick called \"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "a pioneer in watch design, manufacturing and distribution", "Evel Knievel craze of the mid 1970s", "Torah or Bible", "the western coast of Italy", "the first and only U.S. born world grand prix champion", "brass band parades", "mid November", "Facebook", "bajgiel", "Steel Dragon", "Issaquah, Washington (a suburb of Seattle)", "King George's War", "cheated on Miley", "alternative rock", "Fort Snelling, Minnesota", "daguerreotypes", "infrequent rain"], "metric_results": {"EM": 0.125, "QA-F1": 0.2433351370851371}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.2222222222222222, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.28571428571428575, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "retrieved_ids": ["mrqa_squad-validation-6733", "mrqa_naturalquestions-train-19265", "mrqa_naturalquestions-train-8398", "mrqa_naturalquestions-train-41569", "mrqa_naturalquestions-train-36398", "mrqa_naturalquestions-train-73595", "mrqa_naturalquestions-train-73466", "mrqa_naturalquestions-train-11835", "mrqa_naturalquestions-train-56787", "mrqa_naturalquestions-train-63337", "mrqa_naturalquestions-train-82036", "mrqa_naturalquestions-train-21807", "mrqa_naturalquestions-train-20432", "mrqa_naturalquestions-train-75455", "mrqa_naturalquestions-train-43519", "mrqa_naturalquestions-train-629", "mrqa_naturalquestions-train-47028", "mrqa_naturalquestions-train-80873", "mrqa_naturalquestions-train-37773", "mrqa_naturalquestions-train-44124", "mrqa_naturalquestions-train-326", "mrqa_naturalquestions-train-6563", "mrqa_naturalquestions-train-62765", "mrqa_naturalquestions-train-57105", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-62454", "mrqa_naturalquestions-train-4632", "mrqa_naturalquestions-train-84153", "mrqa_naturalquestions-train-44892", "mrqa_naturalquestions-train-63488", "mrqa_naturalquestions-train-22466", "mrqa_naturalquestions-train-8659"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Daniel Hale Williams Preparatory School of Medicine", "maryland", "FX option", "electromagnetic waves", "a Wahhabi/ Salafi", "one of the ten most avoided", "Dimensions in Time", "the Surveyor 3 unmanned lunar probe", "the end of January 1981", "gonadotropin - releasing hormone ( GnRH )", "baptism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "henland", "A compression release engine brake, frequently called a Jake Nett brake or Jacobs brake", "Cheyenne", "fossils in sedimentary rocks", "Hanna-barbera, The Jetsons", "Cortina d'Ampezzo", "efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "an immortal skeletal being who does not have to pay heed to fatigue or even time, he seems to lack comprehension of some human concepts, such as detective novels", "Australian islands", "goalkeeper Timo Hildebrand", "The public sector ( also called the state sector )", "October", "poverty", "a god of the Ammonites, as well as Tyrian Melqart and others", "cornea (the transparent layer at the front of the eye)", "Uncle Fester", "the \" Texas Tower Sniper\""], "metric_results": {"EM": 0.125, "QA-F1": 0.2319510758848994}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.4, 0.5882352941176471, 0.0, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-validation-727", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877"], "retrieved_ids": ["mrqa_naturalquestions-train-25374", "mrqa_naturalquestions-train-26199", "mrqa_naturalquestions-train-32711", "mrqa_naturalquestions-train-30077", "mrqa_naturalquestions-train-14604", "mrqa_naturalquestions-train-24292", "mrqa_naturalquestions-train-85390", "mrqa_naturalquestions-train-59481", "mrqa_naturalquestions-train-12719", "mrqa_naturalquestions-train-27853", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-train-24806", "mrqa_naturalquestions-train-58793", "mrqa_naturalquestions-train-69398", "mrqa_naturalquestions-train-72905", "mrqa_naturalquestions-train-24540", "mrqa_naturalquestions-train-56020", "mrqa_naturalquestions-train-25583", "mrqa_naturalquestions-train-33609", "mrqa_naturalquestions-train-74141", "mrqa_naturalquestions-train-42076", "mrqa_naturalquestions-train-105", "mrqa_naturalquestions-train-6651", "mrqa_naturalquestions-train-17573", "mrqa_naturalquestions-train-52403", "mrqa_naturalquestions-train-60289", "mrqa_naturalquestions-train-20936", "mrqa_naturalquestions-train-71668", "mrqa_naturalquestions-train-69584", "mrqa_naturalquestions-train-85344", "mrqa_naturalquestions-train-34", "mrqa_naturalquestions-train-27288"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["being bitten by radioactive/genetically-altered spiders", "Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Royce da 5'9\" ( bad) and Eminem ( Evil)", "viternicus", "a friend and publicist", "michael ondaatje", "masons'marks", "Theodore Haynes", "Gateshead", "inspired by Motown, Philly soul, and Earth, Wind & Fire ( particularly `` That's the Way of the World '' )", "The neck", "after the Spanish -- American War in the 1898 Treaty of Paris", "a Heavyweight", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's superweapon", "vitese in \"Day for Night\"", "a Curtiss Jenny JN-4HM", "antwerp", "gorillas", "March 15, 1945", "absolute temperature", "the whistle-blowing websiteWiki", "Julius Robert Oppenheimer", "bicuspid", "Aegisthus", "25 November 2015", "tallahassee", "prefabricated housing projects", "brian brian", "WWSB and WOTV"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1282738095238095}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-43519", "mrqa_naturalquestions-train-41838", "mrqa_naturalquestions-train-26191", "mrqa_naturalquestions-train-38998", "mrqa_naturalquestions-train-38393", "mrqa_naturalquestions-train-42472", "mrqa_naturalquestions-train-65896", "mrqa_naturalquestions-train-56941", "mrqa_naturalquestions-train-86835", "mrqa_naturalquestions-train-28710", "mrqa_naturalquestions-train-48308", "mrqa_naturalquestions-train-5582", "mrqa_naturalquestions-train-85411", "mrqa_naturalquestions-train-8659", "mrqa_naturalquestions-train-16721", "mrqa_naturalquestions-train-60830", "mrqa_naturalquestions-train-51482", "mrqa_naturalquestions-train-71337", "mrqa_naturalquestions-train-45944", "mrqa_naturalquestions-train-32796", "mrqa_naturalquestions-train-83976", "mrqa_naturalquestions-train-74971", "mrqa_naturalquestions-train-8163", "mrqa_naturalquestions-train-19075", "mrqa_naturalquestions-train-24924", "mrqa_naturalquestions-train-1634", "mrqa_naturalquestions-train-38540", "mrqa_naturalquestions-train-40233", "mrqa_naturalquestions-train-69993", "mrqa_naturalquestions-train-24601", "mrqa_naturalquestions-train-25554", "mrqa_naturalquestions-train-70439"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["galileo", "cheese", "blessed", "on the lateral side of the tibia", "ferguside royal clan", "the North Sea, through the former Meuse estuary, near Rotterdam", "the kgalagadi Trans-frontier Park", "faldo", "Ximena Sari\u00f1ana Rivera", "Amway", "Mauritius", "Thomas Sowell", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in the absence, the Deputy - Chairman of the Rajya Sabha", "The Show Band Show", "Tanzania", "galileo", "galileo", "an open work crown", "to help him find Purgatory, the afterlife of monsters", "Fulham, Greater London, England", "French", "galileo", "U.S. Marshals", "What's Up (TV series)", "non-transfusion facilities", "galazers", "belgne", "polynomial algebra", "florida", "Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru", "sheepskin", "Honolulu County, Hawaii, United States, on the island of Oahu"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18056632547559964}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3870967741935484, 1.0, 0.30769230769230765]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-2445", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-2287"], "retrieved_ids": ["mrqa_naturalquestions-train-53515", "mrqa_naturalquestions-train-34559", "mrqa_naturalquestions-train-35070", "mrqa_naturalquestions-train-63308", "mrqa_naturalquestions-train-33594", "mrqa_naturalquestions-train-66735", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-train-82530", "mrqa_naturalquestions-train-26357", "mrqa_naturalquestions-train-15930", "mrqa_naturalquestions-train-28444", "mrqa_naturalquestions-train-34736", "mrqa_naturalquestions-train-12489", "mrqa_naturalquestions-train-33259", "mrqa_naturalquestions-train-43519", "mrqa_naturalquestions-train-53077", "mrqa_naturalquestions-train-29484", "mrqa_naturalquestions-train-76439", "mrqa_naturalquestions-train-87310", "mrqa_naturalquestions-train-83530", "mrqa_naturalquestions-train-72409", "mrqa_naturalquestions-train-66122", "mrqa_naturalquestions-train-46282", "mrqa_naturalquestions-train-605", "mrqa_triviaqa-validation-1860", "mrqa_naturalquestions-train-44294", "mrqa_naturalquestions-train-18907", "mrqa_naturalquestions-train-37184", "mrqa_naturalquestions-train-23762", "mrqa_naturalquestions-train-86741", "mrqa_naturalquestions-train-36561", "mrqa_naturalquestions-train-34540"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["belgium", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "stable, non-radioactive rubidium - 85", "James Zeebo", "sovereign states", "president of the United States", "The Discovery Institute (DI) is a politically conservative non-profit think tank based in Seattle, Washington, best known for its advocacy of the pseudoscientific principle of intelligent design (ID)", "Bumblebee", "Australian", "six additional months", "by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Spencer", "\"antiforms\"", "June 9, 2015", "G. V. Prakash Kumar", "Grace Nail Johnson", "Keith Richards", "at least one prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "students learn from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "at which longitude is defined to be 0 \u00b0", "Cartoon Network", "the Presiding Officer on the advice of the parliamentary bureau", "Miami Heat", "33", "vitifolia", "Annual Conference Cabinet", "field hockey player Hannah Macleod", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2847906602254428}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.08333333333333334, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.16, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4347826086956522, 0.25, 0.07407407407407407, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "retrieved_ids": ["mrqa_naturalquestions-train-48825", "mrqa_naturalquestions-train-29142", "mrqa_naturalquestions-train-639", "mrqa_naturalquestions-train-87926", "mrqa_naturalquestions-train-4062", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-19761", "mrqa_naturalquestions-train-31501", "mrqa_naturalquestions-train-69829", "mrqa_naturalquestions-train-60684", "mrqa_naturalquestions-train-45811", "mrqa_naturalquestions-train-43283", "mrqa_naturalquestions-train-28242", "mrqa_naturalquestions-train-86204", "mrqa_naturalquestions-train-45906", "mrqa_naturalquestions-train-52200", "mrqa_naturalquestions-train-1096", "mrqa_naturalquestions-train-64526", "mrqa_naturalquestions-train-46210", "mrqa_naturalquestions-train-1862", "mrqa_naturalquestions-train-32607", "mrqa_naturalquestions-train-76023", "mrqa_naturalquestions-train-45617", "mrqa_naturalquestions-train-29370", "mrqa_naturalquestions-train-6289", "mrqa_naturalquestions-train-57333", "mrqa_naturalquestions-train-34208", "mrqa_naturalquestions-train-21539", "mrqa_naturalquestions-train-7327", "mrqa_naturalquestions-train-24967", "mrqa_naturalquestions-train-50785", "mrqa_naturalquestions-train-40724"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwanda genocide, also known as the genocide against the Tutsi", "creating a climate of learning", "400 metres", "Vili Fualaau and Mary Kay Letourneau", "entertainment", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "12", "the Museum of Manufactures", "Edward Longshanks and the Hammer of the Scots", "naval support", "dundee", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "king James V of Scotland", "\"Grindhouse\"", "women's top 25", "digital transmission", "the Swiss- Austrian border", "lithium-ion battery", "821", "HD channels and Video On Demand content which was not previously carried by cable", "liquid", "Kim Hyun-ah", "colonizers", "transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "the \"Queen of Cool\"", "President Wilson and the American delegation from the Paris Peace Conference", "Socrates", "the fifth season", "dave dors", "Hockey Club Davos", "Michael Crawford", "Qutab Ud - Din - Aibak, founder of the Delhi Sultanate"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3072627314814815}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.2, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.07407407407407408, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.4, 0.4, 0.6666666666666666]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_squad-validation-9827", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-54288", "mrqa_naturalquestions-train-60931", "mrqa_naturalquestions-train-50235", "mrqa_naturalquestions-train-50733", "mrqa_naturalquestions-train-76238", "mrqa_naturalquestions-train-54258", "mrqa_naturalquestions-train-8066", "mrqa_naturalquestions-train-19213", "mrqa_naturalquestions-train-25566", "mrqa_naturalquestions-train-25028", "mrqa_naturalquestions-train-9155", "mrqa_naturalquestions-train-75199", "mrqa_naturalquestions-train-28218", "mrqa_naturalquestions-train-86165", "mrqa_naturalquestions-train-53870", "mrqa_naturalquestions-train-13914", "mrqa_naturalquestions-train-48976", "mrqa_naturalquestions-train-78708", "mrqa_naturalquestions-train-85736", "mrqa_naturalquestions-train-26781", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-72300", "mrqa_naturalquestions-train-43493", "mrqa_naturalquestions-train-25404", "mrqa_squad-validation-10403", "mrqa_naturalquestions-train-8472", "mrqa_naturalquestions-train-63031", "mrqa_naturalquestions-train-56264", "mrqa_naturalquestions-train-55227", "mrqa_naturalquestions-train-26280", "mrqa_naturalquestions-train-2652", "mrqa_naturalquestions-train-25005"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "John Meston", "aragon", "7.8", "trans-Pacific flight", "Sharman Joshi", "students normally have to sit in a classroom and do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "the factorial (p \u2212 1)! + 1 is divisible by p.", "Ana", "New Brunswick", "`` Happy Hour ''", "f. O. Matthiessen", "fredys o'Donnell", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "blackstar", "Star Plus", "fear of the Lord", "1974", "Nicki Minaj", "fernic", "Huguenot", "f1", "friedrich Engels (1820 - 1895) - Genealogy - geni.com", "Drawn Together", "Norman invaders of England", "Tel Aviv", "two", "Corinthian and Saronic Gulfs", "taking blood samples from patients and correctly cataloging them for lab analysis", "Leo Richard Howard", "Sunset Publishing Corporation, part of Southern Progress Corporation, itself a subsidiary of Time Warner"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2895617951127819}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10526315789473684, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333334, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.375]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_triviaqa-validation-6901", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "retrieved_ids": ["mrqa_naturalquestions-train-62777", "mrqa_naturalquestions-train-49952", "mrqa_naturalquestions-train-35039", "mrqa_naturalquestions-train-72269", "mrqa_naturalquestions-train-58634", "mrqa_naturalquestions-train-10810", "mrqa_naturalquestions-train-83494", "mrqa_naturalquestions-train-56131", "mrqa_naturalquestions-train-18938", "mrqa_naturalquestions-train-71340", "mrqa_naturalquestions-train-83071", "mrqa_naturalquestions-train-32377", "mrqa_naturalquestions-train-76795", "mrqa_naturalquestions-train-28808", "mrqa_naturalquestions-train-17257", "mrqa_naturalquestions-train-46398", "mrqa_naturalquestions-train-28545", "mrqa_squad-validation-2757", "mrqa_naturalquestions-train-45499", "mrqa_naturalquestions-train-20665", "mrqa_naturalquestions-train-43362", "mrqa_naturalquestions-train-35422", "mrqa_naturalquestions-train-31305", "mrqa_naturalquestions-train-29227", "mrqa_naturalquestions-train-16982", "mrqa_naturalquestions-train-52096", "mrqa_naturalquestions-train-65486", "mrqa_naturalquestions-train-48090", "mrqa_naturalquestions-train-61686", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-train-505", "mrqa_naturalquestions-train-51326"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating Diesel engines", "Robert Smigel", "the prints and architectural drawings", "is an American action comedy film directed by John Badham, and starring Michael J. Fox and James Woods", "kaleidoscopes", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "AS-204", "ribosomes", "kookaburra", "six-time Silver Slugger Award winner", "Scott Bakula as Dwayne `` King '' Cassius Pride, NCIS Supervisory Special Agent", "is the fourth studio album by American country music artist John Michael Montgomery", "Cozonac", "brian marx", "heliocentric", "Lucius Cornelius Sulla Felix", "Super Bowl LII, following the 2017 season", "Golden Globe", "English and Swahili", "the primacy of core Christian values such as love, patience, charity, and freedom", "islam", "Pantone Matching System (PMS)", "Firoz Shah Tughlaq", "\" My Love from the Star\" (2013)", "San Jose", "sea wasp", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "Thunderbird of Native American tradition", "giving Super Bowl ever", "29.7%", "houlihan"], "metric_results": {"EM": 0.1875, "QA-F1": 0.328145751307516}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.48484848484848486, 0.0, 1.0, 1.0, 0.33333333333333337, 0.3076923076923077, 0.0, 0.2857142857142857, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.888888888888889, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_squad-validation-393", "mrqa_triviaqa-validation-935"], "retrieved_ids": ["mrqa_naturalquestions-train-35233", "mrqa_naturalquestions-train-50435", "mrqa_naturalquestions-train-29142", "mrqa_naturalquestions-train-59064", "mrqa_naturalquestions-train-43817", "mrqa_naturalquestions-train-64391", "mrqa_naturalquestions-train-5004", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-41859", "mrqa_naturalquestions-train-22037", "mrqa_naturalquestions-train-78579", "mrqa_naturalquestions-train-55439", "mrqa_naturalquestions-train-56172", "mrqa_naturalquestions-train-56382", "mrqa_naturalquestions-train-43626", "mrqa_naturalquestions-train-2452", "mrqa_naturalquestions-train-78095", "mrqa_naturalquestions-train-47816", "mrqa_naturalquestions-train-54634", "mrqa_naturalquestions-train-76729", "mrqa_naturalquestions-train-15629", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-train-69712", "mrqa_naturalquestions-train-26869", "mrqa_naturalquestions-train-32466", "mrqa_hotpotqa-validation-3757", "mrqa_naturalquestions-train-69235", "mrqa_naturalquestions-train-55076", "mrqa_naturalquestions-train-55245", "mrqa_naturalquestions-train-28336", "mrqa_naturalquestions-train-20986", "mrqa_naturalquestions-train-18023"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "dust jacket", "various registries", "blood", "Yazoo", "22 April 1894", "a black hole", "dreams", "continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique", "Kris Kristofferson", "ill. (some col.)", "public and private", "French", "Lewis", "Charles Dickens", "World Summit of Nobel Peace Laureates", "proteins", "February 2001", "that there are infinitely many primes", "news", "padlocking the gates", "1969", "Tallemaja \"pine tree Mary\"", "western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British)", "Orthodox Christians", "40F", "640 \u00d7 1136", "Triple Crown of Thoroughbred Racing", "the Western Atlantic ctenophore Mnemiopsis leidyi", "\"Menace II Society\"", "backup quarterback", "40 singles (combining his solo recordings and those with his brothers)"], "metric_results": {"EM": 0.1875, "QA-F1": 0.32031062869878657}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.3157894736842105, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.22222222222222224, 0.4210526315789474]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-20647", "mrqa_naturalquestions-train-47039", "mrqa_naturalquestions-train-79454", "mrqa_naturalquestions-train-19384", "mrqa_naturalquestions-train-34719", "mrqa_naturalquestions-train-44389", "mrqa_hotpotqa-validation-2957", "mrqa_naturalquestions-train-43901", "mrqa_naturalquestions-train-21739", "mrqa_naturalquestions-train-69565", "mrqa_naturalquestions-train-26015", "mrqa_naturalquestions-train-38219", "mrqa_naturalquestions-train-82715", "mrqa_naturalquestions-train-12807", "mrqa_naturalquestions-train-37868", "mrqa_naturalquestions-train-64929", "mrqa_naturalquestions-train-63542", "mrqa_naturalquestions-train-51929", "mrqa_naturalquestions-train-1371", "mrqa_naturalquestions-train-85315", "mrqa_naturalquestions-train-61163", "mrqa_naturalquestions-train-12174", "mrqa_naturalquestions-train-71668", "mrqa_naturalquestions-train-22950", "mrqa_naturalquestions-train-78867", "mrqa_naturalquestions-train-41954", "mrqa_naturalquestions-train-43056", "mrqa_naturalquestions-train-42818", "mrqa_naturalquestions-train-16644", "mrqa_naturalquestions-train-63552", "mrqa_naturalquestions-train-45468", "mrqa_naturalquestions-train-49194"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "Cruises is a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "alison robs Peter to pay Paul", "his friends, Humpty Dumpty and Kitty Softpaws", "the environmentalist Australian Greens", "Royalists", "depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "40 year old French actress L\u00e9a Seydoux", "20,000 leagues under the sea and Around the world in 80 days", "Augustus Waters", "1619", "Tony Blair", "damaging", "June 11, 1973", "Kenya", "critical quotations", "alison dred", "an active supporter of the League of Nations", "Healthcare", "AMC Entertainment Holdings, Inc.", "\"The Gang\"", "3 October 1990", "March 1, 2018", "The weak force is due to the exchange of the heavy W and Z bosons", "fred royale", "Martin Luther King Jr.", "Development of Substitute Materials", "the Chronicles of Barsetshire", "vast"], "metric_results": {"EM": 0.125, "QA-F1": 0.29426767676767673}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 0.4444444444444445, 0.0, 0.33333333333333337, 0.14285714285714288, 0.16, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5714285714285715, 0.0, 0.1818181818181818, 0.6666666666666666, 0.3333333333333333, 0.0, 0.2, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_squad-validation-2828"], "retrieved_ids": ["mrqa_naturalquestions-train-70334", "mrqa_naturalquestions-train-25623", "mrqa_naturalquestions-train-4067", "mrqa_naturalquestions-train-33395", "mrqa_naturalquestions-train-53746", "mrqa_naturalquestions-train-1555", "mrqa_naturalquestions-train-57055", "mrqa_naturalquestions-train-73886", "mrqa_naturalquestions-train-37600", "mrqa_naturalquestions-train-22389", "mrqa_naturalquestions-train-39255", "mrqa_naturalquestions-train-87351", "mrqa_naturalquestions-train-43267", "mrqa_naturalquestions-train-32592", "mrqa_naturalquestions-train-34624", "mrqa_naturalquestions-train-75455", "mrqa_naturalquestions-train-57946", "mrqa_naturalquestions-train-4098", "mrqa_naturalquestions-train-49877", "mrqa_naturalquestions-train-6134", "mrqa_naturalquestions-train-4469", "mrqa_naturalquestions-train-47978", "mrqa_naturalquestions-train-16799", "mrqa_naturalquestions-train-26357", "mrqa_hotpotqa-validation-1021", "mrqa_naturalquestions-train-71828", "mrqa_naturalquestions-train-85123", "mrqa_naturalquestions-train-6578", "mrqa_naturalquestions-train-27535", "mrqa_naturalquestions-train-29371", "mrqa_squad-validation-6677", "mrqa_naturalquestions-train-2717"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["$105 billion", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "pig", "red", "Dreamland", "animated film", "European Union institutions", "381.6 days", "Death Wish Coffee", "CAL IPSO", "lesser celandine", "Ulbricht", "Ronald Ralph \"Ronnie\" Schell", "artemisinin- Based therapy", "Mumbai, Maharashtra", "Leinster", "1938", "2017 / 18", "1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "the Intertropical Convergence Zone ( ITCZ )", "Jane Seymour", "Incudomalleolar joint", "Bobby Riggs", "Leucippus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Corey Brown"], "metric_results": {"EM": 0.25, "QA-F1": 0.3232638888888889}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.25, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "retrieved_ids": ["mrqa_naturalquestions-train-66157", "mrqa_naturalquestions-train-54427", "mrqa_naturalquestions-train-47739", "mrqa_naturalquestions-train-75224", "mrqa_naturalquestions-train-6012", "mrqa_naturalquestions-train-53695", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-train-38745", "mrqa_naturalquestions-train-54903", "mrqa_naturalquestions-train-61763", "mrqa_naturalquestions-train-43207", "mrqa_naturalquestions-train-35233", "mrqa_naturalquestions-train-47228", "mrqa_naturalquestions-train-5160", "mrqa_naturalquestions-train-28833", "mrqa_naturalquestions-train-72066", "mrqa_naturalquestions-train-13385", "mrqa_naturalquestions-train-50166", "mrqa_naturalquestions-train-20614", "mrqa_naturalquestions-train-81054", "mrqa_naturalquestions-train-45547", "mrqa_naturalquestions-train-77461", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-55656", "mrqa_naturalquestions-train-13944", "mrqa_naturalquestions-train-39360", "mrqa_naturalquestions-train-42632", "mrqa_squad-validation-7042", "mrqa_naturalquestions-train-60014", "mrqa_naturalquestions-train-73275", "mrqa_naturalquestions-train-15896", "mrqa_naturalquestions-train-71254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "alchemy", "WBC and lineal titles", "moluccas", "Saturday in May", "Offstage, Goneril, her plans thwarted, commits suicide. The dying Edmund decides, though he admits it is against his own character, to try to save Lear and Cordelia ; however, his confession comes too late", "mary Kissinger had negotiated an Israeli troop withdrawal from parts of the Sinai Peninsula", "1990", "The Number Twelve Looks Like You", "John Elway", "Selena Gomez", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "is an unofficial title sometimes given to new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "letter series", "Fa Ze Rug", "the nine circles of Hell", "Mongols", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "the Friars Minor Conventual (O.F.M. Conv)", "CD Castell\u00f3n", "between 1770 and 1848", "finished the regular season with a 12\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20\u201318 in the AFC Championship Game", "having colloblasts, which are sticky and adhere to prey", "Jon M. Chu", "STS-51-C.", "due to clear weather", "mitterrand"], "metric_results": {"EM": 0.0625, "QA-F1": 0.19705758794120862}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.07692307692307693, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.24000000000000002, 0.0, 0.0, 1.0, 0.0, 0.6486486486486487, 0.0, 0.0, 0.4, 0.7692307692307693, 0.0, 0.4, 0.0, 0.0, 0.2222222222222222, 0.0, 0.4, 0.0, 0.0689655172413793, 0.3636363636363636, 0.0, 0.0, 0.21621621621621626, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-67", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-24708", "mrqa_naturalquestions-train-3077", "mrqa_naturalquestions-train-19", "mrqa_naturalquestions-train-19770", "mrqa_naturalquestions-train-82863", "mrqa_naturalquestions-train-70206", "mrqa_naturalquestions-train-84291", "mrqa_naturalquestions-train-44589", "mrqa_naturalquestions-train-68795", "mrqa_naturalquestions-train-57144", "mrqa_naturalquestions-train-5055", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-train-44900", "mrqa_naturalquestions-train-87093", "mrqa_naturalquestions-train-24610", "mrqa_naturalquestions-train-55300", "mrqa_naturalquestions-train-47477", "mrqa_naturalquestions-train-11217", "mrqa_naturalquestions-train-13530", "mrqa_naturalquestions-train-52871", "mrqa_naturalquestions-train-74056", "mrqa_naturalquestions-train-62240", "mrqa_naturalquestions-train-87281", "mrqa_naturalquestions-train-87509", "mrqa_naturalquestions-train-1867", "mrqa_naturalquestions-train-36667", "mrqa_naturalquestions-train-43300", "mrqa_naturalquestions-train-55947", "mrqa_naturalquestions-train-15326", "mrqa_naturalquestions-train-54891", "mrqa_naturalquestions-train-51752", "mrqa_naturalquestions-train-65486"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["france", "slave", "over 50 million singles", "states'rights to expand slavery", "1923 and 1925", "Metropolitan Statistical Area", "January 19, 1962", "Frigate", "france", "d'Hondt method", "geese", "income inequality will eventually decrease", "jagera", "Peter Davison, Colin Baker and Sylvester McCoy", "August 14, 1848", "lower rates of health and social problems", "juveniles", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "epic verse like \" Inferno\"", "2,664", "iPhone 6", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning services, support services, property services, catering services, security services and facility management services", "Violin Sonata No. 5", "gironde", "1603", "ranked above the two personal physicians of the Emperor", "france", "spiritual awakening", "wrigley"], "metric_results": {"EM": 0.21875, "QA-F1": 0.37457334332334336}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.25, 0.33333333333333337, 0.4615384615384615, 0.923076923076923, 0.0, 1.0, 1.0, 0.2857142857142857, 0.3636363636363636, 0.4444444444444445, 0.28571428571428575, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-499", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "retrieved_ids": ["mrqa_naturalquestions-train-6475", "mrqa_naturalquestions-train-81285", "mrqa_naturalquestions-train-26589", "mrqa_naturalquestions-train-15944", "mrqa_naturalquestions-train-4860", "mrqa_naturalquestions-train-28397", "mrqa_naturalquestions-train-61331", "mrqa_naturalquestions-train-45305", "mrqa_naturalquestions-train-30910", "mrqa_naturalquestions-train-74239", "mrqa_naturalquestions-train-33243", "mrqa_naturalquestions-train-38517", "mrqa_naturalquestions-train-85211", "mrqa_naturalquestions-train-16061", "mrqa_naturalquestions-train-38028", "mrqa_naturalquestions-train-3439", "mrqa_naturalquestions-train-16854", "mrqa_naturalquestions-train-348", "mrqa_naturalquestions-train-10736", "mrqa_naturalquestions-train-62363", "mrqa_naturalquestions-train-48940", "mrqa_naturalquestions-train-7772", "mrqa_naturalquestions-train-54326", "mrqa_naturalquestions-train-11599", "mrqa_naturalquestions-train-54895", "mrqa_naturalquestions-train-32032", "mrqa_naturalquestions-train-77562", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-train-19719", "mrqa_naturalquestions-train-24421", "mrqa_naturalquestions-train-81521", "mrqa_naturalquestions-train-13547"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences", "south", "value investing", "true history of the Kelly Gang", "Honolulu", "1910\u20131940", "non-teaching posts", "Catch Me Who Can", "jazz saxophonist", "Harry Hopman", "4,000", "Khagan", "Heathcliff", "river", "spice", "Homer and the 8th Commandment", "Raymond Unwin", "San Bernardino", "shopping Centre", "Albany High School for Educating People of Color", "Lalbagh Fort", "Sergeant First Class", "Anakin Skywalker", "seek jury nullification", "Cee - Lo", "Anglican", "pacific pacific", "pacific fleet", "magnetism", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "redistributive", "January 12, 1804"], "metric_results": {"EM": 0.25, "QA-F1": 0.3960385101010101}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.8, 1.0, 0.0, 0.33333333333333337, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "retrieved_ids": ["mrqa_naturalquestions-train-871", "mrqa_naturalquestions-train-30029", "mrqa_naturalquestions-train-86984", "mrqa_naturalquestions-train-86581", "mrqa_naturalquestions-train-85891", "mrqa_naturalquestions-train-47365", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-train-1696", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-76315", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-train-58012", "mrqa_naturalquestions-train-50719", "mrqa_naturalquestions-train-50131", "mrqa_naturalquestions-train-13944", "mrqa_naturalquestions-train-37625", "mrqa_naturalquestions-train-61955", "mrqa_hotpotqa-validation-35", "mrqa_naturalquestions-train-54060", "mrqa_naturalquestions-train-70025", "mrqa_naturalquestions-train-49100", "mrqa_naturalquestions-train-51482", "mrqa_naturalquestions-train-55312", "mrqa_naturalquestions-train-23684", "mrqa_naturalquestions-train-68806", "mrqa_naturalquestions-train-72232", "mrqa_naturalquestions-train-6421", "mrqa_naturalquestions-train-61312", "mrqa_naturalquestions-train-56752", "mrqa_naturalquestions-train-38714", "mrqa_naturalquestions-train-63713", "mrqa_naturalquestions-train-40604"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A.", "Jools Holland", "blackberry", "kelly kelly", "\" Big Mamie\"", "margaret", "Hoffa", "a light sky-blue color caused by absorption in the red", "the peasants had to work for free on church land", "1963", "the Atlanta Hawks won his first NBA Championship", "the internal thylakoid system", "sisley", "local festivals such as the popular Port Fairy Folk Festival, Queenscliff Music Festival, Bells Beach Surfclassic and the Bright Autumn Festival", "B SkyB has no veto", "the fourth season", "a more fundamental electrostrong interaction", "the availability of skilled tradespeople", "diamond", "A simple iron boar crest adorns the top of this helmet", "polytechnics became new universities", "kelly kelly", "Lofton", "on kickoffs at the 25 - yard line", "Swedish astronomer Anders Celsius", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "the Bible speaks", "margaret smith", "can be produced with constant technology and resources per unit of time", "Antwerp", "company"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3048022974832185}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.75, 0.28571428571428575, 0.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.631578947368421, 0.8, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-30173", "mrqa_naturalquestions-train-67547", "mrqa_naturalquestions-train-52101", "mrqa_naturalquestions-train-36487", "mrqa_naturalquestions-train-28325", "mrqa_naturalquestions-train-42218", "mrqa_naturalquestions-train-42585", "mrqa_naturalquestions-train-31479", "mrqa_naturalquestions-train-65150", "mrqa_naturalquestions-train-81948", "mrqa_naturalquestions-train-80348", "mrqa_naturalquestions-train-641", "mrqa_naturalquestions-train-76798", "mrqa_naturalquestions-train-51653", "mrqa_naturalquestions-train-41935", "mrqa_naturalquestions-train-76628", "mrqa_naturalquestions-train-53040", "mrqa_naturalquestions-train-86191", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-train-70892", "mrqa_naturalquestions-train-32005", "mrqa_naturalquestions-train-17554", "mrqa_naturalquestions-train-2990", "mrqa_naturalquestions-train-30177", "mrqa_naturalquestions-train-16200", "mrqa_naturalquestions-train-37288", "mrqa_naturalquestions-train-1371", "mrqa_naturalquestions-train-6132", "mrqa_naturalquestions-train-63996", "mrqa_naturalquestions-train-22375", "mrqa_naturalquestions-train-46196", "mrqa_naturalquestions-validation-1565"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["Muslim faith as a tool of the devil", "Chris Weidman", "nullification", "Harishchandra", "writing", "Professor Eobard Thawne", "plum brandy", "a US>10 a week raise over Tesla's US>18 per week salary", "1875", "member states", "clarinets", "management consulting firm McKinsey & Company", "gyphidiophobia", "Cliff Richard", "Crohn's disease", "Alceu Ranzi", "Raya Yarbrough", "Cincinnati", "australia", "John D. Rockefeller", "Old Testament", "UPS", "local talent", "the Football League", "tegan", "mafic", "contemporary accounts were exaggerations", "mary Eugenia Surratt", "1349", "dodo bird", "through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "renoir"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2293208386958387}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.10810810810810811, 0.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_squad-validation-4309", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821", "mrqa_triviaqa-validation-4308"], "retrieved_ids": ["mrqa_naturalquestions-train-50076", "mrqa_naturalquestions-train-72272", "mrqa_naturalquestions-train-78640", "mrqa_naturalquestions-train-81139", "mrqa_naturalquestions-train-62640", "mrqa_naturalquestions-train-32838", "mrqa_naturalquestions-train-51145", "mrqa_naturalquestions-train-3879", "mrqa_naturalquestions-train-26459", "mrqa_squad-validation-3558", "mrqa_naturalquestions-train-57384", "mrqa_naturalquestions-train-37323", "mrqa_naturalquestions-train-38817", "mrqa_naturalquestions-train-7972", "mrqa_naturalquestions-train-16406", "mrqa_naturalquestions-train-46606", "mrqa_naturalquestions-train-59549", "mrqa_naturalquestions-train-6529", "mrqa_naturalquestions-train-29087", "mrqa_naturalquestions-train-57480", "mrqa_naturalquestions-train-6169", "mrqa_naturalquestions-train-63693", "mrqa_naturalquestions-train-65537", "mrqa_naturalquestions-train-13110", "mrqa_naturalquestions-train-26395", "mrqa_squad-validation-3113", "mrqa_naturalquestions-train-76498", "mrqa_naturalquestions-train-74165", "mrqa_naturalquestions-train-5430", "mrqa_naturalquestions-train-10083", "mrqa_naturalquestions-train-75644", "mrqa_naturalquestions-train-80350"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["poisonous", "886 AD", "to finance his own projects with varying degrees of success", "Le Mans", "Xbox 360", "Tokyo", "safety Darian Stewart", "parallelogram rule of vector addition", "abraham lincoln", "364", "startup neutron source", "starry starry night", "the bore, and often the stroke", "performs six major functions ; support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "Doctorin' the Tardis", "National Basketball Development League", "casket letters", "St. Mary's County", "T. J. Ward", "2,615 at the 2010 census", "Pyeongchang", "Kaep (disambiguation)", "a password recovery tool for Microsoft Windows", "Captain John Guidry", "Charles and Ray Eames", "Brazil", "Steve Redgrave", "the smallest subfield of a field", "heartburn", "from 53% in Botswana to -40% in Bahrain", "NADPH"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24166958450046688}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.38095238095238093, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "retrieved_ids": ["mrqa_naturalquestions-train-8291", "mrqa_naturalquestions-train-47902", "mrqa_naturalquestions-train-5479", "mrqa_hotpotqa-validation-3877", "mrqa_naturalquestions-train-62261", "mrqa_naturalquestions-train-63309", "mrqa_squad-validation-93", "mrqa_naturalquestions-train-77353", "mrqa_naturalquestions-train-15141", "mrqa_naturalquestions-train-72268", "mrqa_naturalquestions-train-80539", "mrqa_naturalquestions-train-58230", "mrqa_naturalquestions-train-27365", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-55239", "mrqa_naturalquestions-train-33814", "mrqa_naturalquestions-train-21331", "mrqa_naturalquestions-train-68578", "mrqa_naturalquestions-train-14391", "mrqa_naturalquestions-train-75157", "mrqa_naturalquestions-train-48032", "mrqa_naturalquestions-train-45127", "mrqa_naturalquestions-train-560", "mrqa_naturalquestions-train-63949", "mrqa_naturalquestions-train-84662", "mrqa_naturalquestions-train-31610", "mrqa_naturalquestions-train-39648", "mrqa_naturalquestions-train-44896", "mrqa_naturalquestions-train-28849", "mrqa_naturalquestions-train-59256", "mrqa_naturalquestions-train-9882", "mrqa_naturalquestions-train-52298"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "the Mayor's son", "arthurists", "arthur booth", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "BBC UKTV", "arthur", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "arthur", "arthur", "usernames, passwords, commands and data", "instructions", "The Greens, who won their first lower house seats in 2014, are strongest", "marduk", "arthur", "largest source of foreign direct investment", "arthur", "the South Pacific off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Easy", "Meredith Brody ( Zoe McLellan ), a transfer from the NCIS Great Lakes field office, who has worked as a Special Agent Afloat and is keen to leave her past behind as she moves to New Orleans", "k. kamaraj", "National Lottery", "arthur", "katherine swynford", "in order to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.0625, "QA-F1": 0.12337526138077609}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.125, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.4, 0.12121212121212122, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-63844", "mrqa_naturalquestions-train-15423", "mrqa_naturalquestions-train-3106", "mrqa_naturalquestions-train-82388", "mrqa_naturalquestions-train-57654", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-train-59865", "mrqa_naturalquestions-train-87351", "mrqa_naturalquestions-train-62378", "mrqa_naturalquestions-train-76830", "mrqa_naturalquestions-train-44151", "mrqa_naturalquestions-train-76688", "mrqa_naturalquestions-train-23124", "mrqa_naturalquestions-train-42049", "mrqa_naturalquestions-train-67908", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-train-34076", "mrqa_naturalquestions-train-36274", "mrqa_naturalquestions-train-7772", "mrqa_naturalquestions-train-69992", "mrqa_naturalquestions-train-71208", "mrqa_naturalquestions-train-61501", "mrqa_naturalquestions-train-30032", "mrqa_naturalquestions-train-11709", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-69197", "mrqa_naturalquestions-train-1327", "mrqa_naturalquestions-train-13651", "mrqa_naturalquestions-train-74865", "mrqa_naturalquestions-train-1784", "mrqa_squad-validation-4181", "mrqa_naturalquestions-train-46670"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Guadalupe \"Lupe\" Ontiveros", "Thiel", "skaters", "dav Coleman", "Newell Highway", "daleks", "14 and one-half hands", "shopping mall", "defiant", "DreamWorks Animation", "skylab", "his own men", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "a procession of gods and mortals", "the RAF", "reduce growth in relatively poor countries but encourage growth", "Ibrium", "Hayley Sanderson", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "polly", "ottoexeter", "an estimated 390 billion", "Boston's Combat Zone", "May 10, 1976", "six", "spain", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "dublin", "Paul the Apostle", "surtania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1650094696969697}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45000000000000007, 0.0, 0.0, 0.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "retrieved_ids": ["mrqa_naturalquestions-train-78304", "mrqa_naturalquestions-train-53566", "mrqa_naturalquestions-train-35204", "mrqa_naturalquestions-train-18938", "mrqa_naturalquestions-train-77999", "mrqa_naturalquestions-train-47177", "mrqa_naturalquestions-train-15887", "mrqa_naturalquestions-train-14030", "mrqa_naturalquestions-train-41577", "mrqa_naturalquestions-train-15403", "mrqa_naturalquestions-train-82927", "mrqa_naturalquestions-train-7719", "mrqa_naturalquestions-train-8388", "mrqa_naturalquestions-train-40870", "mrqa_naturalquestions-train-87141", "mrqa_naturalquestions-train-16563", "mrqa_naturalquestions-train-55611", "mrqa_naturalquestions-train-68523", "mrqa_naturalquestions-train-13788", "mrqa_naturalquestions-train-32598", "mrqa_naturalquestions-train-41043", "mrqa_naturalquestions-train-74165", "mrqa_naturalquestions-train-83375", "mrqa_naturalquestions-train-66871", "mrqa_naturalquestions-train-4019", "mrqa_squad-validation-4637", "mrqa_naturalquestions-train-57642", "mrqa_naturalquestions-train-30029", "mrqa_naturalquestions-train-85773", "mrqa_naturalquestions-train-31942", "mrqa_naturalquestions-train-63984", "mrqa_naturalquestions-train-20779"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A cylindrical Service Module", "the Orthodox Christians", "Coca-Cola", "oktoberfest", "800 m", "Majandra Delfino", "kalium", "if a vehicle towing a trailer skids, the trailer can push the towing vehicle from behind until it spins the vehicle around and faces backwards", "T cell receptor", "relatively low", "non-GMO", "Heading Out to the Highway", "moonraker", "$12.99", "Michael Oppenheimer", "England national team", "\"degrees of privilege\" to which they were entitled institutionally and legally", "No Night Today", "Convention", "5,922", "June 4, 1931", "the title character in Luc Besson's \"Valerian and the City of a Thousand Planets\"", "2015\u201316", "the historical Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Potsdam", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "lily- of-the-valley", "Dealey Plaza", "Nairobi", "the chalk ridge line west of the Needles breached", "Neon City"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2220982142857143}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.8571428571428571, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.25000000000000006, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960", "mrqa_hotpotqa-validation-5557"], "retrieved_ids": ["mrqa_naturalquestions-train-6726", "mrqa_naturalquestions-train-33202", "mrqa_naturalquestions-train-38589", "mrqa_naturalquestions-train-87574", "mrqa_naturalquestions-train-82445", "mrqa_hotpotqa-validation-4430", "mrqa_naturalquestions-train-47530", "mrqa_naturalquestions-train-48429", "mrqa_naturalquestions-train-37544", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-72372", "mrqa_naturalquestions-train-34331", "mrqa_naturalquestions-train-62917", "mrqa_naturalquestions-train-16324", "mrqa_naturalquestions-train-12108", "mrqa_naturalquestions-train-15854", "mrqa_naturalquestions-train-87298", "mrqa_naturalquestions-train-43447", "mrqa_naturalquestions-train-16951", "mrqa_naturalquestions-train-28063", "mrqa_naturalquestions-train-75165", "mrqa_naturalquestions-train-20355", "mrqa_naturalquestions-train-34827", "mrqa_naturalquestions-train-28586", "mrqa_naturalquestions-train-67755", "mrqa_naturalquestions-train-39995", "mrqa_naturalquestions-train-6765", "mrqa_naturalquestions-train-28336", "mrqa_naturalquestions-train-14098", "mrqa_naturalquestions-train-15141", "mrqa_naturalquestions-train-66450", "mrqa_naturalquestions-train-1634"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "is a controversial Canadian-American Roman Catholic priest based in the United States near Detroit at Royal Oak, Michigan's National Shrine of the Little Flower church", "1967", "is the amount charged by a bookmaker, or \"bookie\" for taking a bet from a gambler", "the twelfth most populous city in the United States", "115", "bridge", "through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus, causing changes in gene expression", "lower", "Bass", "Chava with Fyedka", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country", "september", "bridge", "Yunnan- Fu", "Mumbai", "Broken Hill and Sydney", "2005", "forgiveness was God's alone to grant", "\"The Doctor's Daughter\"", "september", "bridge", "The project must adhere to zoning and building code requirements. Constructing a project that fails to adhere to codes does not benefit the owner", "1879", "niece of Empress Taitu Bitul, consort of Emperor Menelik II of Ethiopia", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system", "enthusiasm and energy", "Russo-Japanese War", "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking", "Bill Clinton", "Oslo county"], "metric_results": {"EM": 0.125, "QA-F1": 0.17701494805481874}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.06896551724137931, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-24757", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-train-36537", "mrqa_naturalquestions-train-2650", "mrqa_naturalquestions-train-8826", "mrqa_naturalquestions-train-86984", "mrqa_naturalquestions-train-13162", "mrqa_naturalquestions-train-34866", "mrqa_naturalquestions-train-51271", "mrqa_naturalquestions-train-67785", "mrqa_naturalquestions-train-8873", "mrqa_naturalquestions-train-56184", "mrqa_naturalquestions-train-32781", "mrqa_naturalquestions-train-13837", "mrqa_naturalquestions-train-48091", "mrqa_naturalquestions-train-81051", "mrqa_naturalquestions-train-74050", "mrqa_naturalquestions-train-19886", "mrqa_naturalquestions-train-80827", "mrqa_naturalquestions-train-72819", "mrqa_naturalquestions-train-10737", "mrqa_naturalquestions-train-55786", "mrqa_naturalquestions-train-11436", "mrqa_naturalquestions-train-5944", "mrqa_naturalquestions-train-18040", "mrqa_naturalquestions-train-7947", "mrqa_naturalquestions-train-38985", "mrqa_naturalquestions-train-44073", "mrqa_naturalquestions-train-32654", "mrqa_naturalquestions-train-19230", "mrqa_naturalquestions-train-4931", "mrqa_naturalquestions-train-42206"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Chairman", "english firm of Winterthur near Dunfermline", "Threatening government officials", "Veronica", "Victorian College of the Arts", "Britain", "maya", "0.2 inhabitants per square kilometre (0.52/ sq mi)", "emperor", "France", "Ian Paisley", "bataan Death March", "euro", "Graham McPherson", "Taft -- Katsura Agreement", "the late 1970s", "1890", "Sam Bradford", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Oliver Reed", "pole", "Johnny Darrell", "carotid artery disease", "a Belgian law requiring all margarine to be in cube shaped packages infringed article 34", "Euler's totient function", "earwax", "binary strings", "third by international passenger volume", "orange", "Toyota Corona", "Kurt Vonnegut", "8een years before the events of Tangled"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2733936202686203}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.14285714285714288, 0.6153846153846153, 1.0, 0.0, 0.2222222222222222, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.4, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 1.0, 0.4444444444444445, 0.33333333333333337, 0.0, 0.0, 0.0, 0.2857142857142857]}}, "error_ids": ["mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "retrieved_ids": ["mrqa_naturalquestions-train-36875", "mrqa_naturalquestions-train-54019", "mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-train-20273", "mrqa_naturalquestions-train-57221", "mrqa_naturalquestions-train-62092", "mrqa_naturalquestions-train-75406", "mrqa_naturalquestions-train-83796", "mrqa_naturalquestions-train-11599", "mrqa_naturalquestions-train-40331", "mrqa_naturalquestions-train-88241", "mrqa_naturalquestions-train-11177", "mrqa_naturalquestions-train-58887", "mrqa_naturalquestions-train-8979", "mrqa_naturalquestions-train-41668", "mrqa_naturalquestions-train-82478", "mrqa_naturalquestions-train-24609", "mrqa_naturalquestions-train-81285", "mrqa_naturalquestions-train-83474", "mrqa_naturalquestions-train-87066", "mrqa_naturalquestions-train-13209", "mrqa_naturalquestions-train-10249", "mrqa_naturalquestions-train-4912", "mrqa_naturalquestions-train-32604", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-train-12520", "mrqa_naturalquestions-train-20042", "mrqa_naturalquestions-train-46587", "mrqa_triviaqa-validation-5704", "mrqa_naturalquestions-train-26963", "mrqa_naturalquestions-train-61795", "mrqa_naturalquestions-train-65086"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "based on the interplay of supply and demand, which determines the prices of goods and services", "Emma Watson", "the brain, muscles, and liver", "butterfly", "Washington Redskins", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "William Howard Ashton", "the Watergate affair of the Nixon administration in the 1970s", "high and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Broward County", "Lee Byung-hun", "changing display or audio settings quickly", "Battle of Marston Moor", "derived from the spectroscopic notation for the associated atomic orbitals", "if the income share of the top 20 percent (the rich) increases", "Beauty and the Beast", "South Africa", "Scotty Grainger Jr.", "the Alamo", "seal illegally", "the UMC", "Brian Liesegang", "Don Hahn", "Port Moresby", "afghanistan", "National Association for the Advancement of Colored People", "1963\u20131989", "5 star hotel", "president of the House of Commons", "In season two, life changes dramatically when Samantha and Darin give birth to daughter Tabitha Stephens", "6500 - 1500 BC"], "metric_results": {"EM": 0.21875, "QA-F1": 0.339812383286648}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.5, 0.11111111111111112, 0.0, 0.4, 1.0, 0.0, 0.25, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.4, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "retrieved_ids": ["mrqa_naturalquestions-train-79545", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-train-74994", "mrqa_naturalquestions-train-78580", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-train-75460", "mrqa_naturalquestions-train-49561", "mrqa_naturalquestions-train-70702", "mrqa_naturalquestions-train-86195", "mrqa_naturalquestions-train-36177", "mrqa_naturalquestions-train-40388", "mrqa_naturalquestions-train-44378", "mrqa_triviaqa-validation-935", "mrqa_naturalquestions-train-53746", "mrqa_naturalquestions-train-755", "mrqa_naturalquestions-train-58951", "mrqa_naturalquestions-train-4133", "mrqa_naturalquestions-train-70241", "mrqa_naturalquestions-train-10591", "mrqa_naturalquestions-train-64254", "mrqa_naturalquestions-train-27603", "mrqa_naturalquestions-train-41801", "mrqa_naturalquestions-train-18340", "mrqa_naturalquestions-train-32584", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-51423", "mrqa_naturalquestions-train-39681", "mrqa_naturalquestions-train-58310", "mrqa_naturalquestions-train-66018", "mrqa_naturalquestions-train-56271", "mrqa_naturalquestions-train-86197", "mrqa_hotpotqa-validation-1888"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Kevin Costner", "Uranus", "president harding", "Cobham\u2013Edmonds thesis", "Elisabeth Sladen", "Seal II", "March 2012", "jazz musicians and other residents of the city", "Muhammad Ali", "Coldplay", "Spain", "to civil disobedients", "Julius Caesar", "2%", "1979 and 1980", "an MMORPG and as a virtual society, with its currency being the most stable in the real world", "decision problem", "india", "the right side of the heart to the lungs", "Miasma theory", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges (sub-ranges of the Rocky Mountains)", "imperial war flag of the Holy Roman Empire", "The U.S. state of Georgia", "afghanistan", "US$3 per barrel to nearly $12 globally", "a flat rate of 20 %", "Wembley", "in the ARPANET", "roughly west", "Libya"], "metric_results": {"EM": 0.125, "QA-F1": 0.33363546176046177}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.36363636363636365, 0.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.5, 0.21428571428571427, 0.4444444444444445, 0.0, 0.8, 0.0, 0.3333333333333333, 0.5, 0.0, 0.4, 0.0, 0.4444444444444445, 0.4, 0.0, 0.8, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_hotpotqa-validation-1118", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_squad-validation-3635", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-77353", "mrqa_naturalquestions-train-84658", "mrqa_naturalquestions-train-66035", "mrqa_naturalquestions-train-26367", "mrqa_naturalquestions-train-85421", "mrqa_naturalquestions-train-35562", "mrqa_naturalquestions-train-69841", "mrqa_naturalquestions-train-40020", "mrqa_naturalquestions-train-36820", "mrqa_naturalquestions-train-22135", "mrqa_naturalquestions-train-73444", "mrqa_naturalquestions-train-50289", "mrqa_naturalquestions-train-37755", "mrqa_naturalquestions-train-28130", "mrqa_naturalquestions-train-43426", "mrqa_naturalquestions-train-73950", "mrqa_squad-validation-8190", "mrqa_naturalquestions-train-72513", "mrqa_naturalquestions-train-56911", "mrqa_naturalquestions-train-52667", "mrqa_naturalquestions-train-23560", "mrqa_naturalquestions-train-36145", "mrqa_naturalquestions-train-22037", "mrqa_naturalquestions-train-38208", "mrqa_naturalquestions-train-38661", "mrqa_naturalquestions-train-10", "mrqa_naturalquestions-train-35387", "mrqa_naturalquestions-train-3690", "mrqa_naturalquestions-train-45305", "mrqa_naturalquestions-train-10336", "mrqa_naturalquestions-train-10368", "mrqa_naturalquestions-train-44223"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["Union Pacific Railroad", "three of his ribs were broken", "7 December 2000", "Post Alley under Pike Place Market", "mother-of-pearl", "February 20, 1978", "haggis", "Walter Mondale", "96", "De Inventione by Marcus Tullius Cicero", "rms titanic", "a bubble on a black background representing the circle with glossy gold letters", "the alluvial plain", "around 11 miles (18 km) south of San Jose", "Spotty Dog", "Rumplestiltskin", "Carlos Tevez", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "musical venues, including the Teatr Wielki, the Polish National opera, the Chamber Opera, the National Philharmonic Hall and the National Theatre", "riper grapes", "1991", "india", "7 January 1936", "lifetime protection", "twenty- three", "Edwin Hubble", "education, sanitation, and traffic control within the city limits. In addition, residents in unincorporated suburbs had difficulty obtaining municipal services, such as sewage and building code enforcement.", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Nlend Wom\u00e9", "to make a defiant speech, or a speech explaining their actions, in allocution", "a Distinctive Yogurt For Health", "Boston, Massachusetts"], "metric_results": {"EM": 0.1875, "QA-F1": 0.25925936163939256}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.28571428571428575, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 0.10526315789473684, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.8, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "retrieved_ids": ["mrqa_naturalquestions-train-74341", "mrqa_naturalquestions-train-36333", "mrqa_naturalquestions-train-76271", "mrqa_naturalquestions-train-52076", "mrqa_naturalquestions-train-47505", "mrqa_naturalquestions-train-6726", "mrqa_squad-validation-5618", "mrqa_naturalquestions-train-87808", "mrqa_naturalquestions-train-6559", "mrqa_naturalquestions-train-51674", "mrqa_naturalquestions-train-16386", "mrqa_naturalquestions-train-6631", "mrqa_naturalquestions-train-28072", "mrqa_naturalquestions-train-44782", "mrqa_naturalquestions-train-53250", "mrqa_naturalquestions-train-50935", "mrqa_naturalquestions-train-59962", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-train-11314", "mrqa_naturalquestions-train-48556", "mrqa_triviaqa-validation-6944", "mrqa_naturalquestions-train-2782", "mrqa_naturalquestions-train-34452", "mrqa_naturalquestions-train-51539", "mrqa_naturalquestions-train-24292", "mrqa_naturalquestions-train-7459", "mrqa_naturalquestions-train-25908", "mrqa_naturalquestions-train-43056", "mrqa_triviaqa-validation-3300", "mrqa_naturalquestions-train-58847", "mrqa_naturalquestions-train-34902", "mrqa_naturalquestions-train-15636"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "New England", "Etienne de Mestre", "goat", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "military units from their parent countries of Great Britain and France", "The History of Little Goody Two - Shoes", "the longest rotation period ( 243 days ) of any planet in the Solar System", "gathering money from the public", "Thorgan Hazard", "commissioned to purchase their required uniform items", "Jeff Meldrum", "741 weeks", "Norman Painting", "Shoshone, his mother tongue, and other western American Indian languages", "The Paris Sisters", "Suez Canal", "60", "laboratory", "to compensate for the fact that there is no revising chamber", "avatar", "the points of algebro-geometric objects", "most of the items in the collection, unless those were newly accessioned into the collection", "free floating and depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "glycine", "Alta Wind Energy Center in California", "The early modern period began approximately in the early 16th century ; notable historical milestones included the European Renaissance, the Age of Discovery, and the Protestant Reformation.", "Lord's", "Eddy Shah", "Jason Marsden", "in sequence with each heartbeat"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21714947403692658}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.09523809523809523, 0.13333333333333333, 1.0, 0.14285714285714288, 0.47058823529411764, 0.6666666666666666, 0.22222222222222224, 0.25, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 0.2857142857142857, 0.0, 0.4615384615384615, 0.0, 0.0, 0.5, 0.06451612903225806, 0.0, 0.0, 0.3846153846153846, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-1660", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-3320", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "retrieved_ids": ["mrqa_naturalquestions-train-6937", "mrqa_naturalquestions-train-22090", "mrqa_naturalquestions-train-62221", "mrqa_naturalquestions-train-54304", "mrqa_naturalquestions-train-53482", "mrqa_naturalquestions-train-73184", "mrqa_naturalquestions-train-26521", "mrqa_naturalquestions-train-83558", "mrqa_naturalquestions-train-57524", "mrqa_naturalquestions-train-76832", "mrqa_naturalquestions-train-36730", "mrqa_naturalquestions-train-46732", "mrqa_naturalquestions-train-21346", "mrqa_naturalquestions-train-57784", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-84973", "mrqa_naturalquestions-train-68269", "mrqa_naturalquestions-train-62773", "mrqa_naturalquestions-train-37201", "mrqa_naturalquestions-train-81306", "mrqa_naturalquestions-train-72025", "mrqa_squad-validation-4185", "mrqa_naturalquestions-train-39172", "mrqa_naturalquestions-train-28113", "mrqa_naturalquestions-train-43194", "mrqa_naturalquestions-train-32620", "mrqa_naturalquestions-train-49083", "mrqa_naturalquestions-train-44552", "mrqa_naturalquestions-train-88050", "mrqa_naturalquestions-train-77661", "mrqa_naturalquestions-train-51611", "mrqa_naturalquestions-train-15514"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["capital city of Taiwan", "Dan Conner", "westward", "president garfield", "Lois Mae Green", "violence", "Joaquin Phoenix", "keskes", "1977", "John M. Grunsfeld", "New York", "elton john", "2000", "antlers are dropped or shed and grown anew each and every year", "NTV", "the second Sunday of March", "relative units", "lady", "two", "August 10, 1933", "one - mile - wide ( 1.6 km )", "Sochi, Russia", "those who already hold wealth", "B. Traven", "Disney-Pixar film \" Finding Nemo\"", "unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "monetary Sovereignty", "cheagrass", "264,152", "Princeton", "the German Empire", "nearly pure O2 gas"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3007070441813089}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_naturalquestions-validation-3108", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-77702", "mrqa_naturalquestions-train-31729", "mrqa_naturalquestions-train-45490", "mrqa_naturalquestions-train-87801", "mrqa_naturalquestions-train-9602", "mrqa_naturalquestions-train-72372", "mrqa_naturalquestions-train-15888", "mrqa_naturalquestions-train-28015", "mrqa_naturalquestions-train-39414", "mrqa_triviaqa-validation-6881", "mrqa_naturalquestions-train-82834", "mrqa_naturalquestions-train-62651", "mrqa_naturalquestions-train-19450", "mrqa_naturalquestions-train-36648", "mrqa_naturalquestions-train-39812", "mrqa_naturalquestions-train-74023", "mrqa_naturalquestions-train-3171", "mrqa_naturalquestions-train-44403", "mrqa_naturalquestions-train-16813", "mrqa_naturalquestions-train-76678", "mrqa_naturalquestions-train-13583", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-train-41702", "mrqa_naturalquestions-train-27961", "mrqa_naturalquestions-train-22233", "mrqa_naturalquestions-train-74261", "mrqa_naturalquestions-train-79317", "mrqa_naturalquestions-train-80655", "mrqa_naturalquestions-train-21838", "mrqa_naturalquestions-train-74415", "mrqa_naturalquestions-train-7598", "mrqa_naturalquestions-train-2884"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "Vulcan", "every year between 1346 and 1671", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "he broadened the foundations of the Reformation placing them on prophetic faith", "Aristotle", "Charlton Heston", "anti-inflammatory molecules, such as cortisol and catecholamines", "vUHMOaD/JVI aiAQBAJA jaMOJOD", "vick", "one of the uses of money", "may have the force of law, if based on the authority derived from statute or the Constitution itself", "today", "because the Uighur King of Qocho was ranked higher than the Karluk Kara-Khanid ruler", "Sochi, Russia", "right", "from the Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "How soon after the cabin fire incident", "vida Goldstein", "doreen", "30", "Secret Intelligence Service", "100 billion", "kai su, teknon", "photosynthesis", "4 - inch", "Queen City", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25586747491638795}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.375, 0.6666666666666666, 0.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.13333333333333333, 0.125, 0.0, 0.0, 1.0, 0.0, 0.17391304347826084, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.16]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-2475", "mrqa_squad-validation-4953", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "retrieved_ids": ["mrqa_naturalquestions-train-30910", "mrqa_naturalquestions-train-86195", "mrqa_naturalquestions-train-27018", "mrqa_squad-validation-10148", "mrqa_naturalquestions-train-59591", "mrqa_naturalquestions-train-68709", "mrqa_naturalquestions-train-33877", "mrqa_naturalquestions-train-83867", "mrqa_naturalquestions-train-44809", "mrqa_naturalquestions-train-4328", "mrqa_hotpotqa-validation-2184", "mrqa_naturalquestions-train-31525", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-train-68989", "mrqa_naturalquestions-train-4552", "mrqa_naturalquestions-train-32591", "mrqa_naturalquestions-train-41241", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-train-54481", "mrqa_naturalquestions-train-42769", "mrqa_naturalquestions-train-33623", "mrqa_naturalquestions-train-42818", "mrqa_naturalquestions-train-74708", "mrqa_naturalquestions-train-9574", "mrqa_naturalquestions-train-80906", "mrqa_naturalquestions-train-22432", "mrqa_naturalquestions-train-73894", "mrqa_naturalquestions-train-56931", "mrqa_naturalquestions-train-70582", "mrqa_naturalquestions-train-46487", "mrqa_naturalquestions-train-16443", "mrqa_naturalquestions-train-32742"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["all war", "Saira Banu", "Gaels", "Three card brag", "d\u00edsabl\u00f3t", "European or Eurasian cave lion", "drama", "sediment load", "Washington metropolitan area", "scission", "User State Migration Tool", "Ordos City China Science Flying Universe Science and Technology Co.", "pie tins", "PPG Paints Arena, Pittsburgh, Pennsylvania", "philry wall museum", "Section 30 of the Teaching Council Act 2001", "Wilbur", "mid-1988", "quasars", "Monsoon", "Romansh", "Tudor king", "Newstalk radio station 5AA", "James Bond", "Philippians", "describes one of the world's first collected descriptions of what builds nations'wealth", "Gerard Marenghi", "Whitney Houston", "Nebula Award", "conservative", "Saul", "Elvis Presley"], "metric_results": {"EM": 0.09375, "QA-F1": 0.20625742721330959}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.05882352941176471, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.4, 0.4, 0.0, 1.0, 0.26666666666666666, 0.5, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "retrieved_ids": ["mrqa_naturalquestions-train-76701", "mrqa_naturalquestions-train-37521", "mrqa_naturalquestions-train-42705", "mrqa_naturalquestions-train-36368", "mrqa_naturalquestions-train-31569", "mrqa_naturalquestions-train-48612", "mrqa_naturalquestions-train-6205", "mrqa_naturalquestions-train-44294", "mrqa_naturalquestions-train-48555", "mrqa_naturalquestions-train-55663", "mrqa_naturalquestions-train-48935", "mrqa_naturalquestions-train-84084", "mrqa_naturalquestions-train-79782", "mrqa_naturalquestions-train-53409", "mrqa_naturalquestions-train-83744", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-53040", "mrqa_naturalquestions-train-61742", "mrqa_triviaqa-validation-6721", "mrqa_naturalquestions-train-23262", "mrqa_naturalquestions-train-56941", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-2690", "mrqa_naturalquestions-train-25890", "mrqa_naturalquestions-train-43283", "mrqa_naturalquestions-train-37667", "mrqa_naturalquestions-train-56598", "mrqa_naturalquestions-train-11827", "mrqa_naturalquestions-train-1919", "mrqa_naturalquestions-train-42800", "mrqa_naturalquestions-train-19161", "mrqa_naturalquestions-train-75125"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["david carradine", "pasta ( usually cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "brian", "an alliance between the city-state of Geneva and the Swiss Confederation", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "the nature of Abraham Lincoln's war goals", "paid professionals", "explaining their actions", "a large Danish shipping company that operates passenger and freight services across northern Europe", "centre-back", "rommel", "where the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position )", "glucose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "brian gumbel", "one of sixteen personality types", "Thursday", "racing", "dose, route, frequency, and duration", "Mars", "feats of exploration", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "bobby", "The Private Education Student Financial Assistance", "bow", "to raise money to rebuild St. Peter's Basilica in Rome", "colonies", "two forces, one pointing north, and one pointing east", "new laws or amendments to existing laws as a bill", "Jack Murphy Stadium", "the time and space hierarchy theorems"], "metric_results": {"EM": 0.1875, "QA-F1": 0.32322363962988965}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.7272727272727273, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.7567567567567568, 0.0, 0.375, 0.5, 0.0, 0.0, 0.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_squad-validation-9452", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-54216", "mrqa_naturalquestions-train-86251", "mrqa_hotpotqa-validation-5682", "mrqa_naturalquestions-train-56342", "mrqa_naturalquestions-train-69918", "mrqa_naturalquestions-train-5169", "mrqa_naturalquestions-train-5261", "mrqa_naturalquestions-train-39975", "mrqa_naturalquestions-train-33104", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-15542", "mrqa_naturalquestions-train-52458", "mrqa_naturalquestions-train-61795", "mrqa_naturalquestions-train-18368", "mrqa_naturalquestions-train-50171", "mrqa_naturalquestions-train-19242", "mrqa_naturalquestions-train-68989", "mrqa_naturalquestions-train-45305", "mrqa_naturalquestions-train-28133", "mrqa_naturalquestions-train-66188", "mrqa_naturalquestions-train-76665", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-train-36004", "mrqa_naturalquestions-train-532", "mrqa_squad-validation-6734", "mrqa_naturalquestions-train-7807", "mrqa_naturalquestions-train-37124", "mrqa_naturalquestions-train-23704", "mrqa_naturalquestions-train-71491", "mrqa_naturalquestions-train-62868", "mrqa_naturalquestions-train-78256", "mrqa_naturalquestions-train-69421"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["canada", "white", "egypt", "equirrh", "French", "a sailor coming home from a round trip", "domain name www.example.com", "the immune system is less active than normal", "egypt", "natural-ing Ingredients- only personal care products", "egypt", "Rigoletto", "Rossiya Airlines is a Russian airline on Earth by land area, distances within Russia can be very long, and air travel is frequently needed for the President to travel across the country as well as internationally", "third-most", "furniture", "216", "eribe peralta", "Nicholas Stone", "Algernod Lanier Washington", "the Outfield", "singles", "Michael Edwards", "road engines", "eddie", "third quarter ( also known as last quarter )", "egypt", "Yuan T. Lee", "Kentucky, Virginia, and Tennessee", "many areas of technology incidental to rocketry and manned spaceflight", "eve", "about 615 square kilometers", "egypt"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24262218045112782}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.3333333333333333, 0.0, 0.0, 0.6, 0.0, 0.0, 0.05714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4210526315789474, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "retrieved_ids": ["mrqa_naturalquestions-train-81880", "mrqa_naturalquestions-train-69012", "mrqa_naturalquestions-train-73233", "mrqa_naturalquestions-train-59715", "mrqa_naturalquestions-train-61686", "mrqa_naturalquestions-train-65370", "mrqa_naturalquestions-train-4191", "mrqa_naturalquestions-train-67452", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-19309", "mrqa_naturalquestions-train-72068", "mrqa_naturalquestions-train-41983", "mrqa_naturalquestions-train-52754", "mrqa_naturalquestions-train-25028", "mrqa_naturalquestions-train-61417", "mrqa_naturalquestions-train-58522", "mrqa_naturalquestions-train-16539", "mrqa_naturalquestions-train-70126", "mrqa_naturalquestions-train-61719", "mrqa_naturalquestions-train-3371", "mrqa_naturalquestions-train-65166", "mrqa_naturalquestions-train-41726", "mrqa_naturalquestions-train-6578", "mrqa_naturalquestions-train-14276", "mrqa_squad-validation-108", "mrqa_naturalquestions-train-62285", "mrqa_naturalquestions-train-50721", "mrqa_squad-validation-8190", "mrqa_naturalquestions-train-19450", "mrqa_naturalquestions-train-19358", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-train-62960"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["Wakanda", "income", "2003", "hockey", "football", "campaign setting", "2003", "867 feet", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\u00f7", "Dooku / Darth Tyranus", "second most commonly named \"dream college\" both for students and parents in 2013, and was the first nominated by parents in 2009", "picture book", "all health care settings", "increased patient health outcomes and decreased costs to the health care system", "treble clef", "Lecrae", "150 km / h", "between the Piazza di Spagna at the base and Piazzo Trinit\u00e0 dei Monti", "December 1, 2009", "Estelle Sylvia Pankhurst", "27member states", "philosopher, statesman, jurist, orator, and author", "richard burghs", "Indian government ministry", "British", "the site of ancient cult activity as far back as 7th century BCE", "richard burton", "energy-storage molecules", "sn Hubble Space Telescope", "a genuine love of our neighbors as ourselves", "chorale cantatas"], "metric_results": {"EM": 0.09375, "QA-F1": 0.3022035256410256}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.2857142857142857, 0.0, 0.19999999999999998, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.8571428571428571, 0.25, 0.0, 1.0, 0.39999999999999997, 0.0, 0.5, 0.5, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951"], "retrieved_ids": ["mrqa_naturalquestions-train-68221", "mrqa_naturalquestions-train-47444", "mrqa_naturalquestions-train-57484", "mrqa_naturalquestions-train-10672", "mrqa_naturalquestions-train-19243", "mrqa_naturalquestions-train-40141", "mrqa_naturalquestions-train-49455", "mrqa_naturalquestions-train-45857", "mrqa_naturalquestions-train-29555", "mrqa_naturalquestions-train-33619", "mrqa_naturalquestions-train-9346", "mrqa_naturalquestions-train-60830", "mrqa_naturalquestions-train-41998", "mrqa_naturalquestions-train-68474", "mrqa_naturalquestions-train-53991", "mrqa_naturalquestions-train-56698", "mrqa_naturalquestions-train-78797", "mrqa_naturalquestions-train-7088", "mrqa_naturalquestions-train-54868", "mrqa_naturalquestions-train-79489", "mrqa_naturalquestions-train-70671", "mrqa_naturalquestions-train-85770", "mrqa_naturalquestions-train-83939", "mrqa_naturalquestions-train-23538", "mrqa_naturalquestions-train-67383", "mrqa_naturalquestions-train-71646", "mrqa_naturalquestions-train-43248", "mrqa_naturalquestions-train-8742", "mrqa_naturalquestions-train-38055", "mrqa_naturalquestions-train-11208", "mrqa_naturalquestions-train-23220", "mrqa_naturalquestions-train-59597"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["scheptane", "schehematically eliminated from the playoffs", "tobacco", "his leg under `` the immortal Hawke ''", "death penalty", "a stout man with a \"practice chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "south and east coast of  southern Africa", "rich Fisher King is the son of the king", "Mangal Pandey", "Colonia Agrippina", "Cartwright clan", "four of the 50 states of the United States", "curling", "the eighth series", "Pebble Beach", "Los Angeles", "French", "Henry Mills", "\"LOVE Radio\" which featured a limited selection of music genres, was launched on ABC's seven owned-and-operated FM stations in late November 1968", "Miami Marlins", "elected by the court from its members for a three - year term", "Tony Manero", "Donald Henkel", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Cashin' In", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "Santa Clara, California", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Operation Neptune", "Mediterranean Sea"], "metric_results": {"EM": 0.125, "QA-F1": 0.21974755317146621}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17391304347826084, 0.0, 0.18181818181818182, 1.0, 0.0, 0.07142857142857142, 0.0, 1.0, 0.4166666666666667, 0.16666666666666669, 0.8, 0.07142857142857142, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_squad-validation-5852", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-6783", "mrqa_naturalquestions-train-73654", "mrqa_naturalquestions-train-56277", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-train-35082", "mrqa_naturalquestions-train-69235", "mrqa_naturalquestions-train-65794", "mrqa_naturalquestions-train-26392", "mrqa_naturalquestions-train-67439", "mrqa_naturalquestions-train-65166", "mrqa_triviaqa-validation-7032", "mrqa_naturalquestions-train-6539", "mrqa_naturalquestions-train-23410", "mrqa_naturalquestions-train-25259", "mrqa_squad-validation-8966", "mrqa_naturalquestions-train-35314", "mrqa_naturalquestions-train-69374", "mrqa_naturalquestions-train-47541", "mrqa_naturalquestions-train-44544", "mrqa_hotpotqa-validation-2197", "mrqa_naturalquestions-train-45994", "mrqa_naturalquestions-train-56636", "mrqa_naturalquestions-train-24206", "mrqa_naturalquestions-train-11314", "mrqa_naturalquestions-train-8986", "mrqa_naturalquestions-train-79425", "mrqa_naturalquestions-train-11765", "mrqa_naturalquestions-train-27773", "mrqa_naturalquestions-train-61318", "mrqa_naturalquestions-train-20880", "mrqa_triviaqa-validation-4762", "mrqa_naturalquestions-train-23189"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "the seafloor itself moves ( and also carries the continents with it ) as it expands from a central axis", "m.E.N. Arena", "youngest publicly documented people to be identified as transgender", "electric lighting", "used their knowledge of Native American languages as a basis to transmit coded messages", "Einstein", "the absenceistence of the ultraviolet catastrophe", "Premier League", "millais", "Elizabeth Weber", "It was released for PlayStation 4 and Xbox One on May 3, 2016.", "hundreds", "Nightmare Before Christmas", "1999", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "The Five Doctors", "5% abv draught beer", "inefficient", "Chu'Tsai", "Liz", "the least onerous", "lake como", "Grissom, White, and Chaffee", "multinational retail corporation", "passion fruit", "the ancient scholar Bharata Muni", "salt water marshes", "Samuel Ryder", "parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass", "emperor of Austria"], "metric_results": {"EM": 0.3125, "QA-F1": 0.35844494047619047}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.1, 0.0, 0.375, 1.0, 0.06666666666666667, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10489", "mrqa_hotpotqa-validation-1831", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-7792", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "retrieved_ids": ["mrqa_squad-validation-1688", "mrqa_naturalquestions-train-71057", "mrqa_naturalquestions-train-24912", "mrqa_naturalquestions-train-63870", "mrqa_naturalquestions-train-39789", "mrqa_naturalquestions-train-35883", "mrqa_naturalquestions-train-14089", "mrqa_naturalquestions-train-73699", "mrqa_naturalquestions-train-83796", "mrqa_naturalquestions-train-13249", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-train-2987", "mrqa_naturalquestions-train-61316", "mrqa_naturalquestions-train-83106", "mrqa_naturalquestions-train-8496", "mrqa_naturalquestions-train-68799", "mrqa_naturalquestions-train-33064", "mrqa_naturalquestions-train-11996", "mrqa_naturalquestions-train-17568", "mrqa_naturalquestions-train-73939", "mrqa_naturalquestions-train-66450", "mrqa_naturalquestions-train-49380", "mrqa_naturalquestions-train-33560", "mrqa_naturalquestions-train-72600", "mrqa_naturalquestions-train-21912", "mrqa_naturalquestions-train-612", "mrqa_naturalquestions-train-48360", "mrqa_squad-validation-9063", "mrqa_naturalquestions-train-16707", "mrqa_naturalquestions-train-27538", "mrqa_naturalquestions-train-9576", "mrqa_naturalquestions-train-41033"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["George Bush", "cuba", "Burnley and the New Zealand national team", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "big - name lawyers", "Milk Barn Animation", "when they enter the army during initial entry training", "moral tale", "quit", "l Leeds", "A tree - topper or treetopper", "74 per cent", "Heathrow Express", "often social communities with considerable face-to-face interaction among members", "Landon Jones", "monophyletic", "insects and their relationship to humans, other organisms, and the environment", "catechism questions", "a pH indicator", "about 50% oxygen composition at standard pressure", "63,182,000", "John and Charles Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Campbellsville University", "jett rink", "Gavin Kossef", "overweight girls", "works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Jocelyn Flores", "downward pressure"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3150568181818182}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.19999999999999998, 0.6666666666666666, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-2092", "mrqa_squad-validation-7182"], "retrieved_ids": ["mrqa_naturalquestions-train-9779", "mrqa_naturalquestions-train-56357", "mrqa_naturalquestions-train-75317", "mrqa_naturalquestions-train-68896", "mrqa_naturalquestions-train-80628", "mrqa_naturalquestions-train-24328", "mrqa_naturalquestions-train-29742", "mrqa_naturalquestions-train-4559", "mrqa_naturalquestions-train-33482", "mrqa_naturalquestions-train-5543", "mrqa_naturalquestions-train-66664", "mrqa_naturalquestions-train-79259", "mrqa_naturalquestions-train-24281", "mrqa_naturalquestions-train-2237", "mrqa_naturalquestions-train-82915", "mrqa_naturalquestions-train-7906", "mrqa_naturalquestions-train-79164", "mrqa_naturalquestions-train-30353", "mrqa_naturalquestions-train-62641", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-train-44547", "mrqa_naturalquestions-train-35659", "mrqa_naturalquestions-train-21442", "mrqa_naturalquestions-train-44829", "mrqa_naturalquestions-train-39641", "mrqa_naturalquestions-train-30344", "mrqa_naturalquestions-train-72073", "mrqa_naturalquestions-train-47935", "mrqa_naturalquestions-train-61057", "mrqa_naturalquestions-train-50097", "mrqa_naturalquestions-train-38171", "mrqa_hotpotqa-validation-1509"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["Barbara Ryan Coleman", "Good Kid, M.A.D City", "Yosemite National Park", "Interventive treatment", "3 lines of reflection and rotational symmetry of order 3 about its center", "Bishop Reuben H. Mueller", "ray char Charles", "his epic battle with Frieza", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "italy", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater", "sin", "annuity", "Padm\u00e9 Amidala", "Frank Theodore `` Ted '' Levine", "every good work designed to attract God's favor", "semi-independent State of Vietnam, within the French Union", "It was largely determined by President Woodrow Wilson", "cappuccino", "halal", "Hecuba", "a method of imparting the basics of Christianity to the congregations", "Wylie Draper", "political role for Islam", "the university's off- campuses rental policies", "hockey greats Bobby Hull and Dennis Hull", "the defending Super Bowl XLIX champion New England Patriots", "war, famine, and weather"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31117198773448773}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.0, 0.9090909090909091, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2666666666666667, 0.4, 0.6666666666666665, 0.26666666666666666, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_squad-validation-5665", "mrqa_naturalquestions-validation-49", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264"], "retrieved_ids": ["mrqa_squad-validation-4662", "mrqa_naturalquestions-train-85505", "mrqa_naturalquestions-train-14318", "mrqa_naturalquestions-train-45042", "mrqa_squad-validation-2010", "mrqa_naturalquestions-train-64194", "mrqa_naturalquestions-train-77661", "mrqa_naturalquestions-train-51145", "mrqa_naturalquestions-train-26087", "mrqa_naturalquestions-train-11827", "mrqa_naturalquestions-train-105", "mrqa_naturalquestions-train-2206", "mrqa_naturalquestions-train-77070", "mrqa_naturalquestions-train-48842", "mrqa_naturalquestions-train-12564", "mrqa_naturalquestions-train-81857", "mrqa_squad-validation-4369", "mrqa_naturalquestions-train-79788", "mrqa_naturalquestions-train-28072", "mrqa_naturalquestions-train-35053", "mrqa_naturalquestions-train-9800", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-train-82064", "mrqa_naturalquestions-train-27683", "mrqa_naturalquestions-train-44830", "mrqa_squad-validation-1003", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-train-15102", "mrqa_naturalquestions-train-8053", "mrqa_naturalquestions-train-10232", "mrqa_naturalquestions-train-69046", "mrqa_triviaqa-validation-1521"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "Aol", "Timur", "stand", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "fear of flying in a car.", "1937", "solid economic growth", "biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "he had assigned them to the company in lieu of stock", "Marxist and a Leninist", "variation in plants", "constitutional, electricity, coal, oil, gas, nuclear energy, defence and national security, drug policy, employment, foreign policy and relations with Europe", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "3,600", "State Street", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "44 hectares", "george cukor", "shirirakonpolis", "Lawton Mainor Chiles Jr.", "Kobol's Last Gleaming", "Wisconsin", "uneven trade agreements", "Ugali with vegetables, sour milk, meat, fish or any other stew", "energy", "georgia state", "Ruth Elizabeth \"Bette\" Davis", "uranium", "7 December 2004"], "metric_results": {"EM": 0.125, "QA-F1": 0.2274087024087024}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.13333333333333333, 0.0, 1.0, 0.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 0.0, 0.3636363636363636, 0.888888888888889, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "retrieved_ids": ["mrqa_naturalquestions-train-19843", "mrqa_naturalquestions-train-6783", "mrqa_naturalquestions-train-45827", "mrqa_naturalquestions-train-61127", "mrqa_naturalquestions-train-51398", "mrqa_naturalquestions-train-22528", "mrqa_naturalquestions-train-85317", "mrqa_naturalquestions-train-77999", "mrqa_naturalquestions-train-32005", "mrqa_naturalquestions-train-32592", "mrqa_squad-validation-4506", "mrqa_naturalquestions-train-70684", "mrqa_naturalquestions-train-46169", "mrqa_naturalquestions-train-76315", "mrqa_naturalquestions-train-40110", "mrqa_hotpotqa-validation-1906", "mrqa_naturalquestions-train-57384", "mrqa_naturalquestions-train-23165", "mrqa_naturalquestions-train-38156", "mrqa_naturalquestions-train-29989", "mrqa_naturalquestions-train-87100", "mrqa_naturalquestions-train-71205", "mrqa_naturalquestions-train-6347", "mrqa_naturalquestions-train-52639", "mrqa_naturalquestions-train-11177", "mrqa_naturalquestions-train-48270", "mrqa_naturalquestions-train-27365", "mrqa_naturalquestions-train-8062", "mrqa_naturalquestions-train-59833", "mrqa_naturalquestions-train-16498", "mrqa_naturalquestions-train-21258", "mrqa_naturalquestions-train-62445"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.146875, "QA-F1": 0.2548461423820384}, "overall_error_number": 1365, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.69375, "QA-F1": 0.7622826969670455}, "final_upstream_test": {"EM": 0.742, "QA-F1": 0.8495908263824956}}}