{"model_update_steps": 1945, "method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=42_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=42_ckpts/', replay_candidate_size=8, replay_frequency=1, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "The Iroquois", "philanthropy", "chley and settles down with him for the rest of the series", "Virginia Wade", "Gary Morris", "to the anterolateral corner of the spinal cord", "1966", "radioisotope thermoelectric generator", "john Cameron", "The Stock Market crash in New York", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs show Summary", "acmthompson", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "the Exclusive Rights to Superman", "May and June 2010"], "metric_results": {"EM": 0.125, "QA-F1": 0.20679528124381066}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705885, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.5, 0.0, 0.4444444444444445, 0.07407407407407407, 0.4, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "retrieved_ids": ["mrqa_naturalquestions-train-25899", "mrqa_naturalquestions-train-86907", "mrqa_naturalquestions-train-55714", "mrqa_naturalquestions-train-53472", "mrqa_naturalquestions-train-20969", "mrqa_naturalquestions-train-12110", "mrqa_naturalquestions-train-67206", "mrqa_naturalquestions-train-48945", "mrqa_naturalquestions-train-54271", "mrqa_naturalquestions-train-22664", "mrqa_naturalquestions-train-8670", "mrqa_naturalquestions-train-48490", "mrqa_naturalquestions-train-9941", "mrqa_naturalquestions-train-76814", "mrqa_naturalquestions-train-62141", "mrqa_naturalquestions-train-57438", "mrqa_naturalquestions-train-23178", "mrqa_naturalquestions-train-68734", "mrqa_naturalquestions-train-68501", "mrqa_naturalquestions-train-68628", "mrqa_naturalquestions-train-61245", "mrqa_naturalquestions-train-15351", "mrqa_naturalquestions-train-19770", "mrqa_naturalquestions-train-57523", "mrqa_naturalquestions-train-61506", "mrqa_naturalquestions-train-55687", "mrqa_naturalquestions-train-24867", "mrqa_naturalquestions-train-11493", "mrqa_naturalquestions-train-26269", "mrqa_naturalquestions-train-35494", "mrqa_naturalquestions-train-56", "mrqa_naturalquestions-train-86984"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "boxing", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "marioneth and Llantisilly Rail Traction Company Limited", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "a phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "technological advances", "a student in the second year at a high school, college, or university", "Masaharu Iwata", "Terry Reid", "is of sufficient quality", "Elgar", "North America", "Andr\u00e9 3000", "Commander", "Akhenaten", "President Theodore Roosevelt", "the fourth season", "Denver Broncos", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "mares Bizet", "Matt Winer", "1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.0, "QA-F1": 0.14935810717060716}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.25, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.4, 0.0, 0.0, 0.3636363636363636, 0.3333333333333333, 0.6666666666666666, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_hotpotqa-validation-3242", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "retrieved_ids": ["mrqa_naturalquestions-train-47686", "mrqa_naturalquestions-train-29676", "mrqa_naturalquestions-train-85771", "mrqa_naturalquestions-train-49179", "mrqa_naturalquestions-train-22436", "mrqa_naturalquestions-train-17803", "mrqa_naturalquestions-train-85604", "mrqa_naturalquestions-train-17996", "mrqa_naturalquestions-train-39529", "mrqa_naturalquestions-train-87252", "mrqa_naturalquestions-train-58847", "mrqa_naturalquestions-train-30898", "mrqa_naturalquestions-train-54111", "mrqa_naturalquestions-train-75416", "mrqa_naturalquestions-train-73471", "mrqa_naturalquestions-train-44747", "mrqa_naturalquestions-train-46116", "mrqa_naturalquestions-train-4406", "mrqa_naturalquestions-train-66871", "mrqa_naturalquestions-train-81862", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-train-36261", "mrqa_naturalquestions-train-80116", "mrqa_naturalquestions-train-8116", "mrqa_naturalquestions-train-67755", "mrqa_naturalquestions-train-20687", "mrqa_naturalquestions-train-26280", "mrqa_naturalquestions-train-8006", "mrqa_naturalquestions-train-15604", "mrqa_naturalquestions-train-59855", "mrqa_naturalquestions-train-59187", "mrqa_naturalquestions-train-26779"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "is set to host a Formula One grand prix in the 2015 or 2016 season", "id", "between 27 July and 7 August 2022", "New York", "is getting a remake", "2006", "the Uniting for Consensus", "lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "casket letters", "is one of the most successful female jockeys in North American horse racing history", "death mask", "islam", "Overtime", "Sir Henry Cole", "trouble distinguishing between carbon dioxide and oxygen", "is a British sitcom, broadcast in the United Kingdom from 1982 to 1984", "cement City, Texas", "the Democratic Unionist Party", "20 May 1973", "many educational institutions especially within the US", "many traditions of Hinduism - especially those common in the West", "control purposes", "island in the Sun", "Callability", "2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM, either 16 or 32 GB of internal storage, and a 2300 mAh battery", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins", "gallbladder", "berenice Abbott"], "metric_results": {"EM": 0.03125, "QA-F1": 0.10488208470222476}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.10526315789473685, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.19999999999999998, 0.0, 0.0, 0.0, 0.45161290322580644, 0.4, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-5662", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-6341", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-26846", "mrqa_naturalquestions-train-71074", "mrqa_naturalquestions-train-63141", "mrqa_naturalquestions-train-7402", "mrqa_naturalquestions-train-83637", "mrqa_naturalquestions-train-62863", "mrqa_naturalquestions-train-48078", "mrqa_naturalquestions-train-23793", "mrqa_naturalquestions-train-84908", "mrqa_naturalquestions-train-29455", "mrqa_naturalquestions-train-31569", "mrqa_naturalquestions-train-43213", "mrqa_naturalquestions-train-22514", "mrqa_naturalquestions-train-86581", "mrqa_naturalquestions-train-66330", "mrqa_naturalquestions-train-31928", "mrqa_naturalquestions-train-38557", "mrqa_naturalquestions-train-70223", "mrqa_naturalquestions-train-28444", "mrqa_naturalquestions-train-3239", "mrqa_naturalquestions-train-44505", "mrqa_naturalquestions-train-16029", "mrqa_naturalquestions-train-87482", "mrqa_naturalquestions-train-39733", "mrqa_naturalquestions-train-85696", "mrqa_naturalquestions-train-27578", "mrqa_naturalquestions-train-79790", "mrqa_naturalquestions-train-79237", "mrqa_naturalquestions-train-21786", "mrqa_naturalquestions-train-66035", "mrqa_naturalquestions-train-84677"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Arnhem", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "Januarius", "a son of Amram and Jochebed, of the tribe of Levi", "Brooklyn", "tetanus", "bounding the time or space used by the algorithm", "museum", "Alex O'Loughlin", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "a chain or screw stoking mechanism", "in the middle decade of the 19th century", "the Reverse - Flash", "All Hallows'Day", "the A's", "Baku", "Catholics", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "Quebec", "a person's speech or language must be significantly impaired in one ( or several ) of the four communication modalities following acquired brain injury or have significant decline over a short time period", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "rotating discs", "Splodgenessabounds"], "metric_results": {"EM": 0.21875, "QA-F1": 0.30411086309523805}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0625, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_squad-validation-3389", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_naturalquestions-validation-3840"], "retrieved_ids": ["mrqa_naturalquestions-train-66122", "mrqa_naturalquestions-train-57186", "mrqa_naturalquestions-train-12238", "mrqa_naturalquestions-train-22920", "mrqa_naturalquestions-train-84381", "mrqa_naturalquestions-train-12405", "mrqa_naturalquestions-train-57327", "mrqa_naturalquestions-train-79350", "mrqa_naturalquestions-train-8284", "mrqa_naturalquestions-train-68501", "mrqa_naturalquestions-train-8223", "mrqa_naturalquestions-train-73711", "mrqa_naturalquestions-train-56687", "mrqa_naturalquestions-train-48443", "mrqa_naturalquestions-train-28012", "mrqa_naturalquestions-train-63101", "mrqa_naturalquestions-train-29314", "mrqa_naturalquestions-train-70143", "mrqa_naturalquestions-train-84830", "mrqa_naturalquestions-train-9524", "mrqa_naturalquestions-train-73507", "mrqa_naturalquestions-train-46084", "mrqa_naturalquestions-train-1784", "mrqa_naturalquestions-train-7251", "mrqa_naturalquestions-train-38196", "mrqa_naturalquestions-train-29371", "mrqa_naturalquestions-train-74764", "mrqa_naturalquestions-train-184", "mrqa_naturalquestions-train-68591", "mrqa_naturalquestions-train-63152", "mrqa_naturalquestions-train-36368", "mrqa_naturalquestions-train-24789"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["otranto", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric", "Paspahegh Indians", "a branch of the anterior internodal tract that resides on the inner wall of the left atrium", "South Dakota", "7 : 25 a.m.", "swanee or swannee whistle", "a genetically engineered plant", "to start fires, hunt, and bury their dead", "the Department of Physics, Cochin University of science and Technology", "Parietal cells ( also known as oxyntic or delomorphous cells )", "placental", "Ready to Die", "june", "Ming dynasty", "1840", "a defiant speech, or a speech explaining their actions", "George Scherff", "kinks", "A study by the World Institute for Development Economics Research at United Nations University", "nonconservative forces", "averse to wedlock", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "deaths, injuries, and structural collapses", "as a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.0625, "QA-F1": 0.11426767676767677}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_naturalquestions-validation-816", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "retrieved_ids": ["mrqa_naturalquestions-train-44896", "mrqa_naturalquestions-train-2649", "mrqa_naturalquestions-train-60680", "mrqa_naturalquestions-train-49857", "mrqa_naturalquestions-train-72123", "mrqa_naturalquestions-train-52378", "mrqa_naturalquestions-train-60484", "mrqa_naturalquestions-train-75500", "mrqa_naturalquestions-train-11221", "mrqa_naturalquestions-train-26608", "mrqa_naturalquestions-train-58409", "mrqa_naturalquestions-train-23421", "mrqa_naturalquestions-train-27379", "mrqa_naturalquestions-train-11351", "mrqa_naturalquestions-train-28069", "mrqa_naturalquestions-train-52697", "mrqa_naturalquestions-train-71333", "mrqa_naturalquestions-train-7498", "mrqa_naturalquestions-train-57446", "mrqa_naturalquestions-train-82548", "mrqa_naturalquestions-train-40321", "mrqa_naturalquestions-train-2753", "mrqa_naturalquestions-train-82978", "mrqa_naturalquestions-train-72903", "mrqa_naturalquestions-train-50785", "mrqa_naturalquestions-train-46417", "mrqa_naturalquestions-train-61173", "mrqa_naturalquestions-train-9394", "mrqa_naturalquestions-train-45009", "mrqa_naturalquestions-train-22466", "mrqa_naturalquestions-train-65607", "mrqa_naturalquestions-validation-1364"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["clopedia2", "the draft carries more air ( oxygen ) past the flame, helping to produce a smokeless light", "2005 to 2008", "1998", "cliedville", "Bermuda", "90-60's", "independent schools", "dolph Camilli", "the times sign or the dimension sign is the symbol \u00d7", "BAFTA Television Award", "Juice Newton", "Super Bowl LII", "HTTP Secure ( HTTPS )", "typically closes for two and half weeks in late summer so it can be converted into the Haunted Mansion Holiday", "clisholm", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "june", "clangers", "moths", "universal", "widow - maker infarction due to a high death risk", "1.1 \u00d7 1011 metric tonnes", "clangers", "leaf tissue", "Indian club ATK", "land that a nation has conquered and expanded", "Indian Ocean near Grande Comore, Comoros Islands", "`` - s ''", "Norwegian", "the human respiratory system"], "metric_results": {"EM": 0.09375, "QA-F1": 0.16953125}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.08333333333333333, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-validation-5582", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-62378", "mrqa_naturalquestions-train-65417", "mrqa_naturalquestions-train-13985", "mrqa_naturalquestions-train-73993", "mrqa_naturalquestions-train-11342", "mrqa_naturalquestions-train-58010", "mrqa_naturalquestions-train-19467", "mrqa_naturalquestions-train-19382", "mrqa_naturalquestions-train-11616", "mrqa_naturalquestions-train-74715", "mrqa_naturalquestions-train-8972", "mrqa_naturalquestions-train-42403", "mrqa_naturalquestions-train-30692", "mrqa_naturalquestions-train-73343", "mrqa_naturalquestions-train-43142", "mrqa_naturalquestions-train-28113", "mrqa_naturalquestions-train-21738", "mrqa_naturalquestions-train-9356", "mrqa_naturalquestions-train-57746", "mrqa_naturalquestions-train-74912", "mrqa_naturalquestions-train-67500", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-validation-8448", "mrqa_naturalquestions-train-80466", "mrqa_naturalquestions-train-81549", "mrqa_naturalquestions-train-9464", "mrqa_naturalquestions-train-68815", "mrqa_naturalquestions-train-32584", "mrqa_naturalquestions-train-20900", "mrqa_naturalquestions-train-7119", "mrqa_naturalquestions-train-86944", "mrqa_naturalquestions-train-21987"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few", "The U.S. Army Chaplain insignia", "Kairi", "both inner city blacks, who wanted more involvement in government, and whites in the suburbs, who want more services and more control over the central city", "Ray Milland", "Armenia", "the Christian biblical canon", "Beyonc\u00e9", "film", "gallantry", "16 million", "the late 1950s", "work oxen for haulage", "1998", "Ben Keaton", "23.1", "18 - season", "family member", "over-fishing", "William Powell Lear", "the tangential force", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "The `` Blue Peter '' is used as a maritime signal, indicating that the vessel flying it is about to leave, and Reed chose the name to represent'a voyage of adventure'on which the programme would set out", "Abraham Gottlob Werner", "marlborough", "present-day Charleston", "a \"quiescent\" stance", "Adam Karpel", "Heinz Guderian"], "metric_results": {"EM": 0.125, "QA-F1": 0.2236879355400697}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.5365853658536585, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_hotpotqa-validation-3846", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_squad-validation-4318", "mrqa_hotpotqa-validation-1142", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "retrieved_ids": ["mrqa_naturalquestions-train-19020", "mrqa_naturalquestions-train-75556", "mrqa_naturalquestions-train-39773", "mrqa_naturalquestions-train-57203", "mrqa_naturalquestions-train-53798", "mrqa_naturalquestions-train-41034", "mrqa_naturalquestions-train-75368", "mrqa_naturalquestions-train-25158", "mrqa_naturalquestions-train-82501", "mrqa_naturalquestions-train-12929", "mrqa_naturalquestions-train-39918", "mrqa_naturalquestions-train-39276", "mrqa_naturalquestions-train-84539", "mrqa_naturalquestions-train-53720", "mrqa_naturalquestions-train-83622", "mrqa_naturalquestions-train-43601", "mrqa_naturalquestions-train-47479", "mrqa_naturalquestions-train-55672", "mrqa_naturalquestions-train-60138", "mrqa_naturalquestions-train-38365", "mrqa_naturalquestions-train-43647", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-train-12277", "mrqa_naturalquestions-train-38094", "mrqa_naturalquestions-train-30733", "mrqa_naturalquestions-train-76039", "mrqa_naturalquestions-train-58120", "mrqa_naturalquestions-train-13733", "mrqa_naturalquestions-train-82468", "mrqa_naturalquestions-train-57404", "mrqa_naturalquestions-train-30547", "mrqa_naturalquestions-train-51199"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computational complexity theory", "georgeppe Antonio 'Nino' Farina", "37", "6.4 nanometers", "the eighth and eleventh episodes of the season", "Carl Edwards", "400", "endocrine", "Latin liberalia studia", "Forest of Bowland", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "Eureka", "1868", "2018", "wolverhampton Wanderers", "law firm", "Pottawatomie County", "orangutan", "Newton's Law of Gravitation", "The church tower", "george enders", "Toronto", "foreigner", "110 miles", "Kona coast", "Liberal conservatism", "one of the largest gold rushes the world has ever seen", "six", "not guilty", "psychotherapeutic", "Quentin Coldwater", "acidic"], "metric_results": {"EM": 0.28125, "QA-F1": 0.35068655303030305}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true], "QA-F1": [0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032"], "retrieved_ids": ["mrqa_naturalquestions-train-5859", "mrqa_naturalquestions-train-56337", "mrqa_naturalquestions-train-55582", "mrqa_naturalquestions-train-36004", "mrqa_naturalquestions-train-68697", "mrqa_naturalquestions-train-26211", "mrqa_naturalquestions-train-39975", "mrqa_naturalquestions-train-60570", "mrqa_naturalquestions-train-11827", "mrqa_naturalquestions-train-33059", "mrqa_naturalquestions-train-31709", "mrqa_naturalquestions-train-17932", "mrqa_naturalquestions-train-33252", "mrqa_naturalquestions-train-82445", "mrqa_naturalquestions-train-21489", "mrqa_naturalquestions-train-5035", "mrqa_naturalquestions-train-64701", "mrqa_naturalquestions-train-87435", "mrqa_naturalquestions-train-56357", "mrqa_naturalquestions-train-46603", "mrqa_naturalquestions-train-18317", "mrqa_naturalquestions-train-42690", "mrqa_naturalquestions-train-47686", "mrqa_naturalquestions-train-23581", "mrqa_naturalquestions-train-60598", "mrqa_naturalquestions-train-26269", "mrqa_naturalquestions-train-12128", "mrqa_naturalquestions-train-29648", "mrqa_naturalquestions-train-9453", "mrqa_naturalquestions-train-5416", "mrqa_naturalquestions-train-83537", "mrqa_naturalquestions-train-75548"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["the English phrase `` I Seek You ''", "Argentinian", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "to taste sweetened sheeps\u2019 milk ricotta cream", "the efficiency of photosynthesis", "high-dress ball held at Devonshire House in 1897 to celebrate Queen Victoria's diamond jubilee", "the Willard Hotel", "\"The Krypto Report\" a podcast produced by the white supremacist site \"The Daily Stormer\"", "triplet", "water", "president", "officeholders", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy", "Worcester Cold Storage and Warehouse Co.", "acting", "C. W. Grafton", "LED illuminated display", "Americans acting under orders", "iPod Classic", "My Sassy Girl", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "General Hospital", "wood or coal", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd", "the root respiration", "soils", "blades", "medium and heavy- duty diesel trucks", "testes"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2879244075816656}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.13333333333333333, 0.4, 0.0, 1.0, 0.18181818181818182, 1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 0.1290322580645161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_hotpotqa-validation-3428", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-5940", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-57441", "mrqa_naturalquestions-train-16227", "mrqa_naturalquestions-train-38997", "mrqa_naturalquestions-train-8670", "mrqa_naturalquestions-train-41998", "mrqa_naturalquestions-train-18272", "mrqa_naturalquestions-train-54", "mrqa_naturalquestions-train-37282", "mrqa_naturalquestions-train-20169", "mrqa_naturalquestions-train-42478", "mrqa_naturalquestions-train-26144", "mrqa_naturalquestions-train-53137", "mrqa_naturalquestions-train-45468", "mrqa_naturalquestions-train-45712", "mrqa_naturalquestions-train-85021", "mrqa_naturalquestions-train-80180", "mrqa_naturalquestions-train-25995", "mrqa_naturalquestions-train-24246", "mrqa_naturalquestions-train-30391", "mrqa_naturalquestions-train-48565", "mrqa_naturalquestions-train-62389", "mrqa_naturalquestions-train-34559", "mrqa_naturalquestions-train-16422", "mrqa_naturalquestions-train-5582", "mrqa_naturalquestions-train-71187", "mrqa_naturalquestions-train-27900", "mrqa_naturalquestions-train-42408", "mrqa_naturalquestions-train-47776", "mrqa_naturalquestions-train-38606", "mrqa_naturalquestions-train-78345", "mrqa_naturalquestions-train-68734", "mrqa_naturalquestions-train-42561"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["v. v.", "yellow fever", "three legal systems", "Las Vegas, Nevada", "status code and reason message", "globetrotters", "cruiserweight title", "a bridge over the Merderet in the fictional town of Ramelle", "a Dubliner tried to kill Benito Mussolini", "slow", "menhirs", "a strict and elaborate set of rules designed by Victoria, Duchess of Kent, along with her attendant, Sir John Conroy, concerning the upbringing of the Duchess's daughter, the future Queen Victoria", "pH 7 ( 25 \u00b0 C )", "1987", "the MGM Grand Garden Special Events Center", "digital fashion gallery launched at the Chicago History Museum", "C. J. Anderson", "all-encompassing definition of the term is extremely difficult, if not impossible", "to share that story with the world through stories of faith, judgment, dedication and sacrifice of the church organization through today", "60", "Eagle Ridge Mall", "Pel\u00e9", "reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "fire", "Gomer Pyle", "must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Ward", "a Belgian\u2013French explorer, spiritualist, Buddhist, anarchist and writer", "Jamestown", "Rouen Cathedral is a historical capital city of Normandy, in northwestern France on the River Seine, and currently the capital of the Haute- Normandie (Upper Normandy) retinaion", "tree growth stages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.33937364718614715}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.8, 0.28571428571428575, 0.5, 0.6666666666666666, 0.9333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "retrieved_ids": ["mrqa_naturalquestions-train-29535", "mrqa_naturalquestions-train-68054", "mrqa_naturalquestions-train-27730", "mrqa_naturalquestions-train-57435", "mrqa_naturalquestions-train-39376", "mrqa_naturalquestions-train-29746", "mrqa_naturalquestions-train-52760", "mrqa_squad-validation-194", "mrqa_naturalquestions-train-36117", "mrqa_naturalquestions-train-56680", "mrqa_naturalquestions-train-68171", "mrqa_naturalquestions-train-3549", "mrqa_naturalquestions-train-50459", "mrqa_naturalquestions-train-52495", "mrqa_naturalquestions-train-65370", "mrqa_naturalquestions-train-75685", "mrqa_naturalquestions-train-44643", "mrqa_naturalquestions-train-1069", "mrqa_naturalquestions-train-63860", "mrqa_naturalquestions-train-67193", "mrqa_naturalquestions-train-3257", "mrqa_naturalquestions-train-71962", "mrqa_naturalquestions-train-59537", "mrqa_naturalquestions-train-43446", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-18796", "mrqa_naturalquestions-train-61332", "mrqa_naturalquestions-train-86748", "mrqa_naturalquestions-train-63521", "mrqa_naturalquestions-train-3376", "mrqa_naturalquestions-train-9689", "mrqa_naturalquestions-train-5217"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "Traumnovelle", "a Gender pay gap in favor of males in the labor market", "Treaty on the Functioning of the European Union", "at 1 atm pressure", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "Dan Dares", "length of their main span ( i.e. the length of suspended roadway between the bridge's towers )", "trams", "alexander", "joseph smith", "the Pechenegs, the Bulgars, and especially the Seljuk Turks", "died in battle", "Volkswagen Beetle", "joseph smith", "North American Technate", "joseph smith", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world", "the Vittorio Emanuele II Gallery and Piazza della Scala", "catfish aquaculture", "atomic number 53", "J.J. Looney as Young Sparrow and DJ Dragon Nutz", "Iraq", "a co-op of grape growers", "mann", "joseppe Verdi", "June 25, 1952", "the Charlotte Hornets", "the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean Fernel", "chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.24684206769005362}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5454545454545454, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.4444444444444445, 0.08695652173913043, 1.0, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-6442"], "retrieved_ids": ["mrqa_naturalquestions-train-14536", "mrqa_naturalquestions-train-29317", "mrqa_naturalquestions-train-37227", "mrqa_naturalquestions-train-84997", "mrqa_naturalquestions-train-73988", "mrqa_naturalquestions-train-28282", "mrqa_naturalquestions-train-62641", "mrqa_naturalquestions-train-29508", "mrqa_naturalquestions-train-70126", "mrqa_naturalquestions-train-59603", "mrqa_naturalquestions-train-77273", "mrqa_naturalquestions-train-67106", "mrqa_naturalquestions-train-61729", "mrqa_naturalquestions-train-78436", "mrqa_naturalquestions-train-37579", "mrqa_naturalquestions-train-10828", "mrqa_naturalquestions-train-74261", "mrqa_naturalquestions-train-45462", "mrqa_naturalquestions-train-7327", "mrqa_naturalquestions-train-67071", "mrqa_naturalquestions-train-86422", "mrqa_naturalquestions-train-71306", "mrqa_naturalquestions-train-30523", "mrqa_naturalquestions-train-63664", "mrqa_naturalquestions-train-68019", "mrqa_naturalquestions-train-71448", "mrqa_naturalquestions-train-43830", "mrqa_naturalquestions-train-28015", "mrqa_naturalquestions-train-17716", "mrqa_naturalquestions-train-65378", "mrqa_hotpotqa-validation-3632", "mrqa_naturalquestions-train-83227"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay", "Mark Lennon", "fencers", "John Major", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "2014", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Minos", "1860", "american cigars", "byker grove", "New South Wales", "Johnson", "dandy", "american", "Orwell", "Kvapil", "Gregg Popovich", "not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive immune system", "uncle", "a musician", "nodel", "December 1, 1969", "american", "ark of kent", "California State Automobile Association and the Automobile Club of Southern California", "\"faith\"", "Cinderella", "delayed the sealing of the hatch", "a fear of seeming rude"], "metric_results": {"EM": 0.15625, "QA-F1": 0.273241493398143}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.34782608695652173, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.2424242424242424, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.47058823529411764]}}, "error_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_hotpotqa-validation-4826", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-9524", "mrqa_naturalquestions-train-69031", "mrqa_naturalquestions-train-25978", "mrqa_naturalquestions-train-78814", "mrqa_naturalquestions-train-17325", "mrqa_naturalquestions-train-31525", "mrqa_naturalquestions-train-21852", "mrqa_naturalquestions-train-41769", "mrqa_naturalquestions-train-42262", "mrqa_naturalquestions-train-55108", "mrqa_naturalquestions-train-42585", "mrqa_naturalquestions-train-81326", "mrqa_naturalquestions-train-61968", "mrqa_naturalquestions-train-13546", "mrqa_naturalquestions-train-15327", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-train-74221", "mrqa_naturalquestions-train-9229", "mrqa_naturalquestions-train-31394", "mrqa_naturalquestions-train-15598", "mrqa_naturalquestions-train-80574", "mrqa_naturalquestions-train-5035", "mrqa_naturalquestions-train-52894", "mrqa_naturalquestions-train-50420", "mrqa_squad-validation-7819", "mrqa_naturalquestions-train-55139", "mrqa_naturalquestions-train-74173", "mrqa_naturalquestions-train-2331", "mrqa_naturalquestions-train-18794", "mrqa_naturalquestions-train-21812", "mrqa_naturalquestions-train-11728", "mrqa_naturalquestions-train-21249"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister ( 1982 film)", "president of Guggenheim Partners", "Jason Lee", "Napoleon", "bakers", "3.7 percent of the entire student population", "a negative effect on subsequent long-run economic growth", "maryland", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "maryland", "paddington", "amyotrophic lateral sclerosis (ALS)", "\"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "a pioneer in watch design, manufacturing and distribution", "mid 1970s", "Torah or Bible", "the western coast of Italy", "first and only U.S. born world grand prix champion", "a jazz funeral without a body", "mid November", "Facebook", "bajgiel", "The song was covered by the fictional band Steel Dragon in the 2001 film \"Rock Star\"", "Seattle", "King George's War", "cheated on Miley", "alternative rock", "Fort Snelling, Minnesota", "daguerreotypes", "a Mediterranean climate"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18104369588744587}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false], "QA-F1": [0.0, 0.4, 0.0, 0.0, 0.0, 0.2857142857142857, 0.4444444444444445, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913", "mrqa_squad-validation-2656"], "retrieved_ids": ["mrqa_naturalquestions-train-15759", "mrqa_naturalquestions-train-43622", "mrqa_naturalquestions-train-37291", "mrqa_naturalquestions-train-7868", "mrqa_naturalquestions-train-72231", "mrqa_naturalquestions-train-63552", "mrqa_naturalquestions-train-17118", "mrqa_naturalquestions-train-18427", "mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-69376", "mrqa_naturalquestions-train-14604", "mrqa_naturalquestions-train-73300", "mrqa_naturalquestions-train-54823", "mrqa_naturalquestions-train-15384", "mrqa_naturalquestions-train-76238", "mrqa_naturalquestions-train-36077", "mrqa_naturalquestions-train-26211", "mrqa_naturalquestions-train-78190", "mrqa_naturalquestions-train-46890", "mrqa_naturalquestions-train-46898", "mrqa_naturalquestions-train-60699", "mrqa_naturalquestions-train-85118", "mrqa_naturalquestions-train-54446", "mrqa_naturalquestions-train-26674", "mrqa_naturalquestions-train-15280", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-train-60814", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-9875", "mrqa_naturalquestions-train-22567", "mrqa_naturalquestions-train-946", "mrqa_naturalquestions-validation-1364"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["`` Hong Kong comedy film directed by Raymond Yip, and starring Francis Ng, Michelle Reis and Daniel Wu.", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Chicago's first permanent non-native settler", "maryland", "FX option", "electromagnetic waves", "a Wahhabi/ Salafi extremists militant group", "high self", "Dimensions in Time", "Surveyor 3 unmanned lunar probe", "January 1981", "lutein - releasing hormone", "the structure and substance of his questions and answers concerning baptism in the Small Catechism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "john Robertson wide on the left", "slowing the vehicle", "Belle Fourche and Cheyenne rivers", "( Sometimes) absence", "Hanna- Barbera, The Jetsons", "Cortina d'Ampezzo", "efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "Alba Longa", "mary stryjewski a.k.a. whtmnk", "garland island", "Timo Hildebrand", "The public sector ( also called the state sector )", "2 February 1940", "poverty", "`` great king ''", "cornea (the transparent layer at the front of the eye)", "Emmett \"Doc\" Brown", "Charles Whitman"], "metric_results": {"EM": 0.0625, "QA-F1": 0.21384806713754084}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.21052631578947367, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.4000000000000001, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "retrieved_ids": ["mrqa_naturalquestions-train-69417", "mrqa_naturalquestions-train-21331", "mrqa_naturalquestions-train-55397", "mrqa_naturalquestions-train-6205", "mrqa_naturalquestions-train-62430", "mrqa_naturalquestions-train-86038", "mrqa_naturalquestions-train-32781", "mrqa_naturalquestions-train-6934", "mrqa_naturalquestions-train-8187", "mrqa_naturalquestions-train-62322", "mrqa_naturalquestions-train-18873", "mrqa_naturalquestions-train-34757", "mrqa_naturalquestions-train-79323", "mrqa_naturalquestions-train-15149", "mrqa_naturalquestions-train-12103", "mrqa_naturalquestions-train-29909", "mrqa_naturalquestions-train-83842", "mrqa_naturalquestions-train-78094", "mrqa_naturalquestions-train-80302", "mrqa_naturalquestions-train-52715", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-train-61037", "mrqa_naturalquestions-train-1997", "mrqa_naturalquestions-train-40857", "mrqa_naturalquestions-train-37667", "mrqa_naturalquestions-train-48490", "mrqa_naturalquestions-train-68723", "mrqa_naturalquestions-train-74307", "mrqa_naturalquestions-train-43841", "mrqa_naturalquestions-train-69303", "mrqa_naturalquestions-train-78917", "mrqa_naturalquestions-train-84958"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Harry Potter and the Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "Wichita and the state of Kansas", "Proof", "brianius ptolemy", "a friend and publicist", "brian clifton", "King's Chamber", "Theodore Haynes", "the Old Town Hall, Gateshead", "Yogi Berra", "The neck", "1898", "professional wrestler", "Payaya Indians", "to steal the plans for the Death Star", "The Bells of St. Mary's", "boston Aubrey Houston", "tibility for impressions, and an inclination to be touched by emotions", "chimpanzees", "March 15, 1945", "absolute temperature", "Wiki-blowing", "Sam Waterston", "bicuspid", "Aegisthus", "3 December", "tallahassee", "prefabricated housing projects", "boston brian baryledickens", "WWSB and WOTV"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2416666666666667}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_triviaqa-validation-1736", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-72462", "mrqa_naturalquestions-train-29746", "mrqa_naturalquestions-train-2702", "mrqa_naturalquestions-train-9663", "mrqa_naturalquestions-train-61855", "mrqa_naturalquestions-train-54058", "mrqa_naturalquestions-train-26710", "mrqa_naturalquestions-train-26400", "mrqa_naturalquestions-train-9047", "mrqa_naturalquestions-train-36575", "mrqa_naturalquestions-train-85921", "mrqa_naturalquestions-train-82997", "mrqa_naturalquestions-train-45764", "mrqa_naturalquestions-train-73233", "mrqa_naturalquestions-train-58847", "mrqa_naturalquestions-train-47183", "mrqa_naturalquestions-train-53804", "mrqa_naturalquestions-train-24281", "mrqa_naturalquestions-train-26015", "mrqa_naturalquestions-train-8492", "mrqa_naturalquestions-train-2651", "mrqa_naturalquestions-train-14866", "mrqa_naturalquestions-train-47917", "mrqa_naturalquestions-train-75021", "mrqa_naturalquestions-train-35201", "mrqa_naturalquestions-train-49688", "mrqa_naturalquestions-train-36203", "mrqa_naturalquestions-train-77778", "mrqa_naturalquestions-train-25841", "mrqa_naturalquestions-train-68723", "mrqa_naturalquestions-train-40588", "mrqa_naturalquestions-train-63488"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["theatre", "Limburger cheese", "galileo", "on the lateral side of the tibia", "ferguside", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "Colin Montgomerie", "October 29, 1985", "Amway", "Mauritius", "Thomas Sowell", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in His absence, the Deputy - Chairman of the Rajya Sabha", "Golden Globe Award", "kPMG (T) Limited", "Chad", "60-mile-wide", "a statue of fame", "allowing a child to go through a torturous treatment to gain information", "Fulham", "French, English and Spanish", "florida", "U.S. Marshals", "What's Up (TV series)", "supply chain management", "Mars rover", "Stanislaw August Poniatowski", "polynomial algebra", "Michael J. Fox", "Japanese", "sheepskin", "Honolulu"], "metric_results": {"EM": 0.0625, "QA-F1": 0.08633241758241758}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-2287"], "retrieved_ids": ["mrqa_naturalquestions-train-2146", "mrqa_naturalquestions-train-43775", "mrqa_naturalquestions-train-31734", "mrqa_naturalquestions-train-31729", "mrqa_naturalquestions-train-75814", "mrqa_naturalquestions-train-79850", "mrqa_naturalquestions-train-40335", "mrqa_naturalquestions-train-70108", "mrqa_naturalquestions-train-46898", "mrqa_naturalquestions-train-76961", "mrqa_naturalquestions-train-14945", "mrqa_naturalquestions-train-65666", "mrqa_naturalquestions-train-37417", "mrqa_naturalquestions-train-30675", "mrqa_naturalquestions-train-27603", "mrqa_naturalquestions-train-4943", "mrqa_naturalquestions-train-706", "mrqa_naturalquestions-train-6726", "mrqa_naturalquestions-train-21013", "mrqa_naturalquestions-train-45168", "mrqa_naturalquestions-train-62685", "mrqa_naturalquestions-train-25896", "mrqa_naturalquestions-train-75212", "mrqa_naturalquestions-train-47031", "mrqa_naturalquestions-train-84177", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-train-58800", "mrqa_squad-validation-4318", "mrqa_naturalquestions-train-49009", "mrqa_naturalquestions-train-69324", "mrqa_naturalquestions-train-30790", "mrqa_naturalquestions-train-18085"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["belgian", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "gamma ray emission ( energy of 514 keV )", "James Zeebo", "sovereign states", "president of the United States Senate", "\"Teach the Controversy\" campaign", "Megatron", "Australia", "18 months", "geographic area", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Spencer", "\"antiforms\"", "June 9, 2015", "Veyyil", "Grace Nail Johnson", "Keith Richards", "at least one prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Godfrey Army Airfield", "knowledgeable in that one area", "a line of longitude ) in a geographical coordinate system at which longitude is defined to be 0 \u00b0", "Adult Swim", "Presiding Officer", "Miami Heat", "33", "dactylosphaera vitifoliae", "Annual Conference Cabinet", "bronze medal", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.25, "QA-F1": 0.3258312747035573}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 1.0, 0.08333333333333334, 0.0, 1.0, 0.0, 0.0, 0.1818181818181818, 0.0, 0.16, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.4347826086956522, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "retrieved_ids": ["mrqa_triviaqa-validation-116", "mrqa_naturalquestions-train-60255", "mrqa_naturalquestions-train-44905", "mrqa_naturalquestions-train-65923", "mrqa_naturalquestions-train-85035", "mrqa_naturalquestions-train-26507", "mrqa_naturalquestions-train-85849", "mrqa_naturalquestions-train-61167", "mrqa_naturalquestions-train-20536", "mrqa_naturalquestions-train-14897", "mrqa_naturalquestions-train-46928", "mrqa_naturalquestions-train-7526", "mrqa_naturalquestions-train-85948", "mrqa_naturalquestions-train-30709", "mrqa_naturalquestions-train-41464", "mrqa_naturalquestions-train-17567", "mrqa_naturalquestions-train-61658", "mrqa_naturalquestions-train-51555", "mrqa_naturalquestions-train-65679", "mrqa_naturalquestions-train-49399", "mrqa_naturalquestions-train-8638", "mrqa_naturalquestions-train-71147", "mrqa_naturalquestions-train-27531", "mrqa_naturalquestions-train-87100", "mrqa_naturalquestions-train-38085", "mrqa_naturalquestions-train-61458", "mrqa_naturalquestions-train-79675", "mrqa_naturalquestions-train-58634", "mrqa_naturalquestions-train-80874", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-6668", "mrqa_naturalquestions-train-43066"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwandaandan genocide, also known as the genocide against the Tutsi", "working harmoniously to fulfill the needs of every student in the classroom", "400 metres", "Nasim Pedrad", "the entertainment division", "the straight - line distance from A to B", "12", "the Great Exhibition of 1851", "Edward Longshanks", "naval support", "dundee", "the person compelled to pay for reformist programs", "lithgow Palace in Scotland", "\"Grindhouse\" fake trailer", "lyle davenport", "digital transmission", "the Swiss- Austrian border", "lithium-ion battery", "821", "HD channels and Video On Demand content which was not previously carried by cable", "liquid", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposition", "the \"Queen of Cool\"", "President Wilson and the American delegation from the Paris Peace Conference", "araspas", "the fifth season", "the ultimate taboo", "Eisstadion Davos", "Michael Crawford", "a lightning strike"], "metric_results": {"EM": 0.125, "QA-F1": 0.27580830627705627}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2, 0.18181818181818182, 0.5, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.125, 0.0, 0.0, 0.6666666666666665, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-28955", "mrqa_naturalquestions-train-55830", "mrqa_naturalquestions-train-64615", "mrqa_naturalquestions-train-42408", "mrqa_naturalquestions-train-32742", "mrqa_naturalquestions-train-31154", "mrqa_naturalquestions-train-76754", "mrqa_naturalquestions-train-39073", "mrqa_naturalquestions-train-661", "mrqa_naturalquestions-train-67640", "mrqa_naturalquestions-train-6073", "mrqa_naturalquestions-train-21142", "mrqa_naturalquestions-train-72794", "mrqa_naturalquestions-train-60502", "mrqa_naturalquestions-train-38876", "mrqa_naturalquestions-train-30002", "mrqa_naturalquestions-train-84139", "mrqa_naturalquestions-train-29857", "mrqa_naturalquestions-train-84978", "mrqa_naturalquestions-train-20364", "mrqa_naturalquestions-train-72321", "mrqa_naturalquestions-train-6618", "mrqa_naturalquestions-train-49559", "mrqa_naturalquestions-train-77526", "mrqa_naturalquestions-train-70163", "mrqa_naturalquestions-train-16605", "mrqa_naturalquestions-train-43267", "mrqa_naturalquestions-train-87488", "mrqa_naturalquestions-train-56068", "mrqa_naturalquestions-train-70249", "mrqa_naturalquestions-train-14519", "mrqa_naturalquestions-train-11574"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "aragon", "11.1", "trans-Pacific", "Sharman Joshi", "in a classroom", "Forster I, Forster II, and Forster III", "prime", "Tropical Storm Ann", "White Castle in New Brunswick", "The Chump", "Ishmael", "fredys o'Donnell", "a 1993 American comedy - drama film directed by Fred Schepisi", "blackstar", "India", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "1974", "Nicki Minaj", "fredic", "French Huguenot", "belle epoque", "friedrich Engels", "\"Drawn Together\"", "William the Conqueror", "Ben Gurion International Airport", "two degrees of freedom", "Mainland Greece", "taking blood samples from patients", "Guinness World Records", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.03125, "QA-F1": 0.12978896103896104}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.25, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5714285714285715, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_hotpotqa-validation-5710", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "retrieved_ids": ["mrqa_naturalquestions-train-14118", "mrqa_naturalquestions-train-13556", "mrqa_naturalquestions-train-3241", "mrqa_naturalquestions-train-56841", "mrqa_naturalquestions-train-49267", "mrqa_naturalquestions-train-30667", "mrqa_naturalquestions-train-75556", "mrqa_naturalquestions-train-55379", "mrqa_naturalquestions-train-80688", "mrqa_naturalquestions-train-15753", "mrqa_naturalquestions-train-13046", "mrqa_naturalquestions-train-1147", "mrqa_naturalquestions-train-84958", "mrqa_naturalquestions-train-9494", "mrqa_naturalquestions-train-78994", "mrqa_naturalquestions-train-14628", "mrqa_naturalquestions-train-82261", "mrqa_naturalquestions-train-42290", "mrqa_naturalquestions-train-60699", "mrqa_naturalquestions-train-26006", "mrqa_naturalquestions-train-31569", "mrqa_naturalquestions-train-16333", "mrqa_naturalquestions-train-81380", "mrqa_naturalquestions-train-26912", "mrqa_naturalquestions-train-69925", "mrqa_naturalquestions-train-66909", "mrqa_naturalquestions-train-15064", "mrqa_naturalquestions-train-38531", "mrqa_naturalquestions-train-69277", "mrqa_naturalquestions-train-60759", "mrqa_naturalquestions-train-36789", "mrqa_naturalquestions-train-49051"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating", "Robert Smigel", "the prints and architectural drawings", "Mos Def", "David Brewster", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "AS-205", "ribosomal RNA", "kookaburra", "six-time Silver Slugger Award winner", "CCH Pounder", "I Swear", "Cozonac", "Gerry Adams", "heliocentric", "Sulla", "following the 2017 season", "Golden Globe", "Swahili", "change", "ohan Pamuk", "Hexachrome", "Firoz Shah Tughlaq", "\" My Love from the Star\"", "San Jose", "sea wasp", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "Thunderbird", "the most giving Super Bowl ever", "29.7", "Loretta Swit"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3608856421356421}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.48484848484848486, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_hotpotqa-validation-3623", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-935"], "retrieved_ids": ["mrqa_naturalquestions-train-58879", "mrqa_naturalquestions-train-54972", "mrqa_naturalquestions-train-25891", "mrqa_naturalquestions-train-74768", "mrqa_naturalquestions-train-42438", "mrqa_naturalquestions-train-63860", "mrqa_naturalquestions-train-74745", "mrqa_naturalquestions-train-19241", "mrqa_naturalquestions-train-82840", "mrqa_naturalquestions-train-29559", "mrqa_naturalquestions-train-3717", "mrqa_naturalquestions-train-39419", "mrqa_naturalquestions-train-44747", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-train-87124", "mrqa_naturalquestions-train-79489", "mrqa_naturalquestions-train-66650", "mrqa_naturalquestions-train-77593", "mrqa_naturalquestions-train-21101", "mrqa_naturalquestions-train-38055", "mrqa_naturalquestions-train-55282", "mrqa_naturalquestions-train-51938", "mrqa_naturalquestions-train-61331", "mrqa_naturalquestions-train-6283", "mrqa_naturalquestions-train-28130", "mrqa_naturalquestions-train-23220", "mrqa_naturalquestions-train-56674", "mrqa_naturalquestions-train-57277", "mrqa_naturalquestions-train-38520", "mrqa_naturalquestions-train-48215", "mrqa_naturalquestions-train-3770", "mrqa_naturalquestions-train-8053"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "James Blundell", "Yazoo", "1894", "stars exceeding about eight times the mass of the sun", "\"homo enim in hac vita\"", "French chose to cede the former, but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area", "Willie Nelson and Kris Kristofferson", "ill. (chiefly col.)", "research universities and other public and private", "French", "Lewis", "Charles Dickens", "World Summit of Nobel Peace Laureates", "proteins", "2001", "exceeds any given number", "ITV News at Ten", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "Tallemaja \"pine tree Mary\"", "western portions of the Great Lakes region", "Presbyterian", "James Bond", "640 \u00d7 1136 at 326 ppi", "Fillies Triple Crown", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "\"Menace II Society\"", "quarterback", "Larry Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2263988553686614}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21276595744680848, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-61970", "mrqa_naturalquestions-train-25772", "mrqa_naturalquestions-train-40770", "mrqa_naturalquestions-train-37898", "mrqa_naturalquestions-train-18740", "mrqa_naturalquestions-train-32604", "mrqa_naturalquestions-train-87920", "mrqa_naturalquestions-train-77353", "mrqa_naturalquestions-train-56276", "mrqa_naturalquestions-train-68829", "mrqa_triviaqa-validation-3901", "mrqa_naturalquestions-train-20835", "mrqa_naturalquestions-train-35082", "mrqa_naturalquestions-train-82514", "mrqa_naturalquestions-train-80072", "mrqa_naturalquestions-train-53944", "mrqa_naturalquestions-train-71597", "mrqa_naturalquestions-train-36185", "mrqa_naturalquestions-train-83394", "mrqa_naturalquestions-train-84836", "mrqa_naturalquestions-train-78399", "mrqa_naturalquestions-train-11684", "mrqa_naturalquestions-train-27773", "mrqa_naturalquestions-train-23051", "mrqa_naturalquestions-train-40945", "mrqa_naturalquestions-train-58887", "mrqa_naturalquestions-train-43517", "mrqa_naturalquestions-train-57256", "mrqa_naturalquestions-train-47955", "mrqa_naturalquestions-train-58274", "mrqa_naturalquestions-train-36177", "mrqa_naturalquestions-train-73538"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "MSC Crociere S. p.A.", "alison carLSON", "a European fairy tale in 1697", "Liberals' main support lies in Melbourne's more affluent eastern and outer suburbs, and some rural and regional centres", "Royalists", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "memento, homo, quia pulvis es, et in pulverem reverteris", "al\u00e9a seydoux", "alibris Books", "Augustus Waters", "1619", "alison mhta", "casino, tobacco and gambling", "June 11, 1973", "Kenya and in the Masai Mara in particular", "Timeline of Shakespeare criticism", "boudicca", "an active supporter of the League of Nations", "Cargill", "AMC Entertainment Holdings, Inc.", "Paddy's Pub in South Philadelphia", "3 October 1990", "March 1, 2018", "heavy W and Z bosons", "fredalus", "son and oldest living child", "Manhattan Project", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18958818958818957}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.1111111111111111, 0.07692307692307691, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.22222222222222224, 0.0, 0.1818181818181818, 0.0, 0.3333333333333333, 0.4000000000000001, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_naturalquestions-validation-1328"], "retrieved_ids": ["mrqa_naturalquestions-train-58012", "mrqa_naturalquestions-train-20447", "mrqa_naturalquestions-train-13218", "mrqa_naturalquestions-train-47576", "mrqa_naturalquestions-train-1290", "mrqa_naturalquestions-train-80798", "mrqa_naturalquestions-train-21091", "mrqa_naturalquestions-train-64528", "mrqa_naturalquestions-train-62134", "mrqa_naturalquestions-train-46240", "mrqa_naturalquestions-train-52701", "mrqa_naturalquestions-train-67480", "mrqa_naturalquestions-train-872", "mrqa_naturalquestions-train-81981", "mrqa_naturalquestions-train-49997", "mrqa_naturalquestions-train-48360", "mrqa_naturalquestions-train-77618", "mrqa_naturalquestions-train-15691", "mrqa_naturalquestions-train-69289", "mrqa_naturalquestions-train-46670", "mrqa_naturalquestions-train-35968", "mrqa_triviaqa-validation-6389", "mrqa_naturalquestions-train-65794", "mrqa_naturalquestions-train-1741", "mrqa_naturalquestions-train-68267", "mrqa_naturalquestions-train-10094", "mrqa_naturalquestions-train-22937", "mrqa_naturalquestions-train-69923", "mrqa_naturalquestions-train-31232", "mrqa_naturalquestions-train-87883", "mrqa_naturalquestions-train-1567", "mrqa_naturalquestions-train-81659"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "javier (Luna)", "red", "Dreamland", "Best Animated Feature", "European Union institutions", "American record for the most time in space (381.6 days)", "nine other contenders from across the United States", "CAL IPSO", "celandine", "James `` Scotty '' Reston", "Ronnie Schell", "artemisinin- Based therapy", "Mumbai, Maharashtra", "It comprises the ancient Kingdoms of Mide, Osraige and Leinster", "1940", "2017 / 18 Divisional Round game against the New Orleans Saints", "as early as possibly 1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "midlands of the river Niger Benue trough to over 2,000 mm ( 78.7 in ) along the south western escarpment of the Jos Plateau", "benjamin franklin", "Incudomalleolar joint", "billie Jean King", "Leucippus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Corey Brown"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30081487956487957}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.3636363636363636, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.16666666666666666, 1.0, 0.3636363636363636, 0.33333333333333337, 0.0, 0.08, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "retrieved_ids": ["mrqa_naturalquestions-train-30102", "mrqa_naturalquestions-train-47000", "mrqa_naturalquestions-train-2658", "mrqa_naturalquestions-train-4256", "mrqa_naturalquestions-train-55138", "mrqa_naturalquestions-train-1948", "mrqa_naturalquestions-train-87879", "mrqa_naturalquestions-train-59470", "mrqa_naturalquestions-train-80629", "mrqa_squad-validation-7149", "mrqa_naturalquestions-train-35650", "mrqa_naturalquestions-train-16951", "mrqa_naturalquestions-train-24397", "mrqa_naturalquestions-train-10120", "mrqa_naturalquestions-train-35223", "mrqa_naturalquestions-train-63669", "mrqa_naturalquestions-train-14802", "mrqa_naturalquestions-train-17568", "mrqa_naturalquestions-train-7835", "mrqa_naturalquestions-train-55274", "mrqa_naturalquestions-train-30550", "mrqa_naturalquestions-train-15141", "mrqa_naturalquestions-train-73399", "mrqa_naturalquestions-train-79699", "mrqa_naturalquestions-train-13329", "mrqa_naturalquestions-train-55308", "mrqa_naturalquestions-train-28491", "mrqa_naturalquestions-train-40496", "mrqa_hotpotqa-validation-2886", "mrqa_naturalquestions-train-44365", "mrqa_naturalquestions-train-67590", "mrqa_naturalquestions-train-25764"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "gold", "WBO lightweight title", "moluccas", "the first Saturday in May", "Cordelia", "began multilateral negotiations", "1990", "J.R. R. Tolkien", "Peyton Manning", "Selena Gomez", "learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "it is an unofficial title sometimes given to new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "prime numbers that are of the form 2p \u2212 1", "car's version of the Chrysler 300 `` letter series '' ; a large, high - performance luxury coupe sold in very limited numbers", "Fa Ze YouTubers", "the nine circles of Hell", "the Muslim", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "Friars Minor Conventual", "CD Castell\u00f3n", "between 1770 and 1848", "12\u20134", "having colloblasts, which are sticky and adhere to prey", "Jon M. Chu", "STS-51-L", "it will retreat to its den and winter will persist for six more weeks", "minister of industrial restructuring and external trade"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2923979446542933}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.07692307692307693, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.8, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3870967741935484, 0.42857142857142855, 0.1111111111111111, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-62454", "mrqa_hotpotqa-validation-3242", "mrqa_naturalquestions-train-27764", "mrqa_naturalquestions-train-47092", "mrqa_naturalquestions-train-50821", "mrqa_squad-validation-4253", "mrqa_naturalquestions-train-17539", "mrqa_naturalquestions-train-20167", "mrqa_naturalquestions-train-69167", "mrqa_naturalquestions-train-78281", "mrqa_naturalquestions-train-18082", "mrqa_naturalquestions-train-55138", "mrqa_naturalquestions-train-42843", "mrqa_naturalquestions-train-79688", "mrqa_naturalquestions-train-25771", "mrqa_naturalquestions-train-63770", "mrqa_naturalquestions-train-72644", "mrqa_naturalquestions-train-55545", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-train-26018", "mrqa_naturalquestions-train-53240", "mrqa_naturalquestions-train-18629", "mrqa_naturalquestions-train-73962", "mrqa_naturalquestions-train-56236", "mrqa_naturalquestions-train-21529", "mrqa_naturalquestions-train-48735", "mrqa_naturalquestions-train-44987", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-train-39052", "mrqa_naturalquestions-train-52868", "mrqa_naturalquestions-train-86968", "mrqa_naturalquestions-train-4886"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["alpine skiing, cross-country skiing, ski jumping, nordic combined, snowboarding and freestyle skiing", "slave", "over 50 million singles", "secessionists of the Confederate States, who advocated for states'rights to expand slavery", "1923 and 1925", "Metropolitan Statistical Area", "January 19, 1962", "Frigate", "france playhouse", "iteratively", "austin geese", "the move from the manufacturing sector to the service sector", "france", "Peter Davison, Colin Baker and Sylvester McCoy", "August 14, 1848", "lower", "at least some species, juveniles are capable of reproduction before reaching the adult size and shape. The combination of hermaphroditism and early reproduction enables small populations to grow at an explosive rate.", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "A Chorus Line", "2,664 rooms and 220 suites", "LG", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning services, support services, property services, catering services, security services and facility management services", "Symphony No. 7", "france", "uncertain", "ranked above the two personal physicians of the Emperor", "france", "to improve our conscious contact with God as we understood Him", "wrigley frankley Jr."], "metric_results": {"EM": 0.15625, "QA-F1": 0.3366234540653146}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.13333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6046511627906977, 0.4615384615384615, 0.923076923076923, 0.0, 0.33333333333333337, 0.0, 0.2857142857142857, 0.3636363636363636, 0.4444444444444445, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.16666666666666669, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_triviaqa-validation-2098", "mrqa_naturalquestions-validation-6545", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "retrieved_ids": ["mrqa_naturalquestions-train-29421", "mrqa_naturalquestions-train-23352", "mrqa_naturalquestions-train-43373", "mrqa_naturalquestions-train-6321", "mrqa_naturalquestions-train-3078", "mrqa_naturalquestions-train-7022", "mrqa_naturalquestions-train-82577", "mrqa_naturalquestions-train-10517", "mrqa_naturalquestions-train-36608", "mrqa_naturalquestions-train-3236", "mrqa_naturalquestions-train-82365", "mrqa_naturalquestions-train-66748", "mrqa_naturalquestions-train-11512", "mrqa_naturalquestions-train-7204", "mrqa_naturalquestions-train-23881", "mrqa_naturalquestions-train-81061", "mrqa_naturalquestions-train-53909", "mrqa_naturalquestions-train-11856", "mrqa_naturalquestions-train-57842", "mrqa_naturalquestions-train-33604", "mrqa_naturalquestions-train-50507", "mrqa_naturalquestions-train-62566", "mrqa_naturalquestions-train-78075", "mrqa_naturalquestions-train-6148", "mrqa_naturalquestions-train-21952", "mrqa_naturalquestions-train-15141", "mrqa_naturalquestions-train-59566", "mrqa_naturalquestions-train-64200", "mrqa_naturalquestions-train-29815", "mrqa_naturalquestions-train-11058", "mrqa_naturalquestions-train-24757", "mrqa_naturalquestions-train-38661"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences", "north of the Lakes Region and south of the Kancamagus Highway", "Magic formula investing", "true history of the Kelly Gang", "Honolulu", "1910\u20131940", "non-teaching posts", "Salamanca", "jazz saxophonist", "pacific tennis", "4,000", "founder of the Yuan dynasty", "Catherine Earnshaw", "canal", "spice products", "``The Simpsons 138th Episode Spectacular\" and \"The Simpsons Spin-Off Showcase\"", "Barry Parker", "San Bernardino", "an extensive neoclassical centre referred to as Tyneside Classical largely developed in the 1830s by Richard Grainger and John Dobson, and recently extensively restored", "Albany High School for Educating People of Color", "An early textual references about Mughal gardens are found in the memoirs and biographies of the Mughals emperors, including those of Babur, Humayun and Akbar.", "Sergeant First Class", "Anakin Skywalker", "A technical defense may enhance the chances for acquittal but make for more boring proceedings and reduced press coverage", "Cee - Lo", "Anglican", "mammy two Shoes", "warcruiser Renown", "magnetism", "located within nine coastal southern Nigerian states", "opportunity-based entrepreneurship", "January 11, 1755 or 1757"], "metric_results": {"EM": 0.125, "QA-F1": 0.2689551767676768}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false], "QA-F1": [0.8, 0.19999999999999998, 0.0, 0.33333333333333337, 0.33333333333333337, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.08333333333333334, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8000000000000002]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_squad-validation-3176", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_naturalquestions-validation-7801", "mrqa_hotpotqa-validation-945"], "retrieved_ids": ["mrqa_naturalquestions-train-45657", "mrqa_naturalquestions-train-59608", "mrqa_naturalquestions-train-81380", "mrqa_naturalquestions-train-36185", "mrqa_naturalquestions-train-9031", "mrqa_naturalquestions-train-60376", "mrqa_naturalquestions-train-38354", "mrqa_naturalquestions-train-28271", "mrqa_naturalquestions-train-61414", "mrqa_naturalquestions-train-42253", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-train-4019", "mrqa_naturalquestions-train-71357", "mrqa_naturalquestions-train-19534", "mrqa_naturalquestions-train-78399", "mrqa_naturalquestions-train-53870", "mrqa_naturalquestions-train-83173", "mrqa_naturalquestions-train-12880", "mrqa_naturalquestions-train-86076", "mrqa_naturalquestions-train-33677", "mrqa_naturalquestions-train-33921", "mrqa_naturalquestions-train-3938", "mrqa_naturalquestions-train-33125", "mrqa_naturalquestions-train-62848", "mrqa_naturalquestions-train-35233", "mrqa_naturalquestions-train-38075", "mrqa_naturalquestions-train-54518", "mrqa_naturalquestions-train-29571", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-train-10828", "mrqa_naturalquestions-train-19954"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A. and the studio in New York", "hiraani Tiger Hutchence", "blackberry and dewberry", "horsehead Nebula", "\" Big Mamie\"", "smith", "Hoffa", "a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light )", "they had not paid their poll tax", "1963", "boston pachulia", "the internal thylakoid system", "aline charigot", "the V8 Supercars and Australian Motorcycle Grand Prix at Phillip Island", "no veto", "the third season", "gravitational", "the availability of skilled tradespeople", "diamond", "A simple iron boar crest adorns the top of this helmet", "Northumbria at Newcastle", "curtin", "James", "on kickoffs at the 25 - yard line", "from the Latin centum, which means 100, and gradus, which meant steps", "about 7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "Romans 1:17", "margaret smith", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "antwerp", "company"], "metric_results": {"EM": 0.0625, "QA-F1": 0.25225224337066443}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.26666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.631578947368421, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6060606060606061, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-75889", "mrqa_naturalquestions-train-46264", "mrqa_naturalquestions-train-41464", "mrqa_naturalquestions-train-39049", "mrqa_naturalquestions-train-44200", "mrqa_naturalquestions-train-87252", "mrqa_naturalquestions-train-33257", "mrqa_naturalquestions-train-17027", "mrqa_naturalquestions-train-42387", "mrqa_naturalquestions-train-30389", "mrqa_naturalquestions-train-42179", "mrqa_naturalquestions-train-83637", "mrqa_naturalquestions-train-66425", "mrqa_naturalquestions-train-33919", "mrqa_naturalquestions-train-9109", "mrqa_naturalquestions-train-69635", "mrqa_naturalquestions-train-38517", "mrqa_naturalquestions-train-58310", "mrqa_naturalquestions-train-25907", "mrqa_naturalquestions-train-84908", "mrqa_naturalquestions-train-17567", "mrqa_naturalquestions-train-80184", "mrqa_naturalquestions-train-24111", "mrqa_naturalquestions-train-84672", "mrqa_naturalquestions-train-74979", "mrqa_naturalquestions-train-36824", "mrqa_naturalquestions-train-62246", "mrqa_naturalquestions-train-28015", "mrqa_naturalquestions-train-18697", "mrqa_naturalquestions-train-689", "mrqa_naturalquestions-train-7666", "mrqa_naturalquestions-train-9107"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["pamphlets on Islam", "Chris Weidman", "nullification", "Harishchandra", "poetry", "Professor Eobard Thawne", "plum brandy", "a US$10 a week raise over Tesla's US$18 per week salary", "1875", "by member states on a voluntary basis", "clarinet", "McKinsey's offices in Silicon Valley and India", "gypsia", "cliff renoir", "Crohn's disease or ulcerative colitis", "Ondemar Dias", "Raya Yarbrough", "Arizona", "cruiserweight", "John D. Rockefeller", "the Old Testament", "UPS", "local talent", "Football League", "re-encountered tegan", "c Cascade", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations", "Mary Eugenia Surratt", "1349", "mauritius", "through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "Stan Butler"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2913412536257364}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.923076923076923, 0.0, 0.4444444444444445, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.2758620689655173, 0.0, 0.0, 0.0, 0.10810810810810811, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_triviaqa-validation-2953", "mrqa_naturalquestions-validation-7821"], "retrieved_ids": ["mrqa_triviaqa-validation-5231", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-train-27788", "mrqa_hotpotqa-validation-400", "mrqa_naturalquestions-train-41612", "mrqa_naturalquestions-train-81499", "mrqa_naturalquestions-train-51847", "mrqa_naturalquestions-train-47570", "mrqa_naturalquestions-train-34390", "mrqa_naturalquestions-train-73699", "mrqa_naturalquestions-train-50939", "mrqa_naturalquestions-train-40916", "mrqa_naturalquestions-train-23010", "mrqa_naturalquestions-train-73353", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-train-27200", "mrqa_naturalquestions-train-41744", "mrqa_naturalquestions-train-82602", "mrqa_triviaqa-validation-5487", "mrqa_naturalquestions-train-80539", "mrqa_naturalquestions-train-30921", "mrqa_naturalquestions-train-54596", "mrqa_naturalquestions-train-79723", "mrqa_naturalquestions-train-23881", "mrqa_naturalquestions-train-15384", "mrqa_naturalquestions-train-68448", "mrqa_naturalquestions-train-49709", "mrqa_naturalquestions-train-47056", "mrqa_naturalquestions-train-43107", "mrqa_naturalquestions-train-21249", "mrqa_naturalquestions-train-77417", "mrqa_naturalquestions-train-82501"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["public speaking", "886 AD", "used to finance his own projects with varying degrees of success", "Formula One", "Kinect", "Tokyo", "defensive end Kony Ealy", "the parallelogram rule of vector addition", "raulston", "364", "a neutron source", "ferdinand", "the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "The Justified Ancients of Mu Mu", "National Basketball Development League (NBDL)", "ferdinand", "St. Mary's County", "Graham Gano", "2,615", "Pyeongchang", "Kaep", "a password recovery tool for Microsoft Windows", "Captain John Guidry", "Charles and Ray Eames", "Brazil", "Steve Redgrave", "either Q or the finite field with p elements, whence the name", "gyrosis", "53", "NADPH"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23900178661208074}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 1.0, 0.8, 0.0, 0.0, 0.47058823529411764, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 0.8235294117647058, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_squad-validation-7914", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_triviaqa-validation-814", "mrqa_squad-validation-8873"], "retrieved_ids": ["mrqa_naturalquestions-train-214", "mrqa_naturalquestions-train-62961", "mrqa_naturalquestions-train-69752", "mrqa_naturalquestions-train-39908", "mrqa_naturalquestions-train-9941", "mrqa_naturalquestions-train-15351", "mrqa_squad-validation-6733", "mrqa_naturalquestions-train-3264", "mrqa_naturalquestions-train-34615", "mrqa_naturalquestions-train-59591", "mrqa_naturalquestions-train-3478", "mrqa_naturalquestions-train-79038", "mrqa_naturalquestions-train-8343", "mrqa_naturalquestions-train-22090", "mrqa_naturalquestions-train-18667", "mrqa_naturalquestions-train-81210", "mrqa_naturalquestions-train-9917", "mrqa_naturalquestions-train-58357", "mrqa_naturalquestions-train-65050", "mrqa_naturalquestions-train-36951", "mrqa_naturalquestions-train-2693", "mrqa_naturalquestions-train-70671", "mrqa_naturalquestions-train-76411", "mrqa_naturalquestions-train-83771", "mrqa_naturalquestions-train-46084", "mrqa_naturalquestions-train-20728", "mrqa_naturalquestions-train-19710", "mrqa_naturalquestions-train-84836", "mrqa_naturalquestions-train-80651", "mrqa_naturalquestions-train-36288", "mrqa_naturalquestions-train-31758", "mrqa_naturalquestions-train-11462"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales", "the Mayor's son", "arpers Ferry", "arthur boston", "Marty Ingels", "BBC UKTV", "arald", "demographics and economic ties", "three or more separate periods", "The Kickoff Game", "narcolepsy", "arctic monkeys", "arthur koreans", "arthur", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "A computer program", "The centre-right Australian Labor Party (ALP)", "marduk", "Hekla", "$474 million", "nash", "the South Pacific off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Easy", "Meredith Brody ( Zoe McLellan ), a transfer from the NCIS Great Lakes field office, who has worked as a Special Agent Afloat and is keen to leave her past behind as she moves to New Orleans", "tamer youssef", "National Lottery", "arthur", "aragon", "in order to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.0625, "QA-F1": 0.14631716322892793}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.18181818181818182, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.4, 0.12121212121212122, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-24286", "mrqa_naturalquestions-train-54060", "mrqa_naturalquestions-train-87935", "mrqa_naturalquestions-train-11127", "mrqa_naturalquestions-train-24646", "mrqa_naturalquestions-train-59946", "mrqa_naturalquestions-train-33756", "mrqa_naturalquestions-train-63104", "mrqa_naturalquestions-train-72700", "mrqa_naturalquestions-train-86460", "mrqa_naturalquestions-train-3754", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-train-86191", "mrqa_naturalquestions-train-43371", "mrqa_naturalquestions-train-13554", "mrqa_naturalquestions-train-5140", "mrqa_naturalquestions-train-32358", "mrqa_naturalquestions-train-84672", "mrqa_naturalquestions-train-43261", "mrqa_naturalquestions-train-86826", "mrqa_naturalquestions-train-86362", "mrqa_naturalquestions-train-38366", "mrqa_naturalquestions-train-6355", "mrqa_naturalquestions-train-34897", "mrqa_naturalquestions-train-46078", "mrqa_naturalquestions-train-69144", "mrqa_naturalquestions-train-1430", "mrqa_naturalquestions-train-52454", "mrqa_naturalquestions-train-43601", "mrqa_naturalquestions-train-57055", "mrqa_naturalquestions-train-47560", "mrqa_naturalquestions-train-16330"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Silicon Valley", "spain", "sports commentator", "Newell Highway between Melbourne and Brisbane", "cybermen", "a horse is 15 hands, 2", "a shopping mall located in Bloomington, Minnesota, United States ( a suburb of the Twin Cities )", "to avoid responsibility for her actions", "DreamWorks Animation", "skaltz", "his own men", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "spain", "the RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "reduce growth in relatively poor countries but encourage growth", "Ibrium", "spain", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "spain", "aintree", "390 billion", "Washington Street", "May 10, 1976", "six", "lohan", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "spain", "John Smith", "lusitania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.125, "QA-F1": 0.19817364348614347}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.07407407407407407, 0.3636363636363636, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.45000000000000007, 0.0, 1.0, 1.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_squad-validation-3106"], "retrieved_ids": ["mrqa_naturalquestions-train-10736", "mrqa_naturalquestions-train-46084", "mrqa_naturalquestions-train-74173", "mrqa_naturalquestions-train-69669", "mrqa_naturalquestions-train-8707", "mrqa_naturalquestions-train-84481", "mrqa_naturalquestions-train-80543", "mrqa_naturalquestions-train-4401", "mrqa_naturalquestions-train-85086", "mrqa_naturalquestions-train-13420", "mrqa_naturalquestions-train-27548", "mrqa_naturalquestions-train-70116", "mrqa_naturalquestions-train-23573", "mrqa_naturalquestions-train-25465", "mrqa_naturalquestions-train-73282", "mrqa_naturalquestions-train-24767", "mrqa_naturalquestions-train-85846", "mrqa_naturalquestions-train-84557", "mrqa_naturalquestions-train-84636", "mrqa_naturalquestions-train-86003", "mrqa_naturalquestions-train-12030", "mrqa_naturalquestions-train-708", "mrqa_naturalquestions-train-35577", "mrqa_naturalquestions-train-1919", "mrqa_naturalquestions-train-33921", "mrqa_naturalquestions-train-55882", "mrqa_naturalquestions-train-31729", "mrqa_naturalquestions-train-42830", "mrqa_naturalquestions-train-49847", "mrqa_naturalquestions-train-59765", "mrqa_naturalquestions-train-24124", "mrqa_naturalquestions-train-68447"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["an orbital scientific instrument package", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano, S.A.B. de C.V.", "beer", "gender test", "Majandra Delfino", "kalium", "extreme circumstances", "T cell receptor", "generally paid on graduated scales, with income depending on experience", "non-GMO", "Heading Out to the Highway", "Moonraker", "$349", "Michael Oppenheimer", "England national team", "poor and well socially standing Chinese", "No Night Today", "Convention", "5,922", "December 5, 1991", "2016", "Sam Hinkie", "Saint Nicholas", "Potsdam", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "Monday", "Dallas", "Nairobi", "last Ice Age", "Neon City"], "metric_results": {"EM": 0.15625, "QA-F1": 0.29734848484848486}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1818181818181818, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960", "mrqa_hotpotqa-validation-5557"], "retrieved_ids": ["mrqa_naturalquestions-train-80799", "mrqa_naturalquestions-train-39303", "mrqa_naturalquestions-train-51653", "mrqa_naturalquestions-train-19169", "mrqa_naturalquestions-train-85155", "mrqa_naturalquestions-train-64271", "mrqa_naturalquestions-train-65506", "mrqa_naturalquestions-train-58830", "mrqa_naturalquestions-train-310", "mrqa_naturalquestions-train-85003", "mrqa_naturalquestions-train-3286", "mrqa_naturalquestions-train-81210", "mrqa_naturalquestions-train-70299", "mrqa_naturalquestions-train-13914", "mrqa_naturalquestions-train-25566", "mrqa_naturalquestions-train-60808", "mrqa_naturalquestions-train-28938", "mrqa_naturalquestions-train-59430", "mrqa_naturalquestions-train-75185", "mrqa_naturalquestions-train-5024", "mrqa_naturalquestions-train-20335", "mrqa_naturalquestions-train-51542", "mrqa_naturalquestions-train-6188", "mrqa_naturalquestions-train-29379", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-train-62033", "mrqa_naturalquestions-train-43592", "mrqa_naturalquestions-train-70965", "mrqa_naturalquestions-train-22885", "mrqa_naturalquestions-train-6716", "mrqa_naturalquestions-train-70025", "mrqa_naturalquestions-train-30477"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Napoleon", "is a controversial Canadian-American Roman Catholic priest based in the United States near Detroit at Royal Oak, Michigan's National Shrine of the Little Flower church", "1967", "legprints", "the twelfth most populous city in the United States", "50 home run club", "september", "confers an aberrant, ligand - independent, non- regulated growth stimulus to the cancer cells", "lower", "bass", "Tzeitel", "New Orleans, Biloxi, Mississippi, Mobile, Alabama", "september", "bridge", "Yunnan- Fu", "London, United Kingdom", "Broken Hill and Sydney", "2005", "forgiveness was God's alone to grant", "\"The Doctor's Daughter\"", "september", "bridge", "structures that are a matter of custom or expectation, such as isolating businesses to a business district and residences to a residential district", "1879", "She was the daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm", "south-east China", "passenger space", "Arkansas", "Buskerud"], "metric_results": {"EM": 0.1875, "QA-F1": 0.24453947368421053}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7200000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4999999999999999, 0.0, 0.10526315789473684, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61"], "retrieved_ids": ["mrqa_naturalquestions-train-87885", "mrqa_naturalquestions-train-47437", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-train-43511", "mrqa_naturalquestions-train-38228", "mrqa_naturalquestions-train-74662", "mrqa_naturalquestions-train-33085", "mrqa_naturalquestions-train-10745", "mrqa_naturalquestions-train-56733", "mrqa_naturalquestions-train-27955", "mrqa_naturalquestions-train-82343", "mrqa_naturalquestions-train-44481", "mrqa_naturalquestions-train-23728", "mrqa_naturalquestions-train-12972", "mrqa_naturalquestions-train-53456", "mrqa_naturalquestions-train-14723", "mrqa_naturalquestions-train-38514", "mrqa_naturalquestions-train-55409", "mrqa_naturalquestions-train-23969", "mrqa_naturalquestions-train-58677", "mrqa_naturalquestions-train-58901", "mrqa_triviaqa-validation-4068", "mrqa_naturalquestions-train-48882", "mrqa_naturalquestions-train-706", "mrqa_naturalquestions-train-23352", "mrqa_naturalquestions-train-81033", "mrqa_naturalquestions-train-49769", "mrqa_naturalquestions-train-38415", "mrqa_hotpotqa-validation-3618", "mrqa_naturalquestions-train-29557", "mrqa_naturalquestions-train-15826", "mrqa_naturalquestions-train-15740"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Chairman", "king George VI", "engaging in the forbidden speech", "Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "boulangere", "0.2 inhabitants per square kilometre (0.52/ sq mi)", "king edward graham niven", "France", "Ian Paisley", "bataan", "euro", "suggs", "Imperial Japan's victory in the Russo - Japanese War, with its subsequent withdrawal of Russian influence, and the Taft -- Katsura Agreement", "1973", "c. 1886", "2008 NFL season", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Oliver Reed as Antonius Proximo", "pole", "Johnny Darrell", "carotid artery disease", "margarine", "Euler's totient function", "wax", "binary strings", "the second-busiest airport in the United States by passenger volume", "red", "Honda Accord", "Kurt Vonnegut", "king and queen of Corona"], "metric_results": {"EM": 0.125, "QA-F1": 0.1995654199601568}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "retrieved_ids": ["mrqa_naturalquestions-train-35862", "mrqa_naturalquestions-train-15636", "mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-train-24297", "mrqa_naturalquestions-train-79002", "mrqa_naturalquestions-train-25772", "mrqa_naturalquestions-train-14215", "mrqa_triviaqa-validation-1550", "mrqa_naturalquestions-train-36810", "mrqa_naturalquestions-train-57350", "mrqa_naturalquestions-train-72653", "mrqa_naturalquestions-train-19806", "mrqa_naturalquestions-train-11032", "mrqa_squad-validation-8075", "mrqa_naturalquestions-train-1400", "mrqa_naturalquestions-train-36699", "mrqa_naturalquestions-train-27535", "mrqa_naturalquestions-train-66160", "mrqa_naturalquestions-train-85327", "mrqa_naturalquestions-train-21514", "mrqa_squad-validation-5249", "mrqa_naturalquestions-train-25908", "mrqa_naturalquestions-train-26627", "mrqa_naturalquestions-train-39995", "mrqa_naturalquestions-validation-150", "mrqa_naturalquestions-train-51199", "mrqa_naturalquestions-train-56896", "mrqa_naturalquestions-train-2331", "mrqa_naturalquestions-train-50099", "mrqa_naturalquestions-train-66668", "mrqa_naturalquestions-train-1564", "mrqa_naturalquestions-train-17970"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "the interplay of supply and demand, which determines the prices of goods and services", "Emma Watson", "brain, muscles, and liver", "afghanistan", "the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Ravens", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "William Howard Ashton", "national security, big oil companies and bribery and corruption at the highest levels of the government of the United States", "high and persistent unemployment", "Greenacres, Lake Clarke Shores, Lake Worth, and West Palm Beach", "Lee Byung-hun", "changing display or audio settings quickly", "King Charles I of England", "from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "it actually declines over the medium term", "Beautyy and the Beast", "South Africa", "Tyler \" Ty\" Mendoza", "Alamo", "a seal illegally is broken", "United Methodist Church", "David Draiman", "Roger Allers and Rob Minkoff", "Port Moresby, Papua New Guinea", "Alvin Simon Theodore Ross Bagdasarian", "National Association for the Advancement of Colored People", "1963\u20131989", "a special train designated for transporting Titanic survivors was being organized with the New Haven Railroad", "a heart attack", "Dick Sargent", "6500 - 1500 BC"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3162524635690397}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.5, 0.125, 0.0, 0.4, 0.0, 0.34782608695652173, 0.25, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.7499999999999999, 0.4615384615384615, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-5690", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-1136", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "retrieved_ids": ["mrqa_naturalquestions-train-23762", "mrqa_naturalquestions-train-83474", "mrqa_naturalquestions-train-37495", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-train-44452", "mrqa_naturalquestions-train-8124", "mrqa_naturalquestions-train-6958", "mrqa_naturalquestions-train-54319", "mrqa_naturalquestions-train-30558", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-train-69388", "mrqa_naturalquestions-train-11103", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-train-36608", "mrqa_naturalquestions-train-17568", "mrqa_naturalquestions-train-71072", "mrqa_naturalquestions-train-55138", "mrqa_naturalquestions-train-7838", "mrqa_naturalquestions-train-605", "mrqa_naturalquestions-train-76688", "mrqa_naturalquestions-train-71219", "mrqa_naturalquestions-train-35661", "mrqa_naturalquestions-train-5343", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-train-74230", "mrqa_naturalquestions-train-47177", "mrqa_naturalquestions-train-38333", "mrqa_naturalquestions-train-43185", "mrqa_naturalquestions-train-68703", "mrqa_naturalquestions-train-57349", "mrqa_naturalquestions-train-53695", "mrqa_naturalquestions-train-77030"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Amber Laura Heard", "ring", "president rudolf", "Cobham\u2013Edmonds thesis", "a Time Lady", "II", "March 2012", "jazz musicians and other residents of the city", "Muhammad Ali", "Beyonc\u00e9 and Bruno Mars", "Menorca", "to civil disobedients", "Julius Caesar", "2", "1979", "the first person to find it would inherit his entire fortune and the corporation", "decision problem", "elizabeth lecouvreur", "the heart", "Miasma theory", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges", "red", "Georgia", "butterfly", "US$3 per barrel", "a flat rate", "david harting", "the ARPANET", "roughly west", "Sudan"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3571206571206571}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.36363636363636365, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.07692307692307691, 0.4444444444444445, 0.0, 0.4, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-4069", "mrqa_squad-validation-3635", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-63337", "mrqa_hotpotqa-validation-5709", "mrqa_naturalquestions-train-14196", "mrqa_naturalquestions-train-66000", "mrqa_naturalquestions-train-6839", "mrqa_naturalquestions-train-22964", "mrqa_naturalquestions-train-11392", "mrqa_naturalquestions-train-52138", "mrqa_naturalquestions-train-78579", "mrqa_naturalquestions-train-14215", "mrqa_naturalquestions-train-52458", "mrqa_naturalquestions-train-24879", "mrqa_naturalquestions-train-58293", "mrqa_naturalquestions-train-37009", "mrqa_naturalquestions-train-60875", "mrqa_naturalquestions-train-7540", "mrqa_naturalquestions-train-60208", "mrqa_naturalquestions-train-86624", "mrqa_naturalquestions-train-86536", "mrqa_naturalquestions-train-19492", "mrqa_naturalquestions-train-25755", "mrqa_naturalquestions-train-57277", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-45764", "mrqa_naturalquestions-train-12924", "mrqa_naturalquestions-train-70510", "mrqa_naturalquestions-train-8374", "mrqa_naturalquestions-train-36269", "mrqa_naturalquestions-train-86228", "mrqa_naturalquestions-train-44230", "mrqa_naturalquestions-train-68209", "mrqa_naturalquestions-train-86615"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["San Joaquin Valley Railroad", "three of his ribs were broken", "7 December 2000", "Post Alley under Pike Place Market", "art from Thailand, Burma, Cambodia, Indonesia and Sri Lanka", "February 20, 1978", "haggis", "Walter Mondale", "96", "the basic curriculum -- the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "japan", "a black background representing the circle with glossy gold letters", "The period known as the Ubaid period", "at exactly 37\u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "emileen brown", "Rumplestiltskin", "Harry Kane", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "numerous musical venues", "riper grapes", "1991", "india", "7 January 1936", "lifetime protection", "twenty- three", "Edwin Hubble", "much of the city's tax base dissipated, leading to problems with funding education, sanitation, and traffic control within the city limits", "The Republicans, who were loyal to the democratic, left - leaning and relatively urban Second Spanish Republic, in an alliance of convenience with the Anarchists and Communists", "Nlend Wom\u00e9", "no of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "less sugar content", "Boston, Massachusetts"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2730920411642918}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4799999999999999, 0.08695652173913045, 0.8, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1625", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "retrieved_ids": ["mrqa_naturalquestions-train-81897", "mrqa_naturalquestions-train-78374", "mrqa_naturalquestions-validation-816", "mrqa_naturalquestions-train-40831", "mrqa_naturalquestions-train-58981", "mrqa_naturalquestions-train-14856", "mrqa_naturalquestions-train-23483", "mrqa_naturalquestions-train-83927", "mrqa_naturalquestions-train-57332", "mrqa_triviaqa-validation-7530", "mrqa_naturalquestions-train-56569", "mrqa_naturalquestions-train-1304", "mrqa_naturalquestions-train-18317", "mrqa_naturalquestions-train-43674", "mrqa_naturalquestions-train-21952", "mrqa_naturalquestions-train-32422", "mrqa_squad-validation-358", "mrqa_naturalquestions-train-26313", "mrqa_naturalquestions-train-42601", "mrqa_naturalquestions-train-75873", "mrqa_naturalquestions-train-35365", "mrqa_naturalquestions-train-36153", "mrqa_squad-validation-110", "mrqa_naturalquestions-train-74177", "mrqa_naturalquestions-train-37128", "mrqa_naturalquestions-train-61037", "mrqa_naturalquestions-train-48466", "mrqa_naturalquestions-train-49545", "mrqa_naturalquestions-train-87093", "mrqa_naturalquestions-train-52937", "mrqa_naturalquestions-train-62272", "mrqa_triviaqa-validation-5050"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "New England", "Etienne de Mestre", "rabbit", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "The History of Little Goody Two - Shoes", "the longest rotation period ( 243 days ) of any planet in the Solar System", "gathering money from the public", "Ethan", "when commissioned to purchase their required uniform items", "Jeff Meldrum", "741 weeks", "Norman Painting", "Shoshone, his mother tongue, and other western American Indian languages", "The Paris Sisters", "emigrant Ship", "60", "journalist", "the fact that there is no revising chamber", "avatar", "the points of algebro-geometric objects", "most of the items in the collection, unless those were newly accessioned into the collection", "free floating and depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "strychnine", "Texas", "1945", "13 June 2003", "Eddy Shah", "Darleen Carr", "with each heartbeat"], "metric_results": {"EM": 0.125, "QA-F1": 0.22126272190017446}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.09523809523809523, 0.0, 1.0, 0.14285714285714288, 0.47058823529411764, 0.0, 0.4, 0.25, 0.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.2857142857142857, 1.0, 0.6, 0.0, 0.0, 0.5, 0.06451612903225806, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_triviaqa-validation-3118", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "retrieved_ids": ["mrqa_naturalquestions-train-19915", "mrqa_naturalquestions-train-17327", "mrqa_naturalquestions-train-81499", "mrqa_naturalquestions-train-2097", "mrqa_naturalquestions-train-4401", "mrqa_naturalquestions-train-74165", "mrqa_naturalquestions-train-45462", "mrqa_naturalquestions-train-55361", "mrqa_naturalquestions-train-70965", "mrqa_naturalquestions-train-48359", "mrqa_naturalquestions-train-27936", "mrqa_naturalquestions-train-41849", "mrqa_naturalquestions-train-74958", "mrqa_naturalquestions-train-71101", "mrqa_naturalquestions-train-73886", "mrqa_naturalquestions-train-20359", "mrqa_naturalquestions-train-10887", "mrqa_naturalquestions-train-14292", "mrqa_naturalquestions-train-38314", "mrqa_naturalquestions-train-24119", "mrqa_naturalquestions-train-8187", "mrqa_naturalquestions-train-85550", "mrqa_naturalquestions-train-43366", "mrqa_naturalquestions-train-35082", "mrqa_naturalquestions-train-34736", "mrqa_naturalquestions-train-63571", "mrqa_naturalquestions-train-26017", "mrqa_naturalquestions-train-4931", "mrqa_naturalquestions-train-39188", "mrqa_naturalquestions-train-26695", "mrqa_naturalquestions-train-44666", "mrqa_naturalquestions-train-81897"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Taiwan ( a government on the island of Taiwan established in 1949 by Chiang Kai-shek after the conquest of mainland China by the communists led by Mao Zedong", "Dan Conner", "checkpoint Charlie", "JFK assassination", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "keskes", "the 1980s", "John M. Grunsfeld", "New Orleans", "a man who makes potions in a traveling show", "2000", "each and every year", "Fiat S.p.A.", "the second Sunday of March", "relative units of force and mass", "woman", "two", "August 10, 1933", "The Golden Gate Bridge", "Sochi, Russia", "those who already hold wealth", "B. Traven", "Finding Nemo", "unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "oil prices", "cheez", "264,152", "Princeton, New Jersey", "the United States", "high pressure"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3757772864390512}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.15384615384615383, 1.0, 0.0, 0.0, 0.8, 1.0, 0.9230769230769231, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-13477", "mrqa_squad-validation-4118", "mrqa_naturalquestions-train-67517", "mrqa_naturalquestions-train-18812", "mrqa_naturalquestions-train-36066", "mrqa_naturalquestions-train-49305", "mrqa_naturalquestions-train-72149", "mrqa_naturalquestions-train-39773", "mrqa_naturalquestions-train-10511", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-train-65370", "mrqa_naturalquestions-train-42399", "mrqa_naturalquestions-train-85911", "mrqa_squad-validation-6878", "mrqa_naturalquestions-train-71093", "mrqa_naturalquestions-train-36177", "mrqa_naturalquestions-train-45876", "mrqa_squad-validation-8473", "mrqa_naturalquestions-train-76537", "mrqa_naturalquestions-train-6839", "mrqa_naturalquestions-train-65317", "mrqa_naturalquestions-train-34048", "mrqa_naturalquestions-train-49316", "mrqa_naturalquestions-train-641", "mrqa_naturalquestions-train-74024", "mrqa_naturalquestions-train-20829", "mrqa_naturalquestions-train-8223", "mrqa_naturalquestions-train-56674", "mrqa_naturalquestions-train-53148", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-train-28139", "mrqa_naturalquestions-train-39500"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "on the road back to Samarkand", "warren warren", "Isabella (Belle) Baumfree", "warren", "throughout the 14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "he broadened the foundations of the Reformation placing them on prophetic faith", "Bacon", "Charlton Heston", "anti-inflammatory molecules, such as cortisol and catecholamines", "war", "Kevin Kolb", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the force of law, if based on the authority derived from statute or the Constitution itself", "warren d' estaing", "when the Mongols placed the Uighurs of the Kingdom of Qocho over the Koreans at the court the Korean King objected", "Sochi, Russia", "right", "Hudson Bay", "At the insistence of NASA Administrator Webb, North American removed Harrison Storms as Command Module program manager. Webb also reassigned Apollo Spacecraft Program Office (ASPO) Manager Joseph Francis Shea, replacing him with George Low", "south australia", "warren", "30 Major League Baseball teams and their 160 minor league baseball affiliates", "the Secret Intelligence Service", "100 billion", "'Kai su, teknon'", "by the photolysis of ozone by light of short wavelength, and by the immune system as a source of active oxygen", "4.7 / 5.5 - inch", "Queen City", "persons and companies ( typically federal contractors ) who defraud governmental programs"], "metric_results": {"EM": 0.25, "QA-F1": 0.36946694543201897}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.375, 0.6666666666666666, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.9, 0.14285714285714288, 0.0, 0.09523809523809522, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5882352941176471, 0.0, 0.0, 0.0, 0.6153846153846153, 0.28571428571428575, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_triviaqa-validation-1571", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "retrieved_ids": ["mrqa_naturalquestions-train-42636", "mrqa_naturalquestions-train-56427", "mrqa_naturalquestions-train-56152", "mrqa_naturalquestions-train-1104", "mrqa_naturalquestions-train-19658", "mrqa_naturalquestions-train-87091", "mrqa_naturalquestions-train-33085", "mrqa_naturalquestions-train-46380", "mrqa_naturalquestions-train-65506", "mrqa_naturalquestions-train-83235", "mrqa_naturalquestions-train-56980", "mrqa_naturalquestions-train-44617", "mrqa_naturalquestions-train-61299", "mrqa_naturalquestions-train-12277", "mrqa_naturalquestions-train-36440", "mrqa_naturalquestions-train-32838", "mrqa_naturalquestions-train-57626", "mrqa_naturalquestions-train-42420", "mrqa_naturalquestions-train-68063", "mrqa_naturalquestions-train-32633", "mrqa_naturalquestions-train-9940", "mrqa_naturalquestions-validation-8525", "mrqa_naturalquestions-train-21248", "mrqa_naturalquestions-train-60892", "mrqa_naturalquestions-train-38028", "mrqa_naturalquestions-train-66809", "mrqa_naturalquestions-train-72663", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-train-33229", "mrqa_naturalquestions-train-52562", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-45870"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["charged particle beam weapons", "Keshto Mukherjee", "Gaelic and Norse ancestry", "Three card brag", "d\u00edsabl\u00f3t", "cave leopards", "Russian film industry", "sediment load", "the Washington metropolitan area", "GTPase", "User State Migration Tool ( USMT )", "Ordos City China Science Flying Universe Science and Technology Co.", "franscioni", "PPG Paints Arena, Pittsburgh, Pennsylvania", "philry wall museum", "Section 30 of the Teaching Council Act 2001", "Paul Lynde as Templeton, a care - free, egotistical rat who lives on a web in a corner of Homer's barn above Wilbur's pig pen", "mid-1988", "quasars", "Monsoon", "Romansh", "Tudor queen", "5AA (FIVEaa)", "Q Branch (or later Q Division)", "Paul and Timothy", "An Inquiry into the Nature and Causes of the Wealth of Nations", "Gerard Marenghi", "Whitney Houston", "Nebula Award", "tabled a motion of no confidence in James Callaghan's Labour government", "David", "Elvis Presley"], "metric_results": {"EM": 0.03125, "QA-F1": 0.15548126390517694}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.4, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4444444444444445, 0.17391304347826084, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.30769230769230765, 0.5, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "retrieved_ids": ["mrqa_naturalquestions-train-10548", "mrqa_naturalquestions-train-25193", "mrqa_naturalquestions-train-61692", "mrqa_naturalquestions-train-37120", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-train-64473", "mrqa_naturalquestions-train-36138", "mrqa_naturalquestions-train-15074", "mrqa_naturalquestions-train-64533", "mrqa_naturalquestions-train-44247", "mrqa_triviaqa-validation-1423", "mrqa_naturalquestions-train-30590", "mrqa_triviaqa-validation-1298", "mrqa_naturalquestions-train-49458", "mrqa_naturalquestions-train-16043", "mrqa_naturalquestions-train-8734", "mrqa_naturalquestions-train-81278", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-train-17685", "mrqa_naturalquestions-train-73852", "mrqa_naturalquestions-train-35996", "mrqa_naturalquestions-train-42559", "mrqa_naturalquestions-train-5935", "mrqa_naturalquestions-train-65138", "mrqa_naturalquestions-train-13875", "mrqa_naturalquestions-train-4632", "mrqa_naturalquestions-train-44156", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-train-60378", "mrqa_hotpotqa-validation-104", "mrqa_naturalquestions-train-12924", "mrqa_naturalquestions-train-21421"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["david carradine", "cavatelli, acini di pepe, pastina, orzo, etc.", "boston", "independence from the Duke of Savoy through an alliance between the city-state of Geneva and the Swiss Confederation", "various causes", "American Civil War", "chartered", "a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "King of Scandinavia", "second vice-captain", "japan", "the side - chain of the amino acid N - terminal", "disaccharide sucrose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "pastels and oil painting", "Extroverted Thinking ( Te )", "Thursday", "white", "a comprehensive drug therapy plan for patient-specific problems, identifying goals of therapy, and reviewing all prescribed medications prior to dispensing and administration to the patient", "jupiter", "feats of exploration", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "bobby", "The Education Service Contracting scheme of the government", "pernambuco wood", "to sell indulgences to raise money to rebuild St. Peter's Basilica in Rome", "its colonies", "two forces, one pointing north, and one pointing east", "new laws or amendments to existing laws", "Jack Murphy Stadium", "hierarchy theorems"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27488918862999745}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.9333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.4666666666666667, 0.0, 0.375, 0.5, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.13333333333333333, 0.0, 0.1818181818181818, 0.0, 0.47058823529411764, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_squad-validation-1429", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_squad-validation-9569", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_squad-validation-9452", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-80128", "mrqa_naturalquestions-train-88014", "mrqa_naturalquestions-train-60208", "mrqa_naturalquestions-train-80539", "mrqa_naturalquestions-train-4016", "mrqa_naturalquestions-train-8699", "mrqa_naturalquestions-train-68734", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-train-33438", "mrqa_naturalquestions-train-78089", "mrqa_naturalquestions-train-73534", "mrqa_hotpotqa-validation-35", "mrqa_naturalquestions-train-19518", "mrqa_naturalquestions-train-83787", "mrqa_naturalquestions-train-55282", "mrqa_naturalquestions-train-41411", "mrqa_naturalquestions-train-73546", "mrqa_naturalquestions-train-8737", "mrqa_naturalquestions-train-70268", "mrqa_naturalquestions-train-59165", "mrqa_naturalquestions-train-63141", "mrqa_naturalquestions-train-7599", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-train-37328", "mrqa_naturalquestions-train-3078", "mrqa_naturalquestions-train-69712", "mrqa_naturalquestions-train-48091", "mrqa_naturalquestions-train-3171", "mrqa_naturalquestions-train-33594", "mrqa_naturalquestions-train-6706", "mrqa_naturalquestions-train-16061", "mrqa_naturalquestions-train-30761"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["bridal Veil Falls", "4 Points - F, H, V, W and Y", "19th", "september", "French", "a sailor coming home from a round trip", "domain names www.example.com and example.com", "immune system is less active than normal", "Strasbourg", "natural-ing Ingredients- only personal care products", "Burl Ives", "Rigoletto", "Russia", "third-most abundant element in the universe", "iKEA", "216", "egypt", "Nicholas Stone", "Algernod Lanier Washington", "the Outfield", "egypt", "Michael Edwards", "railway locomotives", "eddie Fisher", "third quarter ( also known as last quarter )", "egypt", "chemists Glenn T. Seaborg", "Tennessee", "computers", "eve", "615 square kilometers", "magi"], "metric_results": {"EM": 0.0625, "QA-F1": 0.22686011904761905}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 0.2857142857142857, 0.25, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.4, 0.4, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "retrieved_ids": ["mrqa_naturalquestions-train-5262", "mrqa_naturalquestions-train-53019", "mrqa_naturalquestions-train-52701", "mrqa_naturalquestions-train-19772", "mrqa_naturalquestions-train-10111", "mrqa_naturalquestions-train-32970", "mrqa_naturalquestions-train-85848", "mrqa_naturalquestions-train-10840", "mrqa_naturalquestions-train-9230", "mrqa_naturalquestions-train-23263", "mrqa_naturalquestions-train-34944", "mrqa_naturalquestions-train-55802", "mrqa_naturalquestions-train-63452", "mrqa_naturalquestions-train-20779", "mrqa_naturalquestions-train-16432", "mrqa_naturalquestions-train-53454", "mrqa_naturalquestions-train-42374", "mrqa_naturalquestions-train-13165", "mrqa_naturalquestions-train-29661", "mrqa_naturalquestions-train-13868", "mrqa_naturalquestions-train-20542", "mrqa_naturalquestions-train-60775", "mrqa_naturalquestions-train-32422", "mrqa_naturalquestions-train-76861", "mrqa_triviaqa-validation-7133", "mrqa_naturalquestions-train-44200", "mrqa_naturalquestions-train-60360", "mrqa_naturalquestions-train-44073", "mrqa_naturalquestions-train-85696", "mrqa_naturalquestions-train-81111", "mrqa_naturalquestions-train-33689", "mrqa_naturalquestions-train-8343"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "high school teachers, median salaries in 2007 ranged from $35,000 in South Dakota to $71,000", "England and Wales Cricket Board ( ECB )", "mexico", "mexico", "campaign setting", "the `` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867 feet (265 m)", "the Hebrew name Immanu'el ( \u05e2\u05b4\u05de\u05b8\u05bc\u05e0\u05d5\u05bc\u05d0\u05b5\u05dc \u202c, which means `` God with us. ''", "\u00f7", "Christopher Lee as Count Dooku / Darth Tyranus", "8th", "Tikki tikki tembo-no sa rembo- chari bari ruchi-pip peri pembo", "all health care settings", "increased patient health outcomes and decreased costs to the health care system", "treble clef", "Gabriel Alberto Azucena", "12951 / 52", "between the Piazza di Spagna at the base and PIAzza Trinit\u00e0 dei Monti", "December 1, 2009", "an English campaigner for the suffragette movement", "27member states", "philosopher, statesman, scientist, jurist, orator, and author", "meyer", "Arun Jaitley", "British", "ancient cult activity", "Penguin Classics", "energy-storage molecules ATP and NADPH", "Space telescope", "our neighbors as ourselves", "chorale cantatas"], "metric_results": {"EM": 0.125, "QA-F1": 0.22753496503496504}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 1.0, 0.4, 0.0, 0.15384615384615385, 0.7272727272727273, 0.0, 0.0, 1.0, 0.39999999999999997, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951"], "retrieved_ids": ["mrqa_naturalquestions-train-10879", "mrqa_naturalquestions-train-27025", "mrqa_naturalquestions-train-16605", "mrqa_naturalquestions-train-60977", "mrqa_naturalquestions-train-18357", "mrqa_naturalquestions-train-36177", "mrqa_naturalquestions-train-24447", "mrqa_naturalquestions-train-83661", "mrqa_naturalquestions-train-6332", "mrqa_naturalquestions-train-41174", "mrqa_naturalquestions-train-80906", "mrqa_naturalquestions-train-55444", "mrqa_naturalquestions-train-9888", "mrqa_naturalquestions-train-49013", "mrqa_naturalquestions-train-8807", "mrqa_naturalquestions-train-3793", "mrqa_naturalquestions-train-2797", "mrqa_naturalquestions-train-31569", "mrqa_naturalquestions-train-14084", "mrqa_naturalquestions-train-30187", "mrqa_naturalquestions-train-60145", "mrqa_naturalquestions-train-46476", "mrqa_naturalquestions-train-63876", "mrqa_naturalquestions-train-9490", "mrqa_naturalquestions-train-44616", "mrqa_naturalquestions-validation-150", "mrqa_naturalquestions-train-38494", "mrqa_naturalquestions-train-75814", "mrqa_naturalquestions-train-6387", "mrqa_naturalquestions-train-23168", "mrqa_naturalquestions-train-28428", "mrqa_naturalquestions-train-34634"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["well-boiled", "0\u201316", "perique", "under `` the immortal Hawke ''", "death penalty", "a stout man with a \" Double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "south and east coast", "rich Fisher King", "Mangal Pandey", "Colonia Agrippina", "bran", "four of the 50 states of the United States", "curling", "the eighth series", "Pebble Beach", "He has served as the offensive line coach for eight different NFL teams, one Canadian Football League (CFL) team, and four college teams.", "Korean", "Henry Mills", "\"LOVE Radio\"", "the American League (AL) champion Boston Red Sox", "the court from its members", "richard travolta", "Dan Fogelman", "put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "Santa Clara", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Landing Barge, Kitchen or LBK", "peninsular mainland"], "metric_results": {"EM": 0.125, "QA-F1": 0.19610233516483516}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.4166666666666667, 0.16666666666666669, 1.0, 0.07142857142857142, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-712", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-34866", "mrqa_naturalquestions-train-19384", "mrqa_naturalquestions-train-71867", "mrqa_naturalquestions-train-86907", "mrqa_naturalquestions-train-6013", "mrqa_naturalquestions-train-84157", "mrqa_naturalquestions-train-6618", "mrqa_naturalquestions-train-57249", "mrqa_naturalquestions-train-58421", "mrqa_naturalquestions-train-17327", "mrqa_naturalquestions-train-55281", "mrqa_naturalquestions-train-17806", "mrqa_naturalquestions-train-58612", "mrqa_naturalquestions-train-69894", "mrqa_naturalquestions-train-63770", "mrqa_naturalquestions-train-69584", "mrqa_naturalquestions-train-17116", "mrqa_naturalquestions-train-20483", "mrqa_naturalquestions-train-5686", "mrqa_triviaqa-validation-3760", "mrqa_naturalquestions-train-80655", "mrqa_naturalquestions-train-54972", "mrqa_naturalquestions-train-86422", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-train-15431", "mrqa_naturalquestions-train-36855", "mrqa_naturalquestions-train-65396", "mrqa_squad-validation-3368", "mrqa_naturalquestions-train-82927", "mrqa_naturalquestions-train-5779", "mrqa_naturalquestions-train-87794", "mrqa_naturalquestions-train-28771"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "Take That", "youngest publicly documented people to be identified as transgender", "electric lighting", "their knowledge of Native American languages as a basis to transmit coded messages", "Einstein", "electromagnetic theory", "Premier League", "pre-Raphaelite", "Elizabeth Weber", "It was released for PlayStation 4 and Xbox One on May 3, 2016", "hundreds", "\"Waiting for Guffman\"", "June 22, 1978", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "partial funding", "a 5% abv pale lager produced by Boon Rawd Brewery", "inefficient", "Chu'Tsai", "Liz", "least onerous", "lago di como", "Grissom, White, and Chaffee", "multinational retail corporation", "purple passion fruit", "Bharata Muni", "The magnificent sand dunes at Greenwich are of particular significance", "golf", "parts of the air in the vessel were converted into the classical element fire", "emperor of Austria"], "metric_results": {"EM": 0.375, "QA-F1": 0.4386304159202866}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.375, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "retrieved_ids": ["mrqa_naturalquestions-train-6283", "mrqa_naturalquestions-train-58088", "mrqa_naturalquestions-train-35820", "mrqa_naturalquestions-train-50821", "mrqa_naturalquestions-train-48137", "mrqa_naturalquestions-train-50820", "mrqa_naturalquestions-train-29447", "mrqa_naturalquestions-train-23410", "mrqa_naturalquestions-train-70655", "mrqa_naturalquestions-train-6058", "mrqa_naturalquestions-train-13488", "mrqa_naturalquestions-train-2437", "mrqa_naturalquestions-train-62544", "mrqa_naturalquestions-train-43901", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-25133", "mrqa_naturalquestions-train-49110", "mrqa_squad-validation-5618", "mrqa_naturalquestions-train-11436", "mrqa_naturalquestions-train-50285", "mrqa_naturalquestions-train-31348", "mrqa_naturalquestions-train-80331", "mrqa_naturalquestions-train-62665", "mrqa_naturalquestions-train-53592", "mrqa_naturalquestions-train-4815", "mrqa_naturalquestions-train-83124", "mrqa_naturalquestions-train-42615", "mrqa_naturalquestions-train-69179", "mrqa_hotpotqa-validation-4558", "mrqa_hotpotqa-validation-3428", "mrqa_naturalquestions-train-42135", "mrqa_naturalquestions-train-26211"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["florida", "horse racing", "Burnley and the New Zealand national team", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "William Jennings Bryan", "Milk Barn Animation", "when they enter the army during initial entry training", "moral tale", "hiatus", "leeds", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "around 74 per cent", "London Heathrow Airport by Train", "communities with considerable face-to-face interaction among members", "William Strauss", "monophyletic", "insect pest management programs", "candidates on specific catechism questions", "a pH indicator, a color marker, and a dye", "2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "63,182,000", "John and Charles Wesley", "\"Science and Discovery are the great Forces which will lead to the Consummation of the War\"", "Euclid's fundamental theorem of arithmetic", "Mississippi State", "car crash", "Jude in the musical romance drama film \" across the Universe\" (2007)", "cuba", "bridal shop", "Jocelyn Flores", "downward pressure"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3393229166666667}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.7499999999999999, 0.5, 0.2222222222222222, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.4444444444444445, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-1609", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-2092", "mrqa_squad-validation-7182"], "retrieved_ids": ["mrqa_naturalquestions-train-76250", "mrqa_naturalquestions-train-25382", "mrqa_naturalquestions-train-44151", "mrqa_naturalquestions-train-54838", "mrqa_naturalquestions-train-27545", "mrqa_triviaqa-validation-3238", "mrqa_naturalquestions-train-33039", "mrqa_naturalquestions-train-74764", "mrqa_naturalquestions-train-70950", "mrqa_naturalquestions-train-46764", "mrqa_naturalquestions-train-84389", "mrqa_triviaqa-validation-7538", "mrqa_naturalquestions-train-74338", "mrqa_hotpotqa-validation-5401", "mrqa_naturalquestions-train-2285", "mrqa_naturalquestions-train-61537", "mrqa_naturalquestions-train-6467", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-3340", "mrqa_naturalquestions-train-77417", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-train-55200", "mrqa_naturalquestions-train-72572", "mrqa_naturalquestions-train-9926", "mrqa_naturalquestions-train-7599", "mrqa_naturalquestions-train-29646", "mrqa_naturalquestions-train-17620", "mrqa_squad-validation-10369", "mrqa_naturalquestions-train-61849", "mrqa_naturalquestions-train-8053", "mrqa_naturalquestions-train-9346", "mrqa_naturalquestions-train-2798"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["a heroine who experienced many tragedies, mostly at the hands of her controlling ex-husband, the villainous James Stenbeck ( Anthony Herrera) is a fictional character from the American CBS soap opera \"As the World Turns\"", "Good Kid, M.A. A.D City", "Yosemite National Park", "\"interventive\" conservation", "3", "The Methodist Church", "Ray Charles", "Goku becomes the first Saiyan in a thousand years to transform into a fabled Super Saiyan", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "UNESCO", "Brandon Jennings", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "the 1964 Republican National Convention in San Francisco, California", "sin", "annuity", "Princess Leia Organa of Alderaan", "Frank Theodore `` Ted '' Levine", "justice resides", "the French Union", "a war With the main goal of ending aggressive militarism and indeed ending all wars", "espresso", "halal", "Hecuba", "to be memorised by the people themselves", "Wylie Draper", "political role for Islam", "George Beadle's office in a protest over the university's off- campuses rental policies", "hockey greats Bobby Hull and Dennis Hull, as well as painter Manley MacDonald", "Pittsburgh Steelers", "famine"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3318700396825397}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.13333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.22222222222222218, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.2666666666666667, 0.4, 0.39999999999999997, 0.761904761904762, 1.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_squad-validation-5665", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-17725", "mrqa_naturalquestions-train-24634", "mrqa_naturalquestions-train-41055", "mrqa_naturalquestions-train-36471", "mrqa_naturalquestions-train-54596", "mrqa_naturalquestions-train-80606", "mrqa_naturalquestions-train-17512", "mrqa_naturalquestions-train-35604", "mrqa_naturalquestions-train-85952", "mrqa_naturalquestions-train-14722", "mrqa_naturalquestions-train-6457", "mrqa_naturalquestions-train-11835", "mrqa_naturalquestions-train-11574", "mrqa_squad-validation-3126", "mrqa_naturalquestions-train-29774", "mrqa_naturalquestions-train-75644", "mrqa_naturalquestions-train-56068", "mrqa_naturalquestions-train-63860", "mrqa_naturalquestions-train-66434", "mrqa_naturalquestions-train-28642", "mrqa_naturalquestions-train-32216", "mrqa_naturalquestions-train-4648", "mrqa_naturalquestions-train-13540", "mrqa_naturalquestions-train-24447", "mrqa_naturalquestions-train-62437", "mrqa_naturalquestions-train-4154", "mrqa_naturalquestions-train-76750", "mrqa_naturalquestions-train-62868", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-train-5561", "mrqa_naturalquestions-train-10451", "mrqa_naturalquestions-train-22388"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["suitable for use on rough terrain", "AOL", "Timur", "R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "fear of public speaking", "September 1895", "health care", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "Tesla had assigned them to the company in lieu of stock", "Marxist", "variation in plants", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "3,600", "State Street", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "44 hectares", "georgia cukor", "georgia", "Lawton Mainor Chiles Jr.", "the episode `` Kobol's Last Gleaming ''", "Wisconsin v. Yoder", "uneven trade agreements", "bread, chapati, mahamri, boiled sweet potatoes or yams", "ATP energy", "georgia", "Ruth Elizabeth \"Bette\" Davis", "georgia", "7 December 2004"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21735492212465896}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.888888888888889, 0.0, 0.0, 0.1111111111111111, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.10526315789473685, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "retrieved_ids": ["mrqa_naturalquestions-train-53898", "mrqa_naturalquestions-train-31580", "mrqa_naturalquestions-train-57961", "mrqa_naturalquestions-train-26719", "mrqa_naturalquestions-train-79347", "mrqa_naturalquestions-train-62068", "mrqa_naturalquestions-train-8533", "mrqa_naturalquestions-train-24445", "mrqa_naturalquestions-train-23252", "mrqa_naturalquestions-train-19783", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-train-37732", "mrqa_naturalquestions-train-54858", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-train-85301", "mrqa_naturalquestions-train-44167", "mrqa_naturalquestions-train-59067", "mrqa_naturalquestions-train-33064", "mrqa_triviaqa-validation-7371", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-train-15753", "mrqa_naturalquestions-train-40549", "mrqa_naturalquestions-train-68475", "mrqa_naturalquestions-train-11962", "mrqa_naturalquestions-train-60328", "mrqa_naturalquestions-train-19876", "mrqa_naturalquestions-train-60848", "mrqa_naturalquestions-train-8398", "mrqa_naturalquestions-train-10353", "mrqa_naturalquestions-train-38570", "mrqa_naturalquestions-train-82261", "mrqa_naturalquestions-train-76530"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.1425, "QA-F1": 0.24943029715293202}, "overall_error_number": 1372, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.715, "QA-F1": 0.7678968449500064}, "final_upstream_test": {"EM": 0.734, "QA-F1": 0.8342629050423302}}}