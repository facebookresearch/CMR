{"model_update_steps": 2560, "method_class": "index_cl_bart_index", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, index_rank_method='most_similar', indexing_args_path='exp_results/supervision_data/1012_dm_simple.train_args.json', indexing_method='bart_index', inference_query_size=1, init_memory_cache_path='exp_results/data_streams/bart_index.upstream_memory.full.pkl', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/index_based/ckpt_dir/1022_MixedAllErrors_T=100_index_UR=0.5_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=567_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/index_based/ckpt_dir/1022_MixedAllErrors_T=100_index_UR=0.5_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=567_ckpts/', replay_candidate_size=0, replay_frequency=3, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, upstream_sample_ratio=0.5, use_mir=False, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=100, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "The Iroquois", "a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion", "Catherine Zeta Jones", "Virginia Wade", "The Dallas Lovers' Song", "to the anterolateral corner of the spinal cord", "1966", "for scientific observation", "product or policy that is open and honest", "Stock Market crash in New York", "New York Stadium", "norman Tebbit", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs", "great heroism or of the most conspicuous courage in circumstances of extreme danger", "Michael Douglas", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2086032314353815}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 0.0, 0.23529411764705885, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.07407407407407407, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "marioneth and Llantisilly Rail Traction Company Limited", "acetic acid", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit", "one of the membership who is no longer an exempt", "Bothtec", "Terry Reid", "information about climate change based on published sources", "sept Princesses", "North America", "Andr\u00e9 3000", "rookies", "Akhenaten", "President Theodore Roosevelt", "the fourth season", "four", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "marie fille de Perth", "Matt Winer", "1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2571056190541484}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.9019607843137255, 0.0, 1.0, 0.0, 0.2, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.3333333333333333, 0.0, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "baijan", "id", "between 27 July and 7 August 2022", "New York", "id", "2006", "the Uniting for Consensus", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "boggling", "baze", "death mask", "bollywood", "Overtime", "Sir Henry Cole", "has trouble distinguishing between carbon dioxide and oxygen", "bambi", "cement City, Texas", "The plan was overseen by Arlene Foster of the Democratic Unionist Party (DUP ) the then-minister for Enterprise, Trade and Investment", "23 July 1989", "many educational institutions especially within the US", "gurus often exercising a great deal of control over the lives of their disciples", "for control purposes", "bamboula", "Callability", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins", "bile duct", "berenice Abbott"], "metric_results": {"EM": 0.0625, "QA-F1": 0.11816964285714285}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0, 0.0, 0.15999999999999998, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-34657", "mrqa_naturalquestions-train-12702", "mrqa_naturalquestions-train-41354", "mrqa_naturalquestions-train-36318", "mrqa_naturalquestions-train-81133", "mrqa_naturalquestions-train-24436", "mrqa_naturalquestions-train-79631", "mrqa_naturalquestions-train-41280", "mrqa_naturalquestions-train-85639", "mrqa_naturalquestions-train-38723", "mrqa_naturalquestions-train-43212", "mrqa_naturalquestions-train-74972", "mrqa_naturalquestions-train-23972", "mrqa_naturalquestions-train-76378", "mrqa_naturalquestions-train-66288", "mrqa_naturalquestions-train-28570", "mrqa_hotpotqa-validation-3774", "mrqa_hotpotqa-validation-1201", "mrqa_naturalquestions-validation-3028", "mrqa_squad-validation-3113", "mrqa_squad-validation-3478", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-114", "mrqa_squad-validation-3478", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-3478", "mrqa_squad-validation-3113", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-3028", "mrqa_squad-validation-3478", "mrqa_squad-validation-3113"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Remagen", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "month", "Moses", "museum", "tetanus", "bounding the time or space", "museum", "Alex O'Loughlin", "Eddie Leonski", "Jack Ridley", "a mixture of phencyclidine and cocaine", "bunker", "in the middle decade of the 19th century", "the Reverse - Flash", "All Souls'Day", "1968", "Baku", "Catholics", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America", "to comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "rotor", "Splodgenessabounds"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28253968253968254}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["museum", "International Association of Athletics Federations", "a soft wool fabric with a colorful swirled pattern of curved shapes", "Paspahegh Indians", "atrium", "South Dakota", "2 : 44 p.m. EDT", "swanee or swannee whistle", "nopes.com", "to start fires, hunt, and bury their dead", "India", "the stomach", "placental", "Ready to Die", "a revolver", "imperial rule", "1840", "a defiant speech, or a speech explaining their actions", "Francis Marion Crawford, Robert Underwood Johnson, Stanford White, Fritz Lowenstein, George Scherff, and Kenneth Swezey", "kinks", "A January 2014 report by the World Institute for Development Economics Research at United Nations University", "entropy", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18586458476164358}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "2005 to 2008", "1998", "city-owned parks and parkways", "island in the Mediterranean Sea situated", "70-50's", "unaided school", "dolph Camilli", "times sign", "BAFTA Television Award", "Emily Blunt", "1960", "HTTP Secure ( HTTPS )", "late - September through early January", "aircraft", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "june", "birds", "butterflies", "new research and a better understanding of the complex issues regarding immune", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf tissue", "Indian club ATK", "land", "near Grande Comore, Comoros Islands", "rupees", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.0625, "QA-F1": 0.12317708333333333}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-55792", "mrqa_naturalquestions-train-49817", "mrqa_naturalquestions-train-33527", "mrqa_naturalquestions-train-78613", "mrqa_naturalquestions-train-42732", "mrqa_naturalquestions-train-71522", "mrqa_naturalquestions-train-9984", "mrqa_naturalquestions-train-62061", "mrqa_naturalquestions-train-29476", "mrqa_naturalquestions-train-329", "mrqa_naturalquestions-train-67245", "mrqa_naturalquestions-train-25341", "mrqa_naturalquestions-train-53608", "mrqa_naturalquestions-train-79455", "mrqa_naturalquestions-train-24244", "mrqa_naturalquestions-train-50667", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-4506", "mrqa_triviaqa-validation-2368", "mrqa_triviaqa-validation-1494", "mrqa_hotpotqa-validation-984", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1494", "mrqa_squad-validation-6399", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-254", "mrqa_naturalquestions-validation-1489", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-4506", "mrqa_squad-validation-3021", "mrqa_triviaqa-validation-254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "Only a few common complex biomolecules", "The U.S. Army Chaplain insignia", "Kairi", "both inner city blacks, who wanted more involvement in government, and whites in the suburbs", "boston Becks", "near the Black Sea", "the last book accepted into the Christian biblical canon", "Beyonc\u00e9", "buttermens", "gallantry", "16 million", "1950s", "work oxen", "1998", "a priest", "23.1", "2001", "family member", "long-term environmental changes", "William Powell Lear", "the tangential force", "Terrell Suggs", "decide on all the motions and amendments that have been moved", "a voyage of adventure", "Abraham Gottlob Werner", "june", "present-day Charleston", "quiescent", "Douglas Ross", "Panzer"], "metric_results": {"EM": 0.28125, "QA-F1": 0.37080853174603173}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.5714285714285715, 0.0, 0.25, 0.0, 0.0, 1.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_hotpotqa-validation-1142", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_hotpotqa-validation-2902"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computability theory", "nino", "50", "6.4 nanometers apart", "the eighth and eleventh episodes of the season", "Carl Edwards II", "over 400 games", "the adrenal glands", "artes liberales", "the Bowland Fells", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "St. Louis County, Missouri, United States", "1828", "2018", "the FA Cup", "law firm", "Pottawatomie County", "orangutan", "Newton's Law of Gravitation", "The Parish Church of St Andrew", "bromley- by-Bow", "Toronto", "foreigner", "110 miles (177 km )", "the Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "psychological and psychotherapeutic theories and associated techniques", "Quentin Coldwater", "acidic bogs"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3048487103174603}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.375, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.28571428571428575, 0.0, 0.0, 0.0, 0.6666666666666666, 0.2666666666666667, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_squad-validation-5313", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentinian", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "almond paste", "photosynthesis", "images of different animals and humans perform various actions", "george w. Bush", "The Daily Stormer", "triplet", "water", "president", "the citizens", "George, Margrave of Brandenburg-Ansbach", "Kamba", "3D computer-animated comedy film", "Worcester Cold Storage and Warehouse Co. fire", "acting", "Ross Macdonald", "liquid crystal on silicon ( L CoS ) ( based on an LCoS chip from Himax ), field - sequential color system, LED illuminated display", "Americans acting under orders", "iPod Classic", "My Sassy Girl", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "The Edge of Night", "non-combustible substances that corrode, such as iron, contained very little", "pedagogy", "vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 )", "the root respiration", "land - living organisms, both alive and dead, as well as carbon stored in soils", "death", "medium and heavy- Duty diesel trucks", "testes"], "metric_results": {"EM": 0.25, "QA-F1": 0.41843384691463437}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.1, 0.0, 0.0, 0.8888888888888888, 0.0, 1.0, 0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 0.918918918918919, 1.0, 0.0, 0.3333333333333333, 0.1290322580645161, 1.0, 0.0, 0.0, 1.0, 0.0, 0.23529411764705882, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-133", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-84073", "mrqa_naturalquestions-train-38246", "mrqa_naturalquestions-train-30843", "mrqa_naturalquestions-train-17638", "mrqa_naturalquestions-train-45681", "mrqa_naturalquestions-train-40287", "mrqa_naturalquestions-train-24775", "mrqa_naturalquestions-train-28314", "mrqa_naturalquestions-train-50512", "mrqa_naturalquestions-train-73757", "mrqa_naturalquestions-train-49999", "mrqa_naturalquestions-train-49499", "mrqa_naturalquestions-train-83461", "mrqa_naturalquestions-train-77698", "mrqa_naturalquestions-train-28570", "mrqa_naturalquestions-train-44847", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-7506", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-187", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7157", "mrqa_triviaqa-validation-7506", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-187", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7157", "mrqa_squad-validation-9281", "mrqa_hotpotqa-validation-582"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["Pope, Alexander (1688-1744) (DNB00) - Wikipedia, vi. 189, 201", "yellow fever", "three legal systems", "Las Vegas, Nevada", "a status line", "globetrotters", "cruiserweight", "the fictional town of Ramelle", "1987", "slow", "slow", "a strict and elaborate set of rules designed by Victoria, Duchess of Kent, along with her attendant, Sir John Conroy, concerning the upbringing of the Duchess's daughter, the future Queen Victoria", "7", "1947", "the MGM Grand Garden Special Events Center", "digital fashion gallery", "C. J. Anderson", "a single all-encompassing definition of the term", "1987", "60", "Eagle Ridge Mall", "Pel\u00e9", "reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "the classical element fire", "Gomer Pyle", "at least 18 or 21 years old ( or have a legal guardian present )", "Ward", "a Belgian\u2013French explorer, spiritualist, Buddhist, anarchist and writer", "The Jamestown settlement in the Colony of Virginia", "Monet began the paintings in January or early February 1892", "carbon related emissions"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3716968795093795}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [0.2222222222222222, 0.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.25, 0.2857142857142857, 0.19999999999999998, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["1967", "Traumnovelle", "a Gender pay gap in favor of males in the labor market", "The TEU", "ice melting", "his brother, who died in action in the United States Army", "alexander", "the length of their main span", "trams", "handguns", "King Crimson", "the Pechenegs, the Bulgars, and especially the Seljuk Turks", "died in battle", "Volkswagen Beetle", "alexander", "European Union", "Queen Elizabeth II", "infections, irritation, or allergies", "a average of 25,000 people ascend the tower every day", "the Vittorio Emanuele II Gallery and Piazza della Scala", "catfish aquaculture", "atomic number 53", "Andy Allo, Venzella Joy Williams, and Hannah Fairlight as Calamity, Serenity, Charity, and Veracity", "Iraq", "a co-op of grape growers", "victor willsmeron", "viuseppe Verdi", "1952", "Los Angeles Lakers", "the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the body hair transplantation ( BHT ) on appropriate candidates who have available donor hair on the chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.125, "QA-F1": 0.2303463595583161}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.08695652173913043, 0.22222222222222224, 0.09523809523809523]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["British rock group Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Joe Turano", "fencers", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "2014", "The stability, security, and predictability of British law and government", "Minos and Kokalos", "29 June 1941", "cienfuegos in the Las Villas province of Cuba", "byker grove", "New South Wales", "Fort Bull", "dandy", "ferric", "Orwell", "Bohemia", "Gregg Popovich", "not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive immune system", "under the tutelage of his uncle", "a musician", "nodel", "December 1, 1969", "american", "jK Rowling", "California State Automobile Association", "\"alone\"", "Cinderella", "The astronauts were asphyxiated before the hatch could be opened", "due to a lack of understanding of the legal ramifications"], "metric_results": {"EM": 0.125, "QA-F1": 0.24280111908788377}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.15384615384615385, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.2424242424242424, 0.4, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.8571428571428571]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-1303", "mrqa_naturalquestions-train-51261", "mrqa_naturalquestions-train-47760", "mrqa_naturalquestions-train-7073", "mrqa_naturalquestions-train-38345", "mrqa_naturalquestions-train-63523", "mrqa_naturalquestions-train-20004", "mrqa_naturalquestions-train-43353", "mrqa_naturalquestions-train-4859", "mrqa_naturalquestions-train-66302", "mrqa_naturalquestions-train-2830", "mrqa_naturalquestions-train-46158", "mrqa_naturalquestions-train-6992", "mrqa_naturalquestions-train-30908", "mrqa_naturalquestions-train-27075", "mrqa_naturalquestions-train-69259", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-441", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-441", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-7300"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["The Omega Man", "former president of Guggenheim Partners", "Jason Lee", "Napoleon", "maryland", "3.7", "High and persistent unemployment", "matt Willis and Charlie Quirke", "Little Golden Lion award and the Ecumenical Award", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "9", "Michael Hordern", "amyotrophic lateral sclerosis", "a gimmick called \"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "Swiss watchmaking industry", "1976", "Torah", "the western coast of Italy", "first and only U.S. born world grand prix champion", "the quintessential New Orleans art form -- a jazz funeral without a body", "mid November", "Facebook", "ring", "The song was covered by the fictional band Steel Dragon in the 2001 film \"Rock Star\"", "Seattle", "North America theater", "he cheated on Miley", "punk rock", "Fort Snelling, Minnesota", "sport", "infrequent rain"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2595283189033189}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.33333333333333337, 0.3636363636363636, 0.0, 1.0, 0.0, 0.1111111111111111, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_squad-validation-10168", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Chicago's first permanent non-native settler", "River Phoenix", "FX option", "electromagnetic waves", "Islamic State of Iraq and the Levant", "sport", "Dimensions in Time", "Surveyor 3 unmanned lunar probe", "January 1981", "luteinizing hormone", "the structure and substance of his questions and answers concerning baptism in the Small Catechism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "john Robertson", "releasing the compressed air trapped in the cylinders", "Cheyenne rivers", "appearance of fossils in sedimentary rocks", "Hanna- Barbera", "Cortina d'Ampezzo", "efficient and effective management of money ( funds )", "Alba Longa", "anthropomorphic personification of death", "Australian islands", "Timo Hildebrand", "public sector ( also called the state sector )", "2 February 1940", "lack of access to education", "The name Moloch results from a dysphemic vocalisation in the Second Temple period of a theonym based on the root mlk `` king ''", "cornea (the transparent layer at the front of the eye)", "Uncle Fester", "James MacArthur"], "metric_results": {"EM": 0.0625, "QA-F1": 0.16838023088023085}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.1111111111111111, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-validation-727", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Part 2", "the Flatbush section of Brooklyn, New York City", "Wichita and the state of Kansas", "Proof", "brianius ptolemy", "a friend and publicist", "taszl\u00f3 de Alm\u00e1sy", "masons'marks", "Theodore Haynes (1988) and Julia Rose (1989)", "Gateshead", "The horn line at the end is performed by the Phenix Horns from Earth, Wind & Fire", "The neck", "1898", "professional wrestler", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's superweapon", "Vito Corleone", "bplane", "tibility for impressions, and an inclination to be touched by emotions", "gorillas", "March 15, 1945", "absolute temperature", "The organization has a user base of over 1,800,000", "Sam Waterston", "bccal cusp", "his brother, Menelaus", "3 December", "tallahassee", "prefabricated housing projects", "London", "WOTV"], "metric_results": {"EM": 0.125, "QA-F1": 0.12797619047619047}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-30698", "mrqa_naturalquestions-train-9483", "mrqa_naturalquestions-train-54715", "mrqa_naturalquestions-train-85820", "mrqa_naturalquestions-train-43950", "mrqa_naturalquestions-train-82855", "mrqa_naturalquestions-train-28089", "mrqa_naturalquestions-train-58901", "mrqa_naturalquestions-train-10949", "mrqa_naturalquestions-train-14400", "mrqa_naturalquestions-train-70387", "mrqa_naturalquestions-train-35693", "mrqa_naturalquestions-train-20653", "mrqa_naturalquestions-train-61869", "mrqa_naturalquestions-train-24912", "mrqa_naturalquestions-train-75219", "mrqa_naturalquestions-validation-5944", "mrqa_triviaqa-validation-46", "mrqa_squad-validation-2448", "mrqa_triviaqa-validation-6385", "mrqa_squad-validation-5178", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-8180", "mrqa_hotpotqa-validation-3877", "mrqa_squad-validation-5178", "mrqa_naturalquestions-validation-150", "mrqa_squad-validation-7836", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-2448"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["galileo", "cheeses", "galileo", "the back of the head of the tibia, below the level of the knee joint", "ferguside", "the North Sea", "the kgalagadi Trans-frontier Park", "faldo", "October 29, 1985", "Amway", "the Mauritius Examinations Syndicate", "Thomas Sowell", "the Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "the Royal Albert Hall", "tANU", "Cameroon", "florida", "an open work crown", "using a baby as bait, allowing a child to go through a torturous treatment to gain information, and allowing Dean to become a vampire", "Fulham", "French, English and Spanish", "florida", "U.S. Marshals", "What's Up (TV series)", "supply chain management", "galileo", "Stanislaw August Poniatowski", "polynomial algebra", "Snowbell", "The three wise monkeys ( Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru, alternatively \u4e09 \u5339 \u306e \u733f sanbiki no saru", "sheepskin and Merino Wool products", "Honolulu"], "metric_results": {"EM": 0.0625, "QA-F1": 0.11383928571428571}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8095238095238095, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-2287"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["milk soft cheese", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845 - October 6, 1931", "gamma ray emission ( energy of 514 keV )", "James Zeebo", "sovereign states", "vice president of the United States", "intelligent design movement", "Bumblebee", "Australian", "six additional months for men and 24 months for women", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Spencer", "\"antiforms\"", "June 9, 2015", "V. Prakash Kumar", "Grace Nail Johnson", "Keith Richards -- guitar solo, bass guitar, backing vocals", "prime number p with n < p < 2n \u2212 2", "Bangor International Airport", "teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "180th", "Cartoon Network", "the Presiding Officer on the advice of the parliamentary bureau", "the Miami Heat of the National Basketball Association (NBA)", "33", "vitifolia", "Annual Conference Cabinet", "field hockey player Hannah Macleod", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2765290831244779}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.08333333333333334, 0.0, 0.0, 0.0, 0.0, 0.8421052631578948, 0.19999999999999998, 0.16, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.2857142857142857, 0.25, 0.08333333333333333, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_hotpotqa-validation-5628", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwanda genocide, also known as the genocide against the Tutsi", "Co-teaching is defined as two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "500 metres", "Diondre Cole", "the entertainment division", "A to a point B", "12", "the Great Exhibition of 1851", "King Edward I to Henry VIII", "the Chagos Archipelago", "dundee", "the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "larnley", "\"Grindhouse\" fake trailer", "davenport", "digital transmission", "the Swiss- Austrian border", "lithium-ion battery", "821", "HD channels and Video On Demand content which was not previously carried by cable", "liquid", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposition", "the \"Queen of Cool\"", "President Woodrow Wilson", "davis", "the fifth season", "davis dors", "Hockey Club Davos", "Michael Crawford", "Qutab Ud - Din - Aibak, founder of the Delhi Sultanate"], "metric_results": {"EM": 0.125, "QA-F1": 0.24320762844611526}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2, 0.10526315789473684, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.125, 0.0, 0.0, 0.6666666666666665, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.6666666666666666]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-7842", "mrqa_naturalquestions-train-55", "mrqa_naturalquestions-train-40051", "mrqa_naturalquestions-train-27903", "mrqa_naturalquestions-train-22076", "mrqa_naturalquestions-train-69799", "mrqa_naturalquestions-train-79598", "mrqa_naturalquestions-train-26434", "mrqa_naturalquestions-train-68487", "mrqa_naturalquestions-train-59987", "mrqa_naturalquestions-train-77073", "mrqa_naturalquestions-train-732", "mrqa_naturalquestions-train-868", "mrqa_naturalquestions-train-37245", "mrqa_naturalquestions-train-56078", "mrqa_naturalquestions-train-498", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1298", "mrqa_squad-validation-1904", "mrqa_squad-validation-2053", "mrqa_squad-validation-1904", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-4164", "mrqa_naturalquestions-validation-3483", "mrqa_triviaqa-validation-1298", "mrqa_squad-validation-1904", "mrqa_triviaqa-validation-1298", "mrqa_naturalquestions-validation-9171", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-4164"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "aragon", "11.1", "trans-Pacific flight", "Sharman Joshi", "students normally have to sit in a classroom and do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "prime", "Ana", "Cherry Hill", "Happy Hour", "amhmael", "Jeanne Labrosse", "comedy - drama", "blackstar", "Indian", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "In 1889", "Nicki Minaj", "slave of duty", "Huguenot", "fagioli", "friedrich", "Teen Titans Go", "William the Conqueror", "Ben Gurion International Airport", "two degrees of freedom", "the Corinthian and Saronic Gulfs", "phlebotomists", "Guinness World Records", "Southern Progress Corporation"], "metric_results": {"EM": 0.21875, "QA-F1": 0.29386712519936203}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.10526315789473684, 1.0, 0.25, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating", "Robert Smigel", "the Sackler Centre for arts education", "kathy Najimy", "kaleidoscope", "British Columbia", "AS-205", "ribosomal", "kookaburra", "six-time", "Scott Bakula as Dwayne `` King '' Cassius Pride, NCIS Supervisory Special Agent", "\"I Swear\"", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak ( Bulgarian : \u043a\u043e\u0437\u0443\u043d\u0430\u043a )", "Belfast West", "heliocentric", "Sulla", "Super Bowl LII", "Golden Globe", "Kenyan English", "trust God's word", "snow", "Pantone Matching System (PMS)", "Firoz Shah Tughlaq", "\" My Love from the Star\"", "San Jose", "octopus", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "Thunderbird of Native American tradition", "giving Super Bowl", "29.7", "houlihan"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3852767602767603}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.14814814814814814, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_hotpotqa-validation-3547", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_squad-validation-8464", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-935"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier - Rebecca First Edition", "various registries", "James Blundell", "Yazoo", "April 1894", "black holes", "dreams", "cede", "Willie Nelson and Kris Kristofferson", "ill. (some col.)", "private", "a French pirate", "Lewis", "Charles Dickens and Beatrix Potter", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "carbohydrates", "2001", "it must be infinitely many primes", "news bulletin", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "Huldra", "western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Orthodox Christians", "James Bond", "a 4 in ( 10 cm ) LCD multi-touch Retina display and a screen resolution of 640 \u00d7 1136 at 326 ppi", "fillies", "the Western Atlantic ctenophore Mnemiopsis leidyi", "\"Menace II Society\"", "backup", "Larry Wayne Gatlin & the Gatlin Brothers"], "metric_results": {"EM": 0.1875, "QA-F1": 0.29353802424222514}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.4210526315789474, 0.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.41379310344827586, 0.0, 0.0, 0.19999999999999998, 0.0, 0.7692307692307693, 1.0, 0.19999999999999998, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-51436", "mrqa_naturalquestions-train-36412", "mrqa_naturalquestions-train-24775", "mrqa_naturalquestions-train-37534", "mrqa_naturalquestions-train-64528", "mrqa_naturalquestions-train-26160", "mrqa_naturalquestions-train-28917", "mrqa_naturalquestions-train-78547", "mrqa_naturalquestions-train-3085", "mrqa_naturalquestions-train-11001", "mrqa_naturalquestions-train-27941", "mrqa_naturalquestions-train-68227", "mrqa_naturalquestions-train-48023", "mrqa_naturalquestions-train-47540", "mrqa_naturalquestions-train-51543", "mrqa_naturalquestions-train-40080", "mrqa_naturalquestions-validation-4951", "mrqa_squad-validation-3151", "mrqa_squad-validation-3151", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-4951", "mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-8464", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-3552", "mrqa_triviaqa-validation-6721", "mrqa_squad-validation-8464", "mrqa_hotpotqa-validation-3623"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "Mediterranean Shipping Company S.A. ( MSC )", "1934", "a European fairy tale", "Liberals", "Parliamentarians", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "kill to spies", "Captain Nemo", "Augustus Waters", "1619", "alison", "\"W welfare Cash Card\" in the style of the Supplemental Nutrition Assistance Program", "1973", "Kenya", "a chronological collection of critical quotations about William Shakespeare and his works", "alred", "neutrality", "Cargill", "AMC Entertainment Holdings, Inc.", "\"The Gang\"", "1990", "March 1, 2018", "weak force", "Blandings", "Martin Luther King III", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27061710858585863}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.7499999999999999, 0.0, 0.0, 0.0, 0.33333333333333337, 0.1111111111111111, 0.16, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.33333333333333337, 0.0, 0.3333333333333333, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "javier (Luna)", "red", "Luna Park", "Best Animated Feature", "European Union institutions", "American astronaut who formerly held the American record for the most time in space (381.6 days)", "nine", "NASA's CAL IPSO satellite", "celandine", "John F. Kennedy", "Ronnie Schell", "p. falciparum", "Tata Consultancy Services Limited (TCS) is an Indian multinational information technology (IT) service, consulting and business solutions company Headquartered in Mumbai, Maharashtra", "the east", "1939", "2017 / 18 Divisional Round game against the New Orleans Saints", "possibly 1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "southern Hemisphere", "dambala", "Incudomalleolar joint", "bobby riggs", "Democritus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Corey Brown"], "metric_results": {"EM": 0.125, "QA-F1": 0.3022921522921523}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.5, 0.6666666666666666, 0.25, 0.25, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.0909090909090909, 0.5, 0.0, 0.3636363636363636, 0.6666666666666666, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "photosystem II photolyzes water to obtain and energize new electrons, photosystem I simply reenergizes depleted electrons at the end of an electron transport chain", "Pitt", "alchemy", "WBO lightweight title", "moluccas", "Kentucky Derby and Belmont Stakes", "Cordelia", "began multilateral negotiations", "1990", "J.R. R. Tolkien", "John Elway", "Selena Gomez", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "letter series", "Fa Ze Rug", "nine", "Mongols", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "Friars Minor", "CD Castell\u00f3n", "1946 -- 48", "12\u20134", "by having colloblasts, which are sticky and adhere to prey", "Patrick Wachsberger and Erik Feig of Summit Entertainment produced with Adam Shankman and Jennifer Gibgot of Offspring Entertainment", "Mission Specialist for mission STS-51-L.", "it will retreat to its den and winter will persist for six more weeks", "mitterrand"], "metric_results": {"EM": 0.125, "QA-F1": 0.3084004173099918}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.5714285714285715, 1.0, 0.0, 0.6486486486486487, 0.0, 0.0, 0.5, 0.7692307692307693, 0.0, 0.4, 0.0, 0.0, 0.2222222222222222, 0.0, 0.4, 0.0, 1.0, 0.4615384615384615, 0.0, 0.33333333333333337, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-58633", "mrqa_naturalquestions-train-33769", "mrqa_naturalquestions-train-18550", "mrqa_naturalquestions-train-11686", "mrqa_naturalquestions-train-63150", "mrqa_naturalquestions-train-74365", "mrqa_naturalquestions-train-31635", "mrqa_naturalquestions-train-81986", "mrqa_naturalquestions-train-28936", "mrqa_naturalquestions-train-31904", "mrqa_naturalquestions-train-66866", "mrqa_naturalquestions-train-22967", "mrqa_naturalquestions-train-83845", "mrqa_naturalquestions-train-4282", "mrqa_naturalquestions-train-85943", "mrqa_naturalquestions-train-7794", "mrqa_naturalquestions-validation-4048", "mrqa_triviaqa-validation-6107", "mrqa_hotpotqa-validation-26", "mrqa_squad-validation-9568", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-4951", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2420", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-4048", "mrqa_hotpotqa-validation-1929", "mrqa_hotpotqa-validation-26", "mrqa_triviaqa-validation-3933", "mrqa_triviaqa-validation-3595", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3052"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["alpine skiing, cross-country skiing, ski jumping, nordic combined, snowboarding and freestyle skiing", "amhmael", "over 50 million singles", "states'rights to expand slavery", "between 1923 and 1925", "Winter Park is a suburban city in Orange County, Florida, United States.", "January 19, 1962", "Frigate", "france", "The party with the highest quotient is awarded the seat, which is then added to its constituency seats in allocating the second seat", "yellowish red bills and legs", "the move from the manufacturing sector to the service sector", "Moreton Bay", "Peter Davison, Colin Baker and Sylvester McCoy", "August 14, 1848", "lower rates of health and social problems", "juveniles are capable of reproduction before reaching the adult size and shape", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "co-written the book of the musical \"A Chorus Line\"", "2,664", "Heilsenteger", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning services, support services, property services, catering services, security services and facility management services", "Symphony No. 7 in F major, Opus 24", "dordogne", "1603", "ranked above the two personal physicians of the Emperor", "sugar Plum Fairy", "we admitted we were powerless over alcohol -- that our lives had become unmanageable.   Came to believe that a Power greater than ourselves could restore us to sanity.", "Wrigley Field"], "metric_results": {"EM": 0.15625, "QA-F1": 0.35891046453546455}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.25, 0.7200000000000001, 0.4615384615384615, 0.923076923076923, 0.2857142857142857, 1.0, 0.0, 0.2857142857142857, 0.3636363636363636, 0.4444444444444445, 0.5454545454545454, 0.0, 1.0, 0.22222222222222224, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-499", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences, with the Romans serving as external arbiters on disputes concerning Jewish customs and law", "south of the Kancamagus Highway", "Magic formula investing", "because he gave its name to the prize, does not deal with such matters in its day-to-day business life", "Waialua District of the island of O\u02bb ahu, City and County of Honolulu", "1910\u20131940", "non-teaching posts", "Catch Me Who Can", "jazz saxophonist", "tennis", "4,000", "the founder of the Yuan dynasty", "Heathcliff", "canal", "spice", "He was voiced by Phil Hartman and first appeared in the second season episode \"Homer vs. Lisa and the 8th Commandment\"", "Joseph Rowntree", "San Bernardino", "The city has an extensive neoclassical centre referred to as Tyneside Classical largely developed in the 1830s by Richard Grainger and John Dobson, and recently extensively restored.", "Albany High School for Educating People of Color", "Lalbagh Fort at Dhaka", "Sergeant First Class", "Anakin Skywalker", "seek jury nullification", "Cee - Lo", "Anglican", "mammy two Shoes", "duke of York", "magnetism", "directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based entrepreneurship", "1757"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30474633618015967}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false], "QA-F1": [0.2222222222222222, 0.4, 0.0, 0.0, 0.5882352941176471, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.5454545454545454, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9523809523809523, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_naturalquestions-validation-7801", "mrqa_hotpotqa-validation-945"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["alsivar", "geldof", "blackberry and dewberry", "st Orion", "\" Big Mamie\"", "trinidad and Tobago", "the \"eternal outsider, the sardonic drifter\" someone who rebels against the social structure", "a light sky-blue color caused by absorption in the red", "the lords would take back these privileges and they were prepared to fight for them", "1963", "he was born as Zaza Pachulia", "inner mitochondria membrane", "aline charigot", "Grand Annual Steeplechase at Warrnambool and the Australian International Airshow at Geelong", "The channel which can get carriage on a suitable beam of a satellite", "the fourth season", "a more fundamental electroweak interaction", "availability of skilled tradespeople", "diamond", "A simple iron boar crest adorns the top of this helmet associating it with the Benty Grange helmet and the Guilden Morden boar from the same period", "the University of Northumbria at Newcastle in which polytechnics became new universities", "japan", "Lofton", "award a touchback on kickoffs at the 25 - yard line", "the Latin centum, which means 100, and gradus", "about 7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "by faith", "poker", "can be produced with constant technology and resources per unit of time, such that more of one good could be produced only by diverting resources from the other good, resulting in less production of it", "antwerp", "company"], "metric_results": {"EM": 0.09375, "QA-F1": 0.27530802530802534}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.3076923076923077, 0.5333333333333333, 0.0, 0.0, 0.6666666666666667, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.45454545454545453, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-6276", "mrqa_naturalquestions-train-13920", "mrqa_naturalquestions-train-39116", "mrqa_naturalquestions-train-53469", "mrqa_naturalquestions-train-66389", "mrqa_naturalquestions-train-34065", "mrqa_naturalquestions-train-7961", "mrqa_naturalquestions-train-16609", "mrqa_naturalquestions-train-81715", "mrqa_naturalquestions-train-59814", "mrqa_naturalquestions-train-9049", "mrqa_naturalquestions-train-23985", "mrqa_naturalquestions-train-41523", "mrqa_naturalquestions-train-43315", "mrqa_naturalquestions-train-76378", "mrqa_naturalquestions-train-88118", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3384", "mrqa_hotpotqa-validation-1426", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-2428", "mrqa_triviaqa-validation-6941", "mrqa_hotpotqa-validation-1426", "mrqa_triviaqa-validation-3393", "mrqa_hotpotqa-validation-1426", "mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-3393", "mrqa_hotpotqa-validation-1426", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-3393"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["The Turk", "Chris Weidman", "jury nullification", "Harishchandra", "poet", "Professor Eobard Thawne", "slivovitz", "a US$10 a week raise", "October", "member states", "because of all the instruments", "McKinsey's offices in Silicon Valley and India and created its Internet practice", "gyphidiophobia", "living Doll", "Crohn's disease", "Francisco de Orellana", "Raya Yarbrough", "Arizona", "michael dokes", "John D. Rockefeller", "the Old Testament", "UPS", "local talent", "Football League", "the Doctor and Tegan", "mafic", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "John Surratt", "1340", "dodo bird", "by focusing on positive or negative thoughts people can bring negative or negative experiences into their life", "Stan Butler"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28660714285714284}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.2857142857142857, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.1, 0.0, 0.0, 1.0, 0.8888888888888888, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_squad-validation-4309", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["public speaking", "886 AD", "to finance his own projects with varying degrees of success", "Le Mans", "Kinect", "Tokyo", "defensive end Kony Ealy", "the parallelogram rule of vector addition", "abraham lincoln", "blackbirds", "spontaneous fission", "Van Gogh", "the bore, and often the stroke", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "Doctorin' the Tardis", "National Basketball Development League (NBDL)", "port of Portsmouth", "St. Mary's County", "Ted Ginn Jr.", "2,615", "Pyeongchang", "athlete", "a password recovery tool for Microsoft Windows", "Captain John Guidry", "historical contributions to the development of modern architecture and furniture", "Brazil", "abraham lincoln", "the smallest subfield of a field F containing both 0 and 1", "heartburn", "53", "photosynthesis"], "metric_results": {"EM": 0.125, "QA-F1": 0.22661356209150327}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.47058823529411764, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.058823529411764705, 0.0, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_squad-validation-8873"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "an outgoing, eccentic, big - hearted, loving, sweet, and thoughtful elephant and teacher", "abolitionist", "abraham city of Torquay", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "ABC1", "mikael blomkvist", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "Ecclestone", "Dr. Ian Malcolm", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "A computer program", "North Western and Eastern rural regional areas", "king abraham", "hvannadalshnukur", "representing Kenya's largest source of foreign direct investment, and... bilateral trade", "koreans", "off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Evan Jonigkeit", "Meredith Brody ( Zoe McLellan ), a transfer from the NCIS Great Lakes field office, who has worked as a Special Agent Afloat and is keen to leave her past behind as she moves to New Orleans", "Jawaharlal Nehru", "National Lottery", "Apollo", "abraham", "to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.0625, "QA-F1": 0.13880115493166964}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7777777777777778, 0.0, 0.0, 0.4, 0.12121212121212122, 0.0, 0.11764705882352941, 0.5, 0.0, 0.0, 0.0, 0.4000000000000001]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-34298", "mrqa_naturalquestions-train-60888", "mrqa_naturalquestions-train-1068", "mrqa_naturalquestions-train-24406", "mrqa_naturalquestions-train-6255", "mrqa_naturalquestions-train-73370", "mrqa_naturalquestions-train-8803", "mrqa_naturalquestions-train-52372", "mrqa_naturalquestions-train-84321", "mrqa_naturalquestions-train-20131", "mrqa_naturalquestions-train-3307", "mrqa_naturalquestions-train-5503", "mrqa_naturalquestions-train-13982", "mrqa_naturalquestions-train-84809", "mrqa_naturalquestions-train-61728", "mrqa_naturalquestions-train-5668", "mrqa_naturalquestions-validation-9765", "mrqa_squad-validation-1249", "mrqa_squad-validation-1249", "mrqa_squad-validation-1249", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-1426", "mrqa_naturalquestions-validation-9765", "mrqa_squad-validation-814", "mrqa_naturalquestions-validation-9765", "mrqa_squad-validation-2313", "mrqa_squad-validation-814", "mrqa_squad-validation-1249", "mrqa_triviaqa-validation-3603", "mrqa_naturalquestions-validation-7567", "mrqa_squad-validation-8075", "mrqa_squad-validation-814"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Thiel", "skaters", "football", "Newell Highway", "androids", "14 and one-half hands tall", "a suburb of the Twin Cities", "Some civil disobedience defendants choose to make a defiant speech, or a speech explaining their actions", "DreamWorks Animation", "Liszt Strauss Wagner Dvorak", "his own men", "emissions resulting from human activities", "arthur", "the RAF", "encourage growth", "Ibbi-Sipish", "Hayley Sanderson", "Polish-Jewish", "one of the main leaders ( Spanish : Caudillo ) of the 1936 coup, General Francisco Franco", "polly", "jockeys", "390 billion", "Washington Street", "1978", "six", "lohan", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "spain", "Paul the Apostle, later cited by John Smith in Jamestown, Virginia, and by Lenin during the Russian Revolution", "lusitania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21977588383838387}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.45000000000000007, 0.0, 0.2222222222222222, 1.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_hotpotqa-validation-1444", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_squad-validation-3106"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A high-gain S-band antenna", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "Oktoberfest", "800 m", "Matt Jones", "kOH", "In extreme circumstances, a driver may attempt to jackknife the vehicle deliberately in order to halt it following brake failure", "CD4+ and CD8+", "high school teachers had the lowest median salary earning $39,259", "non-GMO", "Heading Out to the Highway", "Moonraker Reboot", "$12.99", "Michael Oppenheimer", "England national team", "rich and well socially standing Chinese while there were less rich Mongol and Semu who lived in poverty and were ill treated", "Jumping on the Moon", "Convention", "5,922", "December 5, 1991", "\"A Cure for Wellness\"", "Philadelphia 76ers", "Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "Joe Frazier", "23 March 1991", "Sunday", "Dallas", "Nairobi", "last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28848299064753335}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.08695652173913045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.4827586206896552, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13793103448275862, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "\"Boston Herald\" Rumor Clinic", "1967", "\"Footprints in the Sand\"", "the twelfth most populous city in the United States", "115", "september", "the result is an aberrant, ligand - independent, non- regulated growth stimulus to the cancer cells", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "Bass", "Tzeitel", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "east of the Sea of Japan", "seaman", "Yunnan- Fu", "London, United Kingdom", "Broken Hill and Sydney", "2005", "all punishments and granted them salvation", "\"The Doctor's Daughter\"", "september", "colebert", "bridge collapses or explosions", "1879", "She was the daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia.", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm and energy", "north-east China", "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle", "Bill Clinton", "Hordaland, Rogaland and Aust-Agder"], "metric_results": {"EM": 0.125, "QA-F1": 0.23287176639150325}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6153846153846153, 0.08, 0.0, 0.0, 0.12121212121212123, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.10526315789473684, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-75561", "mrqa_naturalquestions-train-14630", "mrqa_naturalquestions-train-80799", "mrqa_naturalquestions-train-85279", "mrqa_naturalquestions-train-4696", "mrqa_naturalquestions-train-58415", "mrqa_naturalquestions-train-47492", "mrqa_naturalquestions-train-4711", "mrqa_naturalquestions-train-58388", "mrqa_naturalquestions-train-81872", "mrqa_naturalquestions-train-57908", "mrqa_naturalquestions-train-76851", "mrqa_naturalquestions-train-44444", "mrqa_naturalquestions-train-17295", "mrqa_naturalquestions-train-366", "mrqa_naturalquestions-train-4151", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-946", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5683", "mrqa_naturalquestions-validation-5631", "mrqa_triviaqa-validation-1079", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-4500", "mrqa_hotpotqa-validation-5683", "mrqa_triviaqa-validation-2245", "mrqa_hotpotqa-validation-26"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Speaker", "wartime", "expressing defiance toward the government and unwillingness to stand for its policies", "Veronica", "Melbourne Conservatorium of Music", "Britain", "onions", "0.2 inhabitants per square kilometre", "Pink Panther", "France", "Ian Paisley", "Bataan Death March", "euro", "wings of a Dove", "United States", "1973", "1886", "St. Louis Rams", "Manhattan", "juba", "pole", "Johnny Darrell", "artery disease", "vegetable fat", "Euler's totient function", "ear canal", "the set of all connected graphs", "Busiest airports in the United States by international passenger traffic", "red", "Subaru DL", "Kurt Vonnegut", "september"], "metric_results": {"EM": 0.125, "QA-F1": 0.18772893772893773}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.14285714285714288, 0.6153846153846153, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.5, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "supply and demand", "Ewan McGregor", "brain, muscles, and liver", "urecchiette", "Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Ravens", "in the courtyard adjoining the Assembly Hall", "William Howard Ashton", "wyo history", "high and persistent unemployment", "Broward County", "Song Kang-ho, Lee Byung-hun, and Jung Woo-sung", "changing display or audio settings quickly", "Battle of Marston Moor", "from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "GDP growth actually declines over the medium term", "Beauty and the Beast", "South Africa", "The Young and the Restless", "alamo", "a seal illegally is broken", "United Methodist Church", "Brian Liesegang", "Roger Allers and Rob Minkoff", "Papua New Guinea", "witch Doctor", "National Association for the Advancement of Colored People", "1963\u20131989", "september", "John Prescott", "fall in love and get married", "septadhyayi"], "metric_results": {"EM": 0.125, "QA-F1": 0.28387399134681746}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.5, 0.28571428571428575, 0.0, 0.4, 0.0, 0.34782608695652173, 0.888888888888889, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 0.4615384615384615, 0.25, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Amber Laura Heard", "Uranus", "president Rudolph Ford, Jr.", "Cobham\u2013Edmonds thesis", "roles of the companion is to remind the Doctor of his \"moral duty\"", "II", "March 2012", "margaret conaway", "Muhammad Ali", "Coldplay", "Britain", "to civil disobedients", "Julius Caesar", "2", "March", "Easter egg", "formal language", "margaret lecouvreur", "heart", "That the plague was caused by bad air", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges", "6:7", "other states", "common nettle (stinging) Urtica dioica L", "$12", "a flat rate of 20 %", "7th", "to build a nationwide network in the UK", "roughly west", "Sudan"], "metric_results": {"EM": 0.3125, "QA-F1": 0.42589285714285713}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4, 1.0, 0.7777777777777778, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.4, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-4069", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-24336", "mrqa_naturalquestions-train-33296", "mrqa_naturalquestions-train-16951", "mrqa_naturalquestions-train-58984", "mrqa_naturalquestions-train-5645", "mrqa_naturalquestions-train-5350", "mrqa_naturalquestions-train-79749", "mrqa_naturalquestions-train-20754", "mrqa_naturalquestions-train-7386", "mrqa_naturalquestions-train-61083", "mrqa_naturalquestions-train-36319", "mrqa_naturalquestions-train-82453", "mrqa_naturalquestions-train-46591", "mrqa_naturalquestions-train-15245", "mrqa_naturalquestions-train-27554", "mrqa_naturalquestions-train-87775", "mrqa_squad-validation-7352", "mrqa_squad-validation-7352", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-4447", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-7495", "mrqa_triviaqa-validation-4829", "mrqa_squad-validation-2751", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-7704", "mrqa_squad-validation-2751"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["San Joaquin Valley Railroad", "three of his ribs were broken", "7 December 2000", "Post Alley under Pike Place Market", "mother-of-pearl", "February 20, 1978", "haggis", "Ronald Reagan", "96", "the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "india", "white, blue, pink, rainbow neon and glittering dotted lines", "Jericho in the Levant region", "around 11 miles (18 km) south of San Jose", "Spotty Dog", "Rumplestiltskin", "Nicolas Anelka", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "events and festivals", "riper", "1991", "india", "7 January 1936", "ten years", "twenty- three", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA", "Many of the city's tax base dissipated, leading to problems with funding education, sanitation, and traffic control within the city limits. In addition, residents in unincorporated suburbs had difficulty obtaining municipal services", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Jan Dahl Tomasson, Abel Xavier, Gheorghe Popescu, Florin R\u0103ducioiu, Maniche, Marko Marin, Eduardo Vargas and Obafemi Martins", "mistreatment from government officials", "distinctive Yogurt for Health", "Boston, Massachusetts"], "metric_results": {"EM": 0.25, "QA-F1": 0.3101489554154995}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8750000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2777777777777778, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1625", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-validation-969", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_triviaqa-validation-2524"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "New England", "Etienne de Mestre", "dragon", "The primary catalyst for secession was slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories.", "military units from their parent countries of Great Britain and France", "a children's story published by John Newbery in London in 1765", "243 days", "gathering money from the public", "Eden and Thorgan", "commissioned", "Jeff Meldrum", "741", "david painting", "French and English", "The Paris Sisters", "suez Canal", "34", "journalist", "the fact that there is no revising chamber", "pillbox hat", "the points of algebro-geometric objects", "most of the items in the collection, unless those were newly accessioned into the collection, probably don't show up in the computer system", "free floating and depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "strychnine", "Texas", "the early 16th century", "Lord's", "Eddy Shah", "charles williams", "in sequence with each heartbeat"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31760046477324344}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 0.07692307692307693, 0.13333333333333333, 0.0, 0.4, 0.47058823529411764, 0.28571428571428575, 0.6666666666666666, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 0.0, 0.0, 0.34782608695652173, 0.06451612903225806, 1.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Republic of Taiwan", "Dan Conner", "checkpoint Charlie", "president k Kennedy assassination", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "keskes", "1977", "Carl Sagan", "New York City", "may be quite simple but now that I can't mind that I put down in words", "2000", "antlers are dropped or shed and grown anew each and every year. They grow from pedicels", "Fiat S.p.A.", "the second Sunday of March, and standard time restarts on the first Sunday in November", "the relative units of force and mass", "woman", "two", "August 10, 1933", "The Golden Gate Bridge", "Sochi, Russia", "those who already hold wealth", "B. Traven", "Finding Nemo", "the subject of unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "no immediate relationship between money supply and inflation", "wooded areas", "264,152", "Princeton, New Jersey", "the German Empire", "air separation technology involves forcing air to dissolve through ceramic membranes based on zirconium dioxide by either high pressure or an electric current, to produce nearly pure O2 gas"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31792736846916414}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.9230769230769231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.47058823529411764, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.10526315789473684, 0.2222222222222222, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_squad-validation-8070", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_naturalquestions-validation-6839", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-62598", "mrqa_naturalquestions-train-30188", "mrqa_naturalquestions-train-29188", "mrqa_naturalquestions-train-58760", "mrqa_naturalquestions-train-41199", "mrqa_naturalquestions-train-80891", "mrqa_naturalquestions-train-59331", "mrqa_naturalquestions-train-15508", "mrqa_naturalquestions-train-63382", "mrqa_naturalquestions-train-64488", "mrqa_naturalquestions-train-46511", "mrqa_naturalquestions-train-17331", "mrqa_naturalquestions-train-38961", "mrqa_naturalquestions-train-4523", "mrqa_naturalquestions-train-78030", "mrqa_naturalquestions-train-40362", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-7630", "mrqa_squad-validation-4108", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-10406", "mrqa_hotpotqa-validation-1884", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2100", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-1015", "mrqa_squad-validation-4108", "mrqa_squad-validation-9032"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "on the road back to Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "warren palace", "every year between 1346 and 1671", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "his main interest was centered on the prophecy of the Little Horn in Daniel 8:9\u201312, 23\u201325", "Aristotle", "Sir Cedric Hardwicke as Sethi, Nina Foch as Bithiah, Martha Scott as Yoshebel, Judith Anderson as Memnet, and Vincent Price as Baka, among others", "cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes) peak in order to elicit an effective response against any intruding pathogens", "garb and the tartan", "Kevin Kolb", "The role of medium of exchange is one of the uses of money", "The ability to make such orders is also based on express or implied Acts of Congress that delegate to the President some degree of discretionary power ( delegated legislation )", "son et lumi\u00e8re", "when the Mongols placed the Uighurs of the Kingdom of Qocho over the Koreans at the court the Korean King objected", "Sochi, Russia", "right", "Hudson Bay", "After the insistence of NASA Administrator Webb, North American removed Harrison Storms as Command Module program manager", "south australia", "smartpen", "160", "the Secret Intelligence Service", "100 billion", "kai su, teknon", "photosynthesis", "4.7 / 5.5 - inch", "Queen City", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"], "metric_results": {"EM": 0.21875, "QA-F1": 0.31337003011488224}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.375, 0.10526315789473682, 0.0, 0.0, 0.6206896551724138, 0.5, 0.0, 0.09523809523809525, 0.7906976744186047, 0.0, 0.09523809523809522, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.16]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_triviaqa-validation-2475", "mrqa_squad-validation-4953", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["charged particle beam weapons", "Hindi film industry", "Gaels", "Crash", "d\u00edsir", "lion", "Russian film industry", "sediment load", "Washington metropolitan area", "GTPase responsible for endocytosis in the eukaryotic cell", "User State Migration Tool", "Ordos City China Science Flying Universe Science and Technology Co.", "pie tins", "PPG Paints Arena, Pittsburgh, Pennsylvania", "archaeology", "Section 30 of the Teaching Council Act 2001", "squirrels", "1984", "quasars", "Retreating Monsoon", "Romansh", "Tudor king", "MIX 94.5", "Q Branch (or later Q Division) the fictional research and development division of the British Secret Service", "Philippi in Greece", "the division of labour, productivity, and free markets", "Gerard Marenghi (born January 24, 1920) known as Jerry Maren", "Whitney Houston", "Hugo Award", "conservative", "king David of Israel", "Hugo Peretti, Luigi Creatore, and George David Weiss"], "metric_results": {"EM": 0.0625, "QA-F1": 0.19635303932178932}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 0.05, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 0.125, 0.0, 0.0, 0.7499999999999999, 0.0, 0.3636363636363636, 0.0, 0.0, 0.5454545454545454]}}, "error_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["Quentin Tarantino", "cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "lila Field Academy for Children", "independence from the Duke of Savoy", "the Roentgen rays, but by the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "physicians, lawyers, engineers, and accountants", "a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Finland", "a midfielder", "jonathan", "the P position", "carbohydrate monosaccharide sucrose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "wendy\u2019s", "by functions ; Introverted Sensing ( Si ), Extroverted Thinking ( Te ), Introverted Feeling ( Fi ) and Extrovert Intuition ( Ne ) )", "Thursday", "yellow", "drug choice, dose, route, frequency, and duration of therapy", "jupiter", "aviator, polar explorer, and organizer of polar logistics", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Jules", "The Private Education Student Financial Assistance", "bamba", "to raise money to rebuild St. Peter's Basilica in Rome", "colonies", "two forces, one pointing north, and one pointing east", "Bills", "Jack Murphy Stadium", "hierarchy theorems"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23202941972243443}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.7000000000000001, 0.0, 0.0, 0.8666666666666666, 0.11764705882352941, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.16, 0.0, 0.375, 0.5, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.13333333333333333, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_squad-validation-1429", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-11001", "mrqa_naturalquestions-train-69661", "mrqa_naturalquestions-train-61339", "mrqa_naturalquestions-train-49285", "mrqa_naturalquestions-train-6424", "mrqa_naturalquestions-train-50570", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-52126", "mrqa_naturalquestions-train-5047", "mrqa_naturalquestions-train-34189", "mrqa_naturalquestions-train-60062", "mrqa_naturalquestions-train-606", "mrqa_naturalquestions-train-8983", "mrqa_naturalquestions-train-64119", "mrqa_naturalquestions-train-78769", "mrqa_naturalquestions-train-4044", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-4330", "mrqa_hotpotqa-validation-4823", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-5007", "mrqa_hotpotqa-validation-4823", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-5336", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_squad-validation-1592"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["robin", "letter", "Indiana", "temple square", "French", "a sailor coming home from a round trip", "domain names www.example.com and example.com", "a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication", "opera", "vegan", "fly", "Rigoletto", "land area", "third-most abundant element in the universe", "IKEA", "216 countries and territories", "brazil", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers", "Algernod Lanier Washington", "the Outfield", "porec", "Michael Edwards ( briefly as the older Connor )", "railway locomotives", "egypt", "third quarter ( also known as last quarter )", "bresslaw", "chemists Glenn T. Seaborg, the developer of the actinide concept", "Kentucky, Virginia, and Tennessee", "avionics, telecommunications, and computers", "eve", "615 square kilometers", "egypt"], "metric_results": {"EM": 0.125, "QA-F1": 0.224740473342447}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.25, 0.21052631578947367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_naturalquestions-validation-7641", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "experience", "2003", "cricket", "sweden", "campaign setting", "2003", "867 feet", "the Hebrew name Immanu'el ( \u05e2\u05b4\u05de\u05b8\u05bc\u05e0\u05d5\u05bc\u05d0\u05b5\u05dc \u202c, which means `` God with us. '' It was possibly brought from the Byzantine Empire", "Shape of You", "Padm\u00e9 Amidala", "8th", "Tikki tikki tembo-no sa rembo- chari bari ruchi-pip peri pembo", "all health care settings", "become more integral within the health care system", "treble clef", "Gabriel Alberto Azucena (born September 23, 1988)", "160 km / h", "Rome", "December 1, 2009", "Estelle Sylvia Pankhurst", "egypt", "editor of the works of Francis Bacon", "margaret", "Ministry of Corporate Affairs", "Irish", "the site of ancient cult activity as far back as 7th century BCE", "bont\u00eb Sisters", "energy- storage molecules ATP and NADPH", "astrology", "a genuine love of God with heart, soul, mind, and strength", "Christ lag"], "metric_results": {"EM": 0.1875, "QA-F1": 0.338328242481203}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.33333333333333337, 0.19999999999999998, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.31578947368421056, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8750000000000001, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.8, 0.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["margaret spillane", "7\u20139", "perique", "cut off close by the hip, and under the left shoulder, he carried a crutch", "death penalty", "stout man with a \"double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "meyerly", "rich Fisher King is later said to be the rich Fisher king who \"was wounded in a battle and completely crippled, so that he's helpless now, has lived there for twelve years", "Mangal Pandey of the 34th BNI", "V Alaudae, a Celtic legion recruited from Gallia Narbonensis and XXI, possibly a Galatian legion from the other side of the empire", "Cartwright clan", "four of the 50 states of the United States", "curling", "eighth series", "Pebble Beach", "Los Angeles", "French", "Henry Mills", "\"LOVE Radio\" which featured a limited selection of music genres", "Miami Marlins", "elected by the court from its members for a three - year term", "travolta", "Don Henkel", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Normandy landings (codenamed Operation Neptune) were the landing operations on Tuesday, 6 June 1944 (termed D-Day) of the Allied invasion of Normandy in Operation Overlord during World War II", "Mediterranean Sea"], "metric_results": {"EM": 0.03125, "QA-F1": 0.11600437995912136}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.18181818181818182, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.07142857142857142, 0.13793103448275862, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_squad-validation-5852", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-712", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-85573", "mrqa_naturalquestions-train-15406", "mrqa_naturalquestions-train-80823", "mrqa_naturalquestions-train-4089", "mrqa_naturalquestions-train-45998", "mrqa_naturalquestions-train-71810", "mrqa_naturalquestions-train-1362", "mrqa_naturalquestions-train-78219", "mrqa_naturalquestions-train-19348", "mrqa_naturalquestions-train-4080", "mrqa_naturalquestions-train-48670", "mrqa_naturalquestions-train-9205", "mrqa_naturalquestions-train-78547", "mrqa_naturalquestions-train-589", "mrqa_naturalquestions-train-65598", "mrqa_naturalquestions-train-27951", "mrqa_hotpotqa-validation-2403", "mrqa_squad-validation-9792", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-2403", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-9569", "mrqa_squad-validation-9569", "mrqa_squad-validation-8625", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-6057", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-5370"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "Take That", "youngest", "electric lighting", "their knowledge of Native American languages", "Einstein", "electromagnetic theory", "Premier League club Swansea City", "lily", "Elizabeth Weber", "an earlier Funcom game, \"The Secret World\"", "hundreds", "\"Waiting for Guffman\"", "1978", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "partial funding", "5% abv draught beer", "inefficient", "Chu'Tsai", "Liz", "least onerous", "lago di como", "Grissom, White, and Chaffee", "multinational retail corporation", "purple", "The Natya Shastra is the foundational treatise for classical dances of India, and this text is attributed to the ancient scholar Bharata Muni", "The geological properties of a white silica sand found at Basin Head are unique in the province ; the sand grains cause a scrubbing noise as they rub against each other when walked on", "pGA", "parts of the air in the vessel", "emperors"], "metric_results": {"EM": 0.375, "QA-F1": 0.4504303493734106}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.25, 1.0, 0.08695652173913045, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 0.046511627906976744, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["George Bush", "taghrooda", "Burnley", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "the big - name lawyers", "Milk Barn Animation", "when they enter the army during initial entry training", "one of The Canterbury Tales by Geoffrey Chaucer", "they announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled", "leeds", "`` crown '' ) of a Christmas tree or Hanukkah bush", "74 per cent", "london heathrow airport", "often social communities with considerable face-to-face interaction among members", "William Strauss and Neil Howe", "monophyletic", "study of insects", "candidates on specific catechism questions", "a pH indicator, a color marker, and a dye", "2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "usually resident population of the UK and its constituent countries", "John and Charles Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Mississippi State", "loner Cal Trask", "appearing as Jude in the musical romance drama film \" Across the Universe\" (2007)", "mike", "work in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "XXXTentacion", "stagnant wages for the working class amidst rising levels of property income for the capitalist class"], "metric_results": {"EM": 0.125, "QA-F1": 0.2414763698194762}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.26086956521739124, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.7499999999999999, 0.5, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.5555555555555556, 1.0, 0.10526315789473682]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_squad-validation-7182"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["James Stenbeck", "Good Kid, M.A.D City", "el Capitan", "Interventive treatment", "3 lines of reflection and rotational symmetry of order 3 about its center", "Bishop Reuben H. Mueller", "wurlitzer Electric Piano", "During his epic battle with Frieza", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "UNESCO", "Anfernee Simons", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "not given at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater", "every good work designed to attract God's favor is a sin", "annuity", "twin sister", "Buffalo Bill", "justice", "France", "It insisted on its neutral rights, which included allowing private corporations and banks to sell or loan money to either side", "dry", "meat torn from an animal by wild beasts", "Arthur Russell", "memorised by the people themselves", "Wylie Draper", "political role for Islam", "the university's off- campus rental policies", "Dennis Hull", "Pittsburgh Steelers", "famine"], "metric_results": {"EM": 0.3125, "QA-F1": 0.44261138167388164}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.0, 1.0, 0.22222222222222218, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.18181818181818182, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.2666666666666667, 0.4, 0.6666666666666665, 0.4, 1.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_naturalquestions-validation-49", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-61623", "mrqa_naturalquestions-train-22558", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-5172", "mrqa_naturalquestions-train-10047", "mrqa_naturalquestions-train-14245", "mrqa_naturalquestions-train-40044", "mrqa_naturalquestions-train-17470", "mrqa_naturalquestions-train-14597", "mrqa_naturalquestions-train-48148", "mrqa_naturalquestions-train-6609", "mrqa_naturalquestions-train-12700", "mrqa_naturalquestions-train-19838", "mrqa_naturalquestions-train-34071", "mrqa_naturalquestions-train-69735", "mrqa_naturalquestions-train-34038", "mrqa_squad-validation-3523", "mrqa_squad-validation-3523", "mrqa_squad-validation-3523", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-4173", "mrqa_squad-validation-3523", "mrqa_hotpotqa-validation-1909", "mrqa_naturalquestions-validation-8728", "mrqa_hotpotqa-validation-1909", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3698", "mrqa_squad-validation-4479", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-4479"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["suitable for use on rough terrain", "AOL", "Timur", "R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "georgia", "1937", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock. He had to work at various electrical repair jobs and even as a ditch digger for $2 per day.", "Marxist", "variation in plants", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades.", "3,600", "State Street", "Kingdom of Saudi Arabia", "110", "georgia cukor", "georgia", "Lawton Mainor Chiles Jr.", "Kobol's Last Gleaming", "Wisconsin", "uneven trade agreements", "tea or porridge with bread, chapati, mahamri, boiled sweet potatoes or yams. Ugali with vegetables, sour milk, meat, fish or any other stew is generally eaten by much of the population", "energy", "georgia", "Ruth Elizabeth \"Bette\" Davis", "uranium", "7 December 2004"], "metric_results": {"EM": 0.125, "QA-F1": 0.2652648142772745}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 0.34782608695652173, 0.0, 0.0, 0.1111111111111111, 0.09523809523809525, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 0.5365853658536585, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 50, "before_eval": {"predictions": ["basketball", "Alex Orff, Paul Hindemith, Richard Strauss, Luigi Nono, Krzysztof Penderecki and Joaqu\u00edn Rodrigo", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie ), typically cooked in a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with mashed potato", "Gatiman express", "kill in...T.S. Eliot: plays, Sweeney Agonistes, the rock, Murder in the cathedral, the family reunion, the cocktail party, the confidential clerk", "Taylor Swift", "The issues of conflicting territorial claims between British and French colonies in North America were turned over to a commission to resolve", "the Hindu sage Valmiki", "the 1719 Epistle to Ramsay by the Scottish poet William Hamilton", "every two to six years", "China was torn by dissension and unrest", "association football", "2016", "Dan Castellaneta", "2007", "Wicked Twister", "subtraction", "rocket designer and creator of the Atlas ICBM.", "shoe", "unclear", "supernatural psychological horror", "originate in the House of Representatives", "The Revenant", "evening", "Blue (Da Ba Dee)", "shirley williams", "Faurot Field", "Dennis C. Stewart as Leo `` Craterface '' Balmudo, head of the Scorpions, a rival greaser gang", "the `` sandbar '' between river of life, with its outgoing `` flood '', and the ocean that lies beyond ( death ), the `` boundless deep '', to which we return", "11", "kahramanmara\u015f", "If the car is slowed initially by manual use of the automatic gear box"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3003763739328755}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.1081081081081081, 0.6666666666666666, 0.10526315789473684, 0.0, 0.3076923076923077, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8400000000000001, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-3395", "mrqa_triviaqa-validation-813", "mrqa_hotpotqa-validation-5623", "mrqa_squad-validation-10171", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-1607", "mrqa_squad-validation-8134", "mrqa_hotpotqa-validation-1528", "mrqa_naturalquestions-validation-7812", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-118", "mrqa_triviaqa-validation-290", "mrqa_hotpotqa-validation-1350", "mrqa_triviaqa-validation-3428", "mrqa_squad-validation-10427", "mrqa_hotpotqa-validation-304", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-3572", "mrqa_hotpotqa-validation-2635", "mrqa_triviaqa-validation-7770", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-71", "mrqa_squad-validation-290", "mrqa_triviaqa-validation-239", "mrqa_naturalquestions-validation-3022"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 51, "before_eval": {"predictions": ["3", "the base of the right ventricle", "a card from a pack of playing cards by Alice", "Los Angeles", "Alex Breckenridge as Monique Valentine, Sebastian's superficial girlfriend", "seven", "Schr\u00f6dinger equation", "patusnaya and mallasol types of what", "Ashland, Holderness, Campton, Rumney, Wentworth, Warren, Ellsworth, Waterville Valley and Thornton", "public schools in Alabama, Arkansas, Georgia, Louisiana, Mississippi, Oklahoma, Tennessee and Texas", "Ghostface mask", "Roger Staubach", "AC induction motor and transformer", "originally a three-part retrospective in tribute to Eric Morecambe", "American rock band Queens of the Stone Age", "the port of Nueva Espa\u00f1a to the Spanish coast", "Robert John Day", "1775\u20131795", "Empiricism", "The Moon and Sixpence", "Pabst Brewing Company", "redistributive", "Tyrion Lannister", "ozzie Owl", "`` something that is to be expressed through some medium, as speech, writing or any of various arts ''", "Saint Peter", "Fourth Home Rule Bill", "t\u1ebdt Nguy\u00ean \u00d0\u00e1n", "Gebhard v Consiglio dell\u2019 Ordine degli Avvocati e Procuratori di Milano", "Belarus", "1835", "The Virgin Queen, Gloriana or Good Queen Bess"], "metric_results": {"EM": 0.125, "QA-F1": 0.2920398482898483}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.4324324324324324, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.4444444444444445, 0.33333333333333337, 0.761904761904762, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.0, 0.4, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 0.4, 0.4444444444444445]}}, "error_ids": ["mrqa_hotpotqa-validation-4919", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4006", "mrqa_squad-validation-2432", "mrqa_naturalquestions-validation-6634", "mrqa_squad-validation-10386", "mrqa_triviaqa-validation-7398", "mrqa_hotpotqa-validation-5740", "mrqa_squad-validation-1932", "mrqa_triviaqa-validation-787", "mrqa_hotpotqa-validation-4795", "mrqa_squad-validation-1185", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-4922", "mrqa_naturalquestions-validation-7484", "mrqa_hotpotqa-validation-4528", "mrqa_naturalquestions-validation-7312", "mrqa_triviaqa-validation-4890", "mrqa_hotpotqa-validation-596", "mrqa_squad-validation-7324", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-1046", "mrqa_naturalquestions-validation-897", "mrqa_hotpotqa-validation-2549", "mrqa_triviaqa-validation-3980", "mrqa_squad-validation-4430", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3357"], "retrieved_ids": ["mrqa_naturalquestions-train-83096", "mrqa_naturalquestions-train-16609", "mrqa_naturalquestions-train-14597", "mrqa_naturalquestions-train-42114", "mrqa_naturalquestions-train-30845", "mrqa_naturalquestions-train-23893", "mrqa_naturalquestions-train-26187", "mrqa_naturalquestions-train-51672", "mrqa_naturalquestions-train-52704", "mrqa_naturalquestions-train-69494", "mrqa_naturalquestions-train-7975", "mrqa_naturalquestions-train-78925", "mrqa_naturalquestions-train-21951", "mrqa_naturalquestions-train-57841", "mrqa_naturalquestions-train-63529", "mrqa_naturalquestions-train-81249", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-validation-49", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-118", "mrqa_squad-validation-2428", "mrqa_hotpotqa-validation-1528", "mrqa_naturalquestions-validation-1008", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-7812", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-7812", "mrqa_hotpotqa-validation-2635"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 52, "before_eval": {"predictions": ["Myllokunmingia", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Sophie Faldo", "Brenda", "the imaginary unit", "`` asphyxia '' ( cutting off the oxygen supply ) and cooling", "edinburgh clapton", "Carey Mulligan, Matthias Schoenaerts, Michael Sheen, Tom Sturridge and Juno Temple", "The Worm", "1908", "five", "German hymns", "Bury Football Club", "Fan S. Noli", "The Thing of It is... is a 1967 novel written by William Goldman about Amos McCracken", "the NFC Championship Game", "Tom Robinson", "Home Alone", "filming began in September 2000 at Leavesden Film Studios and in London, with production ending in July 2001", "Western Asia", "quickly", "about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm)", "The leopard", "service sector", "Florida", "anise-flavoured spirit", "dave williams", "Greater Manchester", "British", "all aspect of public and private life", "anarchists", "Just under 540,800", "the UK's largest digital subscription television company"], "metric_results": {"EM": 0.21875, "QA-F1": 0.34290564150761516}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.4, 0.21052631578947367, 1.0, 0.0, 0.25, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.5, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6188", "mrqa_naturalquestions-validation-8228", "mrqa_squad-validation-9081", "mrqa_naturalquestions-validation-3351", "mrqa_triviaqa-validation-2598", "mrqa_naturalquestions-validation-7688", "mrqa_naturalquestions-validation-1186", "mrqa_hotpotqa-validation-5318", "mrqa_squad-validation-2401", "mrqa_hotpotqa-validation-5482", "mrqa_squad-validation-7240", "mrqa_hotpotqa-validation-182", "mrqa_squad-validation-308", "mrqa_naturalquestions-validation-8759", "mrqa_hotpotqa-validation-1871", "mrqa_squad-validation-8849", "mrqa_hotpotqa-validation-1504", "mrqa_squad-validation-7377", "mrqa_hotpotqa-validation-1402", "mrqa_triviaqa-validation-5068", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-904", "mrqa_squad-validation-2918", "mrqa_squad-validation-2772"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 53, "before_eval": {"predictions": ["Following the election of the UK Labour Party to government in 1997", "Levi's Stadium", "c 1600 from Bishopsgate with elaborately carved wood work and leaded windows", "Professor Moriarty", "Gregory Peck", "after the Seven Years' War", "pomme", "~74,000 (BP = Before Present)", "Chicago fire", "Bronwyn Kathleen Bishop", "guidance and intervention from the European empire to aid in the governing of a more evolved social structure", "pogo hearts", "tony plexwhackers", "domination or control by a group of people over another", "a salvation story", "kuwaita", "Sir Derek George Jacobi", "paul williamsfaraday", "25 June 1932", "patricci", "Edward Anthony Spitzka", "seasonal television specials", "chromosome", "Evey's mother", "10", "things that are a matter of custom or expectation", "Joudeh Al - Goudia family", "Wes Unseld", "1936", "nitrogen dioxide", "more land clearance (Bronze Age agriculture) in the upland areas (central Germany)", "Daniel Handler"], "metric_results": {"EM": 0.125, "QA-F1": 0.2635200736763237}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [0.18181818181818182, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.375, 0.28571428571428575, 0.4, 0.0, 0.0, 0.42857142857142855, 0.0]}}, "error_ids": ["mrqa_squad-validation-4100", "mrqa_squad-validation-169", "mrqa_squad-validation-5390", "mrqa_squad-validation-7700", "mrqa_triviaqa-validation-3107", "mrqa_hotpotqa-validation-4571", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-7626", "mrqa_hotpotqa-validation-123", "mrqa_squad-validation-9867", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-3014", "mrqa_squad-validation-9808", "mrqa_naturalquestions-validation-1161", "mrqa_triviaqa-validation-595", "mrqa_hotpotqa-validation-622", "mrqa_triviaqa-validation-6410", "mrqa_naturalquestions-validation-6485", "mrqa_triviaqa-validation-2821", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-117", "mrqa_squad-validation-6877", "mrqa_naturalquestions-validation-678", "mrqa_hotpotqa-validation-5614", "mrqa_naturalquestions-validation-6970", "mrqa_triviaqa-validation-177", "mrqa_squad-validation-9354", "mrqa_triviaqa-validation-3268"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 54, "before_eval": {"predictions": ["The Christmas Invasion", "Purple Heart Medal", "an expression of Priestley's socialist political principles", "March 9 to 18", "10 November 2017", "Romancing the Stone", "the seventh cranial nerve", "The 8th Habit", "Anishinaabeg", "general medical advice and a range of services that are now performed solely by other specialist practitioners, such as surgery and midwifery", "sunny", "westray", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "22 September 2015", "\"Murder Request\")", "chaste", "unexploded mines", "annually in late January or early February", "Michael Schumacher", "the duodenum", "pastry", "Cuyler Reynolds", "Atlanta", "vodka", "cricketing", "135", "saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac, including caches of supplies destined for New France's western forts and furs destined for Europe", "typically found within a casino", "Hermes, Dior, Cartier, Bottega Veneta, Chanel, Fendi, Gucci, Louis Vuitton, MaxMara, Celine, Tiffany & Co.", "\"Shoot Straight from Your Heart\"", "Emily Bronte", "Kony Ealy"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26397579814148187}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.13333333333333333, 0.7272727272727273, 0.5, 0.2666666666666667, 0.0, 0.0, 0.0, 0.08695652173913042, 0.0, 0.0, 0.4827586206896552, 1.0, 1.0, 0.0, 0.0, 0.4444444444444444, 0.3636363636363636, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0588235294117647, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-1383", "mrqa_hotpotqa-validation-4277", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2684", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-525", "mrqa_squad-validation-6211", "mrqa_naturalquestions-validation-4960", "mrqa_triviaqa-validation-5166", "mrqa_naturalquestions-validation-4021", "mrqa_triviaqa-validation-1396", "mrqa_triviaqa-validation-2356", "mrqa_naturalquestions-validation-8441", "mrqa_hotpotqa-validation-4181", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1707", "mrqa_hotpotqa-validation-3620", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-5057", "mrqa_hotpotqa-validation-2058", "mrqa_squad-validation-10293", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3710", "mrqa_triviaqa-validation-6164", "mrqa_squad-validation-979"], "retrieved_ids": ["mrqa_naturalquestions-train-55861", "mrqa_naturalquestions-train-75809", "mrqa_naturalquestions-train-20768", "mrqa_naturalquestions-train-48123", "mrqa_naturalquestions-train-32795", "mrqa_naturalquestions-train-54697", "mrqa_naturalquestions-train-84855", "mrqa_naturalquestions-train-65990", "mrqa_naturalquestions-train-26324", "mrqa_naturalquestions-train-36503", "mrqa_naturalquestions-train-78837", "mrqa_naturalquestions-train-75174", "mrqa_naturalquestions-train-86643", "mrqa_naturalquestions-train-16999", "mrqa_naturalquestions-train-54154", "mrqa_naturalquestions-train-45694", "mrqa_squad-validation-4100", "mrqa_squad-validation-169", "mrqa_triviaqa-validation-776", "mrqa_naturalquestions-validation-6970", "mrqa_triviaqa-validation-177", "mrqa_naturalquestions-validation-6485", "mrqa_hotpotqa-validation-1901", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-2821", "mrqa_triviaqa-validation-7484", "mrqa_squad-validation-169", "mrqa_triviaqa-validation-2821", "mrqa_triviaqa-validation-7484", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-5318", "mrqa_hotpotqa-validation-5482"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 55, "before_eval": {"predictions": ["2,000", "John Elway", "England", "Gracie the new `` face '' of the FBI", "Brittany, Cornwall, Ireland", "stromal connective tissue", "paris", "around the world", "pangea", "a job that few require (low demand) will result in a low wage for that job", "octagon", "bobby giraldi", "Scott Mosier", "the mysterious Keyser S\u00f6ze", "weaving", "the book and architecture ; and also including ceramics, metal, glass, and gardens", "the Company's army", "DeMarcus Ware as time expired in the half", "Jack Nicholson -- Chinatown as J.J. `` Jake '' Gittes", "daniel brown", "boxing", "Kaffa in the Crimea in 1347", "a group of gardens built by the Mughals in the Persian style of architecture", "Iranian", "paris", "Landwehr", "often given priority because his work was published first", "as a preparation for the Feast of the Divine Mercy, celebrated each year on first Sunday after Easter", "accommodationism", "Parliamentarians ( `` Roundheads '' ) and Royalists ( `` Cavaliers '' )", "the Niger\u2013 Congo language family", "a lower index of refraction, typically a cladding of a different glass, or plastic"], "metric_results": {"EM": 0.09375, "QA-F1": 0.20126175907425908}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.09999999999999999, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.5714285714285715, 0.0, 0.36363636363636365]}}, "error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_hotpotqa-validation-3062", "mrqa_naturalquestions-validation-6918", "mrqa_triviaqa-validation-3568", "mrqa_squad-validation-4700", "mrqa_triviaqa-validation-140", "mrqa_squad-validation-7407", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-5479", "mrqa_hotpotqa-validation-3264", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-938", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4098", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-9536", "mrqa_triviaqa-validation-7421", "mrqa_naturalquestions-validation-9459", "mrqa_squad-validation-4773", "mrqa_naturalquestions-validation-819", "mrqa_hotpotqa-validation-5543", "mrqa_triviaqa-validation-7689", "mrqa_hotpotqa-validation-4215", "mrqa_squad-validation-3450", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-570", "mrqa_hotpotqa-validation-2699", "mrqa_naturalquestions-validation-7078"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 56, "before_eval": {"predictions": ["Randy", "1991", "predictions that can be tested in various ways", "the city council", "moral conservatism, literalism, and the attempt \"to implement Islamic values in all spheres of life\"", "Tesla demonstrated a series of electrical effects previously performed throughout America and Europe,:76 included using high-voltage, high-frequency alternating current to light a wireless gas-discharge lamp", "65,535 bytes ( 8 byte header + 65,527 bytes of data )", "39", "a smartphone - like, hands - free format", "increased their reserves (by expanding their money supplies) in amounts far greater than before", "Friedrich Nietzsche", "transgender", "John Mailer London's Burning", "Robert \"Bumps\" Blackwell, Enotris Johnson, and Little Richard", "five", "negatively impact teachers' mental and physical health, productivity, and students' performance", "Subaru", "d'Artagnan", "when each of the variables is a perfect monotone function of the other", "Melbourne", "1932", "September 25, 1957", "methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid, and formic acid", "Peter Boles\u0142aw Schmeichel MBE", "the highest commissioned SS rank, inferior only to \"Reichsf\u00fchrer-SS\"", "ERINys - the Greek Goddess of Revenge (Greek mythology)", "the lowest known subaerial volcanic vents in the world, at 45 m ( 150 ft ) or more below sea level", "Beyonc\u00e9", "in Egypt, the only part of the country located in Asia", "Joanna Page", "in the retina of mammalian eyes ( e.g. the human eye )", "kaupthing Singer & Friedlander Ltd5"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23305562945508598}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.07692307692307691, 0.3636363636363636, 0.0, 0.0, 0.08333333333333333, 1.0, 0.0, 0.0, 0.5454545454545454, 0.6666666666666666, 0.23076923076923075, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.08695652173913045, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-9608", "mrqa_squad-validation-1397", "mrqa_naturalquestions-validation-1787", "mrqa_squad-validation-371", "mrqa_naturalquestions-validation-752", "mrqa_squad-validation-3720", "mrqa_hotpotqa-validation-627", "mrqa_triviaqa-validation-6223", "mrqa_hotpotqa-validation-4620", "mrqa_hotpotqa-validation-3651", "mrqa_squad-validation-2004", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-326", "mrqa_naturalquestions-validation-486", "mrqa_hotpotqa-validation-1912", "mrqa_squad-validation-3653", "mrqa_hotpotqa-validation-15", "mrqa_hotpotqa-validation-686", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-1349", "mrqa_triviaqa-validation-5813", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-3069", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-2701"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 57, "before_eval": {"predictions": ["Saturn", "texicographer", "60 million", "Bathurst", "1948", "low in Europe and Central Asia but medium to high in respect of labour practices, employment and entrepreneurship and in access to finance", "the Psalms, the books of Hebrews, Romans, and Galatians", "FeO (w\u00fcstite) is written as Fe1 \u2212 xO, where x is usually around 0.05.", "Groucho", "materials melted near an impact crater", "half-penny sales tax", "david pediaview", "tom Brady", "harvinywermod", "oxygen", "Victoria's more affluent eastern and outer suburbs, and some rural and regional centres", "His / Her Majesty's Ship", "reared in South Africa", "October 15, 1997", "Book of Job", "modernized the outside of the building and renovated the inside as part of his first construction project in Manhattan", "Greenland shark", "pembroke", "Lewis Balfour", "kevin williams", "five", "tara air plane", "Mark Antony", "William F. Buckley Jr.", "chicken schnitzel", "Sam the Sham", "as many as two consecutive terms"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21804220085470086}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.07692307692307691, 0.2, 0.13333333333333333, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.25, 0.6666666666666666, 0.3333333333333333, 0.4615384615384615, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.4]}}, "error_ids": ["mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-4974", "mrqa_hotpotqa-validation-1567", "mrqa_hotpotqa-validation-5436", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3969", "mrqa_squad-validation-2255", "mrqa_squad-validation-3598", "mrqa_triviaqa-validation-3648", "mrqa_squad-validation-7226", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-7489", "mrqa_squad-validation-2886", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-6365", "mrqa_naturalquestions-validation-10657", "mrqa_hotpotqa-validation-3802", "mrqa_hotpotqa-validation-3544", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-6425", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-7187", "mrqa_hotpotqa-validation-1975", "mrqa_triviaqa-validation-1742", "mrqa_hotpotqa-validation-1016", "mrqa_naturalquestions-validation-787"], "retrieved_ids": ["mrqa_naturalquestions-train-80857", "mrqa_naturalquestions-train-75777", "mrqa_naturalquestions-train-48429", "mrqa_naturalquestions-train-4517", "mrqa_naturalquestions-train-34371", "mrqa_naturalquestions-train-52314", "mrqa_naturalquestions-train-15330", "mrqa_naturalquestions-train-26826", "mrqa_naturalquestions-train-40672", "mrqa_naturalquestions-train-35942", "mrqa_naturalquestions-train-8039", "mrqa_naturalquestions-train-76164", "mrqa_naturalquestions-train-39394", "mrqa_naturalquestions-train-34700", "mrqa_naturalquestions-train-8236", "mrqa_naturalquestions-train-43864", "mrqa_triviaqa-validation-892", "mrqa_hotpotqa-validation-1912", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-5813", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-3568", "mrqa_triviaqa-validation-5057", "mrqa_hotpotqa-validation-3651", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-2701", "mrqa_naturalquestions-validation-10724", "mrqa_squad-validation-371", "mrqa_hotpotqa-validation-1912", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-10724"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 58, "before_eval": {"predictions": ["Dutch", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "a house edge of between 0.5 % and 1 %", "outer and inner membranes", "in the Ouseburn valley", "1955", "henry v", "200 horsepower (150 kilowatts)", "Thomas Edison and Nikola Tesla", "Lutheranism", "Night Ranger", "the red M&M", "Thocmentony", "organizational interventions, like changing teachers' schedules, providing support networks and mentoring, changing the work environment, and offering promotions and bonuses, may be effective in helping to reduce occupational stress among teachers", "levels of economic inequality", "hen yozhuo", "Enrico Fermi", "Amal Clooney", "henry", "1979", "mainly civil servants recruited in special university classes, called Lehramtstudien (Teaching Education Studies)", "large areas", "0 points", "the medial epicondyle of the humerus from posteriorly, or inferiorly with the elbow flexed", "simple devices such as weighing scales and spring balances", "no man is an island", "Liao, Jin, and Song", "`` central '' or `` middle ''", "pigeons", "electric", "Attack the Block", "William Shakespeare's play Romeo and Juliet, in which Juliet seems to argue that it does not matter that Romeo is from her family's rival house of Montague, that is, that he is named `` Montague ''"], "metric_results": {"EM": 0.21875, "QA-F1": 0.32236383408258407}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.7272727272727273, 0.0, 0.8, 1.0, 0.5, 0.0, 0.15384615384615385, 0.0, 0.4, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.05714285714285715]}}, "error_ids": ["mrqa_squad-validation-9173", "mrqa_naturalquestions-validation-5554", "mrqa_squad-validation-8687", "mrqa_squad-validation-5202", "mrqa_triviaqa-validation-6185", "mrqa_squad-validation-1443", "mrqa_squad-validation-1535", "mrqa_hotpotqa-validation-593", "mrqa_naturalquestions-validation-10626", "mrqa_hotpotqa-validation-5654", "mrqa_squad-validation-2057", "mrqa_triviaqa-validation-5104", "mrqa_hotpotqa-validation-4178", "mrqa_triviaqa-validation-7264", "mrqa_hotpotqa-validation-5127", "mrqa_squad-validation-2042", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-9814", "mrqa_squad-validation-10351", "mrqa_triviaqa-validation-4817", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-6520", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-1706", "mrqa_naturalquestions-validation-3470"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 59, "before_eval": {"predictions": ["GE's four computer sales and service centers (Schenectady, Phoenix, Chicago, and Phoenix)", "The waxy cuticle", "the Naturalization Act of 1790", "Matthew Vaughn", "2 %", "Mountbatten", "Century 16 multiplex", "James Edward Kelly", "LuthorCorp", "24\u201310", "marxian", "Because course [the price of oil] is going to rise", "in the U.S. state of Kansas", "a yolk sac ( protruding from its lower part )", "Kelly Bundy", "Italy", "worked on the keels, boats", "Sir William Bruce", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "Mario Addison", "321,520", "2017 Major League Baseball draft", "2005", "red deer", "kalapatthar", "``Generalfeldmarschall\" (Field Marshal) Helmuth Karl Bernhard von Moltke", "marx", "`` Turkey in the Straw ''", "as soon as 2019", "Cinerama Productions/Palomar theatrical library", "marx", "tala"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2734403259679684}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.06896551724137931, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.16666666666666666, 0.888888888888889, 0.0, 1.0, 0.0, 0.4, 0.0, 0.9565217391304348, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4838", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-875", "mrqa_triviaqa-validation-2965", "mrqa_naturalquestions-validation-7974", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-3719", "mrqa_triviaqa-validation-2223", "mrqa_squad-validation-3730", "mrqa_hotpotqa-validation-740", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-5864", "mrqa_squad-validation-5124", "mrqa_hotpotqa-validation-5253", "mrqa_naturalquestions-validation-7035", "mrqa_squad-validation-825", "mrqa_hotpotqa-validation-1917", "mrqa_squad-validation-7612", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3685", "mrqa_hotpotqa-validation-4456", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-1676", "mrqa_naturalquestions-validation-5649", "mrqa_squad-validation-5887", "mrqa_triviaqa-validation-1933", "mrqa_triviaqa-validation-2525"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 60, "before_eval": {"predictions": ["Currer Bell", "the illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "in positions Arg15 - Ile16 and produces \u03c0 - Chymotrypsin", "The Catcher in the Rye", "The invading Normans and their descendants", "Margiana  Margiana (Greek: \u1f08\u03bd\u03c4\u03b9\u03cc\u03c7\u03b5\u03b9\u03b1 \u03c4\u1fc6\u03c2 \u039c\u03b1\u03c1\u03b3\u03b9\u03b1\u03bd\ufffd\ufffd\u03c2) was a major oasis-city", "Bendigo and its environs", "T.H. Green", "She became a naturalized American citizen in 1994 and also received Hungarian citizenship in June 2007.", "July 1872", "Boston and Lowell Railroad", "James Lofton and Mark Malone", "in Armenia place the fork to the right of the dinner plate", "summer months", "Phillip Schofield and Christine Bleakley returned to co-present", "the third \u00e9tude", "South East Asia", "segues", "d Welsh poet dwyd Theatr Cymru", "iron", "During the reign of King Beorhtric of Wessex ( 786 -- 802 ) three ships of `` Northmen '' landed at Portland Bay in Dorset", "South Australian town", "Flag Day in 1954", "around 300,000", "in 1858, Sir William James Herschel initiated fingerprinting in India", "Sexred", "Veronica Lodge", "The typically melds neo-Nazi ideology with ethnic European paganism and opposition to \"foreign\" religions such as Christianity, Islam and Judaism.", "rik Mayall", "NASA's yearly budget also began to shrink in light of the successful landing, and NASA also had to make funds available for the development of the upcoming Space Shuttle. By 1971, the decision was made to also cancel missions 18 and 19", "inversely proportional to the wave frequency, so gamma rays have very short wavelengths that are fractions of the size of atoms, whereas wavelengths on the opposite end of the spectrum can be as long as the universe", "The story is a satire on corruption in the administration of criminal justice and the concept of the \"celebrity criminal\""], "metric_results": {"EM": 0.15625, "QA-F1": 0.3191741219434634}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.4347826086956522, 0.25, 1.0, 0.0, 0.0, 0.4, 0.0, 0.23529411764705882, 0.6666666666666666, 0.4, 0.0, 0.5714285714285714, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.1904761904761905, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.1, 0.0, 0.3111111111111111, 0.18518518518518515, 0.23529411764705882]}}, "error_ids": ["mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-7225", "mrqa_squad-validation-1126", "mrqa_hotpotqa-validation-2715", "mrqa_squad-validation-2842", "mrqa_naturalquestions-validation-9273", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-1124", "mrqa_squad-validation-559", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1786", "mrqa_squad-validation-5449", "mrqa_hotpotqa-validation-1033", "mrqa_triviaqa-validation-3404", "mrqa_triviaqa-validation-3684", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-5311", "mrqa_naturalquestions-validation-6337", "mrqa_hotpotqa-validation-4293", "mrqa_naturalquestions-validation-6759", "mrqa_hotpotqa-validation-1971", "mrqa_triviaqa-validation-1284", "mrqa_squad-validation-4011", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-3681"], "retrieved_ids": ["mrqa_naturalquestions-train-62482", "mrqa_naturalquestions-train-34090", "mrqa_naturalquestions-train-68720", "mrqa_naturalquestions-train-24189", "mrqa_naturalquestions-train-54316", "mrqa_naturalquestions-train-25873", "mrqa_naturalquestions-train-41573", "mrqa_naturalquestions-train-12656", "mrqa_naturalquestions-train-4130", "mrqa_naturalquestions-train-81579", "mrqa_naturalquestions-train-66258", "mrqa_naturalquestions-train-36186", "mrqa_naturalquestions-train-77160", "mrqa_naturalquestions-train-52838", "mrqa_naturalquestions-train-26215", "mrqa_naturalquestions-train-53786", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1393", "mrqa_squad-validation-2057", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-1173", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-2223", "mrqa_naturalquestions-validation-3470", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-1393", "mrqa_squad-validation-2057"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 61, "before_eval": {"predictions": ["The Two Noble Kinsmen", "john hames", "San Bernardino", "Judy Collins", "King Mondo", "the 2013 non-fiction book of the same name by David Finkel", "liza tarbuck", "The Frost Report", "fell from his horse", "incitement to terrorism", "Henry and Liza", "her father Eoghan Dubhdara \u00d3 M\u00e1ille", "Guadalupe Victoria", "iron", "Elk and Kanawha Rivers", "They circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "'Bucks Point'", "to `` help bring creative projects to life ''", "the LDS Church", "Old World fossil representatives", "in salts and never as the free elements", "Bill Hader", "Michigan", "thicker consistency and a deeper flavour than sauce", "the internal reproductive anatomy ( such as the uterus in females ), and the external genitalia", "new zealand", "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam", "Nine Inch Nails", "AD 14", "1977", "electronic music", "He was taken prisoner and eventually was convicted of crimes against peace"], "metric_results": {"EM": 0.15625, "QA-F1": 0.22497258771929823}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.16666666666666669, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1850", "mrqa_triviaqa-validation-3861", "mrqa_squad-validation-2644", "mrqa_naturalquestions-validation-6118", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-validation-7407", "mrqa_triviaqa-validation-6418", "mrqa_squad-validation-6218", "mrqa_naturalquestions-validation-8368", "mrqa_hotpotqa-validation-2012", "mrqa_hotpotqa-validation-4743", "mrqa_triviaqa-validation-2130", "mrqa_naturalquestions-validation-7483", "mrqa_squad-validation-8560", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-3355", "mrqa_naturalquestions-validation-1699", "mrqa_hotpotqa-validation-4931", "mrqa_triviaqa-validation-3960", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6376", "mrqa_squad-validation-3067", "mrqa_hotpotqa-validation-1114", "mrqa_squad-validation-9370", "mrqa_triviaqa-validation-3985", "mrqa_hotpotqa-validation-3481"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 62, "before_eval": {"predictions": ["no false doctrine", "blanqueamiento", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "the Monarch", "United Kingdom", "Ballarat Bertie", "therefore", "Ricketts Glen State Park", "an openly wounded and unabashedly portentous rock balladeer", "magma", "dennis main Wilson", "fee per unit of connection time", "the \"coordinator\"", "In 1932 the German encyclopedia Knaurs Lexikon stated the length as 1,320 kilometres (820 miles) presumably a typographical error", "2017 -- 18 network television season", "motion pictures", "Egypt", "Michael Collins", "lorraine", "4145 ft", "science", "during the production company vanity cards shown following the closing credits of most programs", "Kansas\u2013Nebraska Act", "2 Constant ( C\u03bc and C\u03b4 ) gene segments", "1910\u20131940", "11:28", "Start Here", "thomas dennis dennis", "Autobahn", "endocrine", "Baron", "poodle"], "metric_results": {"EM": 0.28125, "QA-F1": 0.372294061302682}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.22222222222222224, 0.13333333333333333, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0689655172413793, 0.5, 0.0]}}, "error_ids": ["mrqa_squad-validation-2339", "mrqa_squad-validation-9843", "mrqa_naturalquestions-validation-5396", "mrqa_squad-validation-7766", "mrqa_hotpotqa-validation-507", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-7270", "mrqa_triviaqa-validation-1698", "mrqa_squad-validation-4750", "mrqa_hotpotqa-validation-1510", "mrqa_squad-validation-9303", "mrqa_naturalquestions-validation-8696", "mrqa_squad-validation-2703", "mrqa_naturalquestions-validation-1555", "mrqa_squad-validation-3961", "mrqa_hotpotqa-validation-3469", "mrqa_squad-validation-5816", "mrqa_naturalquestions-validation-538", "mrqa_triviaqa-validation-4095", "mrqa_hotpotqa-validation-5607", "mrqa_naturalquestions-validation-9064", "mrqa_hotpotqa-validation-1692", "mrqa_triviaqa-validation-6254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 63, "before_eval": {"predictions": ["Bank of China Tower", "heat addition (in the boiler) and rejection ( in the condenser) are isobaric (constant pressure) processes in the Rankine cycle", "henry", "ARPANET", "William Shakespeare", "Kohlberg K Travis Roberts", "fleetwood", "probabilistic (or \"Monte Carlo\") and deterministic algorithms", "jamesi hrix", "eight", "Hong Kong\u2013based, Cayman Islands registered Mandarin and Cantonese-language television broadcaster that serves the Chinese mainland and Hong Kong along with other markets with substantial Chinese viewers.", "a narcissistic ex-lover who did the protagonist wrong", "catawba river", "optical microscopy", "mcc hammer", "Haiti", "Sondheim", "Strasbourg", "3.762", "the Iroquois Nations", "1993", "The Daily Mirror", "ACL tears", "malaysia", "ear ossicles", "German", "1999", "belief in the validity of the social contract, which is held to bind all to obey the laws that a government meeting certain standards of legitimacy has established, or else suffer the penalties set out in the law", "Professor Kantorek", "31 - member Senate", "e Edward Hopper", "stone age"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2619438798554652}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.07407407407407407, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.34146341463414637, 0.0, 0.5, 0.8, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-189", "mrqa_squad-validation-3445", "mrqa_triviaqa-validation-7574", "mrqa_squad-validation-4629", "mrqa_hotpotqa-validation-97", "mrqa_triviaqa-validation-1518", "mrqa_squad-validation-9057", "mrqa_triviaqa-validation-54", "mrqa_hotpotqa-validation-366", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-7598", "mrqa_squad-validation-5059", "mrqa_triviaqa-validation-27", "mrqa_triviaqa-validation-5905", "mrqa_naturalquestions-validation-9755", "mrqa_hotpotqa-validation-55", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-3929", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6651", "mrqa_triviaqa-validation-2850", "mrqa_squad-validation-6707", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-4037", "mrqa_triviaqa-validation-2254"], "retrieved_ids": ["mrqa_naturalquestions-train-60465", "mrqa_naturalquestions-train-52829", "mrqa_naturalquestions-train-77450", "mrqa_naturalquestions-train-81701", "mrqa_naturalquestions-train-18568", "mrqa_naturalquestions-train-827", "mrqa_naturalquestions-train-54248", "mrqa_naturalquestions-train-83130", "mrqa_naturalquestions-train-56615", "mrqa_naturalquestions-train-8887", "mrqa_naturalquestions-train-39117", "mrqa_naturalquestions-train-84202", "mrqa_naturalquestions-train-87272", "mrqa_naturalquestions-train-49373", "mrqa_naturalquestions-train-77796", "mrqa_naturalquestions-train-23686", "mrqa_triviaqa-validation-2130", "mrqa_triviaqa-validation-4095", "mrqa_triviaqa-validation-4095", "mrqa_squad-validation-9303", "mrqa_squad-validation-9303", "mrqa_squad-validation-9303", "mrqa_squad-validation-9303", "mrqa_squad-validation-9843", "mrqa_triviaqa-validation-3861", "mrqa_triviaqa-validation-2130", "mrqa_squad-validation-9843", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-validation-8696", "mrqa_triviaqa-validation-2130", "mrqa_squad-validation-6218", "mrqa_squad-validation-9303"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 64, "before_eval": {"predictions": ["MTBF", "Jim Justice", "larvae", "Kyle Broflovski", "kanto", "sorrow regarding the environment", "trial division", "consultant", "Preston", "The Suite Life of Zack & Cody", "bacillus calmette-gu\u00e9rin", "Saturn IB", "H. R. Haldeman", "daniel brian", "Zaha", "guardianobaldo da Montefeltro", "jaw", "badrutt", "charles heen", "reform the lunisolar calendar to provide an accuracy of 365.2425 days of the year", "largest aviation accident to occur in India", "glister", "immediate judgement discrepancy, or cognitive bias, where a person making an initial assessment of another person, place, or thing will assume ambiguous information based upon concrete information", "New Orleans", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "macOS High Sierra", "american heavyweight championship", "three", "Diarmaid MacCulloch", "loire river", "17th Century sources referring to Cardinal Richelieu after he was named to head the royal council in 1624", "appropriates ( gives to, sets aside for ) money to specific federal government departments, agencies, and programs"], "metric_results": {"EM": 0.15625, "QA-F1": 0.20211347794224166}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16949152542372883, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 0.058823529411764705]}}, "error_ids": ["mrqa_triviaqa-validation-2902", "mrqa_naturalquestions-validation-4207", "mrqa_squad-validation-4567", "mrqa_hotpotqa-validation-2542", "mrqa_triviaqa-validation-748", "mrqa_squad-validation-8909", "mrqa_triviaqa-validation-2173", "mrqa_squad-validation-3956", "mrqa_hotpotqa-validation-3489", "mrqa_triviaqa-validation-2032", "mrqa_hotpotqa-validation-811", "mrqa_triviaqa-validation-6398", "mrqa_triviaqa-validation-864", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-3208", "mrqa_squad-validation-8201", "mrqa_hotpotqa-validation-2257", "mrqa_triviaqa-validation-5521", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-4709", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-10533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 65, "before_eval": {"predictions": ["Territory of Colorado", "Arabic numerals", "Diary of a Wimpy Kid : The Long Haul", "the recommended. html filename extension", "pilgrimage", "serous pericardium", "Early Gothic", "Kentucky Derby", "verreaux sifaka", "Brian Keith Bosworth (born March 9, 1965) nicknamed \"The Boz\" is a former American professional football player who played as a linebacker for the Seattle Seahawks in the National Football League (NFL)", "1991", "ABC- DuMont Television Network", "Birmingham, Alabama", "Euler", "2020", "brianess", "apples", "gravitation", "nonconservative forces act to change the internal energies of the system", "bulls", "the applied force is opposed by static friction, generated between the object and the table surface", "ormond sackerians", "whiskey", "Shut Up", "tzotzil", "Karina Smirnoff became the runners - up, and Jack Osbourne and Cheryl Burke received third place", "Seattle Seahawks", "St. Augustine, founded in 1565 but repeatedly attacked and burned by pirates, privateers, and English forces", "four", "Private Mass", "kenya", "mycelium"], "metric_results": {"EM": 0.1875, "QA-F1": 0.25809485653235653}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.07142857142857142, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6153846153846153, 0.0, 0.2666666666666667, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.2222222222222222, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2737", "mrqa_squad-validation-482", "mrqa_naturalquestions-validation-8427", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-2752", "mrqa_squad-validation-1100", "mrqa_triviaqa-validation-1923", "mrqa_hotpotqa-validation-2442", "mrqa_squad-validation-5836", "mrqa_squad-validation-9021", "mrqa_triviaqa-validation-5573", "mrqa_triviaqa-validation-3518", "mrqa_naturalquestions-validation-6075", "mrqa_squad-validation-10420", "mrqa_triviaqa-validation-4080", "mrqa_squad-validation-10313", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-7605", "mrqa_triviaqa-validation-2383", "mrqa_naturalquestions-validation-1783", "mrqa_squad-validation-239", "mrqa_naturalquestions-validation-5468", "mrqa_hotpotqa-validation-4174", "mrqa_squad-validation-2296", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-3691"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 66, "before_eval": {"predictions": ["mayonnaise", "to fund his Colorado Springs experiments", "the eastern shore of the Firth of Clyde, Scotland", "On the Computational Complexity of Algorithms", "from sea level", "Pandavas", "Magnetically soft", "O", "rabbit", "Duke Kent-Brown", "oxygen", "bratwurst", "brad", "better academic results", "Vanessa Block", "insano", "2p + 1 with p prime", "white", "raven", "http://www.example.com/index.html", "Brunswick", "`` Goodbye, Abigail", "two catechisms", "cheddar", "Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen", "Saint-Domingue", "translated the New Testament from Greek into German and poured out doctrinal and polemical writings", "1963", "prokaryotic cell ( or organelle )", "Commissioners", "the traditional name or sometimes the Seven Years' War", "sattu paratha"], "metric_results": {"EM": 0.25, "QA-F1": 0.34687213827838825}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.125, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.28571428571428575, 0.0, 0.6153846153846154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08333333333333334, 1.0, 0.0, 0.0, 0.4, 1.0, 0.13333333333333333, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4099", "mrqa_squad-validation-1416", "mrqa_hotpotqa-validation-1052", "mrqa_naturalquestions-validation-5927", "mrqa_triviaqa-validation-7577", "mrqa_squad-validation-3477", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-3970", "mrqa_squad-validation-7130", "mrqa_hotpotqa-validation-4951", "mrqa_triviaqa-validation-515", "mrqa_squad-validation-8980", "mrqa_triviaqa-validation-5337", "mrqa_triviaqa-validation-6566", "mrqa_naturalquestions-validation-8229", "mrqa_triviaqa-validation-6654", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-5686", "mrqa_squad-validation-3483", "mrqa_squad-validation-2269", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-10357", "mrqa_squad-validation-10167", "mrqa_triviaqa-validation-2087"], "retrieved_ids": ["mrqa_naturalquestions-train-71220", "mrqa_naturalquestions-train-14813", "mrqa_naturalquestions-train-54202", "mrqa_naturalquestions-train-37807", "mrqa_naturalquestions-train-71745", "mrqa_naturalquestions-train-36888", "mrqa_naturalquestions-train-62781", "mrqa_naturalquestions-train-34614", "mrqa_naturalquestions-train-29678", "mrqa_naturalquestions-train-61142", "mrqa_naturalquestions-train-55873", "mrqa_naturalquestions-train-21535", "mrqa_naturalquestions-train-10631", "mrqa_naturalquestions-train-24237", "mrqa_naturalquestions-train-14100", "mrqa_naturalquestions-train-40859", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-7605", "mrqa_triviaqa-validation-7605", "mrqa_naturalquestions-validation-6075", "mrqa_triviaqa-validation-2752", "mrqa_triviaqa-validation-3691", "mrqa_squad-validation-482", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-2752", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-2752", "mrqa_triviaqa-validation-3691", "mrqa_squad-validation-2296"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 67, "before_eval": {"predictions": ["high inequality", "sherry", "funky", "Sweden, Norway and Denmark", "New \u00c5lesund", "desublimation", "Norman origin", "The franchise has also spawned two webcomics: \" Crossed: Wish You Were Here\" which ran from 2012\u20132014, and published by Avatar Press", "payday loans", "Doctor of Philosophy", "1,388", "Otis Timson", "jack flash", "non Governmental and Intergovernmental Organizations", "$150,000 and $250,000", "1963", "Ohio", "first freshman to finish as the runner-up in the Heisman Trophy balloting", "The Crowned Prince of the Philadelphia Mob", "\u201c Resign.\u201d", "bisexual", "stable hierarchy", "Big 12 Conference", "In the 1920s", "American", "Cleopatra VII Philopator", "April Fool's Day", "The Valley Farm", "fox", "chloroplasts", "Salta", "more than 265 million business records worldwide"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3982165404040404}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.8, 0.1818181818181818, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445]}}, "error_ids": ["mrqa_triviaqa-validation-6619", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-4648", "mrqa_triviaqa-validation-1385", "mrqa_hotpotqa-validation-5297", "mrqa_naturalquestions-validation-8911", "mrqa_triviaqa-validation-1583", "mrqa_squad-validation-8526", "mrqa_squad-validation-8837", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-294", "mrqa_squad-validation-6974", "mrqa_squad-validation-6327", "mrqa_hotpotqa-validation-3345", "mrqa_naturalquestions-validation-7089", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-567", "mrqa_triviaqa-validation-5013", "mrqa_triviaqa-validation-3395", "mrqa_squad-validation-8929", "mrqa_hotpotqa-validation-2171"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 68, "before_eval": {"predictions": ["Nuevo Reino de Le\u00f3n", "Delaware", "\"jus sanguinis\"", "his last starring role was as Boston police detective Barry Frost", "The King of Chutzpah", "P $ C", "energy", "Bakerloo", "English", "Glasgow Association for the Higher Education of Women", "the coffee shop Monk's", "T(n)", "The Pilot", "the dot", "dieppe", "1961", "Tyler Posey", "New South Wales", "teaching", "kung-fu", "brontosaurs", "qualifications", "Kelli Goss", "the final episode of the series", "jill officer and Dawn Askin", "1939", "automaticistas, or tolled ( quota ) highways", "redox", "Venezuela and the remainder in Colombia", "Tsarevich Ivan", "Sokovia", "conservative"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3286830357142857}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.125, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6352", "mrqa_triviaqa-validation-4485", "mrqa_hotpotqa-validation-4978", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-8791", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-6896", "mrqa_naturalquestions-validation-339", "mrqa_squad-validation-1784", "mrqa_triviaqa-validation-6302", "mrqa_naturalquestions-validation-10496", "mrqa_hotpotqa-validation-1145", "mrqa_squad-validation-2052", "mrqa_triviaqa-validation-3720", "mrqa_triviaqa-validation-5893", "mrqa_naturalquestions-validation-921", "mrqa_hotpotqa-validation-2867", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-3089", "mrqa_naturalquestions-validation-2632"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 69, "before_eval": {"predictions": ["Eisenhower Freeway", "Max Planck, Albert Einstein, Louis de Broglie, Arthur Compton, Niels Bohr and many others", "music became more expressive and emotional, expanding to encompass literary, artistic, and philosophical themes", "Thermochemical techniques", "North Kest even, Lincolnshire, England", "the Ming", "near major hotels and in the parking areas of major Chinese supermarkets", "al-Maridini", "Hellenismos", "the buttock and down the lower limb", "The FCI accepted the long - haired type in 2010, listing it as the variety b", "John Bull", "tea, horticultural produce, and coffee", "Barclays", "Eda Reiss Merin", "The blood spilled by the heroes who died in the name of their countrymen's Fatherland and Freedom", "island agent", "break off the cathode", "the British Galleries", "21.8 %", "pharmacological effect", "Han Chinese were treated as second-class citizens", "Tony Orlando and Dawn", "Paris", "We Shall Overcome", "Martha Wainwright", "1,462", "island of man", "West Norse sailors", "symphonic", "Boreas the North- Wind", "1698"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2716097206349722}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.125, 1.0, 0.6666666666666665, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.11764705882352941, 0.18181818181818182, 1.0, 0.0, 1.0, 0.10526315789473682, 0.0, 0.0, 0.22222222222222224, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0]}}, "error_ids": ["mrqa_squad-validation-4562", "mrqa_naturalquestions-validation-8059", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-6949", "mrqa_squad-validation-6464", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2464", "mrqa_naturalquestions-validation-8394", "mrqa_triviaqa-validation-1449", "mrqa_squad-validation-1388", "mrqa_squad-validation-5571", "mrqa_naturalquestions-validation-3035", "mrqa_squad-validation-3610", "mrqa_squad-validation-8185", "mrqa_hotpotqa-validation-3435", "mrqa_triviaqa-validation-5494", "mrqa_hotpotqa-validation-665", "mrqa_triviaqa-validation-6541", "mrqa_naturalquestions-validation-7217", "mrqa_triviaqa-validation-6782", "mrqa_triviaqa-validation-5088", "mrqa_hotpotqa-validation-1381"], "retrieved_ids": ["mrqa_naturalquestions-train-58182", "mrqa_naturalquestions-train-6987", "mrqa_naturalquestions-train-24909", "mrqa_naturalquestions-train-80773", "mrqa_naturalquestions-train-86277", "mrqa_naturalquestions-train-18587", "mrqa_naturalquestions-train-23836", "mrqa_naturalquestions-train-26907", "mrqa_naturalquestions-train-88162", "mrqa_naturalquestions-train-56664", "mrqa_naturalquestions-train-70392", "mrqa_naturalquestions-train-45772", "mrqa_naturalquestions-train-72094", "mrqa_naturalquestions-train-38100", "mrqa_naturalquestions-train-64283", "mrqa_naturalquestions-train-20559", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-663", "mrqa_hotpotqa-validation-5872", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-7089", "mrqa_triviaqa-validation-5893", "mrqa_hotpotqa-validation-5872", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-5893", "mrqa_naturalquestions-validation-7089"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 70, "before_eval": {"predictions": ["paratroopers", "The New York and New Jersey campaign", "The 36th season, Survivor : Ghost Island, will premiere on February 28, 2018", "his hotel room", "Ford's Victorian plants", "cobbler", "Eadred Lulisc", "1972", "2015 is a sequel to the 2013 feature film \"Frozen Fever\" with music and lyrics by Kristen Anderson-Lopez and Robert Lopez and performed throughout most of the short.", "a loanword of the Visigothic word guma `` man ''", "Major General Miles Francis Stapleton Fitzalan-Howard, 17th Duke of Norfolk", "the Hebrew Alephbet", "Australian and New Zealand", "iron and nickel", "blue", "Cape Cod", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Indian Standard Time is calculated on the basis of 82.30'E longitude, in Mirzapur, Uttar Pradesh, which is nearly on the corresponding longitude reference line", "a \"theo-democracy\" based on the principles of: tawhid (unity of God) risala (prophethood) and khilafa (caliphate)", "Salman Khan", "Skip Tyler", "the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "WBMA-LD", "two Nobel Peace Prizes", "whiskey", "a minute particle would break off the cathode, pass out of the tube, and physically strike him", "authorized version", "thirty articles affirming an individual's rights which, although not legally binding in themselves, have been elaborated in subsequent international treaties, economic transfers, regional human rights instruments, national constitutions, and other laws", "World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines", "Manning", "International Imitation Hemingway Competition", "Kingsford, Michigan"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31577704219688707}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.0, 1.0, 0.07692307692307693, 0.2857142857142857, 0.7499999999999999, 0.6666666666666666, 0.5333333333333333, 0.0, 0.0, 0.0, 0.2105263157894737, 0.3448275862068965, 0.35294117647058826, 0.0, 1.0, 0.08, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2105263157894737, 0.0, 0.0, 0.28571428571428575, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-831", "mrqa_naturalquestions-validation-2083", "mrqa_squad-validation-2980", "mrqa_triviaqa-validation-5981", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-259", "mrqa_naturalquestions-validation-3019", "mrqa_hotpotqa-validation-5334", "mrqa_triviaqa-validation-2331", "mrqa_naturalquestions-validation-4567", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-4323", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-4891", "mrqa_squad-validation-9661", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-4211", "mrqa_triviaqa-validation-1408", "mrqa_squad-validation-1389", "mrqa_triviaqa-validation-2546", "mrqa_naturalquestions-validation-7938", "mrqa_squad-validation-10477", "mrqa_squad-validation-361", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-2229"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 71, "before_eval": {"predictions": ["oil magnate and wealthiest man in history John D. Rockefeller", "29 days", "electrical, water, sewage, phone, and cable facilities", "Saxe-Coburg and Gotha", "16,000", "no known case of any U.S. citizens buying Canadian drugs", "Dwight D. Eisenhower", "multi-purpose", "Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "twelve", "Somme", "Randal Keith Orton", "June Diane Raphael as Brianna Hanson, older daughter of Grace and Robert, head of the company founded and once run by Grace", "a liquid", "Sunday", "October 16, 2012", "Austria", "2012", "alpha efferent neurons", "whitetail deer, reindeer, elk, moose, mule deer, blacktail deer and caribou", "David Irving", "cabbage", "rock and roll and rockabilly", "Hongwu Emperor of the Ming Dynasty", "heptathlon", "the black core of pencils is still referred to as lead, even though it never contained the element lead", "rapid expansion in telecommunication and financial activity", "parliaments", "Guy Pemberton", "Libertarianism", "london", "R&B"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2942231379731379}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.4615384615384615, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.38095238095238093, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4]}}, "error_ids": ["mrqa_squad-validation-7849", "mrqa_hotpotqa-validation-253", "mrqa_hotpotqa-validation-862", "mrqa_squad-validation-4344", "mrqa_squad-validation-6383", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5619", "mrqa_naturalquestions-validation-2635", "mrqa_triviaqa-validation-6938", "mrqa_hotpotqa-validation-583", "mrqa_naturalquestions-validation-8136", "mrqa_squad-validation-3689", "mrqa_hotpotqa-validation-5203", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2548", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-4291", "mrqa_triviaqa-validation-1185", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-2477", "mrqa_triviaqa-validation-1640", "mrqa_naturalquestions-validation-2847", "mrqa_hotpotqa-validation-1694", "mrqa_triviaqa-validation-4464", "mrqa_hotpotqa-validation-2866"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 72, "before_eval": {"predictions": ["Monk's Caf\u00e9", "a pagan custom, namely, the winter solstice which in Europe occurs in December", "fourth- ranking Republican leader in the House", "Kristian Olaf Bernhard Birkeland", "no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem", "Operation Neptune", "john f kenzie", "Patrick Moore", "Animal fibers", "Muskogean", "Symbolic interactionism", "Long Island", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "Mark, Kevin Costner's brother-in-law", "snow and rain", "Parliament", "4 School of Public Health in the country", "henry", "heat loss", "1883\u201384", "yellow or yellow-green", "Les Surfs as `` A Pr\u00e9sent Tu Peux t'en Aller''' French also recorded", "fox", "Macau Peninsula, Macau", "De novo", "morrissey", "Boeing 747", "satirical erotic romantic comedy film directed by Michael Lehmann, written by Rob Perez and starring Josh Hartnett, Shannyn Sossamon and Paulo Costanzo", "Greg", "eighteenth century", "Puente Hills Mall, located in the City of Industry, California, United States", "l'Arc de triomphe"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1943015318015318}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.4, 0.09999999999999999, 0.7272727272727272, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.125, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.4, 0.33333333333333337, 0.0, 0.0, 0.3076923076923077, 0.0, 0.2857142857142857, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-4815", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-1110", "mrqa_squad-validation-1760", "mrqa_hotpotqa-validation-4061", "mrqa_triviaqa-validation-6606", "mrqa_triviaqa-validation-5439", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-946", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-1359", "mrqa_hotpotqa-validation-5478", "mrqa_squad-validation-3592", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-417", "mrqa_triviaqa-validation-3799", "mrqa_squad-validation-9855", "mrqa_triviaqa-validation-4730", "mrqa_naturalquestions-validation-3638", "mrqa_triviaqa-validation-5393", "mrqa_hotpotqa-validation-1394", "mrqa_squad-validation-8829", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-1162", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1858", "mrqa_triviaqa-validation-7377"], "retrieved_ids": ["mrqa_naturalquestions-train-73540", "mrqa_naturalquestions-train-59434", "mrqa_naturalquestions-train-87250", "mrqa_naturalquestions-train-59894", "mrqa_naturalquestions-train-86273", "mrqa_naturalquestions-train-58105", "mrqa_naturalquestions-train-62011", "mrqa_naturalquestions-train-37216", "mrqa_naturalquestions-train-43060", "mrqa_naturalquestions-train-86686", "mrqa_naturalquestions-train-73582", "mrqa_naturalquestions-train-56065", "mrqa_naturalquestions-train-40875", "mrqa_naturalquestions-train-28284", "mrqa_naturalquestions-train-24337", "mrqa_naturalquestions-train-52829", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-5981", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-5203", "mrqa_triviaqa-validation-5981", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-663", "mrqa_hotpotqa-validation-1381", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-2157", "mrqa_naturalquestions-validation-2083", "mrqa_triviaqa-validation-672", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-3019"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 73, "before_eval": {"predictions": ["northwest", "ishmael", "light", "Boston", "the \"Negro Cavalry\" by the Native American tribes they fought in the Indian Wars", "8 km", "The Future", "1969", "epic historical drama", "Saudi-interpretation", "buzzards", "many practice areas of pharmacy, however, they may also work in information technology departments or for healthcare information technology vendor companies", "the first set of endosymbiotic events", "Danish pronunciation : ( \u02c8\u0251n\u0250sn\u0329 )", "Eddie Collins", "2014", "when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another", "increased productivity", "Purple Rain", "the 1890s", "in 1958 and 1979", "political geographer", "200\u2013300", "his mind", "extended fluids, differences in pressure result in forces being directed along the pressure gradients", "an alternate, but rarely used unit of mass", "the \"Polovetskie plyaski\"", "royal charter", "the government - owned Panama Canal Authority", "Vernier, Switzerland", "harmonica", "\"The Today Show\""], "metric_results": {"EM": 0.3125, "QA-F1": 0.40052782865282865}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.3333333333333333, 0.2222222222222222, 0.14814814814814814, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.3846153846153846, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4967", "mrqa_triviaqa-validation-56", "mrqa_hotpotqa-validation-3483", "mrqa_triviaqa-validation-7082", "mrqa_squad-validation-6388", "mrqa_naturalquestions-validation-9818", "mrqa_hotpotqa-validation-1181", "mrqa_naturalquestions-validation-2069", "mrqa_squad-validation-9871", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9723", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-2567", "mrqa_squad-validation-6550", "mrqa_squad-validation-1510", "mrqa_squad-validation-10430", "mrqa_squad-validation-10455", "mrqa_hotpotqa-validation-4284", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-3407"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 74, "before_eval": {"predictions": ["an warm and swift Atlantic ocean current", "Friedrich Nietzsche", "wbur", "Anglo-Frisian languages group", "waltham Forest borough", "one of Nazi Germany's finest airships", "paul white", "DeWayne Warren", "100 ft", "hard Candy", "George Merrill and Shannon Rubicam", "Santa Fe, New Mexico", "1943", "translation", "wai Momi", "a \"consulting fee\"", "student tuition, endowments, scholarship/voucher funds, and donations and grants", "tolerance of civil disobedience", "ed Miliband", "China", "written by ghost writers, nonfiction books on military subjects, and video games", "British and French Canadian fur traders from before 1810, and American settlers from the mid-1830s, with its coastal areas north from the Columbia River frequented by ships from all nations engaged in the maritime fur trade", "Instagram", "John Sebastian Bach, Karlheinz Stockhausen, Terry Riley, Philip Glass and Moondog", "1986", "beauty", "1922 July 1930", "classically transmit the nuclear force", "written each article of the Creed to express the character of the Father, the Son, or the Holy Spirit", "April 13, 2018", "\"Gosford Park\"", "an American business magnate, investor, and philanthropist"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24025735294117648}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5882352941176471, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 0.1142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.2857142857142857]}}, "error_ids": ["mrqa_naturalquestions-validation-8072", "mrqa_hotpotqa-validation-5520", "mrqa_triviaqa-validation-6002", "mrqa_hotpotqa-validation-2098", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6573", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-4275", "mrqa_naturalquestions-validation-8008", "mrqa_naturalquestions-validation-7462", "mrqa_triviaqa-validation-2938", "mrqa_squad-validation-1462", "mrqa_squad-validation-7086", "mrqa_naturalquestions-validation-642", "mrqa_hotpotqa-validation-2220", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1027", "mrqa_hotpotqa-validation-726", "mrqa_naturalquestions-validation-9487", "mrqa_squad-validation-801", "mrqa_hotpotqa-validation-4964", "mrqa_squad-validation-10447", "mrqa_squad-validation-2391", "mrqa_naturalquestions-validation-177", "mrqa_triviaqa-validation-5108", "mrqa_hotpotqa-validation-83"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 75, "before_eval": {"predictions": ["December 1974", "Jacques Gentilhatre and Antonio Visentini", "Jimmy Page and Eric Clapton", "Gaahl", "three", "armored fighting vehicle", "bannockburn", "author Studs Terkel, American writer, essayist, filmmaker, teacher, and political activist Susan Sontag, analytic philosopher and Stanford University Professor of Comparative Literature Richard Rorty", "bahr", "1902", "Statue of Freedom", "1963", "23 November 1963", "30", "Limbo", "Assam", "350 government officials and climate change experts", "piped masonry", "vietish", "comic", "Environmental Protection Agency", "523", "tube", "Cher", "one person", "Luger P08", "Al-Masjid an-Nabawi", "Akbar the Great", "laundry", "Fall migration usually begins in mid-August and continues through mid-September", "unmanned", "Scandinavian Airlines"], "metric_results": {"EM": 0.21875, "QA-F1": 0.29920447727559796}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.06896551724137931, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8685", "mrqa_squad-validation-5628", "mrqa_hotpotqa-validation-551", "mrqa_hotpotqa-validation-3306", "mrqa_triviaqa-validation-1217", "mrqa_naturalquestions-validation-9426", "mrqa_triviaqa-validation-6051", "mrqa_squad-validation-8035", "mrqa_triviaqa-validation-5878", "mrqa_squad-validation-683", "mrqa_naturalquestions-validation-9546", "mrqa_squad-validation-8525", "mrqa_naturalquestions-validation-9576", "mrqa_triviaqa-validation-2879", "mrqa_naturalquestions-validation-3347", "mrqa_hotpotqa-validation-1298", "mrqa_squad-validation-903", "mrqa_triviaqa-validation-1205", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4413", "mrqa_naturalquestions-validation-6482", "mrqa_triviaqa-validation-1199", "mrqa_naturalquestions-validation-2006", "mrqa_squad-validation-3775", "mrqa_hotpotqa-validation-2646"], "retrieved_ids": ["mrqa_naturalquestions-train-34419", "mrqa_naturalquestions-train-19895", "mrqa_naturalquestions-train-33492", "mrqa_naturalquestions-train-9203", "mrqa_naturalquestions-train-86867", "mrqa_naturalquestions-train-78006", "mrqa_naturalquestions-train-44190", "mrqa_naturalquestions-train-61549", "mrqa_naturalquestions-train-66230", "mrqa_naturalquestions-train-71702", "mrqa_naturalquestions-train-7582", "mrqa_naturalquestions-train-50260", "mrqa_naturalquestions-train-12965", "mrqa_naturalquestions-train-13622", "mrqa_naturalquestions-train-53831", "mrqa_naturalquestions-train-40445", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7189", "mrqa_naturalquestions-validation-9487", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-7189", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-726", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6573", "mrqa_squad-validation-801", "mrqa_triviaqa-validation-6002", "mrqa_squad-validation-801", "mrqa_hotpotqa-validation-83"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 76, "before_eval": {"predictions": ["Tampa Bay defensive end Simeon Rice sacked Raiders quarterback Rich Gannon on third down, forcing Oakland to settle for kicker Sebastian Janikowski's 40 - yard field goal to give them a 3 -- 0 lead", "2007", "Neil Young", "novelization of the 1977 film Star Wars", "May 27, 2016", "Back-to-Africa movement", "directly elected", "flowers", "\"Naked\" (for which he won the Best Actor award at Cannes Film Festival)", "Conrad Lewis", "2017", "Estero, Florida", "2010", "Medicare", "Christian Goldbach", "Grease", "Netrobalane canopus", "federal government's nearly 700 million acres ( 2,800,000 km ) of subsurface mineral estate located beneath federal, state and private lands severed from their surface rights by the Homestead Act of 1862", "be reborn, often many times, in order to continue their Bodhisattva vow is called a Tulku", "Bhpppavageete or Bhavageeth", "Tachycardia, also called tachyarrhythmia", "No\u00ebvadius DeMun Wilburn", "Richard Burbage", "blood poisoning", "Parliament of the United Kingdom", "achievement-oriented motivations", "discarded", "A Song of Ice and Fire", "alain Whyte", "Julia McKenzie and Anton Rodgers", "15 February 1998", "Amazon.com"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23661434331797235}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06451612903225806, 0.33333333333333337, 0.0, 0.28571428571428575, 0.0, 0.25, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-7535", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-5027", "mrqa_naturalquestions-validation-2102", "mrqa_triviaqa-validation-4440", "mrqa_hotpotqa-validation-2786", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-53", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-1314", "mrqa_naturalquestions-validation-6258", "mrqa_squad-validation-9014", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-2550", "mrqa_naturalquestions-validation-6027", "mrqa_squad-validation-1927", "mrqa_hotpotqa-validation-4510", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-2270", "mrqa_squad-validation-5031", "mrqa_squad-validation-9547", "mrqa_triviaqa-validation-7099", "mrqa_triviaqa-validation-5849", "mrqa_naturalquestions-validation-9591"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 77, "before_eval": {"predictions": ["New York Knickerbockers", "400-meter freestyle", "genetic branches", "east-west", "four", "new zealand", "at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "lofsongur", "islay", "troposphere", "Lesser Antilles", "The Fixx", "67,038", "The Swiss Express", "Scholars and observers who don't believe that Islam is merely a political ideology include Fred Halliday, John Esposito and Muslim intellectuals like Javed Ahmad Ghamidi", "handwriting", "the highest quotient", "William Adelin", "SEVENTEEN or SVT", "Jack", "at the conclusion of play", "most northerly of the five major circles of latitude", "The show focuses on the original and founding ( `` mother '' ) charter, Sons of Anarchy Motorcycle Club, Redwood Original, referred to by the acronym SAMCRO or Sam Crow", "flowers", "Great Yuan", "Song Il-gon", "1974", "new SUV", "hypersensitivity response of plants against pathogen attack", "food", "The Emperor of Japan", "naturally produced in the human body from the amino acids glycine and arginine"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5022545734136291}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.23529411764705882, 0.7272727272727273, 0.7894736842105263, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.16666666666666669]}}, "error_ids": ["mrqa_triviaqa-validation-6483", "mrqa_triviaqa-validation-3900", "mrqa_naturalquestions-validation-6452", "mrqa_triviaqa-validation-593", "mrqa_triviaqa-validation-2095", "mrqa_naturalquestions-validation-8584", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-1921", "mrqa_squad-validation-9520", "mrqa_squad-validation-9530", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4766", "mrqa_naturalquestions-validation-4475", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7845", "mrqa_triviaqa-validation-1780", "mrqa_squad-validation-8046", "mrqa_triviaqa-validation-7610", "mrqa_squad-validation-3543", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-686"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 78, "before_eval": {"predictions": ["2005", "healthcare professionals with specialised education and training who perform various roles to ensure optimal health outcomes for their patients through the quality use of medicines", "fox", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "neuro-orthopaedic Irish veterinary surgeon", "criminality", "Anne of Green Gables", "synovial joint", "jazz", "heineken", "East End of London", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "1484", "T cells ( for cell - mediated, cytotoxic adaptive immunity )", "Electronic sound sources, live instrumental playing and digital signal processing", "1814", "( latent ) heat of fusion, is the change in its enthalpy resulting from providing energy, typically heat, to a specific quantity of the substance to change its state from a solid to a liquid", "Chinese", "Gap", "The Washington Post", "Jurchen Aisin Gioro clan in Manchuria", "mortadella", "an expedition through the wilderness of the Maine district and down the Chaudi\u00e8re River to attack the city of Quebec", "friendship and reunion, saying that there can only be one sun in the sky, and he asked for a noble death", "874.3 square miles", "840, and John Stockton leads the points - assists combination with 714", "DTIME(n2)", "the allegedly corrupt machinations of Fran\u00e7ois Bigot", "loud and dirty as possible", "don't tell my heart", "chocolate confectionery"], "metric_results": {"EM": 0.0625, "QA-F1": 0.22310442607032366}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.3448275862068966, 0.0, 0.35294117647058826, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 1.0, 0.2, 0.26666666666666666, 0.0, 0.35000000000000003, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.1212121212121212, 0.1, 0.0, 0.888888888888889, 0.0, 1.0, 0.9090909090909091, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_squad-validation-6280", "mrqa_triviaqa-validation-1117", "mrqa_naturalquestions-validation-5838", "mrqa_hotpotqa-validation-1219", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-4063", "mrqa_triviaqa-validation-1168", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-5011", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6497", "mrqa_naturalquestions-validation-9342", "mrqa_hotpotqa-validation-1296", "mrqa_squad-validation-9445", "mrqa_naturalquestions-validation-1119", "mrqa_squad-validation-8083", "mrqa_squad-validation-421", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-1916", "mrqa_squad-validation-10239", "mrqa_squad-validation-6115", "mrqa_squad-validation-7476", "mrqa_naturalquestions-validation-276", "mrqa_squad-validation-1807", "mrqa_hotpotqa-validation-2437", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-1465"], "retrieved_ids": ["mrqa_naturalquestions-train-27744", "mrqa_naturalquestions-train-86299", "mrqa_naturalquestions-train-82087", "mrqa_naturalquestions-train-9377", "mrqa_naturalquestions-train-41300", "mrqa_naturalquestions-train-85579", "mrqa_naturalquestions-train-52704", "mrqa_naturalquestions-train-21243", "mrqa_naturalquestions-train-65428", "mrqa_naturalquestions-train-68792", "mrqa_naturalquestions-train-45090", "mrqa_naturalquestions-train-30845", "mrqa_naturalquestions-train-11908", "mrqa_naturalquestions-train-1426", "mrqa_naturalquestions-train-64689", "mrqa_naturalquestions-train-79114", "mrqa_hotpotqa-validation-1298", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-9591", "mrqa_hotpotqa-validation-1314", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-2102", "mrqa_hotpotqa-validation-3337", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-5849", "mrqa_naturalquestions-validation-9591", "mrqa_squad-validation-9855"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 79, "before_eval": {"predictions": ["Sir Hiram Stevens Maxim", "1987", "The Forever People", "make direct representations to the Presiding Officer to nominate speakers", "Chesley Burnett \"Sully\" Sullenberger III", "October 12, 2017", "an allegiance oath that must be taken by all immigrants who wish to become United States citizens", "Kevin Whately", "shirley Rush", "pixilation or pixilate animation", "Iowa", "September of that year", "precedes the value", "Industrial Revolution", "The photon sphere is a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "porcino", "Chicago Cubs", "rugged terrain such as the Arctic", "Mexico\u2013united States border", "Denver Broncos", "a ribosome in the cytosol", "Oyster Bay", "Maritza Petrovi\u0107", "le Leicester", "gas turbines", "either by voting or voice vote", "Calendar for Fixing the Seasons", "16 %", "july", "Secretary of Defense", "pancake-shaped circular disks about 300\u2013600 nanometers in diameter", "thirteen American colonies, then at war with the Kingdom of Great Britain"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31119227994227994}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.3636363636363636, 0.33333333333333337, 1.0, 0.35714285714285715, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4516", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-7733", "mrqa_squad-validation-9407", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-1672", "mrqa_triviaqa-validation-7178", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-2846", "mrqa_triviaqa-validation-3844", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-4529", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-5589", "mrqa_squad-validation-3738", "mrqa_squad-validation-1", "mrqa_squad-validation-8960", "mrqa_hotpotqa-validation-701", "mrqa_triviaqa-validation-5529", "mrqa_squad-validation-3369", "mrqa_naturalquestions-validation-3591", "mrqa_squad-validation-8233", "mrqa_naturalquestions-validation-8509", "mrqa_triviaqa-validation-1912", "mrqa_squad-validation-8811", "mrqa_naturalquestions-validation-360"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 80, "before_eval": {"predictions": ["John Mills", "new jersey", "Piet van der Walt", "Sachin Tendulkar", "geoffrey", "Terre Haute", "Tangmere, West Sussex", "the price had also remained fairly stable versus other currencies and commodities", "l Luxembourg", "State Bar of Arizona", "skeletal muscles", "beer", "sunlight", "the end of its life depends on the mass it was born with", "discus fish", "1967", "Mondays at 21:30 (KST) beginning November 17, 2014", "22 miles", "1598", "the coastline peninsula of Davenports Neck called \"Bauffet's Point\"", "geoffrey", "national Front", "Topeka", "500", "the rochdale shirley", "hijab", "1894", "the evidence for these words to be unreliable, since they were inserted before \"May God help me\" only in later versions of the speech and not recorded in witness accounts of the proceedings", "`` Nelson's Sparrow ''", "coldplay", "phowa", "secondary law"], "metric_results": {"EM": 0.125, "QA-F1": 0.2460279304029304}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.4, 0.2857142857142857, 0.4, 0.0, 0.5, 0.4444444444444445]}}, "error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_triviaqa-validation-6493", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2148", "mrqa_triviaqa-validation-6063", "mrqa_hotpotqa-validation-3056", "mrqa_hotpotqa-validation-3282", "mrqa_squad-validation-3699", "mrqa_triviaqa-validation-7358", "mrqa_naturalquestions-validation-3092", "mrqa_naturalquestions-validation-7848", "mrqa_hotpotqa-validation-3779", "mrqa_squad-validation-3570", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-1261", "mrqa_hotpotqa-validation-2640", "mrqa_squad-validation-3038", "mrqa_squad-validation-3318", "mrqa_triviaqa-validation-1100", "mrqa_hotpotqa-validation-2840", "mrqa_squad-validation-10182", "mrqa_triviaqa-validation-5904", "mrqa_squad-validation-4975", "mrqa_squad-validation-2223", "mrqa_naturalquestions-validation-3822", "mrqa_triviaqa-validation-6129", "mrqa_squad-validation-1930", "mrqa_squad-validation-4042"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 81, "before_eval": {"predictions": ["one rotation of the crank and two piston strokes", "Super Bowl L", "the series has also introduced new recurring aliens: Slitheen (Raxacoricofallapatorian) Ood, Judoon, Weeping Angels and the Silence in the 2009\u201310 Specials", "an adviser to churches in new territories", "USD 5.2 billion", "A rear - view mirror ( or rearview mirror ) is a mirror in automobiles and other vehicles, designed to allow the driver to see rearward through the vehicle's rear window ( rear windshield )", "The Birds", "identify rock samples in the laboratory", "French & Saunders", "the European Parliament and the Council of the European Union", "the Beatles song \"A Day in the Life\"", "on either February 28 or March 1, while others only observe birthdays on the authentic intercalary date, February 29", "Thomas Edison", "duo in 2003 by the rhythm section of the band Sleep, OM is currently a trio", "their defeat by the Swabian League at the Battle of Frankenhausen on 15 May 1525", "red", "a new entrance building", "Barack Obama re-elected as US president", "caribbean", "barrel of cask-conditioned beer", "construction service firms (e.g. engineering, architecture) and construction managers (firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project)", "paraiso", "`` Feed Jake '' is a song written by Danny `` Bear '' Mayo, and recorded by the American country music band Pirates of the Mississippi", "several hundred thousand", "photoelectric effect", "Noel Gallagher", "Max West", "the Midlands and the South West", "aluminium", "newspapers, television, radio, cable television, and other businesses", "Lakshmibai", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3089147020435053}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [0.2222222222222222, 0.0, 0.09999999999999999, 0.2857142857142857, 0.375, 0.20689655172413793, 0.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.125, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5294117647058824, 0.0, 0.5384615384615384, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.7692307692307693, 0.4, 1.0]}}, "error_ids": ["mrqa_squad-validation-3267", "mrqa_squad-validation-7", "mrqa_squad-validation-7728", "mrqa_squad-validation-2481", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-8591", "mrqa_hotpotqa-validation-1949", "mrqa_squad-validation-5055", "mrqa_triviaqa-validation-1724", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-5053", "mrqa_hotpotqa-validation-1701", "mrqa_squad-validation-2287", "mrqa_triviaqa-validation-4596", "mrqa_squad-validation-5447", "mrqa_triviaqa-validation-1305", "mrqa_triviaqa-validation-5598", "mrqa_triviaqa-validation-2632", "mrqa_squad-validation-6764", "mrqa_triviaqa-validation-1476", "mrqa_naturalquestions-validation-7838", "mrqa_squad-validation-914", "mrqa_hotpotqa-validation-3695", "mrqa_squad-validation-5288", "mrqa_triviaqa-validation-5741", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-1664"], "retrieved_ids": ["mrqa_naturalquestions-train-87754", "mrqa_naturalquestions-train-41777", "mrqa_naturalquestions-train-27493", "mrqa_naturalquestions-train-15765", "mrqa_naturalquestions-train-68768", "mrqa_naturalquestions-train-63508", "mrqa_naturalquestions-train-65734", "mrqa_naturalquestions-train-85041", "mrqa_naturalquestions-train-13197", "mrqa_naturalquestions-train-84933", "mrqa_naturalquestions-train-36031", "mrqa_naturalquestions-train-52189", "mrqa_naturalquestions-train-15421", "mrqa_naturalquestions-train-11976", "mrqa_naturalquestions-train-30881", "mrqa_naturalquestions-train-43043", "mrqa_squad-validation-1", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2640", "mrqa_squad-validation-6115", "mrqa_squad-validation-3738", "mrqa_squad-validation-9445", "mrqa_hotpotqa-validation-2640", "mrqa_squad-validation-3038", "mrqa_naturalquestions-validation-9184", "mrqa_squad-validation-683", "mrqa_squad-validation-4975", "mrqa_squad-validation-3738", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-9184", "mrqa_squad-validation-3038", "mrqa_squad-validation-9407"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 82, "before_eval": {"predictions": ["robert mor Cliffs", "Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects", "hands and face", "24 hours later", "April 20, 1945", "dachshund racing championships", "A consortium led by the International Crops Research Institute for the Semi-Arid Tropics", "geologic", "129", "Bed and breakfast", "shoes", "a growing sport in southern California, particularly at the high school level, with increasing numbers of schools adding rugby as an official school sport", "geese", "Jamukha", "tentacles and prey", "shirley", "Tagore", "Jason Ravnsborg", "Infiltration", "9.9", "film and short novels", "American-Canadian", "the films \"Play Misty for Me\" and \" Grand Prix\"", "Gardnerville", "Nucleotides", "25-minute episodes", "Wiki", "Zapatista Army of National Liberation", "Vikram Bhatt, Bhushan Patel and Tinu Suresh Desai", "paris", "2003", "Dead Sea"], "metric_results": {"EM": 0.15625, "QA-F1": 0.33149485930735934}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.16, 0.5, 0.8, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.2857142857142857, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 0.33333333333333337, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-2133", "mrqa_squad-validation-10390", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-215", "mrqa_hotpotqa-validation-4153", "mrqa_hotpotqa-validation-4420", "mrqa_squad-validation-8322", "mrqa_squad-validation-2791", "mrqa_squad-validation-9603", "mrqa_triviaqa-validation-5592", "mrqa_squad-validation-2748", "mrqa_triviaqa-validation-1053", "mrqa_squad-validation-6113", "mrqa_squad-validation-4616", "mrqa_triviaqa-validation-1685", "mrqa_naturalquestions-validation-8544", "mrqa_naturalquestions-validation-746", "mrqa_triviaqa-validation-863", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2361", "mrqa_hotpotqa-validation-4493", "mrqa_squad-validation-7708", "mrqa_hotpotqa-validation-1639", "mrqa_triviaqa-validation-2236", "mrqa_hotpotqa-validation-164", "mrqa_triviaqa-validation-2592", "mrqa_naturalquestions-validation-9117"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 83, "before_eval": {"predictions": ["british", "North Africa", "a young girl", "architecture from the gothic, renaissance, baroque and neoclassical periods", "at angles less than vertical", "the aboral organ", "piotr naskrecki", "Bob Cratchit", "Los Angeles", "Cushman", "Kent, particularly Sandwich, Faversham and Maidstone", "kingdoms of Francia on the Lower Rhine, Burgundy on the Upper Rhine and Alemannia", "S\u00fcleyman-\u0131 Evvel", "3", "best director", "difficult and intricate topics", "email", "ABC Radio", "Tahiti", "detritus from the settlement of the sedimentation", "\"Universal R\"", "Matthew 14:22-36", "Steveston Outdoor pool in Richmond, BC", "Nikola Tesla", "Terry the Tomboy", "redbird", "Australia", "dimensionless", "approximately a `` drive - through '' or `` stop and go '' penalty", "North Atlantic Conference", "on location at the group's final British performance on 14 July 2012 at the National Bowl in Milton Keynes", "british"], "metric_results": {"EM": 0.25, "QA-F1": 0.32951007326007326}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.33333333333333337, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9142857142857143, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3493", "mrqa_squad-validation-9912", "mrqa_naturalquestions-validation-1805", "mrqa_squad-validation-920", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-3113", "mrqa_triviaqa-validation-271", "mrqa_squad-validation-9349", "mrqa_hotpotqa-validation-1312", "mrqa_naturalquestions-validation-5214", "mrqa_triviaqa-validation-5960", "mrqa_hotpotqa-validation-2964", "mrqa_triviaqa-validation-90", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-1798", "mrqa_hotpotqa-validation-4966", "mrqa_triviaqa-validation-3987", "mrqa_naturalquestions-validation-7172", "mrqa_squad-validation-1159", "mrqa_triviaqa-validation-570", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-8518", "mrqa_triviaqa-validation-5793"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 84, "before_eval": {"predictions": ["music", "13 years and 48 days", "Major General Nathan Bedford Forrest", "Theodosius I", "oppidum Ubiorum", "Montreal Canadiens defeat the Ottawa Senators 7 - 4, with Joe Malone scoring five of Montreal's seven goals", "brixton", "60", "Assistant Secretary for Operations", "better fuel economy", "september", "evacuate the cylinder, choking it and giving excessive compression (\"kick back\"", "scotland", "glial cells", "Carson City", "biologist", "Egyptians", "christ church", "Poems : Series 1", "United States ambassador to Ghana and to Czechoslovakia and also served as Chief of Protocol of the United States", "Golden State Warriors", "Emma Thompson and Alice Eve", "edible - nest swiftlets using solidified saliva, which are harvested for human consumption", "christ", "travel literature, cartography, geography, and scientific education", "ruler", "priest", "chiang Kai-Shek", "125 lb", "cephalic", "Lexus", "a prison"], "metric_results": {"EM": 0.21875, "QA-F1": 0.36155525846702313}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.11764705882352941, 0.6666666666666666, 0.0, 0.28571428571428575, 0.8571428571428571, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.24000000000000002, 0.2857142857142857, 0.0, 0.5714285714285715, 0.5454545454545454, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-370", "mrqa_naturalquestions-validation-1147", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-6794", "mrqa_triviaqa-validation-2014", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-1628", "mrqa_hotpotqa-validation-4414", "mrqa_triviaqa-validation-5687", "mrqa_squad-validation-3364", "mrqa_triviaqa-validation-1529", "mrqa_naturalquestions-validation-3368", "mrqa_triviaqa-validation-3538", "mrqa_naturalquestions-validation-10461", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-661", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-8660", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1645", "mrqa_hotpotqa-validation-989", "mrqa_triviaqa-validation-159", "mrqa_hotpotqa-validation-1810", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-6660"], "retrieved_ids": ["mrqa_naturalquestions-train-50484", "mrqa_naturalquestions-train-64555", "mrqa_naturalquestions-train-22182", "mrqa_naturalquestions-train-26754", "mrqa_naturalquestions-train-6755", "mrqa_naturalquestions-train-81435", "mrqa_naturalquestions-train-75550", "mrqa_naturalquestions-train-32566", "mrqa_naturalquestions-train-57658", "mrqa_naturalquestions-train-31019", "mrqa_naturalquestions-train-44480", "mrqa_naturalquestions-train-76196", "mrqa_naturalquestions-train-85492", "mrqa_naturalquestions-train-37517", "mrqa_naturalquestions-train-19651", "mrqa_naturalquestions-train-62071", "mrqa_naturalquestions-validation-579", "mrqa_hotpotqa-validation-4153", "mrqa_naturalquestions-validation-5214", "mrqa_squad-validation-9912", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-5214", "mrqa_hotpotqa-validation-4102", "mrqa_squad-validation-9603", "mrqa_squad-validation-9349", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4153", "mrqa_naturalquestions-validation-5214", "mrqa_squad-validation-9912", "mrqa_squad-validation-4616", "mrqa_naturalquestions-validation-579"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 85, "before_eval": {"predictions": ["Europe", "small", "Stevie Young", "fall", "3000 metres", "October 13, 1980", "1765", "mutiny", "heart", "napkin", "meats", "vertebral column ( spine )", "he did not consider the papacy part of the biblical Church", "1837", "November 27, 2017", "Bambi, a Life in the Woods", "inefficiency", "12\u201318", "the Lightning connector", "on the equator and overlies the East African Rift covering a diverse and expansive terrain that extends roughly from Lake Victoria to Lake Turkana ( formerly called Lake Rudolf) and further south-east to the Indian Ocean", "school", "guinea", "pemberley", "lactobacilli", "a school or other place of formal education", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "victorate", "David Hatcher Childress", "EE", "during the 1980s", "Lotus", "a dose of 200 to 500 mg up to 7 ml"], "metric_results": {"EM": 0.25, "QA-F1": 0.3086805555555555}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.2, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669]}}, "error_ids": ["mrqa_naturalquestions-validation-10601", "mrqa_hotpotqa-validation-4719", "mrqa_naturalquestions-validation-3515", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-7425", "mrqa_naturalquestions-validation-2023", "mrqa_triviaqa-validation-710", "mrqa_squad-validation-2267", "mrqa_naturalquestions-validation-7513", "mrqa_hotpotqa-validation-1506", "mrqa_naturalquestions-validation-2883", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2157", "mrqa_squad-validation-8266", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-4988", "mrqa_squad-validation-1891", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-4854", "mrqa_squad-validation-1566", "mrqa_triviaqa-validation-2508", "mrqa_naturalquestions-validation-2907", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 86, "before_eval": {"predictions": ["Native Americans", "thammasat university", "South Sentinel Island", "FCA US LLC", "Aaron Taylor- Johnson", "jay-Z", "mexico", "a gift", "Conservative", "Peter Davison, Colin Baker, Sylvester McCoy and Paul McGann", "Islam", "2015", "maquiladora", "Bigger Than Both of Us", "cells", "Bad Blood", "a problem", "1787", "WTVG", "Great Lakes", "temperatures and sea levels have been rising at or above the maximum rates proposed during the last IPCC report in 2001", "Judaism", "film playback singer", "southern whites", "Marie", "L", "Commonwealth Universities", "drawing letters in the air ( `` penciling '' )", "self-consistent unification", "the Allstars", "he confirmed he was their author, but requested time to think about the answer to the second question", "August 1, 2016"], "metric_results": {"EM": 0.25, "QA-F1": 0.2935574229691877}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.11764705882352941, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3348", "mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-4230", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5195", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-734", "mrqa_squad-validation-7281", "mrqa_squad-validation-9589", "mrqa_squad-validation-782", "mrqa_naturalquestions-validation-3004", "mrqa_hotpotqa-validation-908", "mrqa_naturalquestions-validation-3704", "mrqa_hotpotqa-validation-4401", "mrqa_naturalquestions-validation-9878", "mrqa_triviaqa-validation-5001", "mrqa_squad-validation-8531", "mrqa_squad-validation-2331", "mrqa_hotpotqa-validation-367", "mrqa_naturalquestions-validation-9516", "mrqa_hotpotqa-validation-5241", "mrqa_naturalquestions-validation-3323", "mrqa_squad-validation-2113", "mrqa_naturalquestions-validation-6912"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 87, "before_eval": {"predictions": ["Paris", "electronic gaming machines", "August 21, 2010", "Mohammad Reza Pahlavi", "digital streams", "Life Savers candy, drugstore chain Rexall and New York City radio station WMCA", "on a sound stage in front of a live audience in Burbank, California", "Chinese", "Due to the controversial and explicit nature of many of their songs", "Meghan Trainor", "carbon cycle", "tenant management", "If the citizen's heart was heavier than a feather", "Donna Noble (Catherine Tate) with Mickey Smith (noel Clarke) and Jack Harkness (Jo Barrowman) recurring as secondary companion figures", "salvaging a country usually seen as one of the most stable and prosperous in Africa", "The Kree", "titanium tetrachloride", "\"Loud\" (2010)", "Kirinyaga, Kirenyaa and Kiinyaa", "gerry mcdonald", "domestic legislation", "actions-oriented", "toothbrush", "state being equally represented by two senators, regardless of its population, serving staggered terms of six years", "sherry", "Thutmose III", "two", "Buck Owens and the Buckaroos", "Kansas", "political support", "clownfish", "all England tennis club"], "metric_results": {"EM": 0.21875, "QA-F1": 0.4128262928194993}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.375, 1.0, 0.4, 0.0, 0.0, 1.0, 0.5, 0.18181818181818182, 0.5714285714285714, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.08695652173913045, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571]}}, "error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-3885", "mrqa_naturalquestions-validation-2196", "mrqa_squad-validation-5648", "mrqa_squad-validation-8344", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-3713", "mrqa_squad-validation-7703", "mrqa_squad-validation-8386", "mrqa_hotpotqa-validation-3692", "mrqa_triviaqa-validation-7649", "mrqa_hotpotqa-validation-3391", "mrqa_triviaqa-validation-6049", "mrqa_squad-validation-9504", "mrqa_triviaqa-validation-3943", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-1282", "mrqa_squad-validation-8552", "mrqa_hotpotqa-validation-5743", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-2814"], "retrieved_ids": ["mrqa_naturalquestions-train-56045", "mrqa_naturalquestions-train-83236", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-68606", "mrqa_naturalquestions-train-53870", "mrqa_naturalquestions-train-20644", "mrqa_naturalquestions-train-32188", "mrqa_naturalquestions-train-34038", "mrqa_naturalquestions-train-50646", "mrqa_naturalquestions-train-84073", "mrqa_naturalquestions-train-85347", "mrqa_naturalquestions-train-12544", "mrqa_naturalquestions-train-50137", "mrqa_naturalquestions-train-27935", "mrqa_naturalquestions-train-49522", "mrqa_naturalquestions-train-18563", "mrqa_squad-validation-2331", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-6912", "mrqa_squad-validation-2331", "mrqa_squad-validation-8266", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10601", "mrqa_squad-validation-8266", "mrqa_squad-validation-782", "mrqa_naturalquestions-validation-6912", "mrqa_squad-validation-7281", "mrqa_hotpotqa-validation-367", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-2907", "mrqa_hotpotqa-validation-661", "mrqa_naturalquestions-validation-3515"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 88, "before_eval": {"predictions": ["Fabio Cannavaro", "Janet Jackson", "The Gold Coast", "not being a civil disobedient", "Selena Gomez", "8th", "Forbes", "159 beats per minute ( bpm )", "abdomen", "lead Zeppelin", "\"public\" (state-controlled) and \"independent\" ( which includes traditional private schools and schools which are privately governed[clarification needed]", "New England", "emmy", "English", "the southwestern United States, Mexico, and Central America", "high risk of a conflict of interest and/or the avoidance of absolute powers", "sacerdotalism", "eventually discontinued", "2,000 km", "44", "Roman-era geography (1st century BC) as Greek \u1fec\u1fc6\u03bd\u03bf\u03c2 ( Rh\u0113nos) Latin Rhenus", "the outflow of cash to shareholders", "emmy", "county council", "Solange Knowles & Destiny's Child", "Ward", "Fryda Wolff", "Eddie Gottlieb Trophy", "Elvis Presley", "Afghans", "reversed", "phycobilisomes"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25148809523809523}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.1, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2848", "mrqa_triviaqa-validation-6166", "mrqa_hotpotqa-validation-3930", "mrqa_naturalquestions-validation-5785", "mrqa_hotpotqa-validation-3343", "mrqa_naturalquestions-validation-10162", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-6636", "mrqa_squad-validation-6823", "mrqa_naturalquestions-validation-4351", "mrqa_triviaqa-validation-4242", "mrqa_hotpotqa-validation-799", "mrqa_naturalquestions-validation-8319", "mrqa_squad-validation-6396", "mrqa_squad-validation-2101", "mrqa_naturalquestions-validation-8965", "mrqa_hotpotqa-validation-2668", "mrqa_squad-validation-9246", "mrqa_naturalquestions-validation-5291", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4364", "mrqa_naturalquestions-validation-2264", "mrqa_squad-validation-843", "mrqa_naturalquestions-validation-8622", "mrqa_hotpotqa-validation-5498", "mrqa_naturalquestions-validation-8095", "mrqa_hotpotqa-validation-828"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 89, "before_eval": {"predictions": ["217", "neither conscientious nor of social benefit", "House of Fraser", "Bonkyll Castle", "four", "the time complexity (or any other complexity measure) of different inputs of the same size", "Deadman's Gun", "m\u00e1laga airport", "Philippines", "Z-ring", "david cameron", "1938", "the 2004 Treaty establishing a Constitution for Europe never came into force", "henry", "California Young Reader Medal", "Louis XVIII", "January 30, 1930", "russia", "swissair", "all U.S. territories except American Samoa", "the Gaussian integers Z[i]", "Janis Joplin with the poets Michael McClure and Bob Neuwirth", "Odinga", "homicides", "river severn", "Bergen County", "August 9, 2017", "mongolia", "a sociological perspective", "USC Trojans", "Ian Hart", "sequential proteolytic activation of complement molecules"], "metric_results": {"EM": 0.25, "QA-F1": 0.35694901315789473}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.37499999999999994, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-803", "mrqa_hotpotqa-validation-1756", "mrqa_triviaqa-validation-3639", "mrqa_squad-validation-1700", "mrqa_naturalquestions-validation-2981", "mrqa_triviaqa-validation-6387", "mrqa_naturalquestions-validation-7598", "mrqa_squad-validation-8898", "mrqa_triviaqa-validation-5020", "mrqa_hotpotqa-validation-2774", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-289", "mrqa_naturalquestions-validation-3269", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-7020", "mrqa_hotpotqa-validation-2831", "mrqa_naturalquestions-validation-9419", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-4515", "mrqa_triviaqa-validation-3643", "mrqa_naturalquestions-validation-1450", "mrqa_squad-validation-2793", "mrqa_squad-validation-6645"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 90, "before_eval": {"predictions": ["a very reactive allotrope of oxygen that is damaging to lung tissue", "reactive allotrope of oxygen", "The Daleks' Master Plan", "The opera was first performed at the Op\u00e9ra-Comique in Paris on 3 March 1875", "phylum", "older than the fault", "mackeswell", "complexity classes", "specific terminology", "venus", "four zonal offices at Chennai, Delhi, Kolkata and Mumbai", "venus", "professor of cognitive science", "The Book of Roger", "time", "photolysis", "1998", "henry", "Grisha Alekandrovich Nikolaev", "cytotoxic", "branch roots", "violet", "Ken Howard", "red", "New England Patriots", "proteins", "A74(M)", "baku", "henry", "Morty", "Catherine Earnshaw", "178 Vivienne Westwood costumes"], "metric_results": {"EM": 0.125, "QA-F1": 0.20625}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.26666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-3496", "mrqa_squad-validation-3497", "mrqa_squad-validation-7657", "mrqa_hotpotqa-validation-2585", "mrqa_squad-validation-4421", "mrqa_squad-validation-4971", "mrqa_triviaqa-validation-6726", "mrqa_squad-validation-6812", "mrqa_triviaqa-validation-2940", "mrqa_naturalquestions-validation-7034", "mrqa_triviaqa-validation-5", "mrqa_squad-validation-1070", "mrqa_squad-validation-10358", "mrqa_naturalquestions-validation-2838", "mrqa_squad-validation-5814", "mrqa_triviaqa-validation-1246", "mrqa_naturalquestions-validation-3233", "mrqa_squad-validation-6884", "mrqa_naturalquestions-validation-1704", "mrqa_triviaqa-validation-3988", "mrqa_triviaqa-validation-7407", "mrqa_squad-validation-259", "mrqa_squad-validation-6627", "mrqa_hotpotqa-validation-2128", "mrqa_triviaqa-validation-810", "mrqa_naturalquestions-validation-6248", "mrqa_hotpotqa-validation-5274", "mrqa_squad-validation-5441"], "retrieved_ids": ["mrqa_naturalquestions-train-86861", "mrqa_naturalquestions-train-47379", "mrqa_naturalquestions-train-59682", "mrqa_naturalquestions-train-8443", "mrqa_naturalquestions-train-25012", "mrqa_naturalquestions-train-66282", "mrqa_naturalquestions-train-71745", "mrqa_naturalquestions-train-32678", "mrqa_naturalquestions-train-47351", "mrqa_naturalquestions-train-86667", "mrqa_naturalquestions-train-39722", "mrqa_naturalquestions-train-75606", "mrqa_naturalquestions-train-563", "mrqa_naturalquestions-train-4073", "mrqa_naturalquestions-train-72256", "mrqa_naturalquestions-train-39559", "mrqa_squad-validation-843", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-7598", "mrqa_squad-validation-782", "mrqa_squad-validation-1700", "mrqa_naturalquestions-validation-2907", "mrqa_triviaqa-validation-6636", "mrqa_squad-validation-370", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-7598", "mrqa_squad-validation-782", "mrqa_squad-validation-2793", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-4871", "mrqa_naturalquestions-validation-2196", "mrqa_triviaqa-validation-7020"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 91, "before_eval": {"predictions": ["40%", "Catherine", "Mongolian patrimonial feudalism", "american muffins", "neighboring Pakistan", "biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "American", "100,000", "paris", "tahiti", "montgomery", "North Greenwich Arena", "ovules in their unfertilized state", "Masters Tournament", "South Korean sports drama film starring Ha Ji-won and Bae Doona", "90%", "henry", "communication", "1856", "Hepatocytes", "deities", "american national bank of new york", "Belfast and elsewhere in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the United States", "gregory cowntree", "corvidae", "South Africa", "Four Weddings and a Funeral", "low-skilled workers", "at the center of the Northern Hemisphere", "Pakistan", "psilocybin, psilocin and baeocystin", "cuba and His Teddy Bear"], "metric_results": {"EM": 0.25, "QA-F1": 0.37319186545585975}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.47058823529411764, 1.0, 0.07999999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.967741935483871, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4749", "mrqa_squad-validation-8411", "mrqa_triviaqa-validation-7698", "mrqa_squad-validation-9738", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-3055", "mrqa_triviaqa-validation-544", "mrqa_naturalquestions-validation-2439", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1140", "mrqa_squad-validation-8577", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-1336", "mrqa_hotpotqa-validation-1068", "mrqa_triviaqa-validation-5143", "mrqa_naturalquestions-validation-7799", "mrqa_triviaqa-validation-6126", "mrqa_triviaqa-validation-4353", "mrqa_squad-validation-7538", "mrqa_naturalquestions-validation-2721", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-1473", "mrqa_triviaqa-validation-6594"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 92, "before_eval": {"predictions": ["Somerset County, Pennsylvania", "communism", "\"Barracuda Frank\"", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "phosphate", "1917", "BitInstant", "\"Fame\", Debbie Berwick on \"Phil of the Future\"", "tilla mancha", "railway locomotives, ships, steamboats and road vehicles", "Social Chapter", "Australia", "retinal ganglion cells of one retina", "quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schr\u00f6dinger equation instead of Newtonian equations", "The Lone Ranger", "Robert Sargent Shriver Jr.", "Landry's, Inc.", "George Mifflin Dallas", "Jenny Agutter and Griffin Dunne", "national defence Volunteers", "Chicago", "national air force", "1978", "inversely", "one and only one man who had the first idea about inventing something that a Jet Ski after many modifications", "Irvine Tech Center", "Marine Corps Air Station Kaneohe Bay", "Teen Titans Go!", "national cup", "November 20, 1942", "Hugues Capet, king of France", "Morning Edition"], "metric_results": {"EM": 0.34375, "QA-F1": 0.46983375420875423}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.5, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7272727272727273, 1.0, 1.0, 0.25, 0.07407407407407407, 1.0, 0.0, 0.4, 0.3333333333333333, 0.8333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4163", "mrqa_triviaqa-validation-3064", "mrqa_hotpotqa-validation-3514", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-3366", "mrqa_naturalquestions-validation-3820", "mrqa_hotpotqa-validation-2474", "mrqa_triviaqa-validation-5888", "mrqa_squad-validation-3276", "mrqa_naturalquestions-validation-3316", "mrqa_squad-validation-10388", "mrqa_hotpotqa-validation-5895", "mrqa_naturalquestions-validation-10080", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-1148", "mrqa_triviaqa-validation-6960", "mrqa_triviaqa-validation-7185", "mrqa_triviaqa-validation-980", "mrqa_squad-validation-2696", "mrqa_triviaqa-validation-1267", "mrqa_squad-validation-3189"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 93, "before_eval": {"predictions": ["Saskatchewan", "a method which pre-allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "central area", "Utah", "gregory abbot", "Great Britain", "mid-size four - wheel drive luxury SUV", "brathwaite", "rebecca", "we want to practice Christian love toward them and pray that they convert", "two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic, and a second that lack resistance", "around 1200", "T'Pau", "gestapo", "1815", "National Football Conference", "kimono (short sword) or a tant\u014d (knife)", "in North America where it has a core population in Michigan and surrounding states and provinces", "\"synforms\"", "2015", "man's first Disobedience", "as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "51st", "Transpacific Yacht Race", "German", "graduate profession and the normal route for graduates wishing to teach is to complete a programme of Initial Teacher Education (ITE) at one of the seven Scottish Universities who offer these courses", "ground support services in mining and tunnelling", "yes or no, or alternately either 1 or 0", "Gibraltar", "`` Company Picnic ''", "lie detector", "mass murder of the future king"], "metric_results": {"EM": 0.125, "QA-F1": 0.24499289772727273}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.06060606060606061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.33333333333333337, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18750000000000003, 1.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-7513", "mrqa_squad-validation-4747", "mrqa_triviaqa-validation-7064", "mrqa_naturalquestions-validation-126", "mrqa_triviaqa-validation-2960", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-1586", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6177", "mrqa_squad-validation-2368", "mrqa_squad-validation-4978", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-4204", "mrqa_squad-validation-9", "mrqa_triviaqa-validation-1548", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-421", "mrqa_triviaqa-validation-4668", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-2600", "mrqa_squad-validation-2781", "mrqa_hotpotqa-validation-3298", "mrqa_squad-validation-2040", "mrqa_hotpotqa-validation-1246", "mrqa_squad-validation-1652", "mrqa_naturalquestions-validation-9903", "mrqa_triviaqa-validation-1746"], "retrieved_ids": ["mrqa_naturalquestions-train-43185", "mrqa_naturalquestions-train-16389", "mrqa_naturalquestions-train-86588", "mrqa_naturalquestions-train-31464", "mrqa_naturalquestions-train-27493", "mrqa_naturalquestions-train-11068", "mrqa_naturalquestions-train-2542", "mrqa_naturalquestions-train-55282", "mrqa_naturalquestions-train-21831", "mrqa_naturalquestions-train-15190", "mrqa_naturalquestions-train-87526", "mrqa_naturalquestions-train-8097", "mrqa_naturalquestions-train-83432", "mrqa_naturalquestions-train-7149", "mrqa_naturalquestions-train-9509", "mrqa_naturalquestions-train-47540", "mrqa_squad-validation-5814", "mrqa_naturalquestions-validation-3820", "mrqa_squad-validation-7", "mrqa_hotpotqa-validation-277", "mrqa_naturalquestions-validation-3820", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-7598", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4163", "mrqa_triviaqa-validation-4353", "mrqa_squad-validation-3276", "mrqa_hotpotqa-validation-4163", "mrqa_squad-validation-9738", "mrqa_naturalquestions-validation-2196", "mrqa_triviaqa-validation-6126", "mrqa_squad-validation-10388"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 94, "before_eval": {"predictions": ["nucleus", "horror fiction", "henry", "alberta", "southeast of the city", "Puerto Rico", "the French island of Guadeloupe in the Lesser Antilles, mainly in the commune of Deshaies", "ragman roll", "1938", "63%", "American pharmaceutical company", "2016", "90% to 93% O2", "approximately 11 %", "flags of dependent territories and other areas of special sovereignty", "Chen's theorem", "alberta", "short-tempered", "1987", "dastardly & muttley", "governing body for pharmacy health care professionals", "henry wenworth", "1968", "enlisted sailor", "led about 1,500 army troops and provincial militia on an expedition in June 1755 to take Fort Duquesne", "Taliban", "June 11, 1986", "Cork, Portarlington, Lisburn, Waterford and Youghal", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash in Hindi, Urdu and Punjabi", "Tom Coburn", "short Circuit", "increased and the ground water level fell significantly. Dead branches dried up and the amount of forests on the flood plains decreased sharply"], "metric_results": {"EM": 0.1875, "QA-F1": 0.27773285924601715}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7368421052631579, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0, 0.923076923076923, 0.2857142857142857, 0.0, 0.0, 0.16000000000000003]}}, "error_ids": ["mrqa_naturalquestions-validation-366", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-2404", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-7787", "mrqa_triviaqa-validation-2249", "mrqa_naturalquestions-validation-2624", "mrqa_hotpotqa-validation-4506", "mrqa_naturalquestions-validation-2949", "mrqa_squad-validation-3677", "mrqa_naturalquestions-validation-5420", "mrqa_hotpotqa-validation-562", "mrqa_triviaqa-validation-6433", "mrqa_squad-validation-2486", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-3230", "mrqa_triviaqa-validation-5584", "mrqa_hotpotqa-validation-2328", "mrqa_naturalquestions-validation-4365", "mrqa_squad-validation-10196", "mrqa_squad-validation-3306", "mrqa_naturalquestions-validation-7496", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-659", "mrqa_squad-validation-9252"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 95, "before_eval": {"predictions": ["the \"richest 1 percent in the United States", "universal ruler", "red", "alain Giresse", "the Italian Alps", "ideal", "david duchovny", "cave bug", "Charles Foster Kane, played by Welles", "parashah", "group", "World Music Awards", "six", "8\u20134\u20133 system", "exposed to scrutiny", "79", "banned the growing of coffee, introduced a hut tax, and the landless were granted less and less land in exchange for their labour", "3,000 troupes de la marine", "a compromise between the two", "stimulated his brain cells", "leprechauns", "Daniel Ken", "David Toms", "four", "bee", "11 free suburban weekly newspapers together covering the Adelaide metropolitan area", "Paul Edgecombe", "The Portuguese", "less workers are required", "the university's science club in 1886", "Ming general Wu Sangui", "kronborg castle"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26278409090909094}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.7272727272727272, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-7459", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-717", "mrqa_squad-validation-9135", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-879", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-1688", "mrqa_hotpotqa-validation-4523", "mrqa_triviaqa-validation-1838", "mrqa_squad-validation-8572", "mrqa_squad-validation-10138", "mrqa_squad-validation-6914", "mrqa_squad-validation-1445", "mrqa_triviaqa-validation-2900", "mrqa_hotpotqa-validation-1234", "mrqa_naturalquestions-validation-5267", "mrqa_naturalquestions-validation-3093", "mrqa_triviaqa-validation-1682", "mrqa_hotpotqa-validation-5033", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-9985", "mrqa_squad-validation-7184", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-4796"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 96, "before_eval": {"predictions": ["Silicates of magnesium and iron", "Marcus Atilius Regulus", "The Dome of the Rock", "Public-Private Partnering", "brilliant celebrity with praise", "bicapitalized Microsoft", "Bulgaria", "13,900 nautical miles (25,700 km) downrange in the Pacific ocean", "Army, Navy and other services", "He is the younger brother of comic actor John Belushi and father of actor Robert Belushi, best known for playing the role of James \"Jim\" Orenthal on the long-running sitcom According to Jim (2001)", "Renhe Sports Management Ltd, 100 % owned by Xiu Li Dai and Yongge Dai. 25 % Owned by Narin Niruttinanon", "kansas city", "a free gift of God's grace through faith in Jesus Christ as redeemer from sin", "hydrogen and helium", "They also spread beyond Europe to the Dutch Cape Colony in South Africa, the Dutch East Indies, the Caribbean, and several of the English colonies of North America, and Quebec, where they were accepted and allowed to worship freely", "the radius R of the turntable in that animation, the rate of angular rotation \u03c9, and the speed of the cannonball ( assumed constant ) v", "2006", "gOV.UK", "the Ministry of War compared with native Chinese dynasties", "probably born in 1162 in Del\u00fc\u00fcn Boldog, near Burkhan Khaldun mountain and the Onon and Kherlen rivers in modern-day northern Mongolia", "el Che or simply Che", "Porsche 968 was the final evolution of a line of water-cooled front-engined rear wheel drive models begun almost 20 years earlier with the 924, taking over the entry-level position in the company lineup", "a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "1971", "The \"Huguenot Street Historic District\" in New Paltz has been designated a National Historic Landmark site", "American Christian rock band Needtobreathe", "Americans to explain the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "September 1995", "cake or biscuit was part of a VAT tribunal in 1991, with the court finding in McVitie's favour that the Jaffa cake should be considered a cake for tax purposes", "inverted", "electromagnetic", "kolinio Epeli Vanuacicila Rabuka and Salote Lomaloma Rabuka"], "metric_results": {"EM": 0.1875, "QA-F1": 0.40118727037928703}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.25, 0.0, 1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.2857142857142857, 0.0, 0.4444444444444445, 0.42857142857142855, 0.3720930232558139, 0.1904761904761905, 1.0, 0.0, 0.5454545454545454, 0.09523809523809523, 0.0, 0.0689655172413793, 0.6363636363636364, 0.0, 0.19047619047619047, 1.0, 0.07692307692307693, 0.0, 0.5142857142857142, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_squad-validation-3504", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-3165", "mrqa_squad-validation-3876", "mrqa_hotpotqa-validation-5639", "mrqa_triviaqa-validation-66", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-113", "mrqa_squad-validation-2100", "mrqa_squad-validation-3667", "mrqa_squad-validation-3023", "mrqa_naturalquestions-validation-7297", "mrqa_triviaqa-validation-5093", "mrqa_squad-validation-8189", "mrqa_squad-validation-6158", "mrqa_triviaqa-validation-7768", "mrqa_hotpotqa-validation-1399", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-3771", "mrqa_squad-validation-3057", "mrqa_squad-validation-9574", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-4414", "mrqa_squad-validation-10311", "mrqa_triviaqa-validation-3409"], "retrieved_ids": ["mrqa_naturalquestions-train-59589", "mrqa_naturalquestions-train-45922", "mrqa_naturalquestions-train-33138", "mrqa_naturalquestions-train-7399", "mrqa_naturalquestions-train-71997", "mrqa_naturalquestions-train-68206", "mrqa_naturalquestions-train-17553", "mrqa_naturalquestions-train-87284", "mrqa_naturalquestions-train-67114", "mrqa_naturalquestions-train-80531", "mrqa_naturalquestions-train-5406", "mrqa_naturalquestions-train-1420", "mrqa_naturalquestions-train-67495", "mrqa_naturalquestions-train-20738", "mrqa_naturalquestions-train-42944", "mrqa_naturalquestions-train-42495", "mrqa_hotpotqa-validation-1246", "mrqa_hotpotqa-validation-2328", "mrqa_squad-validation-1445", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2624", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-879", "mrqa_hotpotqa-validation-659", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-4796", "mrqa_squad-validation-1445", "mrqa_triviaqa-validation-2900", "mrqa_naturalquestions-validation-9985", "mrqa_hotpotqa-validation-5033", "mrqa_hotpotqa-validation-421", "mrqa_triviaqa-validation-7064"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 97, "before_eval": {"predictions": ["third under head coach Brian Billick.", "faith", "suburb", "The National Era, an abolitionist periodical, starting with the June 5, 1851, issue", "the American Civil War", "bramble", "July 1, 2005", "a few drops", "Addy Miller", "Staten Island", "President pro tempore of the Senate", "the optic chiasm, where there is a partial decussation ( crossing ) of fibres from the temporal visual fields ( the nasal hemi - retina ) of both eyes", "Manhattan's lower east side", "quote Shelley's Masque of Anarchy to vast audiences during the campaign for a free India", "The 1911 Encyclop\u00e6dia Britannica describes the Gararish", "Yuliya Snigir as Irina Komarova", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "the Holy Land from Galicia", "british society", "science fiction drama", "26", "incandescent lamps", "kolkata", "Nickelback", "Juliet's cousin", "the \"father of the Mongols\"", "indeed caused by chlorine and bromine from manmade organohalogens", "a combined concert/lecture by British progressive folk-rock band Gryphon", "bismarcks", "fomenting rebellion in many of Great Britain", "Psych is an American detective comedy-drama television series created by Steve Franks", "the planned Nazi pre-emptive nuclear strike on Japan, `` Operation Dandelion, '' is apparently being prevented only by Hitler's personal refusal to authorise it"], "metric_results": {"EM": 0.15625, "QA-F1": 0.28064236111111107}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.9523809523809523, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.5555555555555556, 0.0, 0.5714285714285715, 0.14285714285714288, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.625, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4113", "mrqa_squad-validation-2262", "mrqa_hotpotqa-validation-3785", "mrqa_naturalquestions-validation-2536", "mrqa_naturalquestions-validation-7957", "mrqa_triviaqa-validation-5041", "mrqa_naturalquestions-validation-5999", "mrqa_triviaqa-validation-5865", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-3358", "mrqa_squad-validation-1240", "mrqa_squad-validation-6859", "mrqa_hotpotqa-validation-4389", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-1905", "mrqa_triviaqa-validation-636", "mrqa_squad-validation-1423", "mrqa_triviaqa-validation-2161", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-654", "mrqa_squad-validation-5359", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-6540", "mrqa_hotpotqa-validation-2271", "mrqa_naturalquestions-validation-2729"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 98, "before_eval": {"predictions": ["the most devastating stock market crash in the history of the United States ( acting as the most significant predicting indicator of the Great Depression ), when taking into consideration the full extent and duration of its after effects", "SKUM", "S6", "Turin", "the leading flagellum, the eyespot allows the organism to move in response to light, often toward the light to assist in photosynthesis, and to predict day and night, the primary function of circadian rhythms", "Paris", "fossil sequences in which there was datable material, converting the old relative ages into new absolute ages", "heaviest album of all", "choral work", "in San Francisco, California with offices in New York City and Atlanta", "economic recession", "ex as a noun is assumed to refer to a former sexual or romantic partner, especially a former spouse", "plum", "cedar", "\"citizenship\"", "undulating motion", "'I'll Be There for You", "The Blind Boys of Alabama, Tom Waits, The Neville Brothers, DoMaJe, and Steve Earle", "kent", "Sultans", "post-war communist control of the country", "alberta", "less than the approximately 150,000 base pair genome of the more assimilated chloroplast", "3D", "North Greenbush", "theatre", "Germ\u00e1n Efromovich", "detailed public disclosure of an invention", "Tokugawa shogunate", "Madame Medusa from Disney's The Rescuers ( 1977 )", "between the three towns of Doncaster, Scunthorpe and Gainsborough", "1908"], "metric_results": {"EM": 0.28125, "QA-F1": 0.395904939950851}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [0.4878048780487805, 1.0, 0.0, 1.0, 0.0975609756097561, 0.4, 0.2222222222222222, 1.0, 0.0, 0.4, 1.0, 0.47619047619047616, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.2857142857142857, 0.0, 0.0, 0.11764705882352942, 0.0, 0.0, 1.0, 1.0, 0.7142857142857143, 0.0, 0.0, 0.18181818181818182, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8837", "mrqa_hotpotqa-validation-2978", "mrqa_naturalquestions-validation-2213", "mrqa_triviaqa-validation-1952", "mrqa_squad-validation-5008", "mrqa_triviaqa-validation-2353", "mrqa_hotpotqa-validation-2856", "mrqa_naturalquestions-validation-10707", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2725", "mrqa_hotpotqa-validation-5136", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6382", "mrqa_squad-validation-992", "mrqa_triviaqa-validation-5737", "mrqa_squad-validation-8802", "mrqa_triviaqa-validation-2414", "mrqa_hotpotqa-validation-5724", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-4550", "mrqa_naturalquestions-validation-886", "mrqa_hotpotqa-validation-1533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 99, "before_eval": {"predictions": ["Doctor Who", "21st Amendment", "elstow", "acted increasingly aggressively to force the Huguenots to convert", "Gender pay gap in favor of males in the labor market", "Woody's Roundup", "May 5, 1939", "economic growth by collecting resources from colonies", "1998", "October 1, 2017", "Washington, D.C.", "wicked", "Toronto, Ontario, Canada", "a close and special if unknown connection with the royal nunnery of Shaftesbury (Dorset)", "France's claim to the region was superior to that of the British", "a balance sensor consisting of a statolith", "yosemite national park", "field trips", "Roman Jakobson", "practiced law", "the French company R2E Micral CCMC officially appeared in September 1980 at the Sicob show in Paris", "1800 to 1850", "the words spoken to Adam and Eve after their sin, reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "The book explores what those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Twink", "Trinidad and Tobago", "Northern Ireland Civil Rights Association", "9 is 1/6", "Portugal", "Bhaktivedanta Manor", "the largest known species of jellyfish", "Mammals"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3192408225302962}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true], "QA-F1": [0.5, 0.0, 0.0, 0.18181818181818182, 0.2105263157894737, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.14814814814814817, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.23076923076923078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-7714", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-4903", "mrqa_squad-validation-3130", "mrqa_squad-validation-7449", "mrqa_triviaqa-validation-7414", "mrqa_squad-validation-9926", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-7769", "mrqa_hotpotqa-validation-4287", "mrqa_hotpotqa-validation-1086", "mrqa_squad-validation-10231", "mrqa_squad-validation-1870", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-4961", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-251", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-607", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-6226", "mrqa_squad-validation-8972", "mrqa_hotpotqa-validation-5833", "mrqa_naturalquestions-validation-2794"], "retrieved_ids": ["mrqa_naturalquestions-train-22530", "mrqa_naturalquestions-train-55834", "mrqa_naturalquestions-train-73005", "mrqa_naturalquestions-train-719", "mrqa_naturalquestions-train-52161", "mrqa_naturalquestions-train-52127", "mrqa_naturalquestions-train-10522", "mrqa_naturalquestions-train-43956", "mrqa_naturalquestions-train-22986", "mrqa_naturalquestions-train-53466", "mrqa_naturalquestions-train-21548", "mrqa_naturalquestions-train-32400", "mrqa_naturalquestions-train-46611", "mrqa_naturalquestions-train-85637", "mrqa_naturalquestions-train-32118", "mrqa_naturalquestions-train-76306", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-3771", "mrqa_squad-validation-2100", "mrqa_triviaqa-validation-5737", "mrqa_naturalquestions-validation-3109", "mrqa_triviaqa-validation-5737", "mrqa_triviaqa-validation-5737", "mrqa_naturalquestions-validation-677", "mrqa_triviaqa-validation-5093", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-2949", "mrqa_hotpotqa-validation-3785", "mrqa_triviaqa-validation-2353", "mrqa_squad-validation-2100", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-2414"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.1728125, "QA-F1": 0.2838019619012877}, "overall_error_number": 2647, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.671875, "QA-F1": 0.7415453376348295}, "final_upstream_test": {"EM": 0.694, "QA-F1": 0.8043403827428975}}}