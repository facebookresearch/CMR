{"model_update_steps": 2550, "method_class": "index_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, index_rank_method='most_similar', indexing_args_path='exp_results/supervision_data/1012_dm_simple.train_args.json', indexing_method='bart_io_index', inference_query_size=1, init_memory_cache_path='exp_results/data_streams/bart_io_index.init_memory.pkl', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/index_based/ckpt_dir/1020v2_MixedAllErrors_T=100_IOindex_UR=0.5_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=789_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/index_based/ckpt_dir/1020v2_MixedAllErrors_T=100_IOindex_UR=0.5_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=789_ckpts/', replay_candidate_size=0, replay_frequency=3, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, upstream_sample_ratio=0.5, use_mir=False, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=100, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "The Iroquois", "philanthropy", "The Darling Buds of May", "Virginia Wade", "The Dallas Lovers' Song", "to the anterolateral corner of the spinal cord", "1966", "for scientific observation", "nonseal", "The Stock Market crash in New York", "New York Stadium", "norman Tebbit", "continental units", "David Copperfield", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the religious text of Islam", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "light bulbs within 100 feet of the lab glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs", "the great heroism or of the most conspicuous courage in circumstances of extreme danger", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24890448603683896}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.23529411764705885, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.19999999999999998, 0.5882352941176471, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_squad-validation-1516", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "marioneth and Llantisilly Rail Traction Company Limited", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "a phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit", "a student in the second year at a high school, college, or university", "Bothtec", "Terry Reid", "reports from government agencies and non-governmental organizations", "is the theme of the Variations", "North America", "Andr\u00e9 3000", "rookies", "the Aten, a representation of the Egyptian god, Ra", "Theodore Roosevelt", "the fourth season", "Denver Broncos", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "Georges Bizet", "Matt Winer", "1671", "Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.09375, "QA-F1": 0.29091243723596666}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.9019607843137255, 0.18181818181818182, 1.0, 0.0, 0.0, 0.3333333333333333, 0.4444444444444445, 0.4, 1.0, 1.0, 0.5, 0.0, 0.0, 0.3636363636363636, 0.3333333333333333, 0.6666666666666666, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "baijan", "id", "between 27 July and 7 August 2022", "New York", "is getting a remake", "2006", "the least of the Great Powers", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "boggling", "is a retired professional jockey and one of the most successful English flat racing jockeys of all time", "coronary thrombosis", "bollywood", "Overtime", "Sir Henry Cole", "has trouble distinguishing between carbon dioxide and oxygen", "is a British sitcom, broadcast in the United Kingdom from 1982 to 1984", "cement City, Texas", "the Democratic Unionist Party (DUP )", "23 July 1989", "many educational institutions especially within the US", "often exercising a great deal of control over the lives of their disciples", "for control purposes", "bamboula", "Callability", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins", "bile duct or artery", "berenice Abbott"], "metric_results": {"EM": 0.0625, "QA-F1": 0.11316964285714284}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-21096", "mrqa_naturalquestions-train-29940", "mrqa_naturalquestions-train-9344", "mrqa_naturalquestions-train-14729", "mrqa_naturalquestions-train-33937", "mrqa_naturalquestions-train-84386", "mrqa_naturalquestions-train-18912", "mrqa_naturalquestions-train-3255", "mrqa_naturalquestions-train-81986", "mrqa_naturalquestions-train-21849", "mrqa_naturalquestions-train-69709", "mrqa_naturalquestions-train-40456", "mrqa_naturalquestions-train-43803", "mrqa_naturalquestions-train-88115", "mrqa_naturalquestions-train-17181", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-671", "mrqa_squad-validation-8513", "mrqa_hotpotqa-validation-5802", "mrqa_naturalquestions-validation-8448", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1364", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-4283", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-5325", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-7369"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Arnhem", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "Janus", "Moses", "museum", "nigeria", "bounding the time or space used by the algorithm", "museum", "Alex O'Loughlin", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "nigeria Lovett", "Detective Eddie Thawne", "All Hallows'Day", "sue barker", "baku", "new converts", "Mona Vanderwaal", "sue barker", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the Dutch Cape Colony in South Africa, the Dutch East Indies, the Caribbean, and several of the English colonies of North America, and Quebec", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "turbine rotor", "Splodgenessabounds"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28430059523809526}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.3333333333333333, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-5168", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_squad-validation-3467"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["june", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric with a colorful swirled pattern of curved shapes", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "2 : 44 p.m. EDT", "swanee or swannee whistle", "rapeseed", "to start fires, hunt, and bury their dead", "Cochin University of science and Technology", "parietal cells ( also known as oxyntic or delomorphous cells )", "placental", "September 13, 1994", "june", "imperial rule", "1840", "a defiant speech, or a speech explaining their actions", "Francis Marion Crawford, Robert Underwood Johnson, Stanford White, Fritz Lowenstein, George Scherff, and Kenneth Swezey", "kinks", "a greater tendency to take on debts", "heat", "averse to wedlock", "8.7 -- 9.2", "People's Republic of China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1512774806892454}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "2005 to 2008", "1998", "n Carolina", "islands capital is Mahon", "90-60's", "unaided independent school", "dolph Camilli", "the times sign or the dimension sign", "BAFTA Television Award", "Juice Newton", "1960", "HTTP Secure ( HTTPS )", "late summer", "kansas River", "monatomic", "Palm Springs", "june", "all invertebrates", "dune", "universal", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf tissue", "Indian club ATK", "land", "Indian Ocean near Grande Comore, Comoros Islands", "\u20b9 \u200d5L '' ( for `` rupees 5 lakhs '' )", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.0625, "QA-F1": 0.15163690476190478}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-48013", "mrqa_naturalquestions-train-61127", "mrqa_naturalquestions-train-1981", "mrqa_naturalquestions-train-26926", "mrqa_naturalquestions-train-42729", "mrqa_naturalquestions-train-81329", "mrqa_naturalquestions-train-33625", "mrqa_naturalquestions-train-62196", "mrqa_naturalquestions-train-40776", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-14180", "mrqa_naturalquestions-train-39010", "mrqa_naturalquestions-train-63655", "mrqa_naturalquestions-train-63655", "mrqa_naturalquestions-train-84412", "mrqa_naturalquestions-train-40776", "mrqa_squad-validation-5360", "mrqa_squad-validation-5622", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-365", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-validation-3490", "mrqa_hotpotqa-validation-5899", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-2248", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-1437", "mrqa_squad-validation-4283", "mrqa_squad-validation-2191", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few common complex biomolecules, such as squalene and the carotenes", "The U.S. Army Chaplain insignia", "Kairi", "the suburbs, who wanted more services and more control over the central city", "film", "near the Black Sea", "the last book accepted into the Christian biblical canon", "Beyonc\u00e9", "% IACS", "George Cross", "high viewership levels for the evening on which the episode is broadcast", "1950s", "work oxen for haulage", "1998", "a priest", "third most abundant chemical element in the universe", "18 - season", "family member", "long-term environmental changes", "William Powell Lear", "the unbalanced centripetal force", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "Abraham Gottlob Werner", "Anne", "present-day Charleston", "the Muslim Brotherhood in Palestine took a \"quiescent\" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's \"indulgence\" to build up a network of mosques and charitable organizations", "Andy Cohen", "German general (colonel-general from 1940 )"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3023385496671787}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.19999999999999998, 0.0, 0.25, 0.0, 0.0, 1.0, 0.4444444444444445, 0.4, 0.0, 0.5, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.06451612903225806, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-3146"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computability theory", "guseppe Antonio 'Nino' Farina", "37.7", "6.4 nanometers apart", "the eighth and eleventh episodes of the season", "Kyle Busch", "over 400", "endocrine", "liberal arts ( artes liberales )", "Forest of Bowland", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "St. Louis County, Missouri, United States, 17 mi southwest of St. Missouri and 2 mi east of Eureka", "1868", "2018", "wolverhampton Wanderers", "law firm", "Pottawatomie County", "oangutan", "general relativity", "The church tower", "red pukka\u2019 service on the District line", "Toronto, Ontario, Canada", "foreign", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to the south", "the Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "one of the largest gold rushes the world has ever seen", "six", "not guilty", "psychoanalytic", "Quentin Coldwater", "acidic bogs"], "metric_results": {"EM": 0.1875, "QA-F1": 0.34169924100022786}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.3333333333333333, 0.375, 0.10526315789473684, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.2666666666666667, 0.5, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentine", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "to taste sweetened sheeps\u2019 milk ricotta cream", "photosynthesis", "a wide range of society figures of the period", "White House is", "The Daily Stormer", "triplet", "water", "president", "officeholders annually", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy film", "Worcester Cold Storage and Warehouse Co. fire", "acting", "C. W. Grafton", "LED illuminated display", "Americans acting under orders", "iPod Classic", "My Sassy Girl", "the elimination of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "The Edge of Night", "non-combustible substances that corrode", "pedagogy", "vaskania", "ATP", "soils", "drug dealer", "medium and heavy-duty diesel trucks", "testes"], "metric_results": {"EM": 0.25, "QA-F1": 0.39620961196062}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.1, 0.0, 0.0, 0.125, 0.8, 1.0, 0.0, 0.13333333333333333, 0.4, 0.0, 1.0, 0.18181818181818182, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 0.1290322580645161, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_hotpotqa-validation-832", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-49614", "mrqa_naturalquestions-train-43992", "mrqa_naturalquestions-train-46860", "mrqa_naturalquestions-train-29940", "mrqa_naturalquestions-train-48495", "mrqa_naturalquestions-train-52218", "mrqa_naturalquestions-train-75065", "mrqa_naturalquestions-train-64850", "mrqa_naturalquestions-train-75809", "mrqa_naturalquestions-train-56867", "mrqa_naturalquestions-train-24997", "mrqa_naturalquestions-train-51481", "mrqa_naturalquestions-train-57418", "mrqa_naturalquestions-train-81443", "mrqa_naturalquestions-train-75023", "mrqa_naturalquestions-train-70998", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-4734", "mrqa_squad-validation-6947", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-2753", "mrqa_triviaqa-validation-3915", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-5231", "mrqa_squad-validation-5360", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-7821", "mrqa_hotpotqa-validation-1897"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["1688", "yellow fever", "three legal systems", "Las Vegas", "optional message body", "globetrotters", "Anthony Bellew", "a bridge over the Merderet in the fictional town of Ramelle", "1987", "no plan", "monolith", "a strict and elaborate set of rules designed by Victoria, Duchess of Kent, along with her attendant, Sir John Conroy, concerning the upbringing of the Duchess's daughter, the future Queen Victoria", "pH 7 ( 25 \u00b0 C )", "Kon-Tiki", "the MGM Grand Garden Special Events Center", "digital fashion gallery", "Ronnie Hillman", "all-encompassing", "1838", "more than 60", "Eagle Ridge Mall", "Pel\u00e9", "to reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "the classical element fire", "Ron Howard", "at least 18 or 21 years old ( or have a legal guardian present )", "Ann Ward", "novelist and poet", "The Jamestown settlement in the Colony of Virginia", "Monet", "tree growth stages"], "metric_results": {"EM": 0.125, "QA-F1": 0.269629329004329}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_squad-validation-4506"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "\" Traumnovelle\"", "other factors are accounted for", "Treaty on the Functioning of the European Union", "thermodynamic", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "bactrians and dromedaries", "a longer span", "trams", "gun ban", "cadence & Cascade", "the Bulgars, and especially the Seljuk Turks", "died in battle", "Volkswagen Beetle", "alexander", "continental integration", "Queen Elizabeth I", "infection, irritation, or allergies", "to avoid the long queues", "Town House Galleria", "catfish aquaculture", "atomic number 53", "Rebecca Rose, Andy Allo, Venzella Joy Williams, and Hannah Fairlight as Calamity, Serenity, Charity, and Veracity, respectively, members of the band Evermoist", "Iraq", "a co-op of grape growers", "victor willsmeron", "Verdi", "1952", "Los Angeles Lakers", "the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.22731645441444032}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.23529411764705882, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.28571428571428575, 0.4, 0.0, 0.0909090909090909, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.08695652173913043, 0.22222222222222224, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["British rock group Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Mark Lennon", "fencers", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "August 31, 2014", "The stability, security, and predictability of British law and government", "Minos and Kokalos", "18 November", "cienfuegos in the Las Villas province of Cuba", "byker grove", "New South Wales", "Fort Williams", "Dandy", "arpis", "an English narrator, possibly Orwell himself", "Czech Kingdom (Czech: \"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\" ; German: \"K\u00f6nigreich B\u00f6hmen\" ; Latin: \" Regnum Bohemiae\"", "Gregg Popovich", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "immunological memory", "Mexican", "a musician", "danish", "December 1, 1969", "american", "jK Rowling", "California State Automobile Association", "\"alone\"", "Cinderella", "delayed the sealing of the hatch", "due to a lack of understanding of the legal ramifications"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3355707881443175}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [0.15384615384615385, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.2857142857142857, 0.3076923076923077, 0.0, 0.23529411764705882, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_triviaqa-validation-5852", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-85124", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-65552", "mrqa_naturalquestions-train-35862", "mrqa_naturalquestions-train-81981", "mrqa_naturalquestions-train-85593", "mrqa_naturalquestions-train-77272", "mrqa_naturalquestions-train-76379", "mrqa_naturalquestions-train-78339", "mrqa_naturalquestions-train-33921", "mrqa_naturalquestions-train-26631", "mrqa_naturalquestions-train-15747", "mrqa_naturalquestions-train-1303", "mrqa_naturalquestions-train-58312", "mrqa_naturalquestions-train-69102", "mrqa_naturalquestions-train-57543", "mrqa_triviaqa-validation-1550", "mrqa_naturalquestions-validation-6644", "mrqa_triviaqa-validation-5487", "mrqa_naturalquestions-validation-9688", "mrqa_hotpotqa-validation-187", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-9281", "mrqa_squad-validation-7799", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-3387", "mrqa_hotpotqa-validation-1021", "mrqa_squad-validation-273", "mrqa_hotpotqa-validation-5682", "mrqa_squad-validation-6677", "mrqa_naturalquestions-validation-2900"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister", "president of Guggenheim Partners", "Buddy Pine / Incredi - Boy / Syndrome, a former superhero fanatic who has no super powers of his own but uses advanced technology to give himself equivalent abilities", "Napoleon", "rings", "3.7 percent of the entire student population", "a negative effect on subsequent long-run economic growth", "ringsham", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "9", "paddington", "amyotrophic lateral sclerosis", "\"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "a pioneer in watch design, manufacturing and distribution", "mid 1970s", "Torah or Bible", "on the western coast of Italy", "the first and only U.S. born world grand prix champion", "brass band parades", "in mid November and lit in a public ceremony in late November or early December", "Facebook", "beigel", "Steel Dragon in the 2001 film \"Rock Star\"", "Issaquah, Washington (a suburb of Seattle)", "King George's War", "cheated on Miley", "alternative rock", "Fort Saint Anthony", "ring", "infrequent rain and many sunny days"], "metric_results": {"EM": 0.125, "QA-F1": 0.24111931106174528}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.4, 0.0, 0.0, 0.0, 0.2857142857142857, 0.4444444444444445, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.5263157894736842, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.5]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6913", "mrqa_squad-validation-2656"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Chicago, Illinois", "maryland", "FX option", "electromagnetic waves", "caliphate", "good luck, protection and auspiciousness", "Dimensions in Time", "Surveyor 3", "January 1981", "lutein", "the structure and substance of his questions and answers concerning baptism in the Small Catechism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "john Robertson", "slowing the vehicle", "Cheyenne", "appearance of fossils in sedimentary rocks", "Hanna- Barbera", "in the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "efficient and effective management of money ( funds )", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "duke and Duchess of Sto Helit", "maryland", "Timo Hildebrand", "state sector", "February 1940", "weak government institutions", "a god of the Ammonites", "sclera", "Fester Addams", "\"Danno\" Williams"], "metric_results": {"EM": 0.0625, "QA-F1": 0.21817912479677187}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.5882352941176471, 0.18181818181818182, 0.5882352941176471, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Part 2", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Christina Aguilera and Taio Cruz", "jupiter", "a friend and publicist", "l\u00e1szl\u00f3 de Alm\u00e1sy", "masons'marks", "Theodore Haynes (1988) and Julia Rose (1989)", "Old Town Hall, Gateshead", "The horn line at the end is performed by the Phenix Horns from Earth, Wind & Fire", "The head contains the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg", "after the Spanish -- American War in the 1898 Treaty of Paris", "Heavyweight", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's superweapon", "Vito Corleone", "bplane", "art", "chimpanzees", "March 15, 1945", "absolute temperature", "The organization has a user base of over 1,800,000", "Julius Robert Oppenheimer", "bicuspid", "his brother, Menelaus", "3 December", "tallahassee", "prefabricated housing", "brian brian", "WOTV"], "metric_results": {"EM": 0.09375, "QA-F1": 0.10818452380952381}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-75368", "mrqa_naturalquestions-train-5744", "mrqa_naturalquestions-train-8124", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-24054", "mrqa_naturalquestions-train-45796", "mrqa_naturalquestions-train-40776", "mrqa_naturalquestions-train-82895", "mrqa_naturalquestions-train-68593", "mrqa_naturalquestions-train-52136", "mrqa_naturalquestions-train-11301", "mrqa_naturalquestions-train-30349", "mrqa_naturalquestions-train-44750", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-36106", "mrqa_naturalquestions-train-19445", "mrqa_squad-validation-3478", "mrqa_squad-validation-7554", "mrqa_squad-validation-6947", "mrqa_naturalquestions-validation-8119", "mrqa_squad-validation-1879", "mrqa_squad-validation-9598", "mrqa_triviaqa-validation-2327", "mrqa_hotpotqa-validation-1932", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-3677", "mrqa_hotpotqa-validation-1626", "mrqa_squad-validation-3467", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-6556", "mrqa_squad-validation-5517"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["florida", "Bruder Basil", "brian", "on the lateral side of the tibia", "ferguside royal clan", "the North Sea, through the former Meuse estuary, near Rotterdam", "kgalagadi Trans-frontier Park", "Colin Montgomerie", "October 29, 1985", "Amway", "Mauritius", "George Stigler", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in His absence, the Deputy - Chairman of the Rajya Sabha", "Sinatra", "Tanzanian Legal System and Legal Research", "Chad", "GMAT Sentence Correction (SC)", "an open work crown", "a child to go through a torturous treatment to gain information", "London", "French, English and Spanish", "florida", "U.S. Marshals", "What's Up (TV series)", "supply chain management", "Mars rover", "Stanislaw August Poniatowski", "polynomial algebra", "Stuart Little", "three mystic apes", "sheepskin and Merino Wool products", "Honolulu"], "metric_results": {"EM": 0.0625, "QA-F1": 0.09905753968253969}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21428571428571425, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-2287"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["pasteurised cow's milk soft cheese", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "decay energy", "Lula", "sovereign states", "president of the United States", "The Discovery Institute (DI) is a politically conservative non-profit think tank based in Seattle, Washington, best known for its advocacy of the pseudoscientific principle of intelligent design (ID)", "Bumblebee", "Australian", "36 months for men and 24 months for women", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Spencer", "\"antiforms\"", "June 9, 2015", "\"Veyyil\" (2006)", "Grace Nail Johnson", "Keith Richards", "at least one prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "students learn from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "the 180th meridian in a 360 \u00b0 - system", "Cartoon Network", "the Presiding Officer on the advice of the parliamentary bureau", "the Miami Heat of the National Basketball Association (NBA)", "33", "grapevines", "Annual Conference Cabinet", "Hannah Macleod", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28583232689210947}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.08333333333333334, 0.0, 0.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.16, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.4347826086956522, 0.25, 0.07407407407407407, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwanda genocide, also known as the genocide against the Tutsi", "Co-teaching is defined as two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "400 metres", "Vili Fualaau and Mary Kay Letourneau, a student and teacher who made news for their sexual relationship", "the entertainment division", "A to a point B", "12", "Great Exhibition of 1851", "Edward Longshanks and the Hammer of the Scots", "the Chagos Archipelago", "dundee palgrave.rlp.5100074", "the person compelled to pay for reformist programs", "February 8, 1587", "\"Grindhouse\" fake trailer", "davenport", "digital transmission", "the Swiss- Austrian border", "lithium-ion battery", "821", "V On Demand content", "liquid", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposition", "the \" King of Cool\"", "President Woodrow Wilson", "davis", "the fifth season", "davis dors", "Hockey Club Davos", "Michael Patrick Smith", "Aibak's successor and son - in - law Iltutmish"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28357221177944864}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.2, 0.10526315789473684, 0.5, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.25]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-5036", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-2781", "mrqa_naturalquestions-train-3473", "mrqa_naturalquestions-train-30221", "mrqa_naturalquestions-train-69102", "mrqa_naturalquestions-train-75865", "mrqa_naturalquestions-train-4053", "mrqa_naturalquestions-train-69102", "mrqa_naturalquestions-train-5218", "mrqa_naturalquestions-train-41200", "mrqa_naturalquestions-train-50954", "mrqa_naturalquestions-train-13544", "mrqa_naturalquestions-train-33813", "mrqa_naturalquestions-train-9702", "mrqa_naturalquestions-train-19148", "mrqa_naturalquestions-train-85143", "mrqa_naturalquestions-train-31670", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-8924", "mrqa_squad-validation-8869", "mrqa_triviaqa-validation-7253", "mrqa_hotpotqa-validation-2970", "mrqa_triviaqa-validation-671", "mrqa_squad-validation-7821", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2782", "mrqa_triviaqa-validation-46", "mrqa_hotpotqa-validation-1201", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-5231", "mrqa_squad-validation-8966"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "aragon", "11.1", "trans-Pacific flight", "Sharman Joshi", "students normally have to sit in a classroom and do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "prime", "Ana", "White Castle in New Brunswick", "The Chump", "brian smalley", "beryl markham", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "Blackstar", "Indian", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "1889", "Nicki Minaj", "mabel", "French Huguenot ancestry", "brian cheney", "friedrich", "Drawn Together", "William the Conqueror", "Ben Gurion International Airport", "two degrees of freedom", "Mainland Greece", "taking blood samples from patients and correctly cataloging them for lab analysis", "Guinness World Records", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21529569662793346}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.10526315789473684, 1.0, 0.25, 1.0, 0.25, 0.0, 0.0, 0.0, 0.08333333333333334, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2857142857142857, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating", "David Feldman", "the Sackler Centre for arts education", "maryland", "kaleidoscope", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "Apollo", "ribosomal", "kookaburra", "six", "CCH Pounder", "I Swear", "Cozonac", "brian idlin", "heliocentric", "Sulla", "Super Bowl LII, following the 2017 season", "Golden Globe", "Swahili", "bring about necessary change", "williams", "Pantone Matching System", "Firoz Shah Tughlaq", "My Love from the Star", "San Jose", "sea wasp", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "Thunderbird of Native American tradition", "giving Super Bowl ever", "29.7", "b.J. Hunnicutt"], "metric_results": {"EM": 0.21875, "QA-F1": 0.32318722943722944}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.48484848484848486, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.888888888888889, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_hotpotqa-validation-3547", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_squad-validation-393", "mrqa_triviaqa-validation-935"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "James Blundell", "Yazoo", "22 April 1894", "a star with less mass than 20 suns", "tired from his daily labour (defatigus diurno labore) who at night enters his bedchamber (sub noctem intrat in cubiculum suum) and whose sleep is interrupted by dreams", "as defence of their North American colonies would no longer be an issue and also because they already had ample places from which to obtain sugar", "Willie Nelson and Kris Kristofferson", "ill. (chiefly col.)", "5 University of California", "a French pirate active in the Caribbean and off the coast of Africa", "Lewis", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "proteins", "2001", "(i.e. exceeds any given number)", "10:30 in 2004", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "Skogsr\u00e5", "western portions of the Great Lakes region", "Orthodox Christians", "James Bond", "4 in ( 10 cm )", "fillies", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "\"Menace II Society\"", "backup", "trio"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25498504226251906}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.3111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4210526315789474, 1.0, 1.0, 0.888888888888889, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.19999999999999998, 0.2222222222222222]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-16836", "mrqa_naturalquestions-train-40071", "mrqa_naturalquestions-train-62037", "mrqa_naturalquestions-train-29465", "mrqa_naturalquestions-train-48080", "mrqa_naturalquestions-train-16989", "mrqa_naturalquestions-train-28613", "mrqa_naturalquestions-train-69709", "mrqa_naturalquestions-train-30782", "mrqa_naturalquestions-train-85994", "mrqa_naturalquestions-train-84527", "mrqa_naturalquestions-train-59170", "mrqa_naturalquestions-train-38884", "mrqa_naturalquestions-train-70661", "mrqa_naturalquestions-train-60425", "mrqa_naturalquestions-train-10153", "mrqa_squad-validation-9598", "mrqa_squad-validation-358", "mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-2191", "mrqa_naturalquestions-validation-3951", "mrqa_squad-validation-2053", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-7512", "mrqa_triviaqa-validation-4583", "mrqa_naturalquestions-validation-8877", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-6736", "mrqa_triviaqa-validation-1736"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "Mediterranean Shipping Company S.A.", "alison marx", "his friends, Humpty Dumpty and Kitty Softpaws", "The Liberals' main support lies in Melbourne's more affluent eastern and outer suburbs", "Royalists", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "al\u00e9a seydoux", "jules verne's 20,000 leagues under the sea, but also invented key characters which have passed into world literature - the essential Englishman of Phileas Fogg and the mad, brilliant scientist Captain Nemo", "Augustus Waters", "1619", "Tony Blair: The Journey", "\u2018expensive damaging\"", "June 11, 1973", "Kenya", "Timeline of Shakespeare criticism", "boudicca", "an active supporter of the League of Nations", "United Healthcare", "AMC Entertainment Holdings, Inc.", "The Gang", "3 October 1990", "March 1, 2018", "The weak force is due to the exchange of the heavy W and Z bosons", "Blandings Castle", "Martin Luther King III", "Manhattan Project", "Chronicles of Barsetshire", "undeveloped"], "metric_results": {"EM": 0.09375, "QA-F1": 0.25810696248196247}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.4, 0.8571428571428571, 0.0, 0.4444444444444445, 0.0, 0.33333333333333337, 0.1111111111111111, 0.16, 0.0, 0.06666666666666667, 0.3636363636363636, 1.0, 0.8, 0.5, 0.0, 0.0, 0.22222222222222224, 0.0, 0.1818181818181818, 1.0, 0.3333333333333333, 0.0, 0.2, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_naturalquestions-validation-1328", "mrqa_squad-validation-2828"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "javier (Luna)", "red", "Luna Park", "Best Animated Feature", "European Union institutions", "(381.6 days)", "nine", "CAL IPSO", "celandine flowers", "James `` Scotty '' Reston", "Ronald Ralph \"Ronnie\" Schell", "artemisinin- Based therapy", "Mumbai, Maharashtra", "in the east of Ireland", "1940", "the 2017 / 18 Divisional Round game against the New Orleans Saints", "possibly 1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "southern part of Nigeria", "dambala", "synovial joint", "bobby riggs", "Democritus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Ted Ginn Jr."], "metric_results": {"EM": 0.21875, "QA-F1": 0.3902994574869575}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.5, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.3636363636363636, 0.6666666666666666, 0.0, 0.2, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "electrons are taken by NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "the Duke of Cumberland", "alchemy", "WBC and lineal titles", "moluccas", "the first Saturday in May", "Cordelia", "Israeli troop withdrawal from parts of the Sinai Peninsula", "1971", "The Number Twelve Looks Like You", "John Elway", "Selena Gomez", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "is an unofficial title sometimes given to new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "letter series", "Fa Ze Rug", "nine circles", "Mongols and a Muslim", "along the eastern coast of the continent", "Poor Clares", "CD Castell\u00f3n", "between 1770 and 1848", "12\u20134", "ctenophores are distinguished from all other animals by having colloblasts, which are sticky and adhere to prey", "Jon M. Chu and choreographed by Jamal Sims, Nadine \" Hi Hat\" Ruffin and Dave Scott", "STS-51-C.", "it will retreat to its den and winter will persist for six more weeks", "mitterrand"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3230448007043752}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.06666666666666667, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.6486486486486487, 0.0, 0.0, 0.4, 0.7692307692307693, 0.0, 0.4, 0.0, 0.5, 0.5714285714285715, 1.0, 0.4, 0.0, 1.0, 0.3, 0.0, 0.0, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-37199", "mrqa_naturalquestions-train-33873", "mrqa_naturalquestions-train-47597", "mrqa_naturalquestions-train-61570", "mrqa_naturalquestions-train-50560", "mrqa_naturalquestions-train-64936", "mrqa_naturalquestions-train-79980", "mrqa_naturalquestions-train-10226", "mrqa_naturalquestions-train-48080", "mrqa_naturalquestions-train-74365", "mrqa_naturalquestions-train-77895", "mrqa_naturalquestions-train-43069", "mrqa_naturalquestions-train-84498", "mrqa_naturalquestions-train-22607", "mrqa_naturalquestions-train-61954", "mrqa_naturalquestions-train-65285", "mrqa_naturalquestions-validation-8617", "mrqa_triviaqa-validation-2442", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-4520", "mrqa_triviaqa-validation-2797", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-6259", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-2753", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1758", "mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-3714"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["synchronized skating", "Ishmael", "over 50 million singles", "secessionists of the Confederate States, who advocated for states'rights to expand slavery", "1923 and 1925", "Metropolitan Statistical Area", "January 19, 1962", "Frigate", "Buck Barrow", "d'Hondt method", "Pomeranian", "the move from the manufacturing sector to the service sector", "jagera peoples", "Peter Davison, Colin Baker and Sylvester McCoy", "August 14, 1848", "lower level of economic growth when human capital is neglected for high-end consumption", "In at least some species, juveniles are capable of reproduction before reaching the adult size and shape", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "A Chorus Line", "2,664 rooms and 220 suites", "dudxDQgf", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning, support services, property services, catering services, security services and facility management services", "Symphony No. 7", "dordogne", "1603", "ranked above the two personal physicians of the Emperor", "flute", "Sought through prayer and meditation to improve our conscious contact with God as we understood Him, praying only for knowledge of His will for us and the power to carry that out", "wrigley"], "metric_results": {"EM": 0.125, "QA-F1": 0.30203440350499167}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.13333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.23529411764705882, 0.9333333333333333, 0.4615384615384615, 0.923076923076923, 0.0, 0.33333333333333337, 0.0, 0.2857142857142857, 0.3636363636363636, 0.47058823529411764, 1.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.13636363636363635, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences, with the Romans serving as external arbiters on disputes concerning Jewish customs and law", "south of the Kancamagus Highway", "an investment technique outlined by Joel Greenblatt that uses the principles of value investing", "austin-born Peter Carey", "Haleiwa Ali'i Beach Park", "1910", "non-teaching posts", "Catch Me Who Can", "jazz saxophonist", "Harry Hopman", "4,000", "Khagan", "Catherine Earnshaw", "canal", "spice", "``The Simpsons Spin-Off Showcase\"", "Raymond Unwin", "San Bernardino", "an extensive neoclassical centre referred to as Tyneside Classical largely developed in the 1830s by Richard Grainger and John Dobson", "Albany High School for Educating People of Color", "Agra garden", "a non-commissioned officer in the United States Army's premier special operations unit, the 1st Special Forces Operational Detachment- Delta (1SFOD-D) or \" Delta Force\"", "Anakin Skywalker", "seek jury nullification", "Cee - Lo", "Anglican", "Hattie McDaniel", "battlecruiser Renown", "animal magnetism", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "redistributive", "1757"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2837121212121212}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false], "QA-F1": [0.2222222222222222, 0.4, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.09999999999999999, 0.5454545454545454, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-7435", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A. producer Bones Howe", "jools Holland", "blackberry", "dark dust and gases", "\" Big Mamie\"", "ocelots", "the \"eternal outsider, the sardonic drifter\" someone who rebels against the social structure", "a light sky-blue color caused by absorption in the red", "the peasants who had not paid their poll tax", "2009", "the Golden State Warriors", "the internal thylakoid system", "marisot", "the Bright Autumn Festival", "B SkyB has no veto over the presence of channels", "the fourth season", "the weak and electromagnetic forces are expressions of a more fundamental electrostrong interaction", "availability of skilled tradespeople", "diamond", "A simple iron boar crest adorns the top of this helmet", "polytechnics became new universities", "curtin", "James David Lofton", "25 - yard line", "the Latin centum, which means 100, and gradus", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "by faith", "margaret", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Antwerp", "company"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31908534174159175}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.39999999999999997, 0.28571428571428575, 0.0, 0.6153846153846153, 1.0, 0.0, 0.5, 0.375, 0.0, 1.0, 1.0, 0.8, 0.0, 0.6060606060606061, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-22454", "mrqa_naturalquestions-train-54675", "mrqa_naturalquestions-train-81443", "mrqa_naturalquestions-train-51038", "mrqa_naturalquestions-train-11301", "mrqa_naturalquestions-train-31694", "mrqa_naturalquestions-train-65328", "mrqa_naturalquestions-train-75553", "mrqa_naturalquestions-train-81443", "mrqa_naturalquestions-train-6646", "mrqa_naturalquestions-train-61434", "mrqa_naturalquestions-train-63952", "mrqa_naturalquestions-train-72993", "mrqa_naturalquestions-train-5882", "mrqa_naturalquestions-train-5744", "mrqa_naturalquestions-train-29041", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5810", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-46", "mrqa_triviaqa-validation-313", "mrqa_triviaqa-validation-4055", "mrqa_hotpotqa-validation-1436", "mrqa_triviaqa-validation-6125", "mrqa_squad-validation-3181", "mrqa_squad-validation-2629", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-4856", "mrqa_hotpotqa-validation-945", "mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-9532"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["The Turk", "Chris Weidman", "nullification", "Raghu II", "writer", "Professor Eobard Thawne", "slivovitz", "a US$10 a week raise over Tesla's US$18 per week salary", "October 25, 1825 \u2013 June 3, 1899", "member states", "clarinets", "management consulting firm McKinsey & Company", "Cynophobia", "Cliff Richard", "Crohn's disease or ulcerative colitis", "Ondemar Dias", "Raya Yarbrough", "Arizona", "renoir", "Charles L. Hutchinson", "the Old Testament", "UPS", "touring productions", "Football League", "tristan Farnon", "mafic", "that contemporary accounts were exaggerations", "John Surratt, Jr.", "1349", "dodo bird", "a person can improve their own health, wealth and personal relationships", "Stan Butler"], "metric_results": {"EM": 0.1875, "QA-F1": 0.30081332101806235}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.25, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.13793103448275862, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_naturalquestions-validation-10687", "mrqa_squad-validation-5086", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["abraham lincoln", "1,100 years ago", "used to finance his own projects with varying degrees of success", "24 Hours of Le Mans", "Kinect motion controller", "Tokyo", "safety Darian Stewart", "parallelogram rule of vector addition", "abraham lincoln", "364", "a neutron source used for stable and reliable initiation of nuclear chain reaction in nuclear reactors, when they are loaded with fresh nuclear fuel, whose neutron flux from spontaneous fission is insufficient for a reliable startup, or after prolonged shutdown periods", "Van Gogh", "volume as this steam occupies a greater volume", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "National Basketball Development League (NBDL)", "river medway", "St. Mary's County", "Graham Gano", "2,615", "Pyeongchang", "athlete", "a password recovery tool for Microsoft Windows", "Homeless Man", "Charles and Ray Eames", "florida", "abraham lincoln", "the smallest subfield of a field F containing both 0 and 1", "heartburn", "a Gender pay gap in favor of males in the labor market", "light reactions"], "metric_results": {"EM": 0.15625, "QA-F1": 0.29342522265316384}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 1.0, 0.14285714285714288, 1.0, 0.22222222222222224, 0.47058823529411764, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.058823529411764705, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_naturalquestions-validation-8653", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "Fort Frederick", "an outgoing, eccentic, big - hearted, loving, sweet, and thoughtful elephant and teacher", "abraham lincoln", "arthur", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "ABC", "arthur", "historical political divisions", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "monza", "arthur rex", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "A computer program", "National Party", "marduk", "polar bear", "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "pike", "off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Easy", "Meredith Brody ( Zoe McLellan ), a transfer from the NCIS Great Lakes field office, who has worked as a Special Agent Afloat and is keen to leave her past behind as she moves to New Orleans", "Nehru", "National Lottery", "arthur", "aragon", "in order to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1492007927572943}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 0.0, 0.4, 0.12121212121212122, 0.0, 0.11764705882352941, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3636363636363636]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-45137", "mrqa_naturalquestions-train-85757", "mrqa_naturalquestions-train-20716", "mrqa_naturalquestions-train-67582", "mrqa_naturalquestions-train-11890", "mrqa_naturalquestions-train-81632", "mrqa_naturalquestions-train-27296", "mrqa_naturalquestions-train-40101", "mrqa_naturalquestions-train-72194", "mrqa_naturalquestions-train-66569", "mrqa_naturalquestions-train-40685", "mrqa_naturalquestions-train-23109", "mrqa_naturalquestions-train-75065", "mrqa_naturalquestions-train-41190", "mrqa_naturalquestions-train-41121", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-7157", "mrqa_naturalquestions-validation-5175", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3757", "mrqa_triviaqa-validation-4791", "mrqa_triviaqa-validation-3472", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-8280", "mrqa_triviaqa-validation-254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Thiel", "skaters", "alexander", "the crossroads of the Newell Highway between Melbourne and Brisbane", "androids", "a horse is 15 hands, 2", "shopping mall", "the ones who are violating the greater law are the members of the Navy", "DreamWorks Animation", "Johann Strauss II", "his own men", "emissions resulting from human activities", "poseidon", "the RAF", "reduce growth", "Ibrium", "surt Daly", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "alexander", "surtree", "an estimated 390 billion individual trees divided into 16,000 species", "Washington Street", "May 10, 1976", "six", "Lexy Gold", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "surtsey", "John Smith", "surtania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1746159511784512}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.45000000000000007, 0.0, 1.0, 0.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A cylindrical Service Module (SM) supported the Command Module, with a service propulsion engine and an RCS with propellants, and a fuel cell power generation system with liquid hydrogen and liquid oxygen reactants", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "beer", "gender test", "Matt Jones", "kalium", "In extreme circumstances, a driver may attempt to jackknife the vehicle deliberately in order to halt it following brake failure", "CD4+ and CD8+ (\u03b1\u03b2) T cells", "relatively low salaries", "sweet corn", "Heading Out to the Highway", "Moonraker", "$12.99", "Michael Oppenheimer", "England national team", "\"degrees of privilege\"", "No Night Today", "Convention", "5,922", "December 5, 1991", "\"Chronicle\"", "2016 NBA draft", "the historical Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Potsdam", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "Saturday or Sunday", "Dealey Plaza", "Nairobi", "the last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2498156055900621}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.08695652173913045, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "\" Boston Herald\" Rumor Clinic", "1967", "legprints in the Sand", "the twelfth most populous city in the United States", "115", "bridge of the Titanic, the Olympic", "through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus, causing changes in gene expression", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "Bass", "Chava with Fyedka", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "japan", "ship's deck or sides", "Yunnan- Fu (\u4e91\u5357\u5e9c, \"Y\u00fann\u00e1nf\u01d4\")", "Mumbai, India", "Broken Hill and Sydney", "2005", "all punishments and granted them salvation", "\"The Doctor's Daughter\"", "bridge", "bridge", "bridge collapses or explosions", "1879", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "learning from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "prevent any contaminants in the sink from flowing into the potable water system", "enthusiasm and energy of the teacher", "pasternak", "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle", "Bill Clinton", "Oslo county"], "metric_results": {"EM": 0.09375, "QA-F1": 0.17105924406475878}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.08, 0.0, 0.0, 0.12121212121212123, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_squad-validation-1903", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-76559", "mrqa_naturalquestions-train-32574", "mrqa_naturalquestions-train-82868", "mrqa_naturalquestions-train-68154", "mrqa_naturalquestions-train-84074", "mrqa_naturalquestions-train-20824", "mrqa_naturalquestions-train-21041", "mrqa_naturalquestions-train-82203", "mrqa_naturalquestions-train-51316", "mrqa_naturalquestions-train-54122", "mrqa_naturalquestions-train-86032", "mrqa_naturalquestions-train-61346", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-36452", "mrqa_naturalquestions-train-75924", "mrqa_naturalquestions-train-55299", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3780", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-7017", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1850", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-2802", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-1588", "mrqa_naturalquestions-validation-9327", "mrqa_hotpotqa-validation-3632"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Speaker", "september", "Threatening government officials", "Veronica", "Victorian College of the Arts", "Winston Churchill", "is a simple dish of stock, potatoes, and onions", "0.2 inhabitants per square kilometre", "a Matter of Life and Death", "France", "Ian Paisley", "bataan", "euro", "polly", "Imperial Japan", "1973", "London and New York, c. 1886", "2008", "Manhattan", "Oliver Reed as Antonius Proximo", "pole", "Johnny Darrell", "a waxy substance called plaque", "a Belgian law requiring all margarine to be in cube shaped packages", "Euler's totient function", "when enough earwax accumulates to cause symptoms or to prevent a needed assessment of the ear by your doctor", "binary strings", "second-busiest airport in the United States by passenger volume", "red", "Honda Accord", "Kurt Vonnegut", "september"], "metric_results": {"EM": 0.0625, "QA-F1": 0.12870115995115994}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.14285714285714288, 0.6153846153846153, 0.0, 0.2222222222222222, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.1111111111111111, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_naturalquestions-validation-951", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["average predator populations during hypothetical outbreaks of 14- and 15-year cicadas would be up to 2% higher", "based on the interplay of supply and demand, which determines the prices of goods and services", "Nathan Mack", "brain, muscles, and liver", "butterfly", "Washington Redskins", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "William Howard Ashton", "national security, big oil companies and bribery and corruption", "high and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Broward County", "\"Joint Security Area\"", "to combine keys which are usually kept separate", "king Charles I", "from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "if the income share of the top 20 percent (the rich) increases", "Beauty and the Beast", "south afghanistan", "Tyler \" Ty\" Mendoza", "alamo", "a seal illegally is broken", "the UMC", "Brian Liesegang", "Roger Allers and Rob Minkoff", "Port Moresby, Papua New Guinea", "Alvin Simon Theodore Ross Bagdasarian David Seville", "National Association for the Advancement of Colored People", "1963\u20131989", "unsinkable ship", "John Smith", "1960s and '70s suburban America", "6500 - 1500 BC"], "metric_results": {"EM": 0.25, "QA-F1": 0.3477560455271852}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.38095238095238093, 0.11111111111111112, 0.0, 0.4, 1.0, 0.0, 0.25, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0625, 1.0, 0.4615384615384615, 0.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_triviaqa-validation-6450", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-1136", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Kevin Costner", "planet Uranus", "president Ford's genealogy", "Cobham\u2013Edmonds thesis", "usually human, or humanoid aliens", "prefix", "March 2012", "jazz musicians and other residents of the city", "Muhammad Ali", "Bruno Mars", "Spain", "to civil disobedients", "Julius Caesar", "2", "1898", "a virtual reality simulator", "formal language", "h Hexham, Northumberland, England", "right side of the heart to the lungs", "Miasma theory", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges", "white cross", "significant production of peaches as early as 1571, with exports to other states occurring around 1858", "utica dioica", "US$3 per barrel", "flat rate", "love is All Around - Top Of The Pops", "in the ARPANET", "roughly west", "Sudan"], "metric_results": {"EM": 0.15625, "QA-F1": 0.31486238434767844}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.8, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 0.0, 0.7272727272727273, 0.8, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_squad-validation-3635", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-7597", "mrqa_naturalquestions-train-51432", "mrqa_naturalquestions-train-27101", "mrqa_naturalquestions-train-72578", "mrqa_naturalquestions-train-27562", "mrqa_naturalquestions-train-21913", "mrqa_naturalquestions-train-39475", "mrqa_naturalquestions-train-74951", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-23698", "mrqa_naturalquestions-train-8862", "mrqa_naturalquestions-train-65380", "mrqa_naturalquestions-train-45598", "mrqa_naturalquestions-train-63601", "mrqa_naturalquestions-train-11744", "mrqa_naturalquestions-train-51879", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-7567", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3189", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-3474", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-3971", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-7704", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-1168", "mrqa_squad-validation-4417", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-2465", "mrqa_hotpotqa-validation-2016"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["Southern Pacific", "three of his ribs were broken", "2007", "Post Alley under Pike Place Market", "mother-of-pearl", "February 20, 1978", "haggis", "Walter Mondale", "96", "the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "south koreans", "black", "the alluvial plain", "37 \u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "Spotty Dog", "Rumplestiltskin", "Carlos Tevez", "mammals", "numerous musical venues, including the Teatr Wielki, the Polish National Opera, the Chamber opera, the National Philharmonic Hall and the National Theatre, as well as the Roma and Buffo music theatres", "riper grapes", "1991", "polyphemus", "7 January 1936", "lifetime protection", "twenty- three", "Edwin Hubble", "Many of the city's tax base dissipated, leading to problems with funding education, sanitation, and traffic control within the city limits. In addition, residents in unincorporated suburbs had difficulty obtaining municipal services", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Christian Bj\u00f8rnsh\u00f8j Poulsen", "mistreatment", "sour cream", "Boston, Massachusetts"], "metric_results": {"EM": 0.25, "QA-F1": 0.2813608776844071}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2777777777777778, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_triviaqa-validation-2524"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["NP-complete Boolean satisfiability problem", "Dan Stevens", "New England", "Bart Cummings", "buffalo", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "a children's story published by John Newbery in London in 1765", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "gathering money from the public, which circumvents traditional avenues of investment", "Eden and Thorgan", "commissioned to purchase their required uniform items", "Don Jeffrey \" Jeff\" Meldrum", "741 weeks", "Phil Archer", "Shoshone, his mother tongue, and other western American Indian languages", "The Paris Sisters", "suez Canal", "57", "journalist", "the fact that there is no revising chamber", "women", "the points of algebro-geometric objects, via the notion of the spectrum of a ring", "most of the items in the collection, unless those were newly accessioned into the collection, probably don't show up in the computer system", "free floating and depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "glycine receptor", "Alta Wind Energy Center in California", "the early 16th century", "Lord's", "is a Manchester-based businessman, the founder of the then technologically advanced UK newspaper Today in 1986, and of the short-lived tabloid The Post", "King Louie - The orangutan who leads the Bandar - log. In the 2016 film, he is a Gigantopithecus. He is voiced by Louis Prima in the first movie", "in sequence with each heartbeat"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2849027785053004}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.09523809523809523, 0.0, 0.0, 0.0606060606060606, 0.8695652173913044, 0.28571428571428575, 0.22222222222222224, 0.4444444444444445, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0, 1.0, 0.6, 0.0, 0.0, 0.34782608695652173, 0.06451612903225806, 0.0, 0.0, 0.7499999999999999, 0.0, 0.10526315789473684, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-3320", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Taipei", "Dan Conner", "Berlin", "Kennedy assassination", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "keskes", "the 1980s", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA", "New York City", "a man who makes potions in a traveling show", "2003", "the antlers are dropped or shed and grown anew each and every year", "NTV", "the second Sunday of March", "relative units of force and mass", "woman", "two", "August 10, 1933", "The Golden Gate Bridge", "Sochi, Russia", "those who already hold wealth", "bilingual German author B. Traven", "Finding Nemo", "unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH)", "oil", "wooded areas", "264,152", "Princeton, New Jersey", "the German Empire", "high pressure or an electric current"], "metric_results": {"EM": 0.3125, "QA-F1": 0.42569444444444443}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.8, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_squad-validation-8070", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-9244", "mrqa_naturalquestions-train-26711", "mrqa_naturalquestions-train-39006", "mrqa_naturalquestions-train-41598", "mrqa_naturalquestions-train-17531", "mrqa_naturalquestions-train-30000", "mrqa_naturalquestions-train-7779", "mrqa_naturalquestions-train-59170", "mrqa_naturalquestions-train-4796", "mrqa_naturalquestions-train-52136", "mrqa_naturalquestions-train-9545", "mrqa_naturalquestions-train-7779", "mrqa_naturalquestions-train-76403", "mrqa_naturalquestions-train-31248", "mrqa_naturalquestions-train-75553", "mrqa_naturalquestions-train-64729", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-2797", "mrqa_hotpotqa-validation-4868", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-6389", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-4283", "mrqa_naturalquestions-validation-3808", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3870", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-3716", "mrqa_naturalquestions-validation-3962"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "Sarajevo", "Isabella (Belle) Baumfree", "corgis", "1623\u201340", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "The antichrist of 2 Thessalonians 2", "Aristotle", "Charlton Heston", "anti-inflammatory molecules", "tARTAN", "0-2", "one of the uses of money", "the force of law, if based on the authority derived from statute or the Constitution itself", "ch\u00e2teau de chambord", "when the Mongols placed the Uighurs of the Kingdom of Qocho over the Koreans at the court the Korean King objected", "Sochi, Russia", "left", "The North Saskatchewan River is a glacier - fed river that flows from the Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "NASA immediately convened an accident review board", "new Zealand", "detroit", "30", "Secret Intelligence Service", "100 billion", "kai su, teknon", "photolysis", "4.7 / 5.5 - inch", "Queen City", "an American federal law that imposes liability on persons and companies ( typically federal contractors ) who defraud governmental programs"], "metric_results": {"EM": 0.25, "QA-F1": 0.31800595238095236}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.14285714285714288, 0.0, 0.09523809523809522, 1.0, 0.0, 0.125, 0.2857142857142857, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-2475", "mrqa_squad-validation-4953", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["war", "Hindi film industry", "Gaels", "Three card brag", "d\u00edsabl\u00f3t", "Panthera leo spelaea", "Russian film industry", "delta growth", "the Washington metropolitan area", "GTPase responsible for endocytosis", "User State Migration Tool", "Ordos City China Science Flying Universe Science and Technology Co.", "Franscioni", "PPG Paints Arena, Pittsburgh, Pennsylvania", "archaeology", "Section 30", "Paul Lynde as Templeton, a care - free, egotistical rat who lives on a web in a corner of Homer's barn above Wilbur's pig pen", "mid-1988", "quasars", "Retreating Monsoon", "Romansh", "ltd", "5AA", "James Bond", "Philippi in Greece", "an Inquiry into the Nature and Causes of the Wealth of Nations", "Gerard Marenghi", "Whitney Houston", "Hugo Award", "Conservative Party", "king david", "Elvis Presley"], "metric_results": {"EM": 0.0625, "QA-F1": 0.17447927977819283}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.5, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.17391304347826084, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.30769230769230765, 0.5, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_squad-validation-9355", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["david carradine", "pasta ( usually cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "dancing", "independence from the Duke of Savoy through an alliance between the city-state of Geneva and the Swiss Confederation", "various causes", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "physicians, lawyers, engineers, and accountants", "the ones who are violating the greater law are the members of the Navy", "Danish", "centre-back", "rommel", "in the duodenum where it performs proteolysis, the breakdown of proteins and polypeptides", "glucose to form the disaccharide sucrose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "Paul Laxalt", "Extroverted Feeling ( Fi )", "Thursday", "racing", "drug choice, dose, route, frequency, and duration of therapy", "jupiter", "navigator and expedition leader", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "jules", "The Private Education Student Financial Assistance", "bow", "to sell indulgences to raise money to rebuild St. Peter's Basilica in Rome", "colonies", "two forces, one pointing north, and one pointing east", "new laws or amendments to existing laws as a bill", "Qualcomm Stadium", "hierarchy theorems"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2871091071918278}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.15384615384615383, 1.0, 1.0, 0.0, 0.1764705882352941, 0.0, 0.375, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.47058823529411764, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_squad-validation-1429", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_squad-validation-9452", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-36048", "mrqa_naturalquestions-train-16411", "mrqa_naturalquestions-train-10975", "mrqa_naturalquestions-train-52730", "mrqa_naturalquestions-train-2521", "mrqa_naturalquestions-train-24181", "mrqa_naturalquestions-train-13544", "mrqa_naturalquestions-train-11682", "mrqa_naturalquestions-train-20070", "mrqa_naturalquestions-train-23248", "mrqa_naturalquestions-train-14565", "mrqa_naturalquestions-train-77786", "mrqa_naturalquestions-train-84288", "mrqa_naturalquestions-train-14690", "mrqa_naturalquestions-train-14775", "mrqa_naturalquestions-train-45260", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-4506", "mrqa_hotpotqa-validation-2436", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2037", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-3971", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-3033", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4604", "mrqa_triviaqa-validation-1588", "mrqa_triviaqa-validation-1722", "mrqa_hotpotqa-validation-104"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["elon", "4 Points - F, H, V, W and Y.", "elizabeth", "jonathan smith", "French", "a \"homeward bounder\"", "labels", "the immune system is less active than normal", "Py", "natural-ing Ingredients- only personal care products", "a children's rhyme and song of a kind known as cumulative", "Rigoletto", "land area", "the most abundant element by mass in the Earth's crust as part of oxide compounds such as silicon dioxide", "iKEA", "216 countries and territories around the world", "south koreans", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers", "Algernod Lanier Washington", "the Outfield", "the U.S.", "Edward Furlong", "railway locomotives", "eddie fisher", "at slightly different times when viewed from different points on Earth", "Bill Fraser", "chemists Glenn T. Seaborg, the developer of the actinide concept", "Kentucky, Virginia, and Tennessee", "many areas of technology incidental to rocketry and manned spaceflight", "africa", "237 square miles", "magi"], "metric_results": {"EM": 0.09375, "QA-F1": 0.17272228867623604}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.060606060606060615, 0.0, 0.5454545454545454, 1.0, 0.4210526315789474, 0.0, 0.5, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "graduation scales, with income depending on experience", "2003", "cricket", "mexico", "campaign setting", "2003", "867 feet", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal", "Shape of You", "Ewan McGregor as Obi - Wan Kenobi : A Jedi Master and mentor of Anakin Skywalker", "14th", "picture book", "other healthcare professionals", "increased patient health outcomes and decreased costs to the health care system", "treble clef", "Gabriel Alberto Azucena (born September 23, 1988) who goes by the stage name Gawvi, formerly G-Styles", "12951 / 52 Mumbai Rajdhani Express considering average speed including halts 12049 / 50 : Agra Cantonment - H. Nizamuddin Gatimaan Express - maximum speed 160 km / h", "Rome", "May 18, 2010", "Estelle Sylvia Pankhurst", "meastricht", "Francis Bacon", "meyer", "Ministry of Corporate Affairs", "British", "the site of ancient cult activity", "mexico", "oxygen", "Hubble Space Telescope", "Sanctifying Grace", "chorale cantatas"], "metric_results": {"EM": 0.15625, "QA-F1": 0.28290550595238095}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.16666666666666666, 0.19999999999999998, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.6875000000000001, 0.0, 0.375, 0.0, 0.0, 0.0, 0.39999999999999997, 0.0, 0.0, 0.3571428571428571, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_squad-validation-9951"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["richard frank spillane", "79th", "perique", "under `` the immortal Hawke ''", "death penalty", "stout man with a \" double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "15 Sv", "rich Fisher king is the son of the king who is served from the grail", "Mangal Pandey of the 34th BNI", "Colonia Agrippina, descending to Cologne, V Alaudae, a Celtic legion recruited from Gallia Narbonensis and XXI, possibly a Galatian legion from the other side of the empire", "Lorne Greene", "four of the 50 states of the United States in their full official state names", "sweden", "eighth series", "the main road through the gated community of Pebble Beach", "Los Angeles", "French", "Henry Mills is a fictional character in ABC's television series \"once Upon a Time\"", "\"LOVE Radio\" which featured a limited selection of music genres", "Miami Marlins", "the court from its members for a three - year term", "richard travolta", "Sven Davison and David Dobkin", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "the official residence of the President of the Russian Federation", "Operation Neptune", "Mediterranean Sea"], "metric_results": {"EM": 0.125, "QA-F1": 0.23256075521700523}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.7692307692307693, 0.0, 1.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.2222222222222222, 0.0, 0.5714285714285715, 0.07142857142857142, 0.0, 0.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.18181818181818182, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_squad-validation-5852", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-5396", "mrqa_naturalquestions-train-79119", "mrqa_naturalquestions-train-1791", "mrqa_naturalquestions-train-34979", "mrqa_naturalquestions-train-49652", "mrqa_naturalquestions-train-9534", "mrqa_naturalquestions-train-84317", "mrqa_naturalquestions-train-19760", "mrqa_naturalquestions-train-67569", "mrqa_naturalquestions-train-86807", "mrqa_naturalquestions-train-53019", "mrqa_naturalquestions-train-9545", "mrqa_naturalquestions-train-33212", "mrqa_naturalquestions-train-10152", "mrqa_naturalquestions-train-85074", "mrqa_naturalquestions-train-44855", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-9400", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-6210", "mrqa_triviaqa-validation-3876", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-2092", "mrqa_naturalquestions-validation-7233", "mrqa_hotpotqa-validation-2682", "mrqa_hotpotqa-validation-389", "mrqa_squad-validation-3490", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-6019"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["bat-and-ball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "Take That, East 17 and Boyzone", "youngest publicly documented people to be identified as transgender", "electric lighting", "used their knowledge of Native American languages as a basis to transmit coded messages", "Galileo Galilei and Sir Isaac Newton", "the absenceistence of the ultraviolet catastrophe", "Premier League club Swansea City", "art", "Elizabeth Weber", "a first-person psychological horror adventure game", "hundreds of television and radio channels", "Waiting for Guffman", "1999", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "partial funding", "5% abv draught beer", "not be produced using currently available resources", "Chu'Tsai", "Liz", "least onerous", "cernobbio", "Grissom, White, and Chaffee", "multinational retail corporation", "passion fruit", "The Natya Shastra", "the sand grains cause a scrubbing noise as they rub against each other when walked on", "golf", "parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass", "emperor of austro-Hungarian Empire"], "metric_results": {"EM": 0.3125, "QA-F1": 0.38994264535751605}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.375, 1.0, 0.06666666666666667, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.1818181818181818, 0.2857142857142857, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0689655172413793, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_squad-validation-10489", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["Vice President Al Gore and Connecticut Senator Joe Lieberman", "horse racing", "Burnley and the New Zealand national team", "the famous Tibetan monastery of Kumbum Monastery or Ta'er Shi near Xining", "Styal Mill", "big - name lawyers", "CGI computer animation", "when they enter the army during initial entry training", "moral tale", "they announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled ( 2004 )", "leeds", "A tree - topper or treetopper is a decorative ornament placed on the top ( or `` crown '' ) of a Christmas tree or Hanukkah bush", "260", "heathrow", "often social communities with considerable face-to-face interaction among members", "William Strauss and Neil Howe", "monophyletic", "insects", "specific catechism questions", "a pH indicator, a color marker, and a dye", "about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "63,182,000", "John Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Campbellsville University", "jett Rink", "appearing as Jude in the musical romance drama film \" Across the Universe\" (2007)", "mcdonalds", "Maria works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "XXXTentacion", "stagnant wages for the working class amidst rising levels of property income for the capitalist class"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4383428759493001}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.5, 0.8, 0.7777777777777778, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5384615384615384, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.7368421052631579, 1.0, 0.10526315789473682]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_squad-validation-7182"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["Anthony John Herrera", "Good Kid, M.A.A,D City", "el Capitan", "Interventive treatment", "3", "Bishop Reuben H. Mueller", "Ray Charles", "During his epic battle with Frieza", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "rue Miollis", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "not given at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater", "sin", "annuity", "twin sister", "Buffalo Bill", "justice", "France", "a peace", "cappuccino", "halal meat", "Arthur Russell (born Charles Arthur Russell, Jr. May 21, 1951 \u2013 April 4, 1992)", "pastors and teachers", "Wylie Draper", "it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, while the contrary idea that Islam is, or can be, apolitical is an error", "over the university's off- campuses rental policies", "hockey greats Bobby Hull and Dennis Hull", "New England Patriots", "war, famine, and weather"], "metric_results": {"EM": 0.3125, "QA-F1": 0.39861111111111114}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2666666666666667, 0.06666666666666667, 0.6, 0.26666666666666666, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264"], "retrieved_ids": ["mrqa_naturalquestions-train-30422", "mrqa_naturalquestions-train-84492", "mrqa_naturalquestions-train-61069", "mrqa_naturalquestions-train-32053", "mrqa_naturalquestions-train-59571", "mrqa_naturalquestions-train-47922", "mrqa_naturalquestions-train-17984", "mrqa_naturalquestions-train-60171", "mrqa_naturalquestions-train-64565", "mrqa_naturalquestions-train-37194", "mrqa_naturalquestions-train-4927", "mrqa_naturalquestions-train-64796", "mrqa_naturalquestions-train-36960", "mrqa_naturalquestions-train-44859", "mrqa_naturalquestions-train-77155", "mrqa_naturalquestions-train-26879", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-7731", "mrqa_hotpotqa-validation-57", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-4823", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-6438", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-6091", "mrqa_hotpotqa-validation-945", "mrqa_squad-validation-13"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["suitable for use on rough terrain", "AOL", "Timur", "\"Losing My Religion\"", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "chiraptophobia", "1937", "improved", "biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "he had assigned them to the company in lieu of stock", "Marxist and a Leninist", "variation in plants", "reserved to, and dealt with at, Westminster", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "3,600 Frenchmen", "State Street", "al - Mamlakah al - \u02bbArab\u012byah", "44 hectares", "georgia cukor", "sun", "Lawton Mainor Chiles Jr.", "In the episode `` Kobol's Last Gleaming ''", "Meyer v. Nebraska", "all kinds of domination or control by a group of people over another", "tea or porridge with bread, chapati, mahamri, boiled sweet potatoes or yams. Ugali with vegetables, sour milk, meat, fish or any other stew is generally eaten by much of the population", "energy", "Nebraska", "Ruth Elizabeth \"Bette\" Davis", "rhenium", "7 December 2004"], "metric_results": {"EM": 0.125, "QA-F1": 0.2646879077900419}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 0.888888888888889, 0.0, 0.0, 0.25, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.5365853658536585, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 50, "before_eval": {"predictions": ["basketball", "Igor Stravinsky, Carl Orff, Paul Hindemith, Richard Strauss, Luigi Nono, Krzysztof Penderecki and Joaqu\u00edn Rodrigo", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie ), typically cooked in a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with mashed potato", "Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "murder in...T.S. Eliot", "Big Machine Records", "conflicting territorial claims between British and French colonies in North America were turned over to a commission to resolve, but it reached no decision", "the Hindu sage Valmiki", "Epistle to Ramsay", "every two to six years ( depending on the positions being filled with most positions good for four years )", "Chinese", "association football", "2016", "Dan Castellaneta", "2007", "Wicked Twister", "counting and working with numbers and fractions", "Atlas ICBM", "shoe", "unclear as to how or whether this connection is relevant on microscales", "American supernatural horror", "originate in the House of Representatives", "The Revenant", "the evening of the same day", "\"Blue (Da Ba Dee\") is a song by the Italian music group Eiffel 65.", "USS Constitution Heavy Frigate Sailing Warship", "Faurot Field", "Annette Charles", "the `` sandbar '' between river of life, with its outgoing `` flood '', and the ocean that lies beyond ( death ), the `` boundless deep '', to which we return.", "5", "kahramanmara\u015f", "If the car is slowed initially by manual use of the automatic gear box"], "metric_results": {"EM": 0.125, "QA-F1": 0.2979840267504741}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.4210526315789474, 0.1081081081081081, 0.2222222222222222, 0.3333333333333333, 1.0, 0.28571428571428575, 0.6666666666666666, 0.0, 0.45454545454545453, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.4, 0.15384615384615385, 0.5, 1.0, 0.0, 0.8400000000000001, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-3395", "mrqa_triviaqa-validation-813", "mrqa_squad-validation-10171", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-1607", "mrqa_naturalquestions-validation-7707", "mrqa_squad-validation-8134", "mrqa_hotpotqa-validation-1528", "mrqa_naturalquestions-validation-7812", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-118", "mrqa_triviaqa-validation-290", "mrqa_hotpotqa-validation-1350", "mrqa_triviaqa-validation-3428", "mrqa_squad-validation-10427", "mrqa_hotpotqa-validation-304", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-3572", "mrqa_squad-validation-2275", "mrqa_hotpotqa-validation-2635", "mrqa_triviaqa-validation-7770", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-71", "mrqa_squad-validation-290", "mrqa_triviaqa-validation-239", "mrqa_naturalquestions-validation-3022"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 51, "before_eval": {"predictions": ["third", "the pulmonary arteries", "a card from a pack of playing cards by Alice, yet somehow she is able to talk and is the ruler of the lands in the story, alongside her tiny husband, the King of Hearts", "Los Angeles, Orange, San Diego, San Bernardino, and Riverside", "Alex Breckenridge", "seven", "Schr\u00f6dinger equation", "meat", "Plymouth Regional High School (PRHS) is a public secondary school", "usually given privately in the principal's office", "Ghostface mask", "Roger Thomas Staubach", "AC induction motor and transformer", "originally a three-part retrospective in tribute to Eric Morecambe", "American rock band Queens of the Stone Age", "the cocoa plant was discovered in regions of Mesoamerica, until the present", "Robert John Day", "1775\u20131795", "Empiricism", "w Somerset maugham", "Pabst Brewing Company", "redistributive", "Tyrion Lannister", "Ozzie Owl", "Content", "Saint Peter ( the keeper of the `` keys to the kingdom '' )", "Fourth Home Rule Bill", "t\u00e1o qu\u00e2n", "Gebhard v Consiglio dell\u2019Ordine degli Avvocati e Procuratori di Milano", "Ukraine ( ; Ukrainian: \u0423\u043a\u0440\u0430\u0457\u043d\u0430, \" Ukrajina\") sometimes called the Ukraine, is a sovereign state in Eastern Europe, bordered by Russia to the east and northeast, Belarus", "1835", "King Christopher Hatton"], "metric_results": {"EM": 0.09375, "QA-F1": 0.24117245601620602}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.6, 0.33333333333333337, 0.761904761904762, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.0, 0.4, 0.0, 0.3636363636363636, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.15384615384615383, 0.0909090909090909, 0.4, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4919", "mrqa_naturalquestions-validation-5589", "mrqa_squad-validation-2432", "mrqa_naturalquestions-validation-6634", "mrqa_squad-validation-10386", "mrqa_triviaqa-validation-7398", "mrqa_hotpotqa-validation-5740", "mrqa_squad-validation-1932", "mrqa_triviaqa-validation-787", "mrqa_hotpotqa-validation-4795", "mrqa_squad-validation-1185", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-4922", "mrqa_naturalquestions-validation-7484", "mrqa_hotpotqa-validation-4528", "mrqa_naturalquestions-validation-7312", "mrqa_triviaqa-validation-4890", "mrqa_hotpotqa-validation-596", "mrqa_squad-validation-7324", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-1046", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-91", "mrqa_hotpotqa-validation-2549", "mrqa_triviaqa-validation-3980", "mrqa_squad-validation-4430", "mrqa_hotpotqa-validation-3033", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3357"], "retrieved_ids": ["mrqa_naturalquestions-train-51471", "mrqa_naturalquestions-train-59724", "mrqa_naturalquestions-train-83640", "mrqa_naturalquestions-train-29041", "mrqa_naturalquestions-train-75553", "mrqa_naturalquestions-train-87759", "mrqa_naturalquestions-train-25842", "mrqa_naturalquestions-train-46447", "mrqa_naturalquestions-train-69740", "mrqa_naturalquestions-train-14381", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-22245", "mrqa_naturalquestions-train-12403", "mrqa_naturalquestions-train-86122", "mrqa_naturalquestions-train-7597", "mrqa_naturalquestions-train-58802", "mrqa_triviaqa-validation-3320", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-3181", "mrqa_naturalquestions-validation-4212", "mrqa_squad-validation-3151", "mrqa_hotpotqa-validation-3241", "mrqa_hotpotqa-validation-1391", "mrqa_triviaqa-validation-3213", "mrqa_hotpotqa-validation-1414", "mrqa_squad-validation-358", "mrqa_triviaqa-validation-6207", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-4367", "mrqa_naturalquestions-validation-6787"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 52, "before_eval": {"predictions": ["the Myllokunmingia", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Sophie Faldo", "edinburgh", "the set of complex numbers of the form a + bi where i denotes the imaginary unit", "`` asphyxia '' ( cutting off the oxygen supply ) and cooling", "electric guitars", "Carey Mulligan, Matthias Schoenaerts, Michael Sheen, Tom Sturridge and Juno Temple", "Dennis Keith Rodman (born May 13, 1961) is an American retired professional basketball player, who played for the Detroit Pistons, San Antonio Spurs, Chicago Bulls, Los Angeles Lakers, and Dallas Mavericks", "The construction of Olympic and Titanic took place virtually in parallel, with Olympic's keel laid down first on 16 December 1908 and Titanic's on 31 March 1909", "five", "German hymns", "Bury Football Club", "Benazir Bhutto", "American novelist, playwright, and screenwriter", "NFC Championship Game", "Tom Robinson", "old man", "2001", "the port city of Aden", "quickly to meet the needs of major national and international patient information projects and health system interoperability goals", "about two-thirds the size of cytoplasmic ribosomes ( around 17 nm vs 25 nm)", "The leopard", "the service sector", "Florida", "\"\"la f\u00e9e verte\" (the green fairy) beverage", "double science", "Bolton", "British-American", "all aspect of public and private life", "anarchists", "Just under 540,800 students", "British Sky Broadcasting Group plc"], "metric_results": {"EM": 0.125, "QA-F1": 0.30715885716169294}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.4, 0.21052631578947367, 0.0, 0.0, 0.25, 0.0, 0.3076923076923077, 0.0, 0.20689655172413793, 0.0, 0.6666666666666666, 0.28571428571428575, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5, 0.4, 0.1111111111111111, 0.4, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6188", "mrqa_naturalquestions-validation-8228", "mrqa_triviaqa-validation-4073", "mrqa_squad-validation-9081", "mrqa_naturalquestions-validation-3351", "mrqa_triviaqa-validation-2598", "mrqa_naturalquestions-validation-7688", "mrqa_hotpotqa-validation-4188", "mrqa_naturalquestions-validation-1186", "mrqa_hotpotqa-validation-5318", "mrqa_squad-validation-2401", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-182", "mrqa_squad-validation-308", "mrqa_triviaqa-validation-498", "mrqa_naturalquestions-validation-8759", "mrqa_hotpotqa-validation-1871", "mrqa_squad-validation-6389", "mrqa_squad-validation-8849", "mrqa_hotpotqa-validation-1504", "mrqa_squad-validation-7377", "mrqa_hotpotqa-validation-1402", "mrqa_triviaqa-validation-5068", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-904", "mrqa_squad-validation-2918", "mrqa_squad-validation-2772"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 53, "before_eval": {"predictions": ["Following the election of the UK Labour Party to government in 1997", "two", "c 1600", "Professor Moriarty", "Atticus Finch", "after the Seven Years' War", "raspberries", "around 70,000 BP", "catherine o'Leary", "Bronwyn Kathleen Bishop", "guidance", "pogo heath", "texas state", "Informal rule", "explores a violation of nature", "kairobi", "Nicol Williamson", "david faraday", "25 June 1932", "piccolo pinaini", "James Abram Garfield", "seasonal television specials", "chromosome", "Evey's mother", "10", "things that are a matter of custom or expectation", "Joudeh Al - Goudia family", "Wes Unseld", "1936", "nitrogen", "sediment load", "Daniel Handler"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2956629308191808}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [0.18181818181818182, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.375, 0.28571428571428575, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4100", "mrqa_squad-validation-5390", "mrqa_squad-validation-7700", "mrqa_hotpotqa-validation-4571", "mrqa_triviaqa-validation-776", "mrqa_squad-validation-9144", "mrqa_triviaqa-validation-7626", "mrqa_hotpotqa-validation-123", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-3014", "mrqa_squad-validation-9808", "mrqa_naturalquestions-validation-1161", "mrqa_triviaqa-validation-595", "mrqa_hotpotqa-validation-622", "mrqa_triviaqa-validation-6410", "mrqa_naturalquestions-validation-6485", "mrqa_triviaqa-validation-2821", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-117", "mrqa_squad-validation-6877", "mrqa_naturalquestions-validation-678", "mrqa_hotpotqa-validation-5614", "mrqa_naturalquestions-validation-6970", "mrqa_triviaqa-validation-177", "mrqa_squad-validation-9354", "mrqa_triviaqa-validation-3268"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 54, "before_eval": {"predictions": ["The Christmas Invasion", "the United States", "an expression of Priestley's socialist political principles", "March 9 to 18", "10 November 2017", "Romancing the Stone", "the back part of the tongue", "The 8th Habit", "Anishinaabeg", "general medical advice and a range of services that are now performed solely by other specialist practitioners, such as surgery and midwifery", "tropical desert climate, K\u00f6ppen classification Bwh, because of its location within the Northern desert belt", "westray", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "22 September 2015", "Murder Request", "achilles heel", "kenreb", "annually in late January or early February", "Marigold Newey", "the duodenum", "pastry", "Cuyler Reynolds", "Lacoste, France", "rum, gin, and whiskey", "cricket", "135 (VAQ-135) is a United States Navy electronic attack squadron that currently operates the EA-18G Growler carrier-based electronic warfare jet aircraft", "saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac", "various possible random outcomes or combinations of outcomes", "Chanel, Fendi, Gucci, Louis Vuitton, MaxMara, Celine, Tiffany & Co.", "\"Shoot Straight from Your Heart\"", "james austen", "Newton"], "metric_results": {"EM": 0.15625, "QA-F1": 0.28206848976358523}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.13333333333333333, 0.7272727272727273, 0.5, 0.2666666666666667, 0.0, 0.0, 0.0, 0.08695652173913042, 0.35294117647058826, 0.0, 0.4827586206896552, 1.0, 1.0, 0.0, 0.0, 0.4444444444444444, 0.0, 0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.18181818181818182, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6980", "mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-1383", "mrqa_hotpotqa-validation-4277", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2684", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-525", "mrqa_squad-validation-6211", "mrqa_naturalquestions-validation-4960", "mrqa_triviaqa-validation-5166", "mrqa_naturalquestions-validation-4021", "mrqa_triviaqa-validation-1396", "mrqa_triviaqa-validation-2356", "mrqa_naturalquestions-validation-8441", "mrqa_hotpotqa-validation-4181", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1707", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-5057", "mrqa_hotpotqa-validation-2058", "mrqa_squad-validation-10293", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3710", "mrqa_triviaqa-validation-6164", "mrqa_squad-validation-979"], "retrieved_ids": ["mrqa_naturalquestions-train-60468", "mrqa_naturalquestions-train-82754", "mrqa_naturalquestions-train-54154", "mrqa_naturalquestions-train-14477", "mrqa_naturalquestions-train-46721", "mrqa_naturalquestions-train-33424", "mrqa_naturalquestions-train-52629", "mrqa_naturalquestions-train-53971", "mrqa_naturalquestions-train-49353", "mrqa_naturalquestions-train-31451", "mrqa_naturalquestions-train-64821", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-36048", "mrqa_naturalquestions-train-19007", "mrqa_naturalquestions-train-79846", "mrqa_naturalquestions-train-25023", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-83", "mrqa_squad-validation-3463", "mrqa_squad-validation-7319", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-2914", "mrqa_hotpotqa-validation-2762", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-8095", "mrqa_hotpotqa-validation-2680", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-3074", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2150"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 55, "before_eval": {"predictions": ["over 10,000", "Denver's Executive Vice President of Football Operations and General Manager", "France", "Gracie the new `` face '' of the FBI", "Brittany, Cornwall, Ireland, Isle of Man, Scotland and Wales", "stromal connective tissue", "paris", "there are now numerous such communities across the United States and around the world", "pangea", "a low wage for that job", "octagon", "Billie Jean", "Scott Mosier", "kenya", "weaving", "the art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "sepoys of the Company's army", "Broncos were sacked by DeMarcus Ware as time expired in the half", "Jack Nicholson -- Chinatown as J.J. `` Jake '' Gittes", "dan brown", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "at the port city of Kaffa in the Crimea in 1347", "the Persian style of architecture", "Iranian", "paris", "Landwehr", "his work was published first", "as a preparation for the Feast of the Divine Mercy, celebrated each year on first Sunday after Easter", "accommodationism", "Parliamentarians ( `` Roundheads '' ) and Royalists ( `` Cavaliers '' )", "the Niger\u2013 Congo language", "a lower index of refraction, typically a cladding of a different glass, or plastic"], "metric_results": {"EM": 0.0625, "QA-F1": 0.20817740373661425}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.8571428571428571, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27272727272727276, 0.16666666666666669, 0.0, 0.0, 0.9824561403508771, 0.14285714285714285, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.36363636363636365]}}, "error_ids": ["mrqa_squad-validation-5518", "mrqa_squad-validation-378", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-6918", "mrqa_triviaqa-validation-3568", "mrqa_squad-validation-4700", "mrqa_triviaqa-validation-140", "mrqa_squad-validation-7407", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-5479", "mrqa_hotpotqa-validation-3264", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-938", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4098", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-9536", "mrqa_triviaqa-validation-7421", "mrqa_naturalquestions-validation-9459", "mrqa_squad-validation-4773", "mrqa_naturalquestions-validation-819", "mrqa_hotpotqa-validation-5543", "mrqa_triviaqa-validation-7689", "mrqa_hotpotqa-validation-4215", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-570", "mrqa_hotpotqa-validation-2699", "mrqa_naturalquestions-validation-7078"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 56, "before_eval": {"predictions": ["Randy", "1922 to 1991", "predictions that can be tested in various ways", "the city council", "moral conservatism, literalism, and the attempt \"to implement Islamic values in all spheres of life\"", "Tesla Polyphase System", "65,507 bytes ( 65,535 \u2212 8 byte UDP header \u2212 20 byte IP header )", "39, Newton was 26", "smart glasses", "increased their reserves (by expanding their money supplies) in amounts far greater than before", "\u00dcbermensch", "the model for one of the characters in Jordan Mechner's game \" Prince of Persia\"", "kevin williams", "Enotris Johnson", "five", "teachers' mental and physical health, productivity, and students' performance", "fHI", "Count de La F\u00e8re (born c. 1595 ; died 1661) is a fictional character, a Musketeer of the Guard in the novels The Three Musketeers", "if no repeated data values, a perfect Spearman correlation of + 1 or \u2212 1 occurs when each of the variables is a perfect monotone function of the other", "wurundjeri", "1932", "Angus King", "formaldehyde, glutaraldehyde, citric acid, acetic anhydride, and acetamide", "former Manchester United and Danish international goalkeeper Peter Schmeichel", "the highest commissioned SS rank", "Erinyes", "Dallol is a cinder cone volcano in the Danakil Depression, northeast of the Erta Ale Range in Ethiopia. It has been formed by the intrusion of basaltic magma into Miocene salt deposits", "kevin will Champion", "in Egypt, the only part of the country located in Asia", "Joanna Page", "the macula", "Lehman Bros International"], "metric_results": {"EM": 0.125, "QA-F1": 0.18550857843137256}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.15, 0.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.35294117647058826, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-9608", "mrqa_naturalquestions-validation-1787", "mrqa_squad-validation-371", "mrqa_naturalquestions-validation-752", "mrqa_squad-validation-3720", "mrqa_triviaqa-validation-4390", "mrqa_hotpotqa-validation-627", "mrqa_triviaqa-validation-6223", "mrqa_hotpotqa-validation-4620", "mrqa_hotpotqa-validation-3651", "mrqa_squad-validation-2004", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-326", "mrqa_naturalquestions-validation-486", "mrqa_triviaqa-validation-4640", "mrqa_hotpotqa-validation-1912", "mrqa_squad-validation-3653", "mrqa_hotpotqa-validation-15", "mrqa_hotpotqa-validation-686", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-1349", "mrqa_triviaqa-validation-5813", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-3069", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-2701"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 57, "before_eval": {"predictions": ["Saturn", "dictionary of the English Language", "60 million", "Bathurst", "April 1948", "In inequality of opportunity was higher in the transition economies of Central and Eastern Europe and Central Asia", "the use of terms such as penance and righteousness by the Catholic Church in new ways", "FeO (w\u00fcstite) is written as Fe1 \u2212 xO", "Groucho", "dustrometeoroid impact craters", "half-penny sales tax", "david frost", "Charles Haley", "paris", "glucose", "National Party", "United States Ship", "reared", "October 15, 1997", "in the Hebrew Bible in the Book of Job, Psalms, and Isaiah", "renovated the inside", "Greenland shark", "wales", "renoir", "tom Sly", "five", "tescoense", "elizabeth Taylor", "American conservative author and commentator", "chicken", "Sam the Sham", "three"], "metric_results": {"EM": 0.125, "QA-F1": 0.23278040382819795}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.3, 0.35294117647058826, 0.2222222222222222, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.8571428571428571, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-4974", "mrqa_hotpotqa-validation-1567", "mrqa_hotpotqa-validation-5436", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3969", "mrqa_squad-validation-2255", "mrqa_squad-validation-3598", "mrqa_triviaqa-validation-3648", "mrqa_squad-validation-4067", "mrqa_squad-validation-7226", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-7489", "mrqa_squad-validation-2886", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-6365", "mrqa_hotpotqa-validation-3802", "mrqa_hotpotqa-validation-3544", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-6425", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-7187", "mrqa_hotpotqa-validation-1975", "mrqa_triviaqa-validation-1742", "mrqa_hotpotqa-validation-1016", "mrqa_naturalquestions-validation-787"], "retrieved_ids": ["mrqa_naturalquestions-train-3173", "mrqa_naturalquestions-train-33717", "mrqa_naturalquestions-train-43346", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-43322", "mrqa_naturalquestions-train-48080", "mrqa_naturalquestions-train-48891", "mrqa_naturalquestions-train-24753", "mrqa_naturalquestions-train-36648", "mrqa_naturalquestions-train-14301", "mrqa_naturalquestions-train-60425", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-35923", "mrqa_naturalquestions-train-4136", "mrqa_naturalquestions-train-46447", "mrqa_naturalquestions-train-50258", "mrqa_naturalquestions-validation-1383", "mrqa_squad-validation-3671", "mrqa_naturalquestions-validation-7812", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-633", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-3980", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-2551", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-1494", "mrqa_hotpotqa-validation-4571", "mrqa_squad-validation-8456", "mrqa_hotpotqa-validation-4528", "mrqa_triviaqa-validation-5050", "mrqa_naturalquestions-validation-4288"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 58, "before_eval": {"predictions": ["Dutch", "in the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "2 : 1 blackjack payouts", "phagosomal membrane", "in areas such as South Heaton in Newcastle but once dominated the streetscape on both sides of the Tyne", "1987", "henry vii", "200 horsepower (150 kilowatts)", "Nikola Tesla", "Lutheranism", "Night Ranger", "Buzz, the Honey Nut Cheerios Bee", "Thocmentony", "Organizational interventions, like changing teachers' schedules, providing support networks and mentoring, changing the work environment, and offering promotions and bonuses, may be effective in helping to reduce occupational stress among teachers", "levels of economic inequality", "james boswell", "Arthur H. Compton", "Angelina Jolie, Brad Pitt and Amal Clooney", "henry", "1979", "mainly civil servants recruited in special university classes", "large areas", "E \u00d7 12", "the medial epicondyle of the humerus", "weight", "john donne", "Liao, Jin, and Song", "`` Han dynasty '' ( Hanchao \u6f22 \u671d )", "pigeons", "kinetic energy", "Attack the Block", "Juliet"], "metric_results": {"EM": 0.21875, "QA-F1": 0.32925347222222223}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.25, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.4, 0.7499999999999999, 1.0, 0.125, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9173", "mrqa_naturalquestions-validation-5554", "mrqa_squad-validation-8687", "mrqa_squad-validation-5202", "mrqa_hotpotqa-validation-4035", "mrqa_triviaqa-validation-6185", "mrqa_squad-validation-1443", "mrqa_squad-validation-1535", "mrqa_hotpotqa-validation-593", "mrqa_naturalquestions-validation-10626", "mrqa_hotpotqa-validation-5654", "mrqa_squad-validation-2057", "mrqa_triviaqa-validation-5104", "mrqa_squad-validation-7880", "mrqa_hotpotqa-validation-4178", "mrqa_triviaqa-validation-7264", "mrqa_hotpotqa-validation-5127", "mrqa_squad-validation-2042", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-9814", "mrqa_squad-validation-10351", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-6520", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-1706"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 59, "before_eval": {"predictions": ["GE's four computer sales and service centers (Schenectady, Phoenix, Chicago, and Phoenix) to facilitate a computer time-sharing service", "skin", "the Naturalization Act of 1790", "Guy Ritchie", "around 2 %", "king George VI", "9", "John Christopher Lujack Jr.", "neptune", "24\u201310", "kabuki and bunraku", "It's only fair that, from now on, you should pay more for oil. Let's say ten times more", "in the U.S. state of Kansas", "Transvaginal ultrasonography", "Kelly Bundy", "Italy", "worked on the keels, boats that were used to transfer coal from the river banks to the waiting colliers, for export to London and elsewhere", "Sir William Douglas", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "Mario Addison", "321,520", "June 12, 2017", "2005", "red deer", "kalapatthar", "Moltke Nunatak", "Mowgli", "`` Turkey in the Straw ''", "2020 National Football League ( NFL ) season ( although a move to Las Vegas could happen as soon as 2019 with Sam Boyd Stadium )", "Cinerama Productions/Palomar theatrical library", "marx", "samoan t\u0101l\u0101"], "metric_results": {"EM": 0.125, "QA-F1": 0.24592179867115932}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.14814814814814814, 0.888888888888889, 0.0, 1.0, 0.0, 0.08695652173913045, 0.0, 0.9565217391304348, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4838", "mrqa_squad-validation-6436", "mrqa_naturalquestions-validation-10009", "mrqa_hotpotqa-validation-1035", "mrqa_naturalquestions-validation-875", "mrqa_triviaqa-validation-2965", "mrqa_naturalquestions-validation-7974", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-3719", "mrqa_triviaqa-validation-2223", "mrqa_squad-validation-3730", "mrqa_hotpotqa-validation-740", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-5864", "mrqa_squad-validation-5124", "mrqa_hotpotqa-validation-5253", "mrqa_naturalquestions-validation-7035", "mrqa_squad-validation-825", "mrqa_squad-validation-7612", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3685", "mrqa_hotpotqa-validation-4456", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-1676", "mrqa_naturalquestions-validation-5649", "mrqa_squad-validation-5887", "mrqa_triviaqa-validation-1933", "mrqa_triviaqa-validation-2525"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 60, "before_eval": {"predictions": ["Currer Bell", "the illegitimate son of Ned Stark", "in positions Arg15 - Ile16 and produces \u03c0 - Chymotrypsin", "J. D. Salinger's novel \"The Catcher in the Rye\"", "The invading Normans and their descendants", "Margiana", "Bendigo and its environs", "Thomas Jefferson", "She became a naturalized American citizen in 1994 and also received Hungarian citizenship in June 2007.", "1872", "Boston and Maine Railroad's Southern Division", "James Lofton and Mark Malone", "the right of the dinner plate", "summer months", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts returned for their respective ninth, eighth, seventh and second series", "third \u00e9tude", "South East Asia", "segues", "Welsh poet Dylan Thomas", "tin and copper", "During the reign of King Beorhtric of Wessex ( 786 -- 802 )", "South Australian town", "Flag Day in 1954", "around 300,000", "1858", "Sexred", "Veronica Lodge instead of Betty Cooper", "neo-Nazi ideology with ethnic European paganism and opposition to \"foreign\" religions", "rik Mayall", "NASA's yearly budget also began to shrink in light of the successful landing, and NASA also had to make funds available for the development of the upcoming Space Shuttle", "2.4 \u00d7 10 Hz ( 1 GeV gamma rays ) down to the local plasma frequency of the ionized interstellar medium ( ~ 1 kHz )", "the \"celebrity criminal\""], "metric_results": {"EM": 0.25, "QA-F1": 0.42455363936040047}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.25, 0.6, 0.0, 0.0, 0.4, 0.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.6666666666666665, 0.0, 0.19999999999999998, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.16666666666666669, 0.0, 0.42424242424242425, 0.9268292682926829, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-7225", "mrqa_hotpotqa-validation-949", "mrqa_squad-validation-1126", "mrqa_hotpotqa-validation-2715", "mrqa_squad-validation-2842", "mrqa_naturalquestions-validation-9273", "mrqa_hotpotqa-validation-3039", "mrqa_squad-validation-559", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1786", "mrqa_squad-validation-5449", "mrqa_hotpotqa-validation-1033", "mrqa_triviaqa-validation-3404", "mrqa_triviaqa-validation-3684", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-5311", "mrqa_naturalquestions-validation-6337", "mrqa_hotpotqa-validation-4293", "mrqa_naturalquestions-validation-6759", "mrqa_hotpotqa-validation-1971", "mrqa_triviaqa-validation-1284", "mrqa_squad-validation-4011", "mrqa_naturalquestions-validation-5798"], "retrieved_ids": ["mrqa_naturalquestions-train-24984", "mrqa_naturalquestions-train-791", "mrqa_naturalquestions-train-63867", "mrqa_naturalquestions-train-64951", "mrqa_naturalquestions-train-46051", "mrqa_naturalquestions-train-65314", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-22385", "mrqa_naturalquestions-train-81948", "mrqa_naturalquestions-train-16060", "mrqa_naturalquestions-train-78021", "mrqa_naturalquestions-train-33405", "mrqa_naturalquestions-train-81443", "mrqa_naturalquestions-train-11792", "mrqa_naturalquestions-train-86509", "mrqa_naturalquestions-train-87353", "mrqa_squad-validation-7407", "mrqa_triviaqa-validation-1616", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-10169", "mrqa_triviaqa-validation-2368", "mrqa_triviaqa-validation-3847", "mrqa_hotpotqa-validation-5165", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-4524", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-4856", "mrqa_squad-validation-2733", "mrqa_naturalquestions-validation-10356", "mrqa_hotpotqa-validation-3854", "mrqa_squad-validation-6947"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 61, "before_eval": {"predictions": ["The Raigne of King Edward the Third", "John Hughes", "San Bernardino", "Judy Collins", "King Mondo", "the 2013 non-fiction book of the same name by David Finkel", "little Arrows", "Monty Python", "fell from his horse while hunting and died because of the injury", "incitement to terrorism", "Henry and Liza", "lord of the \u00d3 M\u00e1ille dynasty", "Free and Sovereign State of Tamaulipas", "iron", "Elk and Kanawha Rivers", "They circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "'Bucks Point'", "to `` help bring creative projects to life ''", "The Church of Latter-day Saints", "Old World fossil representatives", "the group 1 elements, excluding hydrogen ( H ), which is nominally a group 1 element but not normally considered to be an alkali metal", "Matt Damon", "daniel smith", "thicker consistency and a deeper flavour than sauce", "external genitalia", "neupommern", "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam", "``100 Greatest Artists of Hard Rock\"", "after AD 70", "1985", "Techno", "It fell to him to report Hess's illegal May 1941 flight to Scotland to Hitler and his recollections and notes have been the subject of debate by historians."], "metric_results": {"EM": 0.15625, "QA-F1": 0.25029726304397354}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 0.4, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.45714285714285713]}}, "error_ids": ["mrqa_hotpotqa-validation-1850", "mrqa_triviaqa-validation-3861", "mrqa_squad-validation-2644", "mrqa_naturalquestions-validation-6118", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-validation-7407", "mrqa_hotpotqa-validation-1715", "mrqa_squad-validation-6218", "mrqa_naturalquestions-validation-8368", "mrqa_hotpotqa-validation-2012", "mrqa_hotpotqa-validation-4743", "mrqa_triviaqa-validation-2130", "mrqa_naturalquestions-validation-7483", "mrqa_squad-validation-8560", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-3355", "mrqa_naturalquestions-validation-1699", "mrqa_hotpotqa-validation-4931", "mrqa_triviaqa-validation-3960", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6376", "mrqa_squad-validation-3067", "mrqa_squad-validation-9370", "mrqa_hotpotqa-validation-327", "mrqa_triviaqa-validation-3985", "mrqa_hotpotqa-validation-3481"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 62, "before_eval": {"predictions": ["enter a prepared bedchamber in which they sleep in peace", "white", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "the Monarch", "United Kingdom, Australia, Canada and the United States", "Ballarat Bitter", "therefore sign", "Ricketts Glen State Park", "change the world", "new magma", "Bill Kerr", "circuit switching, a method which pre-allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Robert Moses", "In 1932", "February 7, 2018", "farmlands", "Egypt", "Buzz Aldrin", "lorraine", "4145 ft", "advocacy of young earth creationism and intelligent design", "during the production company vanity cards shown following the closing credits of most programs", "Kansas\u2013Nebraska Act", "two Constant ( C\u03bc and C\u03b4 ) gene segments", "1910\u20131940", "11:28", "Start Here", "thomas austinley", "Autobahn", "endocrine", "The theatre's first production was Holberg's comedy \"Den V\u00e6gelsindede\" and the opening was on 2 January 1850", "poodle"], "metric_results": {"EM": 0.3125, "QA-F1": 0.37628711685823757}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.25, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0689655172413793, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2339", "mrqa_squad-validation-9843", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-507", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-7270", "mrqa_triviaqa-validation-1698", "mrqa_squad-validation-4750", "mrqa_hotpotqa-validation-1510", "mrqa_squad-validation-9303", "mrqa_naturalquestions-validation-8696", "mrqa_squad-validation-2703", "mrqa_naturalquestions-validation-1555", "mrqa_squad-validation-3961", "mrqa_hotpotqa-validation-3469", "mrqa_squad-validation-5816", "mrqa_naturalquestions-validation-538", "mrqa_triviaqa-validation-4095", "mrqa_hotpotqa-validation-5607", "mrqa_naturalquestions-validation-9064", "mrqa_hotpotqa-validation-1692", "mrqa_triviaqa-validation-6254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 63, "before_eval": {"predictions": ["Bank of China Tower", "heat addition (in the boiler) and rejection ( in the condenser)", "jonathan kenya", "ARPANET", "he died in 1616", "Kohlberg K Travis Roberts", "fylde coast", "probabilistic (or \"Monte Carlo\") and deterministic algorithms", "Janis Joplin", "eight", "Phoenix Hong Kong Channel", "a narcissistic ex-lover who did the protagonist wrong, with Bieber singing in a snappy and spiteful tone while `` hating on a girl for loving herself too much. ''", "catawba river", "petrographic microscope", "mike", "taino", "Sondheim", "Strasbourg", "3.762", "The colonists used wampum as money. But then, they used everything as money, including coins from many different European nations, all at the same time.", "1993", "The Daily Mirror", "broken arm", "marx", "thomas", "German", "1999", "their belief in the validity of the social contract, which is held to bind all to obey the laws that a government meeting certain standards of legitimacy has established, or else suffer the penalties set out in the law", "Professor Kantorek", "31 - member Senate", "kenya", "kenya"], "metric_results": {"EM": 0.125, "QA-F1": 0.19246031746031747}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.38095238095238093, 0.0, 0.5, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-189", "mrqa_squad-validation-3445", "mrqa_triviaqa-validation-7574", "mrqa_squad-validation-4629", "mrqa_triviaqa-validation-3153", "mrqa_hotpotqa-validation-97", "mrqa_triviaqa-validation-1518", "mrqa_squad-validation-9057", "mrqa_triviaqa-validation-54", "mrqa_hotpotqa-validation-366", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-27", "mrqa_triviaqa-validation-5905", "mrqa_naturalquestions-validation-9755", "mrqa_hotpotqa-validation-55", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-3929", "mrqa_triviaqa-validation-5992", "mrqa_squad-validation-306", "mrqa_triviaqa-validation-6651", "mrqa_triviaqa-validation-2850", "mrqa_squad-validation-6707", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-4037", "mrqa_triviaqa-validation-2254"], "retrieved_ids": ["mrqa_naturalquestions-train-5574", "mrqa_naturalquestions-train-64911", "mrqa_naturalquestions-train-47347", "mrqa_naturalquestions-train-71729", "mrqa_naturalquestions-train-46477", "mrqa_naturalquestions-train-49084", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-45612", "mrqa_naturalquestions-train-88142", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-64139", "mrqa_naturalquestions-train-52136", "mrqa_naturalquestions-train-72194", "mrqa_naturalquestions-train-52136", "mrqa_naturalquestions-train-20771", "mrqa_naturalquestions-train-41094", "mrqa_triviaqa-validation-6887", "mrqa_hotpotqa-validation-3854", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-2899", "mrqa_squad-validation-6023", "mrqa_triviaqa-validation-6905", "mrqa_naturalquestions-validation-9284", "mrqa_squad-validation-7664", "mrqa_hotpotqa-validation-4173", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-6751", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-8034", "mrqa_naturalquestions-validation-4067"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 64, "before_eval": {"predictions": ["failure rate", "Jim Justice", "miniature cydippid adults", "Trey Parker and Matt Stone", "kanto", "sorrow regarding the environment", "trial division", "consultant", "The Harris Museum, Art Gallery & Preston Free Public Library", "The Suite Life of Zack & Cody", "tuberculosis", "Saturn IB", "H. R. Haldeman", "brian walsh", "player of the Year", "raphael sanzio - Robin UrtonRaphael Sanzio", "brain surgery", "switzerland", "charliesheen", "granaries were ordered built throughout the empire", "less dangerous", "John Simm", "immediate judgement discrepancy, or cognitive bias, where a person making an initial assessment of another person, place, or thing will assume ambiguous information based upon concrete information", "New Orleans", "the final episode of the series", "macOS High Sierra", "kinshasa", "parallelogram", "Diarmaid MacCulloch", "loire river", "17th Century", "provide funding at a rate or formula based on the previous year's funding"], "metric_results": {"EM": 0.3125, "QA-F1": 0.37889627439415574}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.16949152542372883, 0.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998]}}, "error_ids": ["mrqa_triviaqa-validation-2902", "mrqa_naturalquestions-validation-4207", "mrqa_squad-validation-4567", "mrqa_triviaqa-validation-748", "mrqa_squad-validation-8909", "mrqa_hotpotqa-validation-548", "mrqa_squad-validation-3956", "mrqa_hotpotqa-validation-3489", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-6398", "mrqa_triviaqa-validation-864", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-3208", "mrqa_hotpotqa-validation-2257", "mrqa_triviaqa-validation-5521", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-4709", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-10533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 65, "before_eval": {"predictions": ["the lightly populated \"Cow Counties\" of southern California", "Arabic numerals", "Diary of a Wimpy Kid : The Long Haul", "the four - letter suffix", "pilgrimage", "serous pericardium", "the Early Gothic", "kentucky", "ring-Tailed Lemur", "The Boz", "1991", "ABC- DuMont", "Birmingham", "due to Euler,", "2020", "brianess wenham", "tanyaY", "gravitation", "nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat", "bullfights", "the applied force is opposed by static friction, generated between the object and the table surface", "Sherlock Holmes", "sazerac", "the BPI gold-selling \" Shut Up\" which was initially released as a freestyle on YouTube", "tzetzal", "Karina Smirnoff", "the Arizona Cardinals", "Florida", "four", "the Private Mass", "kenya", "mycelium"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3295138888888889}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.38095238095238093, 1.0, 0.2666666666666667, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2737", "mrqa_squad-validation-482", "mrqa_naturalquestions-validation-8427", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-2752", "mrqa_squad-validation-1100", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-1923", "mrqa_squad-validation-5836", "mrqa_hotpotqa-validation-183", "mrqa_squad-validation-9021", "mrqa_triviaqa-validation-5573", "mrqa_triviaqa-validation-3518", "mrqa_naturalquestions-validation-6075", "mrqa_squad-validation-10420", "mrqa_squad-validation-10313", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-7605", "mrqa_hotpotqa-validation-875", "mrqa_triviaqa-validation-2383", "mrqa_naturalquestions-validation-5468", "mrqa_hotpotqa-validation-4174", "mrqa_squad-validation-2296", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-3691"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 66, "before_eval": {"predictions": ["mayonnaise", "to fund his Colorado Springs experiments", "the eastern shore of the Firth of Clyde, Scotland, at the north-western corner of the county of Ayrshire", "On the Computational Complexity of Algorithms", "from sea level", "Pandavas", "low coercivity", "La Nouba", "cake", "Duke Kent-Brown", "O2", "oysters", "platypus", "better academic results than government schools formerly reserved for other race groups", "Vanessa Block", "insano", "2p + 1", "white", "alaudine", "http://www.example.com/index.html", "b Brunswick-Wolfenb\u00fcttel", "`` Abigail ''", "two catechisms", "cheddar", "Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel", "Saint-Domingue", "disappearance during his return trip back to Wittenberg", "1963", "Organisms in the domains of Archaea and Bacteria", "Commissioners", "Queen Anne's War", "sattu paratha (stuffed with fried chickpea flour)"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2941913886766828}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.125, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.375, 0.28571428571428575, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.07142857142857142, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4099", "mrqa_squad-validation-1416", "mrqa_hotpotqa-validation-1052", "mrqa_naturalquestions-validation-5927", "mrqa_hotpotqa-validation-5712", "mrqa_triviaqa-validation-7577", "mrqa_squad-validation-3477", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-3970", "mrqa_squad-validation-7130", "mrqa_hotpotqa-validation-4951", "mrqa_triviaqa-validation-515", "mrqa_squad-validation-8980", "mrqa_triviaqa-validation-5337", "mrqa_triviaqa-validation-6566", "mrqa_naturalquestions-validation-8229", "mrqa_triviaqa-validation-6654", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-5686", "mrqa_squad-validation-3483", "mrqa_squad-validation-2269", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-10357", "mrqa_squad-validation-10167", "mrqa_triviaqa-validation-2087"], "retrieved_ids": ["mrqa_naturalquestions-train-19969", "mrqa_naturalquestions-train-48080", "mrqa_naturalquestions-train-19894", "mrqa_naturalquestions-train-65971", "mrqa_naturalquestions-train-22300", "mrqa_naturalquestions-train-24442", "mrqa_naturalquestions-train-35357", "mrqa_naturalquestions-train-54353", "mrqa_naturalquestions-train-14073", "mrqa_naturalquestions-train-84192", "mrqa_naturalquestions-train-63437", "mrqa_naturalquestions-train-50258", "mrqa_naturalquestions-train-9737", "mrqa_naturalquestions-train-31526", "mrqa_naturalquestions-train-84535", "mrqa_naturalquestions-train-42823", "mrqa_triviaqa-validation-3014", "mrqa_squad-validation-3442", "mrqa_triviaqa-validation-3960", "mrqa_squad-validation-9569", "mrqa_naturalquestions-validation-2222", "mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-7473", "mrqa_triviaqa-validation-6913", "mrqa_naturalquestions-validation-1282", "mrqa_squad-validation-7700", "mrqa_squad-validation-4773", "mrqa_triviaqa-validation-1698", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-9814", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-572"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 67, "before_eval": {"predictions": ["high inequality", "henry", "james brown", "Sweden, Norway and Denmark", "Oscar II Land on the island of Spitsbergen in Svalbard, Norway", "desublimation", "a surname of Norman origin, deriving from the Norman given name Robert, meaning `` bright renown '' -- from the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "Crossed is a comic book written by Garth Ennis and drawn by Jacen Burrows", "payday loans", "Doctor of Philosophy", "1,388", "an unknown person", "slide", "climate change experts", "$150,000 and $250,000", "1963", "grew up in Ohio and briefly attended the University of Pittsburgh before transferring to Mount Union", "first freshman to finish as the runner-up", "The Crowned Prince of the Philadelphia Mob", "\u201c Resign.\u201d", "romantic attraction, sexual attraction, or sexual behavior", "remained largely intact until the Meiji Restoration (1868)", "Big 12 Conference", "In the 1920s", "American", "Cleopatra VII Philopator", "April Fool's Day", "henry", "american revolution", "chloroplasts", "Salta", "more than 265 million business records worldwide"], "metric_results": {"EM": 0.25, "QA-F1": 0.35310363247863247}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 0.16, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445]}}, "error_ids": ["mrqa_triviaqa-validation-6619", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-2813", "mrqa_naturalquestions-validation-6514", "mrqa_hotpotqa-validation-4648", "mrqa_triviaqa-validation-1385", "mrqa_hotpotqa-validation-5297", "mrqa_naturalquestions-validation-8911", "mrqa_triviaqa-validation-1583", "mrqa_squad-validation-8526", "mrqa_squad-validation-8837", "mrqa_naturalquestions-validation-1976", "mrqa_squad-validation-6974", "mrqa_hotpotqa-validation-1283", "mrqa_squad-validation-6327", "mrqa_hotpotqa-validation-3345", "mrqa_naturalquestions-validation-7089", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-567", "mrqa_triviaqa-validation-5013", "mrqa_triviaqa-validation-3395", "mrqa_squad-validation-8929", "mrqa_hotpotqa-validation-2171"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 68, "before_eval": {"predictions": ["Nuevo Reino de Le\u00f3n", "american state", "the law of Italy governing the acquisition, transmission and loss of Italian citizenship", "last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles ( 2010 -- 14 )", "The King of Chutzpah", "P $ C featuring T.I. & Lil Scrappy, Mike Jones featuring Nicole Wray, Trillville, Juvenile featuring Skip & Wacko, Nasty Nardo, 8Ball & MJG", "energy", "hampstead", "English", "george iv", "Monk's", "worst-case complexity", "chip", "the dot", "dieppe", "1961", "Tyler Posey", "New South Wales", "hard-to-fill positions", "kung-fu", "brontosaurs", "qualifications", "Laura Vallejo", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "shannon Kleibrink", "1939", "tolled ( quota ) highways", "redox", "eastern and interior Venezuela", "tsar", "Lagos", "politically conservative"], "metric_results": {"EM": 0.28125, "QA-F1": 0.33818581780538304}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.8695652173913044, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_naturalquestions-validation-6352", "mrqa_triviaqa-validation-4485", "mrqa_hotpotqa-validation-4978", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-8791", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-6896", "mrqa_naturalquestions-validation-339", "mrqa_squad-validation-1784", "mrqa_triviaqa-validation-6302", "mrqa_naturalquestions-validation-10496", "mrqa_hotpotqa-validation-1145", "mrqa_triviaqa-validation-3720", "mrqa_triviaqa-validation-5893", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-7286", "mrqa_hotpotqa-validation-2867", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-3089", "mrqa_naturalquestions-validation-2632", "mrqa_hotpotqa-validation-4275"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 69, "before_eval": {"predictions": ["Eisenhower Freeway", "the work of Max Planck, Albert Einstein, Louis de Broglie, Arthur Compton, Niels Bohr and many others", "expansion in the size of the orchestra and in the dynamic range and diversity of instruments used in this ensemble", "the uplift of mountain ranges", "North Kest even", "the Ming", "near major hotels and in the parking areas of major Chinese supermarkets", "al-Maridini of Baghdad and Cairo", "Hellenismos", "the buttock", "German and UK Kennel Clubs", "unknown", "tea, horticultural produce, and coffee", "parliaments", "Christina Applegate", "Blue", "island of island", "break off the cathode, pass out of the tube, and physically strike him", "Silver Gallery", "21.8 %", "a placebo effect", "Yuan dynasty", "Tony Orlando and Dawn", "Paris", "american civil rights movement", "Martha Wainwright", "1,462", "island of man", "West Norse sailors", "paul dukas", "island of Aiolia", "1698"], "metric_results": {"EM": 0.15625, "QA-F1": 0.277724358974359}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.9333333333333333, 0.9714285714285714, 0.0, 0.28571428571428575, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.0, 0.09523809523809523, 1.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4562", "mrqa_naturalquestions-validation-8645", "mrqa_naturalquestions-validation-8059", "mrqa_squad-validation-4929", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-6949", "mrqa_squad-validation-6464", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2464", "mrqa_naturalquestions-validation-8394", "mrqa_triviaqa-validation-1449", "mrqa_squad-validation-1388", "mrqa_squad-validation-5571", "mrqa_naturalquestions-validation-3035", "mrqa_squad-validation-3610", "mrqa_squad-validation-8185", "mrqa_hotpotqa-validation-3435", "mrqa_triviaqa-validation-5494", "mrqa_hotpotqa-validation-665", "mrqa_triviaqa-validation-6541", "mrqa_naturalquestions-validation-7217", "mrqa_triviaqa-validation-5088", "mrqa_hotpotqa-validation-1381"], "retrieved_ids": ["mrqa_naturalquestions-train-1574", "mrqa_naturalquestions-train-88054", "mrqa_naturalquestions-train-39282", "mrqa_naturalquestions-train-8781", "mrqa_naturalquestions-train-23486", "mrqa_naturalquestions-train-60442", "mrqa_naturalquestions-train-86352", "mrqa_naturalquestions-train-86563", "mrqa_naturalquestions-train-43344", "mrqa_naturalquestions-train-14541", "mrqa_naturalquestions-train-82770", "mrqa_naturalquestions-train-72194", "mrqa_naturalquestions-train-75187", "mrqa_naturalquestions-train-3613", "mrqa_naturalquestions-train-17730", "mrqa_naturalquestions-train-85155", "mrqa_squad-validation-3971", "mrqa_naturalquestions-validation-9105", "mrqa_triviaqa-validation-2961", "mrqa_naturalquestions-validation-7483", "mrqa_hotpotqa-validation-4620", "mrqa_naturalquestions-validation-3363", "mrqa_triviaqa-validation-2007", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-3333", "mrqa_triviaqa-validation-2770", "mrqa_hotpotqa-validation-182", "mrqa_naturalquestions-validation-1649", "mrqa_hotpotqa-validation-573", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-2201"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 70, "before_eval": {"predictions": ["paratroopers", "The New York and New Jersey campaign", "Ghost Island", "the park", "Australia will no longer be a base for the global car industry", "leather", "the Viking army based in Yorkshire", "1972", "2013", "derived from the given name Gomes which is a loanword of the Visigothic word guma `` man ''.", "Major General Miles Francis Stapleton Fitzalan- Howard", "a Mnemonic for the Hebrew Alephbet", "Australian and New Zealand", "chromium", "white", "a domestic passenger flight that was hijacked by five al- Qaeda members on September 11, 2001, as part of the September 11 attacks", "derived from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "82.30'E longitude, in Mirzapur, Uttar Pradesh, which is nearly on the corresponding longitude reference line", "the principles of: tawhid ( unity of God) risala (prophethood) and khilafa (caliphate)", "Salman Khan", "Skip Tyler", "the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "WBMA-LD", "two", "whiskey", "physically strike him", "authorized", "thirty articles affirming an individual's rights which, although not legally binding in themselves, have been elaborated in subsequent international treaties, economic transfers, regional human rights instruments, national constitutions, and other laws", "a straight line (see world line) traveling through time", "Newton", "International Imitation Hemingway Competition", "Oak Island"], "metric_results": {"EM": 0.25, "QA-F1": 0.37135417214364586}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.15384615384615383, 0.4, 0.5333333333333333, 1.0, 0.0, 0.0, 0.19047619047619047, 0.4761904761904762, 0.42857142857142855, 0.0, 1.0, 0.08, 1.0, 0.4, 0.0, 0.0, 0.0, 0.2105263157894737, 0.4, 0.0, 0.28571428571428575, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-831", "mrqa_squad-validation-1632", "mrqa_squad-validation-2980", "mrqa_triviaqa-validation-5981", "mrqa_hotpotqa-validation-471", "mrqa_naturalquestions-validation-3019", "mrqa_hotpotqa-validation-5334", "mrqa_triviaqa-validation-2331", "mrqa_naturalquestions-validation-4567", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-4323", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-4891", "mrqa_squad-validation-9661", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-4356", "mrqa_triviaqa-validation-1408", "mrqa_squad-validation-1389", "mrqa_triviaqa-validation-2546", "mrqa_naturalquestions-validation-7938", "mrqa_squad-validation-10477", "mrqa_squad-validation-361", "mrqa_hotpotqa-validation-4543"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 71, "before_eval": {"predictions": ["oil magnate and wealthiest man in history John D. Rockefeller", "42", "electrical, water, sewage, phone, and potentially hazardous situations", "Saxe-Coburg and Gotha", "16,000", "no known case", "Dwight David \"Ike\" Eisenhower", "multi-purpose", "Khitan Liao and Jurchen Jin", "twelve", "rowing", "Randal Keith Orton", "Neil Tomlin as Frances `` Frankie '' Bergstein ( n\u00e9e Mengela ), a hippie art teacher", "as a liquid", "Sunday evenings", "October 16, 2012", "Czech Republic", "2012", "alpha efferent neurons", "antlers", "David Irving", "cabbage", "rock and roll", "the Hongwu Emperor of the Ming Dynasty", "decathlon", "1565", "rapid expansion in telecommunication and financial activity", "parliaments", "Caroline Sterling, n\u00e9e Bone, formerly Pemberton ( born 3 April 1955 ; died 2017 )", "Libertarianism", "london", "American R&B"], "metric_results": {"EM": 0.15625, "QA-F1": 0.286831311050061}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.4615384615384615, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 0.0, 0.0, 0.625, 0.1111111111111111, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.19999999999999998, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-7849", "mrqa_hotpotqa-validation-253", "mrqa_squad-validation-7123", "mrqa_hotpotqa-validation-862", "mrqa_squad-validation-4344", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5619", "mrqa_squad-validation-8291", "mrqa_naturalquestions-validation-2635", "mrqa_triviaqa-validation-6938", "mrqa_hotpotqa-validation-583", "mrqa_naturalquestions-validation-8136", "mrqa_squad-validation-3689", "mrqa_hotpotqa-validation-4911", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2548", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-4291", "mrqa_triviaqa-validation-1185", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-2477", "mrqa_triviaqa-validation-1640", "mrqa_naturalquestions-validation-2847", "mrqa_hotpotqa-validation-1694", "mrqa_triviaqa-validation-4464", "mrqa_hotpotqa-validation-2866"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 72, "before_eval": {"predictions": ["Monk's Caf\u00e9", "ancient Rome with gift - giving during the Saturnalia holiday, which took place that month", "fourth- ranking Republican leader in the House", "seven", "the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem", "Operation Neptune", "john f kenna", "Patrick Moore", "alpaca fiber and mohair from Angora goats", "a Muskogean language called Apalachee", "Symbolic interactionism", "the fictional town of West Egg on prosperous Long Island", "a gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "Field of Dreams", "ice core samples that are up to several hundreds of thousands of years old", "member of Parliament", "# 4 School of Public Health in the country", "king shakespeare", "fish", "1883\u201384", "yellow or yellow-green", "A Pr\u00e9sent Tu Peux t'en Aller", "green bay", "Macau Peninsula, Macau", "produce \"de novo\"", "morrissey", "747", "satirical erotic romantic comedy", "Greg (Shearsmith) Fran ( Sarah Hadland)", "late eighteenth century", "Puente Hills Mall, located in the City of Industry, California, United States", "james vi"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30876857517482514}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.4, 0.7272727272727274, 0.7272727272727272, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.125, 1.0, 0.18181818181818182, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.30769230769230765, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-4815", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-1110", "mrqa_squad-validation-1760", "mrqa_hotpotqa-validation-4061", "mrqa_triviaqa-validation-6606", "mrqa_triviaqa-validation-5439", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-946", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-1359", "mrqa_squad-validation-3592", "mrqa_hotpotqa-validation-2192", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-417", "mrqa_triviaqa-validation-3799", "mrqa_squad-validation-9855", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-5393", "mrqa_hotpotqa-validation-1394", "mrqa_squad-validation-8829", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-1162", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1858", "mrqa_triviaqa-validation-7377"], "retrieved_ids": ["mrqa_naturalquestions-train-16543", "mrqa_naturalquestions-train-36932", "mrqa_naturalquestions-train-71770", "mrqa_naturalquestions-train-29042", "mrqa_naturalquestions-train-9737", "mrqa_naturalquestions-train-7544", "mrqa_naturalquestions-train-24469", "mrqa_naturalquestions-train-23276", "mrqa_naturalquestions-train-61468", "mrqa_naturalquestions-train-30145", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-82542", "mrqa_naturalquestions-train-81443", "mrqa_naturalquestions-train-21217", "mrqa_naturalquestions-train-82113", "mrqa_naturalquestions-train-48013", "mrqa_triviaqa-validation-2530", "mrqa_naturalquestions-validation-8645", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8545", "mrqa_squad-validation-8037", "mrqa_squad-validation-2659", "mrqa_squad-validation-9532", "mrqa_squad-validation-10042", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-455", "mrqa_squad-validation-6218", "mrqa_squad-validation-1126", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-585"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 73, "before_eval": {"predictions": ["northwest", "ishmael", "energy carriers adenosine triphosphate (ATP) and nicotinamide adenine dinucleotide phosphate (NADP+)", "Boston", "the 10th Cavalry Regiment of the United States Army", "8 km", "The Future", "1969", "Indian epic historical drama film", "Wahhabism or Salafism", "buzzards", "many practice areas of pharmacy, however, they may also work in information technology departments or for healthcare information technology vendor companies", "that first set of endosymbiotic events", "a Danish - Norwegian patronymic surname meaning `` son of Anders '' ( itself derived from the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew )", "Edward Trowbridge Collins Sr.", "Microsoft Windows, PlayStation 4, and Xbox One in September 2014 and PlayStation 3 and Xbox 360 in November 2014", "when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another", "increased productivity, trade, and secular economic trends", "songfacts", "After World War I, all new housing in London and its suburbs had indoor toilets", "1989", "geographer", "around 200\u2013300", "his mind", "structure and forces that act on one part of an object", "metric counterpart", "the \" Polovtsian Dances, or Polovetskie plyaski\"", "tain", "the government - owned Panama Canal Authority", "Vernier, Switzerland", "harmonica", "The Today Show"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3550808533907445}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.9090909090909091, 0.10526315789473684, 1.0, 0.6896551724137931, 0.14814814814814814, 0.0, 0.0, 0.4210526315789474, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5714285714285715, 0.0, 0.3846153846153846, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-8727", "mrqa_hotpotqa-validation-4967", "mrqa_triviaqa-validation-56", "mrqa_hotpotqa-validation-3483", "mrqa_squad-validation-9592", "mrqa_triviaqa-validation-7082", "mrqa_squad-validation-6388", "mrqa_squad-validation-8801", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-2069", "mrqa_squad-validation-9871", "mrqa_naturalquestions-validation-9530", "mrqa_triviaqa-validation-7642", "mrqa_naturalquestions-validation-9723", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-2567", "mrqa_squad-validation-6550", "mrqa_squad-validation-1510", "mrqa_squad-validation-10430", "mrqa_squad-validation-10455", "mrqa_hotpotqa-validation-4284", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-3407"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 74, "before_eval": {"predictions": ["North Atlantic Drift", "Friedrich Nietzsche", "Fern", "Anglo-Frisian languages", "smiths", "smiths", "xyltenham", "DeWayne Warren", "2", "hard Candy", "George Merrill and Shannon Rubicam of the band Boy Meets Girl", "Santa Fe, New Mexico", "1943", "the translation of the Old Testament", "wai Momi", "a \"consulting fee\" to get around Tesla's aversion to accept charity", "student tuition, endowments, scholarship/voucher funds, and donations and grants from religious organizations or private individuals", "civil disobedience", "smiths", "the British East India company to sell tea from China in American colonies without paying any taxes", "film scripts written by ghost writers, nonfiction books on military subjects, and video games", "British and French Canadian fur traders from before 1810, and American settlers from the mid-1830s", "Instagram's own account", "John Sebastian Bach, Karlheinz Stockhausen, Terry Riley, Philip Glass and Moondog", "soon after birth", "listen to her beautiful voice", "22 July 1930", "gluons", "each article of the Creed to express the character of the Father, the Son, or the Holy Spirit", "released in the United States on March 23, 2018, by Fox Searchlight Pictures, with a wide release on for April 13, 2018", "smiths", "business magnate, investor, and philanthropist"], "metric_results": {"EM": 0.0625, "QA-F1": 0.21856510254143263}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 0.125, 0.6666666666666666, 0.0, 0.3157894736842105, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2608695652173913, 0.0, 0.33333333333333337]}}, "error_ids": ["mrqa_naturalquestions-validation-8072", "mrqa_hotpotqa-validation-5520", "mrqa_triviaqa-validation-6002", "mrqa_hotpotqa-validation-2098", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6573", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-4275", "mrqa_naturalquestions-validation-8008", "mrqa_naturalquestions-validation-7462", "mrqa_squad-validation-2371", "mrqa_triviaqa-validation-2938", "mrqa_squad-validation-1462", "mrqa_squad-validation-7086", "mrqa_squad-validation-6794", "mrqa_triviaqa-validation-4614", "mrqa_naturalquestions-validation-642", "mrqa_hotpotqa-validation-2220", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1027", "mrqa_hotpotqa-validation-726", "mrqa_naturalquestions-validation-9487", "mrqa_squad-validation-801", "mrqa_hotpotqa-validation-4964", "mrqa_squad-validation-10447", "mrqa_squad-validation-2391", "mrqa_naturalquestions-validation-177", "mrqa_triviaqa-validation-5108", "mrqa_hotpotqa-validation-83"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 75, "before_eval": {"predictions": ["around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Andrea Palladio", "guitarist", "Gaahl", "dart", "AMX - 13", "flodden", "American writer and satirist Kurt Vonnegut", "Persian Gulf", "1902", "The Statue of Freedom", "18 November 1963", "23 November 1963", "30", "Limbo", "the state legislators of Assam", "350 government officials and climate change experts", "Gothic", "nazi", "comic", "Attorney General", "523 km", "tube", "musical director", "one person", "Luger P08", "Al- Masjid an-Nabawi", "1556", "laundry", "from Manitoba, Ontario and Nova Scotia in southern Canada to northern Florida and from the Atlantic coast to the Missouri River and the eastern Great Plains", "manned lunar landing", "spanish"], "metric_results": {"EM": 0.25, "QA-F1": 0.3091517857142857}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-5628", "mrqa_hotpotqa-validation-551", "mrqa_hotpotqa-validation-3306", "mrqa_triviaqa-validation-1217", "mrqa_naturalquestions-validation-9426", "mrqa_triviaqa-validation-6051", "mrqa_squad-validation-8035", "mrqa_triviaqa-validation-5878", "mrqa_hotpotqa-validation-5643", "mrqa_squad-validation-683", "mrqa_naturalquestions-validation-9546", "mrqa_squad-validation-8525", "mrqa_naturalquestions-validation-9576", "mrqa_triviaqa-validation-2879", "mrqa_naturalquestions-validation-3347", "mrqa_hotpotqa-validation-1298", "mrqa_squad-validation-903", "mrqa_triviaqa-validation-1205", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4413", "mrqa_naturalquestions-validation-6482", "mrqa_triviaqa-validation-1199", "mrqa_naturalquestions-validation-2006", "mrqa_hotpotqa-validation-2646"], "retrieved_ids": ["mrqa_naturalquestions-train-23030", "mrqa_naturalquestions-train-35838", "mrqa_naturalquestions-train-63105", "mrqa_naturalquestions-train-48080", "mrqa_naturalquestions-train-58525", "mrqa_naturalquestions-train-86888", "mrqa_naturalquestions-train-5949", "mrqa_naturalquestions-train-60171", "mrqa_naturalquestions-train-51682", "mrqa_naturalquestions-train-60391", "mrqa_naturalquestions-train-5533", "mrqa_naturalquestions-train-31961", "mrqa_naturalquestions-train-63947", "mrqa_naturalquestions-train-76071", "mrqa_naturalquestions-train-10336", "mrqa_naturalquestions-train-19661", "mrqa_triviaqa-validation-2368", "mrqa_squad-validation-9405", "mrqa_triviaqa-validation-1034", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-1301", "mrqa_squad-validation-2191", "mrqa_triviaqa-validation-6881", "mrqa_naturalquestions-validation-1607", "mrqa_naturalquestions-validation-10626", "mrqa_squad-validation-7746", "mrqa_naturalquestions-validation-5944", "mrqa_hotpotqa-validation-1855", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-validation-3609", "mrqa_squad-validation-2042"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 76, "before_eval": {"predictions": ["Raiders", "2007", "many bands, including a 1990 single by Saint Etienne", "novelization", "9 November 1967", "the Colonization movement or Black Zionism", "Representatives", "mulberry leaves", "The Boy in the Striped Pyjamas", "Conrad Lewis", "2017 - 12 - 10", "Thrifty Automotive Group", "2007", "Medicaid", "Leonhard Euler", "Grease", "Republic of the Sudan", "federal government's nearly 700 million acres ( 2,800,000 km ) of subsurface mineral estate located beneath federal, state and private lands severed from their surface rights by the Homestead Act of 1862", "continue their Bodhisattva vow", "Bhpppavageete or Bhavageeth", "Tachycardia, also called tachyarrhythmia", "Pluto", "Richard Burbage", "blood poisoning", "the Parliament of the United Kingdom at Westminster", "achievement", "discarded", "A Song of Ice and Fire", "Morrissey", "Julia McKenzie and Anton Rodgers", "15 February 1998", "Amazon.com"], "metric_results": {"EM": 0.25, "QA-F1": 0.3219566052227342}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 1.0, 0.0, 0.0, 0.06451612903225806, 0.0, 0.0, 0.28571428571428575, 0.0, 0.25, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-7535", "mrqa_triviaqa-validation-4440", "mrqa_hotpotqa-validation-2786", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-53", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-1314", "mrqa_naturalquestions-validation-6258", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-2550", "mrqa_naturalquestions-validation-6027", "mrqa_squad-validation-1927", "mrqa_hotpotqa-validation-4510", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-2270", "mrqa_squad-validation-5031", "mrqa_squad-validation-9547", "mrqa_squad-validation-7323", "mrqa_triviaqa-validation-5849", "mrqa_naturalquestions-validation-9591"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 77, "before_eval": {"predictions": ["the 1948 NBL champion Minneapolis Lakers", "400", "genetic branches", "east-west through the centre of Victoria", "four", "central city", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "lofsongur", "islay", "troposphere", "Greater", "The Fixx", "1,382", "The Swiss Express", "many in the West to support illiberal Islamic regimes", "alexco", "quotient", "Matilda of Anjou", "K-pop artists After School, Orange Caramel, NU'EST, Han Dong Geun, Kye Bumzu, Seventeen and Pristin", "Jack", "at any time after the auction", "north", "Redwood Original", "central Liverpool", "Great Yuan", "Song Il-gon", "1974", "Acura", "hydrogen peroxide (H2O2)", "food and clothing", "Lt. Col. Masahiko Takeshita", "the enzyme arginine : glycine amidinotransferase ( AGAT, EC : 2.1. 4.1 ) to form guanidinoacetate"], "metric_results": {"EM": 0.25, "QA-F1": 0.3402999420922215}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.07692307692307693, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.125, 0.0, 0.2857142857142857, 0.0, 0.23529411764705882, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669]}}, "error_ids": ["mrqa_naturalquestions-validation-70", "mrqa_triviaqa-validation-6483", "mrqa_squad-validation-2913", "mrqa_triviaqa-validation-3900", "mrqa_naturalquestions-validation-6452", "mrqa_triviaqa-validation-593", "mrqa_triviaqa-validation-2095", "mrqa_naturalquestions-validation-8584", "mrqa_hotpotqa-validation-1921", "mrqa_squad-validation-9520", "mrqa_triviaqa-validation-6204", "mrqa_hotpotqa-validation-1558", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4766", "mrqa_naturalquestions-validation-4475", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7845", "mrqa_triviaqa-validation-1780", "mrqa_squad-validation-8046", "mrqa_triviaqa-validation-7610", "mrqa_squad-validation-3543", "mrqa_naturalquestions-validation-8163", "mrqa_hotpotqa-validation-1237", "mrqa_naturalquestions-validation-686"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 78, "before_eval": {"predictions": ["introduced to the New Zealand Parliament as a private members bill by Green Party Member of Parliament Sue Bradford in 2005, after being drawn from the ballot", "healthcare professionals with specialised education and training who perform various roles to ensure optimal health outcomes for their patients through the quality use of medicines", "austin", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "neuro-orthopaedic Irish veterinary surgeon best known for his work in small animal practice", "crim\u00b7nal\u00b7i\u00b7ty", "Anne of Green Gables", "synovial joint", "The Votex Jazz Club", "Schlitz", "jack the ripper", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "1484", "one of the subtypes of white blood cell in a vertebrate's immune system", "a number of influences including drum and bass, acid house, jazz and electroacoustic music.", "established by Napoleon, as a French client state, in 1806 and lasted until 1814", "a specific quantity of the substance to change its state from a solid to a liquid, ( or resulting from the release of energy from a substance during transition from liquid to solid ), at constant pressure", "Mongolian", "Gap", "The Indianapolis Times and the Cleveland Press", "founded by the Jurchen Aisin Gioro clan in Manchuria", "how the Sausage is Made", "1756", "friendship and reunion", "86.66% (757.7 sq mi or 1,962 km2)", "840", "DTIME(n2)", "a poor harvest in 1757", "loud and dirty as possible", "achy breaky heart", "chocolate confectionery"], "metric_results": {"EM": 0.0625, "QA-F1": 0.2251774927394269}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.07692307692307693, 0.3448275862068966, 0.0, 0.35294117647058826, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 1.0, 0.0, 0.33333333333333337, 0.14285714285714288, 0.14634146341463414, 0.0, 0.0, 0.5714285714285715, 0.2222222222222222, 0.4, 0.0, 0.5, 0.0, 0.14285714285714288, 0.0, 1.0, 0.9090909090909091, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_squad-validation-6280", "mrqa_triviaqa-validation-1117", "mrqa_naturalquestions-validation-5838", "mrqa_hotpotqa-validation-1219", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-4063", "mrqa_triviaqa-validation-1168", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-5011", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6497", "mrqa_naturalquestions-validation-9342", "mrqa_hotpotqa-validation-1296", "mrqa_squad-validation-9445", "mrqa_naturalquestions-validation-1119", "mrqa_squad-validation-8083", "mrqa_squad-validation-421", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-1916", "mrqa_squad-validation-10239", "mrqa_squad-validation-6115", "mrqa_squad-validation-7476", "mrqa_naturalquestions-validation-276", "mrqa_squad-validation-1807", "mrqa_hotpotqa-validation-2437", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-1465"], "retrieved_ids": ["mrqa_naturalquestions-train-67420", "mrqa_naturalquestions-train-39904", "mrqa_naturalquestions-train-46447", "mrqa_naturalquestions-train-22049", "mrqa_naturalquestions-train-75784", "mrqa_naturalquestions-train-78573", "mrqa_naturalquestions-train-60171", "mrqa_naturalquestions-train-22067", "mrqa_naturalquestions-train-50050", "mrqa_naturalquestions-train-23892", "mrqa_naturalquestions-train-9737", "mrqa_naturalquestions-train-84096", "mrqa_naturalquestions-train-6042", "mrqa_naturalquestions-train-75073", "mrqa_naturalquestions-train-2349", "mrqa_naturalquestions-train-10206", "mrqa_squad-validation-825", "mrqa_hotpotqa-validation-1606", "mrqa_triviaqa-validation-4279", "mrqa_squad-validation-3543", "mrqa_squad-validation-2010", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-7700", "mrqa_hotpotqa-validation-3031", "mrqa_triviaqa-validation-4584", "mrqa_naturalquestions-validation-98", "mrqa_triviaqa-validation-831", "mrqa_hotpotqa-validation-4173", "mrqa_squad-validation-9808", "mrqa_triviaqa-validation-1353", "mrqa_naturalquestions-validation-2666", "mrqa_squad-validation-482"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 79, "before_eval": {"predictions": ["Sir Hiram Stevens Maxim", "1987", "`` 200 ''", "make direct representations to the Presiding Officer to nominate speakers", "Chesley Burnett \"Sully\" Sullenberger III", "Thursdays at 8 : 00 pm ( ET )", "an allegiance oath that must be taken by all immigrants who wish to become United States citizens", "Kevin Whately", "tom hanks", "clay animation or \"clay-mation\"", "caucuses", "September of that year", "precedes the value ( for instance, \u20ac 10, not 10 \u20ac", "the Industrial Revolution", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "Boletus edulis", "white Sox", "rugged terrain such as the Arctic", "the Mexico\u2013united States border", "Carolina Panthers", "a ribosome in the cytosol", "Massapequa", "Ringo Starr", "leicestershire", "gas turbines", "either by voting or voice vote", "the Shoushi Li (\u6388\u6642\u66a6) or Calendar for Fixing the Seasons", "16 %", "july 2004", "Secretary of Defense", "pancake-shaped circular disks about 300\u2013600 nanometers in diameter", "thirteen American colonies, then at war with the Kingdom of Great Britain"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3064225431001747}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.3636363636363636, 0.33333333333333337, 0.0, 0.35714285714285715, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.9473684210526316, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 0.4, 0.2222222222222222, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4516", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-7733", "mrqa_squad-validation-9407", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-1672", "mrqa_triviaqa-validation-7178", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-2846", "mrqa_triviaqa-validation-3844", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-4529", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-5589", "mrqa_triviaqa-validation-5983", "mrqa_squad-validation-3738", "mrqa_squad-validation-8960", "mrqa_hotpotqa-validation-3538", "mrqa_triviaqa-validation-5529", "mrqa_squad-validation-3369", "mrqa_naturalquestions-validation-3591", "mrqa_squad-validation-8233", "mrqa_naturalquestions-validation-8509", "mrqa_triviaqa-validation-1912", "mrqa_squad-validation-8811", "mrqa_naturalquestions-validation-360"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 80, "before_eval": {"predictions": ["John Mills", "new jersey", "Lucia Iipumbu", "Sachin Tendulkar", "fREEWAY", "Charles Silverstein", "the Supermarine Spitfire", "the oil shock", "l Luxembourg", "State Bar of Arizona", "the electrical activity produced by skeletal muscles", "beer", "HgO", "red", "david river", "1967", "Mondays", "22", "1598", "the coastline peninsula of Davenports Neck called \"Bauffet's Point\"", "geoffrey", "james tyndall", "Topeka", "500", "red", "the veil", "1894", "since they were inserted before \"May God help me\" only in later versions of the speech and not recorded in witness accounts of the proceedings", "`` Nelson's Sparrow ''", "red", "phowa", "secondary law"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2517322954822955}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.4, 0.3571428571428571, 0.4, 0.0, 0.5, 0.4444444444444445]}}, "error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_triviaqa-validation-6493", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2148", "mrqa_triviaqa-validation-6063", "mrqa_hotpotqa-validation-3056", "mrqa_hotpotqa-validation-3282", "mrqa_triviaqa-validation-7358", "mrqa_naturalquestions-validation-3092", "mrqa_naturalquestions-validation-7848", "mrqa_hotpotqa-validation-3779", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-1261", "mrqa_squad-validation-3038", "mrqa_squad-validation-3318", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-2009", "mrqa_hotpotqa-validation-2840", "mrqa_squad-validation-10182", "mrqa_triviaqa-validation-5904", "mrqa_naturalquestions-validation-870", "mrqa_squad-validation-4975", "mrqa_squad-validation-2223", "mrqa_naturalquestions-validation-3822", "mrqa_triviaqa-validation-6129", "mrqa_squad-validation-1930", "mrqa_squad-validation-4042"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 81, "before_eval": {"predictions": ["one rotation of the crank and two piston strokes", "Super Bowl L", "2009\u201310 Specials", "an adviser", "USD 3.1 billion dollars", "A rear - view mirror ( or rearview mirror ) is a mirror in automobiles and other vehicles, designed to allow the driver to see rearward through the vehicle's rear window ( rear windshield )", "The Birds", "identify rocks in the laboratory are through optical microscopy and by using an electron microprobe", "French & Saunders", "European Parliament and the Council of the European Union", "the Beatles song \"A Day in the Life\"", "either February 28 or March 1, while others only observe birthdays on the authentic intercalary date, February 29", "Thomas Edison", "trio", "the Battle of Frankenhausen", "South Africa", "a new entrance building", "national unity", "nine", "tarugula", "construction service firms (e.g. engineering, architecture) and construction managers (firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project)", "pariso, Indiana", "`` Feed Jake '' is a song written by Danny `` Bear '' Mayo, and recorded by the American country music band Pirates of the Mississippi", "several hundred thousand", "why Einstein never received a Nobel prize for relativity", "Noel Gallagher", "Max West (1920s outfielder)", "Northern Rail", "aluminium", "newspapers, television, radio, cable television, and other businesses", "Lakshmibai", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2728334603362961}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.2222222222222222, 0.0, 0.0, 1.0, 0.47058823529411764, 0.20689655172413793, 0.0, 0.13333333333333336, 1.0, 1.0, 0.0, 0.21052631578947367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5294117647058824, 0.0, 0.5384615384615384, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.7692307692307693, 0.4, 1.0]}}, "error_ids": ["mrqa_squad-validation-3267", "mrqa_squad-validation-7", "mrqa_squad-validation-7728", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-8591", "mrqa_hotpotqa-validation-1949", "mrqa_squad-validation-5055", "mrqa_triviaqa-validation-1724", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-5053", "mrqa_hotpotqa-validation-1701", "mrqa_squad-validation-2287", "mrqa_triviaqa-validation-4596", "mrqa_squad-validation-5447", "mrqa_triviaqa-validation-1305", "mrqa_triviaqa-validation-5598", "mrqa_triviaqa-validation-2632", "mrqa_squad-validation-6764", "mrqa_triviaqa-validation-1476", "mrqa_naturalquestions-validation-7838", "mrqa_squad-validation-914", "mrqa_triviaqa-validation-296", "mrqa_hotpotqa-validation-3695", "mrqa_squad-validation-5288", "mrqa_triviaqa-validation-5741", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-1664"], "retrieved_ids": ["mrqa_naturalquestions-train-79547", "mrqa_naturalquestions-train-87019", "mrqa_naturalquestions-train-43831", "mrqa_naturalquestions-train-87230", "mrqa_naturalquestions-train-16746", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-57019", "mrqa_naturalquestions-train-10511", "mrqa_naturalquestions-train-5441", "mrqa_naturalquestions-train-55299", "mrqa_naturalquestions-train-72578", "mrqa_naturalquestions-train-87708", "mrqa_naturalquestions-train-10401", "mrqa_naturalquestions-train-39904", "mrqa_naturalquestions-train-15740", "mrqa_naturalquestions-train-33729", "mrqa_squad-validation-10167", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-1780", "mrqa_naturalquestions-validation-53", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1126", "mrqa_hotpotqa-validation-3440", "mrqa_squad-validation-3961", "mrqa_squad-validation-108", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-2164", "mrqa_hotpotqa-validation-2145", "mrqa_hotpotqa-validation-2287", "mrqa_squad-validation-3330", "mrqa_hotpotqa-validation-5727", "mrqa_hotpotqa-validation-3481"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 82, "before_eval": {"predictions": ["north west coast of Scotland", "Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects", "the opposite ( contralateral ) side of the body", "24 hours", "April 20, 1945", "dachshund racing championships sponsored by Wienerschnitzel.", "helped to increase local producer prices by 20\u201325% in Nairobi and Mombasa", "geologic, topographic, and natural ecosystem landscapes", "129", "Boutique hotel", "rudolf dassler", "an official school sport", "geese fair", "his friend and future rival, Jamukha", "tentacles and prey", "konakry", "Ramanaa", "Lawrence County State's Attorney John Fitzgerald, Chief Deputy Attorney General Charlie McGuigan, and attorney and 2014 U.S. Senate candidate Jason Ravnsborg", "water on the ground surface enters the soil", "9.9", "film and short novels", "American-Canadian mystery-drama television series", "Arrested Development", "Gardnerville", "Nucleotides", "25-minute episodes", "Julian Paul Assange ( ; born Julian Paul Hawkins, 3 July 1971) is an Australian computer programmer and the founder ofWiki, an organisation which he founded in 2006.", "Zapatista Army of National Liberation", "Vikram Bhatt, Bhushan Patel and Tinu Suresh Desai", "green with jealousy", "2003", "salt harvesting projects"], "metric_results": {"EM": 0.15625, "QA-F1": 0.32053683385579934}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [0.33333333333333337, 0.16, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5517241379310345, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2133", "mrqa_squad-validation-10390", "mrqa_naturalquestions-validation-579", "mrqa_hotpotqa-validation-4153", "mrqa_hotpotqa-validation-4420", "mrqa_squad-validation-8322", "mrqa_squad-validation-2791", "mrqa_squad-validation-9603", "mrqa_naturalquestions-validation-2347", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-1053", "mrqa_squad-validation-6113", "mrqa_squad-validation-4616", "mrqa_triviaqa-validation-1685", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-8544", "mrqa_naturalquestions-validation-746", "mrqa_triviaqa-validation-863", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2361", "mrqa_squad-validation-7708", "mrqa_hotpotqa-validation-1639", "mrqa_triviaqa-validation-2236", "mrqa_hotpotqa-validation-164", "mrqa_triviaqa-validation-2592", "mrqa_naturalquestions-validation-9117", "mrqa_triviaqa-validation-7419"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 83, "before_eval": {"predictions": ["september", "North Africa", "a young girl", "architecture from the gothic, renaissance, baroque and neoclassical periods", "usually fall back with terminal velocities much lower than their muzzle velocity when they leave the barrel of a firearm", "the aboral organ", "piotr Naskrecki", "Scrooge", "l Angeles", "Berlin", "Kent, particularly Sandwich, Faversham and Maidstone", "kingdoms of Francia on the Lower Rhine, Burgundy on the Upper Rhine and Alemannia", "Selim II", "4 ( etiology of disease ) ; 5 ( body part affected ) ; 6 ( severity of illness ) ; and 7 ( placeholder for extension of the code to increase specificity )", "Avatar", "difficult and intricate topics", "email fax", "ABC Radio", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "bacteria", "Rated R", "paris", "Steveston Outdoor pool in Richmond, BC", "electricity supply system", "Terry the Tomboy", "redbird", "South Australia", "Mach number", "pit road speed", "North Atlantic Conference", "at the group's final British performance on 14 July 2012 at the National Bowl in Milton Keynes", "bbc"], "metric_results": {"EM": 0.25, "QA-F1": 0.3407531980901546}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.14814814814814814, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.15384615384615385, 0.0, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8484848484848485, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3493", "mrqa_squad-validation-9912", "mrqa_naturalquestions-validation-1805", "mrqa_squad-validation-920", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-3113", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-7486", "mrqa_hotpotqa-validation-2221", "mrqa_squad-validation-9349", "mrqa_hotpotqa-validation-1312", "mrqa_naturalquestions-validation-5214", "mrqa_triviaqa-validation-5960", "mrqa_hotpotqa-validation-2964", "mrqa_triviaqa-validation-90", "mrqa_naturalquestions-validation-1798", "mrqa_triviaqa-validation-3987", "mrqa_naturalquestions-validation-7172", "mrqa_squad-validation-1159", "mrqa_triviaqa-validation-570", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-8518", "mrqa_triviaqa-validation-5793"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 84, "before_eval": {"predictions": ["music", "26", "Nathan Bedford Forrest", "Goths", "Ubiorum", "in front of only 700 fans", "Brixton", "60", "Assistant Secretary for Administration and Management", "better fuel economy, which Orenda Aerospace felt would be attractive for older aircraft whose engines were reaching the end of their lifespan", "september", "evacuate the cylinder, choking it and giving excessive compression (\"kick back\")", "tree", "brain", "Carson City", "biologist", "Egyptians", "september", "published under the title `` The Chariot ''", "ambassador to Ghana", "Cleveland Cavaliers", "Emma Thompson and Alice Eve", "edible - nest swiftlets", "christmas", "travel literature, cartography, geography, and scientific education", "prince at Wi\u015bniowiec, magnate", "king", "chiang Kai-Shek", "125 lb (57 kg)", "the trunk", "Lexus", "a prison"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4139121642246642}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2222222222222222, 0.24, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3830", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-6794", "mrqa_triviaqa-validation-2014", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-1628", "mrqa_hotpotqa-validation-4414", "mrqa_triviaqa-validation-5687", "mrqa_squad-validation-3364", "mrqa_triviaqa-validation-1529", "mrqa_naturalquestions-validation-3368", "mrqa_triviaqa-validation-3538", "mrqa_naturalquestions-validation-10461", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-661", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-8660", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1645", "mrqa_hotpotqa-validation-989", "mrqa_triviaqa-validation-159", "mrqa_naturalquestions-validation-6660"], "retrieved_ids": ["mrqa_naturalquestions-train-39904", "mrqa_naturalquestions-train-50281", "mrqa_naturalquestions-train-9545", "mrqa_naturalquestions-train-9885", "mrqa_naturalquestions-train-13129", "mrqa_naturalquestions-train-7227", "mrqa_naturalquestions-train-28980", "mrqa_naturalquestions-train-49953", "mrqa_naturalquestions-train-9545", "mrqa_naturalquestions-train-70275", "mrqa_naturalquestions-train-60929", "mrqa_naturalquestions-train-12930", "mrqa_naturalquestions-train-46616", "mrqa_naturalquestions-train-68722", "mrqa_naturalquestions-train-33455", "mrqa_naturalquestions-train-77786", "mrqa_triviaqa-validation-831", "mrqa_squad-validation-9358", "mrqa_squad-validation-7571", "mrqa_squad-validation-9081", "mrqa_triviaqa-validation-3844", "mrqa_squad-validation-7836", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-7605", "mrqa_squad-validation-7476", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-8136", "mrqa_squad-validation-6550", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8907", "mrqa_squad-validation-10390"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 85, "before_eval": {"predictions": ["Europe, North America, East Asia and South Asia", "4.0", "Malcolm Young", "fall", "3000 metres", "October 13, 1980", "1765", "september", "paris", "on a bread plate, sometimes in the napkin ), napkin, and flatware ( knives and spoons to the right of the central plate, and forks to the left )", "september", "bilaterally symmetrical", "he did not consider the papacy part of the biblical Church", "March 1, 1837", "November 27, 2017", "Austria as Bambi: Eine Lebensgeschichte aus dem Walde", "point on the frontier indicates efficient use of the available inputs ( such as points B, D and C in the graph )", "from ages 12\u201318", "Retina Display", "on the equator and overlies the East African Rift covering a diverse and expansive terrain that extends roughly from Lake Victoria to Lake Turkana ( formerly called Lake Rudolf) and further south-east to the Indian Ocean", "in school at a given time in the school day", "guinea", "Jane Austen", "lactobacilli", "a school or other place of formal education", "due to a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "victoria davison", "David Hatcher Childress", "EE", "in the 1970s and'80s", "mexico", "a dose of 200 to 500 mg up to 7 ml"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2744593253968254}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.5, 0.0, 0.8571428571428571, 0.32, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.25, 0.4, 0.5, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669]}}, "error_ids": ["mrqa_naturalquestions-validation-10601", "mrqa_squad-validation-2529", "mrqa_hotpotqa-validation-4719", "mrqa_naturalquestions-validation-3515", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-7425", "mrqa_naturalquestions-validation-2023", "mrqa_triviaqa-validation-710", "mrqa_naturalquestions-validation-7767", "mrqa_squad-validation-2267", "mrqa_hotpotqa-validation-3220", "mrqa_naturalquestions-validation-7513", "mrqa_hotpotqa-validation-1506", "mrqa_naturalquestions-validation-2883", "mrqa_hotpotqa-validation-244", "mrqa_squad-validation-8266", "mrqa_squad-validation-1941", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-4988", "mrqa_squad-validation-1891", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-4854", "mrqa_squad-validation-1566", "mrqa_triviaqa-validation-2508", "mrqa_naturalquestions-validation-2907", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 86, "before_eval": {"predictions": ["Native Americans", "thammasat", "South Sentinel Island", "FCA US LLC", "Aaron Taylor- Johnson", "4", "japan", "a gift", "Conservative", "Colin Baker, Sylvester McCoy and Paul McGann", "Islam", "Super Bowl XXVIII at the end of the 1993 season", "maquiladora", "Bigger Than Both of Us", "in human and animals as a short - lived product in biochemical processes", "fourth studio album, \" Damn\"", "a problem", "13 May 1787", "WJRT-TV", "Great Lakes", "temperatures and sea levels have been rising at or above the maximum rates proposed during the last IPCC report in 2001", "the Anabaptists, Zwinglianism, and the papacy", "film playback singer, director, writer and producer", "southern whites", "Marie", "L", "Commonwealth Universities", "drawing letters in the air", "a standard model of particle physics", "the All Stars", "Johann Eck, speaking on behalf of the Empire as assistant of the Archbishop of Trier, presented Luther with copies of his writings laid out on a table and asked him if the books were his, and whether he stood by their contents", "1980"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21531498015873016}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.8750000000000001, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3348", "mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-4230", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5195", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-734", "mrqa_squad-validation-7281", "mrqa_squad-validation-7679", "mrqa_squad-validation-9589", "mrqa_squad-validation-782", "mrqa_naturalquestions-validation-3004", "mrqa_hotpotqa-validation-908", "mrqa_naturalquestions-validation-3704", "mrqa_hotpotqa-validation-4401", "mrqa_naturalquestions-validation-9878", "mrqa_squad-validation-5911", "mrqa_triviaqa-validation-5001", "mrqa_squad-validation-8531", "mrqa_squad-validation-2331", "mrqa_hotpotqa-validation-367", "mrqa_naturalquestions-validation-9516", "mrqa_hotpotqa-validation-5241", "mrqa_naturalquestions-validation-3323", "mrqa_squad-validation-10506", "mrqa_triviaqa-validation-1431", "mrqa_squad-validation-2113"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 87, "before_eval": {"predictions": ["on First Street in downtown Dayton, Ohio, United States", "electronic gaming machines", "August 21, 1995", "the last Shah of Iran", "digital media players", "American Broadcasting Company", "Burbank, California", "Mongolian, Tibetan, and Chinese", "Due to the controversial and explicit nature of many of their songs", "Meghan Trainor", "carbon cycle", "tenant management", "recite the 42 negative confessions of Maat", "Donna Noble (Catherine Tate) with Mickey Smith (noel Clarke) and Jack Harkness (Henry Barrowman) recurring as secondary companion figures", "salvaging a country usually seen as one of the most stable and prosperous in Africa", "The Kree, briefly known as the Ruul", "titanium metal", "Loud", "Kirinyaga, Kirenyaa and Kiinyaa", "graham kenzie kenzie", "domestic legislation", "actions-oriented", "toothbrush", "population", "beer", "Thutmose III", "a somewhat larger number of \"contributing authors\"", "Buck Owens", "Kansas", "political support", "Finding Nemo", "all England tennis club"], "metric_results": {"EM": 0.3125, "QA-F1": 0.461604020979021}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [0.3636363636363636, 0.0, 1.0, 0.0, 0.4, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 0.5, 0.2, 0.5714285714285714, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571]}}, "error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-2196", "mrqa_squad-validation-531", "mrqa_squad-validation-5648", "mrqa_naturalquestions-validation-6012", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-3713", "mrqa_squad-validation-7703", "mrqa_squad-validation-8386", "mrqa_hotpotqa-validation-3692", "mrqa_triviaqa-validation-7649", "mrqa_triviaqa-validation-6049", "mrqa_squad-validation-9504", "mrqa_triviaqa-validation-3943", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-1282", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-2814"], "retrieved_ids": ["mrqa_naturalquestions-train-52322", "mrqa_naturalquestions-train-65608", "mrqa_naturalquestions-train-47593", "mrqa_naturalquestions-train-2469", "mrqa_naturalquestions-train-84096", "mrqa_naturalquestions-train-9381", "mrqa_naturalquestions-train-19694", "mrqa_naturalquestions-train-35268", "mrqa_naturalquestions-train-37879", "mrqa_naturalquestions-train-84293", "mrqa_naturalquestions-train-72194", "mrqa_naturalquestions-train-76120", "mrqa_naturalquestions-train-1574", "mrqa_naturalquestions-train-47961", "mrqa_naturalquestions-train-7222", "mrqa_naturalquestions-train-71838", "mrqa_triviaqa-validation-4620", "mrqa_hotpotqa-validation-2375", "mrqa_squad-validation-7664", "mrqa_triviaqa-validation-3538", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-1053", "mrqa_squad-validation-8560", "mrqa_triviaqa-validation-3906", "mrqa_squad-validation-308", "mrqa_squad-validation-4637", "mrqa_squad-validation-2660", "mrqa_naturalquestions-validation-2847", "mrqa_hotpotqa-validation-4648", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2085", "mrqa_squad-validation-10202"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 88, "before_eval": {"predictions": ["Paolo Cannavaro, (] ; born 13 September 1973) is an Italian former professional footballer and current manager of Chinese club Tianjin Quanjian.", "Janet Jackson", "The Gold Coast", "escaping attribution", "Selena Gomez", "8th", "Forbes", "137", "umbilical", "Led Zeppelin", "The South African Schools Act of 1996", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "blue dare", "eponymous protagonist", "in arid lowland or mountainous shrubland, widely dispersed in dry open country with scattered brush", "the high risk of a conflict of interest and/or the avoidance of absolute powers", "sacerdotalism", "discontinued", "2,000 km", "44", "Roman-era geography (1st century BC) as Greek \u1fec\u1fc6\u03bd\u03bf\u03c2 ( Rh\u0113nos) Latin Rhenus", "the company generates income", "duke of england", "ceremonial counties", "popular hits by popular music artists such as Alicia Keys, Jhen\u00e9 Aiko, India Arie, and Solange Knowles & Destiny's Child", "Ward", "Fryda Wolff", "Eddie Gottlieb Trophy", "Elvis Presley", "Afghans", "reversed", "phycobilisomes"], "metric_results": {"EM": 0.125, "QA-F1": 0.2619364830571727}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8, 0.3636363636363636, 0.0, 0.0, 0.9655172413793104, 0.0, 0.0, 0.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2848", "mrqa_triviaqa-validation-6166", "mrqa_hotpotqa-validation-3930", "mrqa_squad-validation-6775", "mrqa_naturalquestions-validation-5785", "mrqa_hotpotqa-validation-3343", "mrqa_naturalquestions-validation-10162", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-6636", "mrqa_squad-validation-6823", "mrqa_naturalquestions-validation-4351", "mrqa_triviaqa-validation-4242", "mrqa_hotpotqa-validation-799", "mrqa_naturalquestions-validation-8319", "mrqa_squad-validation-6396", "mrqa_squad-validation-2101", "mrqa_naturalquestions-validation-8965", "mrqa_hotpotqa-validation-2668", "mrqa_squad-validation-9246", "mrqa_naturalquestions-validation-5291", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4364", "mrqa_naturalquestions-validation-2264", "mrqa_squad-validation-843", "mrqa_naturalquestions-validation-8622", "mrqa_hotpotqa-validation-5498", "mrqa_naturalquestions-validation-8095", "mrqa_hotpotqa-validation-828"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 89, "before_eval": {"predictions": ["217", "conscientious lawbreakers must be punished", "House of Fraser", "Bonkyll Castle", "meat", "time complexity", "Far Away", "m\u00e1laga airport", "the Philippines", "filaments", "margaret", "1843", "the referendum in the Netherlands", "henry", "William Allen White Book Award and the California Young Reader Medal", "Louis XVIII", "January 30, 1930", "russia", "swissair", "all U.S. territories except American Samoa", "the Gaussian integers Z[i] that is, the set of complex numbers of the form a + bi where i denotes the imaginary unit and a and b are arbitrary integers", "Janis Joplin with the poets Michael McClure and Bob Neuwirth", "Odinga", "homicides", "shrewsbury river festival", "Bergen County", "August 9, 2017", "mongol", "the American philosophy of pragmatism", "USC Trojans", "Ian Hart", "sequential proteolytic activation of complement molecules"], "metric_results": {"EM": 0.25, "QA-F1": 0.3684760278099094}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.37499999999999994, 0.23076923076923078, 0.9473684210526316, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-803", "mrqa_squad-validation-6773", "mrqa_hotpotqa-validation-1756", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-6387", "mrqa_naturalquestions-validation-7598", "mrqa_squad-validation-8898", "mrqa_triviaqa-validation-5020", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-289", "mrqa_naturalquestions-validation-3269", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-7020", "mrqa_hotpotqa-validation-2831", "mrqa_squad-validation-9079", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-5430", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-4515", "mrqa_triviaqa-validation-3643", "mrqa_naturalquestions-validation-1450", "mrqa_squad-validation-2793", "mrqa_squad-validation-6645"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 90, "before_eval": {"predictions": ["Ozone is produced in the upper atmosphere when O2 combines with atomic oxygen made by the splitting of O2 by ultraviolet (UV) radiation.", "reactive allotrope", "The Daleks' Master Plan", "tragedy", "phylum", "older", "magnetic field", "complexity", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "clef", "20 regional offices and 11 sub-offices", "black Bottom Stomp", "professor of cognitive science", "The Book of Roger", "time derivative", "photolysis", "1998\u20132002", "national Horatio Hornblower novels", "Grisha Alekandrovich Nikolaev", "Type I", "rootlets", "lily", "Ken Howard", "red", "Pittsburgh Steelers", "essential epitopes", "south-analysis motorway", "park Inn", "september", "Morty", "Charlotte Louise Riley", "the Costiff collection of 178 Vivienne Westwood costumes"], "metric_results": {"EM": 0.125, "QA-F1": 0.19791666666666669}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445]}}, "error_ids": ["mrqa_squad-validation-3496", "mrqa_squad-validation-3497", "mrqa_squad-validation-7657", "mrqa_hotpotqa-validation-2585", "mrqa_squad-validation-4421", "mrqa_squad-validation-4971", "mrqa_triviaqa-validation-6726", "mrqa_squad-validation-1689", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-5", "mrqa_squad-validation-1070", "mrqa_squad-validation-10358", "mrqa_naturalquestions-validation-2838", "mrqa_squad-validation-5814", "mrqa_triviaqa-validation-1246", "mrqa_naturalquestions-validation-3233", "mrqa_squad-validation-6884", "mrqa_naturalquestions-validation-1704", "mrqa_triviaqa-validation-3988", "mrqa_triviaqa-validation-7407", "mrqa_squad-validation-259", "mrqa_squad-validation-6627", "mrqa_hotpotqa-validation-2128", "mrqa_triviaqa-validation-3575", "mrqa_triviaqa-validation-810", "mrqa_naturalquestions-validation-6248", "mrqa_hotpotqa-validation-5274", "mrqa_squad-validation-5441"], "retrieved_ids": ["mrqa_naturalquestions-train-24134", "mrqa_naturalquestions-train-25957", "mrqa_naturalquestions-train-3458", "mrqa_naturalquestions-train-87353", "mrqa_naturalquestions-train-51761", "mrqa_naturalquestions-train-21722", "mrqa_naturalquestions-train-56302", "mrqa_naturalquestions-train-10221", "mrqa_naturalquestions-train-30970", "mrqa_naturalquestions-train-63655", "mrqa_naturalquestions-train-17995", "mrqa_naturalquestions-train-42602", "mrqa_naturalquestions-train-48080", "mrqa_naturalquestions-train-60425", "mrqa_naturalquestions-train-40340", "mrqa_naturalquestions-train-54215", "mrqa_triviaqa-validation-3568", "mrqa_hotpotqa-validation-908", "mrqa_triviaqa-validation-6425", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-9227", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5087", "mrqa_squad-validation-3887", "mrqa_squad-validation-9081", "mrqa_naturalquestions-validation-10148", "mrqa_hotpotqa-validation-3929", "mrqa_naturalquestions-validation-2445", "mrqa_squad-validation-5618", "mrqa_naturalquestions-validation-4192", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-9426"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 91, "before_eval": {"predictions": ["40%", "catherine linton", "Mongolian patrimonial feudalism", "ivy lane", "Afghanistan", "biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "American", "100,000 writes", "aragon", "hiva oa", "tuscaloosa", "North Greenwich Arena", "ovules in their unfertilized state", "The 2015 Masters Tournament", "\"Endless Flight\" in omnibus \"Horror Stories\" and the table tennis sports film \"As One\"", "90%", "henry Bessemer process", "improved firearms", "1856", "Hepatocytes", "deities and spirits", "ulysses S. Grant", "Belfast and elsewhere in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the United States", "glastonbury", "crow", "South Africa", "Four Weddings and a Funeral", "low-skilled workers", "at the center of the Northern Hemisphere", "UAE", "Psilocybin, psilocin and baeocystin", "cuba and His Teddy Bear"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31448765300164727}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.967741935483871, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4749", "mrqa_squad-validation-8411", "mrqa_triviaqa-validation-7698", "mrqa_squad-validation-9738", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-3055", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-888", "mrqa_naturalquestions-validation-2439", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1140", "mrqa_squad-validation-8577", "mrqa_triviaqa-validation-6102", "mrqa_squad-validation-9812", "mrqa_triviaqa-validation-1336", "mrqa_hotpotqa-validation-1068", "mrqa_triviaqa-validation-5143", "mrqa_naturalquestions-validation-7799", "mrqa_triviaqa-validation-6126", "mrqa_triviaqa-validation-4353", "mrqa_squad-validation-7538", "mrqa_naturalquestions-validation-2721", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-1473", "mrqa_triviaqa-validation-6594"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 92, "before_eval": {"predictions": ["Somerset County, Pennsylvania", "communism", "The Gentle Don\" or \"the Docile Don\"", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "pacific", "July 25, 1898", "BitInstant", "\"Stuck in the Suburbs\" (2007)", "talavera de la Reina", "steamboats", "Social Chapter", "Australia", "retinal ganglion cells of one retina", "the physics is now described by the Schr\u00f6dinger equation instead of Newtonian equations", "The Lone Ranger", "President John F. Kennedy", "Landry's, Inc.", "James Knox Polk", "David Naughton, Jenny Agutter and Griffin Dunne", "national defence Volunteers", "Chicago", "john alcock and brown", "Airline Deregulation Act of December 1978", "inversely to member state size", "buoyancy", "Irvine", "Marine Corps Air Station Kaneohe Bay", "Teen Titans Go!", "south african", "November 20, 1942", "Hugues Capet, king of France", "Morning Edition"], "metric_results": {"EM": 0.40625, "QA-F1": 0.55058170995671}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.5, 0.0, 1.0, 0.25, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.2857142857142857, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4163", "mrqa_triviaqa-validation-3064", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-3366", "mrqa_naturalquestions-validation-3820", "mrqa_hotpotqa-validation-2474", "mrqa_triviaqa-validation-5888", "mrqa_naturalquestions-validation-3316", "mrqa_squad-validation-10388", "mrqa_naturalquestions-validation-10080", "mrqa_hotpotqa-validation-2914", "mrqa_triviaqa-validation-6960", "mrqa_triviaqa-validation-7185", "mrqa_hotpotqa-validation-4387", "mrqa_squad-validation-4210", "mrqa_triviaqa-validation-980", "mrqa_squad-validation-2696", "mrqa_triviaqa-validation-1267", "mrqa_squad-validation-3189"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 93, "before_eval": {"predictions": ["prairie region", "a fee per unit of information transmitted", "London Underground", "nearly 100 locations across Utah", "paris", "Great Britain", "mid-size four - wheel drive luxury SUV", "Afro-Guyanese", "kill mountain", "we want to practice Christian love toward them and pray that they convert", "two populations of rodents", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "T'Pau", "geheime Staatspolizei", "1815", "National Football Conference", "seppuku", "North America", "\"synforms\"", "2015", "man's disobedience toward God", "a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "\"Winnie the Pooh\" (2011)", "Transpacific Yacht Race", "German", "one of the seven Scottish Universities", "major provider of ground support services in mining and tunnelling", "either yes or no, or alternately either 1 or 0", "British Overseas Territory", "`` Company Picnic ''", "probable lie test", "mass murder"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2612468117706247}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.2702702702702703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.10526315789473684, 1.0, 0.0, 0.0, 0.34782608695652173, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7513", "mrqa_squad-validation-4747", "mrqa_triviaqa-validation-7064", "mrqa_naturalquestions-validation-126", "mrqa_triviaqa-validation-2960", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-1586", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6177", "mrqa_squad-validation-2368", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-4204", "mrqa_squad-validation-9", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-421", "mrqa_triviaqa-validation-4668", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-2600", "mrqa_squad-validation-2781", "mrqa_hotpotqa-validation-3298", "mrqa_squad-validation-2040", "mrqa_hotpotqa-validation-1246", "mrqa_squad-validation-1652", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-9903", "mrqa_triviaqa-validation-2387", "mrqa_triviaqa-validation-1746"], "retrieved_ids": ["mrqa_naturalquestions-train-26115", "mrqa_naturalquestions-train-39180", "mrqa_naturalquestions-train-22059", "mrqa_naturalquestions-train-58582", "mrqa_naturalquestions-train-42742", "mrqa_naturalquestions-train-24122", "mrqa_naturalquestions-train-68670", "mrqa_naturalquestions-train-52812", "mrqa_naturalquestions-train-87873", "mrqa_naturalquestions-train-7387", "mrqa_naturalquestions-train-13544", "mrqa_naturalquestions-train-52255", "mrqa_naturalquestions-train-44591", "mrqa_naturalquestions-train-58158", "mrqa_naturalquestions-train-2293", "mrqa_naturalquestions-train-5794", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-2368", "mrqa_squad-validation-5571", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-875", "mrqa_triviaqa-validation-5496", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-4485", "mrqa_naturalquestions-validation-2839", "mrqa_squad-validation-6627", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-4314", "mrqa_naturalquestions-validation-3395", "mrqa_triviaqa-validation-2098", "mrqa_naturalquestions-validation-5146", "mrqa_triviaqa-validation-6475"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 94, "before_eval": {"predictions": ["the nucleus", "horror fiction", "cornwall", "alberta", "southeast of the city", "Puerto Rico", "the French island of Guadeloupe in the Lesser Antilles, mainly in the commune of Deshaies ( which doubles for the town of Honor\u00e9 on the fictional island of Saint Marie )", "the \u201cstatute of Rageman\u201d (De Ragemannis)", "1938", "63%", "American pharmaceutical company", "2015", "90% to 93% O2", "3 to 6 % of total solar radiation", "special sovereignty", "Chen's theorem", "belt", "short-tempered", "1987", "stop the Pigeon", "regulates the practice of pharmacists and pharmacy technicians", "henry Wentworth", "1968", "senior enlisted sailor ( `` E-9 '' )", "led about 1,500 army troops and provincial militia on an expedition in June 1755 to take Fort Duquesne.", "Islamic fundamentalist or neofundamentalist", "June 11, 1986", "Killeshandra in County Cavan", "Indian singer Tanvi Shah", "Tom Coburn", "short Circuit 2", "increased and the ground water level fell significantly. Dead branches dried up and the amount of forests on the flood plains decreased sharply."], "metric_results": {"EM": 0.1875, "QA-F1": 0.2531406810035842}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.45161290322580644, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.1, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.16000000000000003]}}, "error_ids": ["mrqa_naturalquestions-validation-366", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-2404", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-7787", "mrqa_triviaqa-validation-2249", "mrqa_naturalquestions-validation-2624", "mrqa_hotpotqa-validation-4506", "mrqa_squad-validation-3677", "mrqa_naturalquestions-validation-5420", "mrqa_hotpotqa-validation-562", "mrqa_triviaqa-validation-6433", "mrqa_squad-validation-2486", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-3230", "mrqa_triviaqa-validation-5584", "mrqa_hotpotqa-validation-2328", "mrqa_naturalquestions-validation-4365", "mrqa_squad-validation-10196", "mrqa_squad-validation-9737", "mrqa_squad-validation-3306", "mrqa_naturalquestions-validation-7496", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-659", "mrqa_squad-validation-9252"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 95, "before_eval": {"predictions": ["the \"richest 1 percent in the United States", "universal ruler", "red", "alain giresse", "the Alps of northern Italy's Lombardy region", "prime ideal", "david duchovny", "carrie", "Charles Foster Kane", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "the s - block", "World Music Awards", "six", "8\u20134\u20134 system", "exposed to scrutiny", "79", "banned the growing of coffee", "no French regular army troops were stationed in North America", "creative plea", "stimulated his brain cells", "leprechaun", "Daniel Ken \"Daniel\" Inouye", "David Toms", "Charles Haley", "hexagonal", "11 free suburban weekly newspapers", "Dean Stanton", "The Portuguese", "a situation of relatively stagnant wages", "the university's science club in 1886", "the Jurchen Aisin Gioro clan", "kronborg castle"], "metric_results": {"EM": 0.15625, "QA-F1": 0.29741790555935294}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.22222222222222224, 0.7272727272727272, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-7459", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-717", "mrqa_squad-validation-9135", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-879", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-1688", "mrqa_hotpotqa-validation-4523", "mrqa_triviaqa-validation-1838", "mrqa_squad-validation-8288", "mrqa_squad-validation-10138", "mrqa_squad-validation-6914", "mrqa_squad-validation-1445", "mrqa_triviaqa-validation-2900", "mrqa_hotpotqa-validation-1234", "mrqa_naturalquestions-validation-5267", "mrqa_naturalquestions-validation-3093", "mrqa_triviaqa-validation-1682", "mrqa_hotpotqa-validation-5033", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-9985", "mrqa_squad-validation-7184", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-4796"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 96, "before_eval": {"predictions": ["silicates", "Marcus Atilius Regulus", "The Dome of the Rock", "Public- private Partnering (PPPs) aka private finance initiatives (PFIs)", "the word is used to translate several Hebrew words, including Hod ( \u05d4\u05d5\u05d3 ) and kabod", "bicapitalized Microsoft", "Bulgaria", "1,142.9 km", "Air Force, Army, Navy and other services", "Mr. Destiny", "Renhe Sports Management Ltd, 100 % owned by Xiu Li Dai and Yongge Dai. 25 % Owned by Narin Niruttinanon", "philadelphia", "salvation and subsequently eternal life is not earned by good deeds but is received only as a free gift of God's grace through faith in Jesus Christ as redeemer from sin", "hydrogen and helium", "where they were accepted and allowed to worship freely", "the cannonball ( assumed constant ) v", "2011", "74", "Chinese dynasties", "1162", "el Che or simply Che", "Porsche 944", "a primer", "1971", "The \"Huguenot Street Historic District\" in New Paltz", "American Christian rock band Needtobreathe", "the Iranian Islamic Revolution and apolitical Islam", "1986", "cake or biscuit", "the object is placed further away from the mirror / lens than the focal point and this real image is inverted", "electromagnetic", "kolinio Epeli Vanuacicila Rabuka and Salote Lomaloma Rabuka"], "metric_results": {"EM": 0.125, "QA-F1": 0.29001596850861555}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.2857142857142857, 0.0, 0.23529411764705882, 0.42857142857142855, 0.9411764705882353, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.16666666666666666, 0.1111111111111111, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_squad-validation-3504", "mrqa_naturalquestions-validation-5675", "mrqa_squad-validation-6778", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-3165", "mrqa_squad-validation-3876", "mrqa_hotpotqa-validation-5639", "mrqa_triviaqa-validation-66", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-113", "mrqa_squad-validation-2100", "mrqa_squad-validation-3667", "mrqa_squad-validation-3023", "mrqa_naturalquestions-validation-7297", "mrqa_hotpotqa-validation-1088", "mrqa_triviaqa-validation-5093", "mrqa_squad-validation-8189", "mrqa_triviaqa-validation-7768", "mrqa_hotpotqa-validation-1399", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-3771", "mrqa_squad-validation-3057", "mrqa_squad-validation-9574", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-3432", "mrqa_squad-validation-10311", "mrqa_triviaqa-validation-3409"], "retrieved_ids": ["mrqa_naturalquestions-train-13544", "mrqa_naturalquestions-train-11064", "mrqa_naturalquestions-train-33212", "mrqa_naturalquestions-train-59528", "mrqa_naturalquestions-train-60362", "mrqa_naturalquestions-train-3325", "mrqa_naturalquestions-train-54368", "mrqa_naturalquestions-train-76035", "mrqa_naturalquestions-train-43736", "mrqa_naturalquestions-train-52255", "mrqa_naturalquestions-train-18558", "mrqa_naturalquestions-train-33992", "mrqa_naturalquestions-train-87284", "mrqa_naturalquestions-train-4846", "mrqa_naturalquestions-train-73118", "mrqa_naturalquestions-train-67060", "mrqa_triviaqa-validation-2802", "mrqa_naturalquestions-validation-3284", "mrqa_squad-validation-874", "mrqa_naturalquestions-validation-10625", "mrqa_hotpotqa-validation-187", "mrqa_triviaqa-validation-2952", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-2848", "mrqa_triviaqa-validation-1408", "mrqa_squad-validation-4567", "mrqa_triviaqa-validation-7598", "mrqa_hotpotqa-validation-1897", "mrqa_squad-validation-8849", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-6499"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 97, "before_eval": {"predictions": ["the third under head coach Brian Billick.", "faith", "suburb", "The National Era", "American Civil War", "bramble", "July 1, 2005", "a few drops", "Andrew Lincoln", "new york city", "Vice president", "the optic chiasm", "the Continental Edison Company in France", "the campaign for a free India", "the Gararish", "Jai Courtney as John `` Jack '' McClane, Jr", "difficulties of the pulmonary circulation", "Holy Land", "bbc", "jazz", "26", "induction motor", "b\u00f4j B\u00f4j", "the Canadian rock band Nickelback", "Romeo Montague (Italian: \"Romeo Montecchi\" )", "the \"father of the Mongols\"", "indeed caused by chlorine and bromine from manmade organohalogens", "a combined concert/lecture by British progressive folk-rock band Gryphon", "australian", "bbc", "Psych", "the planned Nazi pre-emptive nuclear strike on Japan, `` Operation Dandelion, '' is apparently being prevented only by Hitler's personal refusal to authorise it"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23920367826617828}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.4444444444444445, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4113", "mrqa_squad-validation-2262", "mrqa_hotpotqa-validation-3785", "mrqa_naturalquestions-validation-2536", "mrqa_naturalquestions-validation-7957", "mrqa_triviaqa-validation-5041", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7679", "mrqa_triviaqa-validation-5865", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-3358", "mrqa_squad-validation-1240", "mrqa_hotpotqa-validation-4389", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-1905", "mrqa_triviaqa-validation-636", "mrqa_hotpotqa-validation-2802", "mrqa_squad-validation-1423", "mrqa_triviaqa-validation-2161", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-654", "mrqa_squad-validation-5359", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-6540", "mrqa_hotpotqa-validation-2271", "mrqa_naturalquestions-validation-2729"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 98, "before_eval": {"predictions": ["the most devastating stock market crash in the history of the United States", "SKUM", "S6", "polly", "to predict day and night", "vie", "fossil sequences in which there was datable material, converting the old relative ages into new absolute ages", "heaviest album of all", "classical music", "in San Francisco, California with offices in New York City and Atlanta", "economic recession", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "plum", "alashia", "\"citizenship\", so that people had rights to empower them to become economically and socially active, rather than economic activity being a precondition for rights", "foot-oriented steps combined with fluid movements in the torso, as well as floor work", "'I'll Be There for You\"", "the Baltimore teenagers Ivan Ashford, Markel Steele, Cameron Brown, Tariq Al - Sabir and Avery Bargasse", "kent", "Sultans", "post-war communist control of the country", "roger colne", "Paulinella chromatophora is an exception that acquired a photosynthetic cyanobacterial endosymbiont more recently.", "penrose stairs", "Wynantskill is located at the north town line and the northeast corner of the town of North Greenbush.", "polly", "Synergy Group", "a limited period of time", "edo", "Stritch", "between the three towns of Doncaster, Scunthorpe and Gainsborough", "1908, the first five - day workweek in the United States was instituted by a New England cotton mill so that Jewish workers would not have to work on the Sabbath from sundown Friday to sundown Saturday"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24991464826023646}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.4, 1.0, 0.08333333333333333, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.7058823529411764, 0.0, 0.0, 0.0, 0.0, 0.30769230769230765, 0.0, 0.0, 0.18181818181818182, 0.06060606060606061]}}, "error_ids": ["mrqa_hotpotqa-validation-2978", "mrqa_triviaqa-validation-6634", "mrqa_naturalquestions-validation-2213", "mrqa_triviaqa-validation-1952", "mrqa_squad-validation-5008", "mrqa_triviaqa-validation-2353", "mrqa_hotpotqa-validation-2856", "mrqa_naturalquestions-validation-10707", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2725", "mrqa_squad-validation-4433", "mrqa_hotpotqa-validation-5136", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6382", "mrqa_squad-validation-992", "mrqa_triviaqa-validation-5737", "mrqa_squad-validation-8802", "mrqa_triviaqa-validation-2414", "mrqa_hotpotqa-validation-5724", "mrqa_triviaqa-validation-4769", "mrqa_hotpotqa-validation-4093", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-4550", "mrqa_naturalquestions-validation-886", "mrqa_hotpotqa-validation-1533", "mrqa_naturalquestions-validation-2713"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 99, "before_eval": {"predictions": ["Ron Grainer", "1919", "elstow", "sent missionaries, backed by a fund to financially reward converts to Catholicism", "Gender pay gap in favor of males in the labor market", "Woody's horse", "December 19, 1967", "economic growth by collecting resources from colonies, in combination with assuming political control by military and political means", "1997", "October 1, 2017", "in Washington, D.C. on a 99 acre campus", "wild witches", "Toronto, Ontario, Canada", "a saint", "France's claim to the region was superior to that of the British", "a balance sensor consisting of a statolith, a solid particle supported on four bundles of cilia, called \"balancers\" that sense its orientation", "yosemite national park", "extracurricular activities", "Roman Jakobson", "saloon-keeper", "the studies and developments department of the French firm R2E Micral in 1980 at the request of the company CCMC specializing in payroll and accounting", "1800 to 1850", "the words spoken to Adam and Eve after their sin", "the author recounts how his own opinions changed about that line when he talks to the different people about their beliefs", "Twink", "india", "india civil rights association", "at most one prime number", "porto", "Bhaktivedanta Manor", "jellyfish", "mammals"], "metric_results": {"EM": 0.21875, "QA-F1": 0.352970236232536}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.88, 0.2105263157894737, 0.6666666666666666, 0.0, 0.19999999999999998, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 0.14814814814814817, 0.4166666666666667, 1.0, 0.0, 0.0, 1.0, 0.25, 0.23076923076923078, 0.0, 0.34782608695652173, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-7714", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-4903", "mrqa_squad-validation-3130", "mrqa_squad-validation-7449", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-4384", "mrqa_squad-validation-9926", "mrqa_hotpotqa-validation-3347", "mrqa_triviaqa-validation-7769", "mrqa_hotpotqa-validation-4287", "mrqa_squad-validation-10231", "mrqa_squad-validation-4494", "mrqa_squad-validation-1870", "mrqa_hotpotqa-validation-5590", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-251", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-607", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-6226", "mrqa_triviaqa-validation-4125", "mrqa_hotpotqa-validation-5833", "mrqa_naturalquestions-validation-2794"], "retrieved_ids": ["mrqa_naturalquestions-train-13569", "mrqa_naturalquestions-train-83151", "mrqa_naturalquestions-train-45612", "mrqa_naturalquestions-train-31204", "mrqa_naturalquestions-train-13698", "mrqa_naturalquestions-train-68279", "mrqa_naturalquestions-train-76071", "mrqa_naturalquestions-train-52255", "mrqa_naturalquestions-train-17713", "mrqa_naturalquestions-train-64371", "mrqa_naturalquestions-train-73147", "mrqa_naturalquestions-train-42402", "mrqa_naturalquestions-train-54816", "mrqa_naturalquestions-train-60474", "mrqa_naturalquestions-train-64448", "mrqa_naturalquestions-train-72578", "mrqa_naturalquestions-validation-10080", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-1782", "mrqa_squad-validation-9547", "mrqa_hotpotqa-validation-3557", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-4291", "mrqa_squad-validation-6602", "mrqa_naturalquestions-validation-53", "mrqa_hotpotqa-validation-2361", "mrqa_naturalquestions-validation-9064", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-5026", "mrqa_naturalquestions-validation-3058", "mrqa_naturalquestions-validation-10612", "mrqa_naturalquestions-validation-5214"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.170625, "QA-F1": 0.28254474326930945}, "overall_error_number": 2654, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.68, "QA-F1": 0.7495826692132975}, "final_upstream_test": {"EM": 0.679, "QA-F1": 0.8036888266432648}}}