{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2', diff_loss_weight=0, ewc_gamma=0.9, ewc_lambda=250.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4180, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["positive divisors", "quietist/non-political", "Jonathan Stewart", "surface condensers", "Anglo-Saxons", "one of the first peer-to-peer network architectures", "Tanzania", "structure", "ABC Cable News", "Turner and Vernon", "$2 million", "German-language publications", "-40%", "BBC 1", "the \"blurring of theological and confessional differences in the interests of unity.\"", "pamphlets on Islam", "mad dogs", "Mnemiopsis", "both Kenia and Kegnia", "electricity", "student tuition, endowments, scholarship/voucher funds", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "European Council", "826", "1999", "Latin", "semantical problems", "$2 million", "committee", "(trunnion", "South Pacific", "Spanish moss", "1850s", "Abercrombie was recalled and replaced by Jeffery Amherst", "saturating them unconsciously with electricity", "slightly more than normal sea-level O2 partial pressure", "Associating forces with vectors", "showmanship", "social networking support", "Children of Earth", "Soviet", "Brock Osweiler", "San Diego", "Economist", "liquid", "Jerricho Cotchery", "suggested it for use in the ARPANET", "disrupting their plasma membrane", "Genghis Khan", "Robert Boyle", "feigned retreat", "Rotterdam", "the problem of multiplying two integers", "he was illiterate in Czech", "the Monarch", "4.7 yards per carry", "Sports Programs, Inc.", "only pharmacists", "ideological", "behavioral and demographic data", "Kuviasungnerk/Kangeiko", "94", "October 16, 2012", "transportation, sewer, hazardous waste and water"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7783752554812338}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.11111111111111112, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-4676", "mrqa_squad-validation-10430", "mrqa_squad-validation-5326", "mrqa_squad-validation-2291", "mrqa_squad-validation-1530", "mrqa_squad-validation-7086", "mrqa_squad-validation-8412", "mrqa_squad-validation-2478", "mrqa_squad-validation-3590", "mrqa_squad-validation-1913", "mrqa_squad-validation-3771", "mrqa_squad-validation-6293", "mrqa_squad-validation-1766", "mrqa_squad-validation-1187", "mrqa_squad-validation-288"], "SR": 0.734375, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 1, "before_eval_results": {"predictions": ["1929", "the lack of a Parliament of Scotland", "islands", "US$10 a week", "a force is required to maintain motion, even at a constant velocity", "Horace Walpole", "any object can be, essentially uniquely, decomposed into its prime components", "1968", "straight", "complexity classes", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "80%", "leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "Informal", "seven", "aircraft manufacturing", "1671", "the highest duty of a citizen", "the headwaiter", "a comb jelly", "expositions", "1784", "terrorist organisation", "\"winds up\"", "Golden Gate Bridge", "Hulu", "National Galleries of Scotland", "Northern Rhodesia", "Budapest Telephone Exchange", "Joanna Lumley", "Gateshead", "tentacles", "soluble components (molecules)", "East Smithfield burial site in England", "Jerome Schurf", "satellite television", "we are neither making maximum effort nor achieving results necessary", "Isaac Newton", "dangerous enemies", "Robert Underwood Johnson", "a protest", "kinetic friction", "X-rays", "Roger NFL", "Abu al-Rayhan al-Biruni", "colonial powers", "Spreading throughout the Mediterranean and Europe, the Black Death is estimated to have killed 30\u201360% of Europe's total population", "almost a month", "\"cellular\" and \"humoral\"", "traditional old boy network", "anti-Semitic policies", "the Scottish Government", "the Lisbon Treaty", "emerging market", "Bible", "24\u201310", "cellular respiration", "The \"Big Five\"", "computer problems", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "From Russia", "Balvenie Castle", "geological evidence shows that this 5000-mile mountain chain may extend south into Antarctica", "Annie Ida Jenny No\u00eb Haesendonck"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7216039913510987}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1290322580645161, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1273", "mrqa_squad-validation-10338", "mrqa_squad-validation-9525", "mrqa_squad-validation-2704", "mrqa_squad-validation-3113", "mrqa_squad-validation-6759", "mrqa_squad-validation-739", "mrqa_squad-validation-1277", "mrqa_squad-validation-4902", "mrqa_squad-validation-10410", "mrqa_squad-validation-85", "mrqa_squad-validation-9732", "mrqa_squad-validation-4856", "mrqa_squad-validation-2497", "mrqa_squad-validation-9488", "mrqa_squad-validation-3516", "mrqa_newsqa-validation-911", "mrqa_naturalquestions-validation-1802", "mrqa_triviaqa-validation-1415", "mrqa_searchqa-validation-187", "mrqa_hotpotqa-validation-3155"], "SR": 0.671875, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 2, "before_eval_results": {"predictions": ["Body of Proof", "estimated 16,000 to 35,000", "second", "phagocytes", "Jochi", "Alsace", "West Lothian question", "Wiesner", "representatives elected to either house of parliament", "receiveing medication, pharmacists are increasingly expected to be compensated for their patient care skills", "trial division", "August 2004", "noble brother or blood brother", "warships", "if they were non-discriminatory, \"justified by imperative requirements in the general interest\" and proportionately applied.", "lower sixth", "2002", "Organizational", "the number of social services that people can access wherever they move", "cytokines", "The individual is the final judge of right and wrong", "civil disobedience", "eighteenth century", "glaucophyte", "jellyfish", "existing level of inequality", "well before Braddock's departure for North America", "class of owners", "Hughes Hotel", "Golden Gate Bridge", "Annual Status of Education Report", "seven", "Pauli exclusion principle", "1 September 1939", "Mexico", "Battle of Dalan Balzhut", "relative", "Russell T Davies", "Innate", "1903", "photosynthesis", "private research university", "article 49", "wine", "Arabic numerals", "the application of electricity", "bilaterians", "risen with increased income inequality", "life on Tyneside", "student-teacher relationships", "external combustion engines", "that each side is capable of performing the obligations set out", "the Russian defense ministry said Wednesday.Russia's Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "\"wipe out\" the United States if provoked", "it lies just north of the state capital, Raleigh", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "were the first ever winners of the FA Cup, in 1872", "communion", "Robert Noyce", "Eliot Spitzer", "from local pubs got him dubbed \"Beer Drinker of the Year 2002\"", "the title of a 1985 movie, this Native American rite of passage or of spiritual renewal often includes fasting", "\"big ringing it\"", "the Restoration"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7351634043040293}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.8000000000000002, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.761904761904762, 0.4166666666666667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5928", "mrqa_squad-validation-9697", "mrqa_squad-validation-6404", "mrqa_squad-validation-8909", "mrqa_squad-validation-6106", "mrqa_squad-validation-3378", "mrqa_squad-validation-6970", "mrqa_squad-validation-7514", "mrqa_squad-validation-2724", "mrqa_squad-validation-10428", "mrqa_squad-validation-7201", "mrqa_newsqa-validation-3489", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3648", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-2265", "mrqa_hotpotqa-validation-1174", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-12652", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-4178"], "SR": 0.671875, "CSR": 0.6927083333333333, "EFR": 1.0, "Overall": 0.8463541666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["New Orangery", "parish churches", "Michael Mullett", "Tibetan Buddhism", "North American", "intracellular vesicle", "the Smalcald Articles", "return home", "1788", "an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system", "Several thousand", "Get Carter", "the source of most of the chemical energy released", "the college", "bones", "Roger Goodell", "purple skin patches", "Apollo 17", "1562", "cilia", "a majority of all MEPs (not just those present) to block or suggest changes", "Blum complexity axioms", "the Diffie\u2013Hellman key exchange", "America's Funniest Home Videos", "16", "seven-layer OSI-compliant networking protocol", "the Rh\u00f4ne", "0 \u00b0C (32 \u00b0F)", "May 1754", "infected corpses", "2002", "Australian public X.25 network operated by Telstra", "even greater inequality and potential economic instability", "the Association of American Universities", "economic utility in society from resources devoted on high-end consumption", "cut in half", "uncertain", "1835", "720p high definition", "mainline Protestant Methodist denomination", "comb-bearing", "Tate Britain", "CBS", "Treaty of Rome 1957 and the Maastricht Treaty 1992", "M. Theo Kearney", "a higher wage", "Lenin", "most awarded female act of all-time", "the Mayor of the City of New York", "Vishal Bhardwaj", "Congress passed the Chinese Exclusion Act in 1882 which targeted a single ethnic group by specifically limiting further Chinese immigration", "a network connection device", "The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "Antoine Lavoisier", "CeeLo Green", "Ted Stillwell", "Zelaya and Roberto Micheletti", "Chinese nationals", "Senator Evan Bayh", "robinson", "robinson", "a type of large cushion", "the last of whom was his grandfather", "World leaders"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7544754837553751}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 0.0, 0.8666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6473", "mrqa_squad-validation-3355", "mrqa_squad-validation-3530", "mrqa_squad-validation-4074", "mrqa_squad-validation-4675", "mrqa_squad-validation-9103", "mrqa_squad-validation-2914", "mrqa_squad-validation-7502", "mrqa_squad-validation-7300", "mrqa_squad-validation-545", "mrqa_squad-validation-7526", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-1676", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-1834", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-7746", "mrqa_newsqa-validation-294"], "SR": 0.671875, "CSR": 0.6875, "EFR": 0.9523809523809523, "Overall": 0.8199404761904762}, {"timecode": 4, "before_eval_results": {"predictions": ["9\u201318", "Norman Foster", "Battle of Hastings", "9 October 2006", "Apollo 13", "Robert R. Gilruth", "hermaphroditism and early reproduction", "Moscone Center", "The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France, and sent funds to Britain's ally on the mainland, Prussia,", "1994", "patients' prescriptions and patient safety issues", "December 12", "June 6, 1951", "three", "John Wesley", "a project that fails to adhere to codes does not benefit the owner", "21 January 1788", "Gryphon", "between 1859 and 1865", "ESPN Deportes", "1784", "LeGrande", "St. Lawrence", "informal rule", "Golden Super Bowl", "TEU articles 4 and 5", "a Standard Model", "Westinghouse Electric", "Sayyid Abul Ala Maududi", "$5,000,000", "Mike Tolbert", "phagocytes", "glaucophyte", "In case of a shared physical medium (such as radio or 10BASE5), the packets may be delivered according to a multiple access scheme", "the Magnetophon tape recorder", "random access machines", "a trade magazine for the construction industry", "noisiest", "wars", "Ogr\u00f3d Saski", "Zygons", "Dorothy Skerrit", "Stratfor", "The new Touch, now the most popular iPod, will be available in both black and white and get a $30 price cut, to $199 for 8GB of storage", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients", "two", "The Abolition of Corporal Punishment Act, 1997 ( Act No. 33 of 1997 )", "Colonists objected to the Tea Act because they believed that it violated their rights as Englishmen to `` No taxation without representation ''", "Rent", "James Intveld", "Collective Noun", "Lorelei", "Salvador Dal\u00ed", "The Combination Acts of 1799-1800", "Ricky Marco", "Frank Sinatra", "Little Richard", "John Hume", "Tell", "lop off the hund (German for dog), and you end up with dachs, which is German", "a hat", "Anubis", "Frank Sinatra", "The average number of children for each president is 4.1,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6930671045845161}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6206896551724138, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.16666666666666669, 0.5614035087719298, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10295", "mrqa_squad-validation-6877", "mrqa_squad-validation-5620", "mrqa_squad-validation-9810", "mrqa_squad-validation-8643", "mrqa_squad-validation-4658", "mrqa_squad-validation-718", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-3012", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-1403", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-265", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-6600"], "SR": 0.640625, "CSR": 0.678125, "EFR": 1.0, "Overall": 0.8390625}, {"timecode": 5, "before_eval_results": {"predictions": ["atmospheric", "the constituting General Conference in Dallas, Texas", "central business district", "the third", "The mermaid", "Mick Mixon", "1992", "a course of study", "Mansfeld", "because it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "peptidoglycan", "in Genesis", "15", "the port of Salerno", "84 hours", "Jordan Norwood", "Doritos", "four", "the American Revolutionary War", "the League of Nations", "the temperance movement", "the Albany Congress", "miners", "Giovanni Branca", "Tiffany & Co.", "NASA discontinued the manned Block I program", "Andy Warhol", "2011", "five", "The ability to make probabilistic decisions", "the Divine Right of Kings", "teaching", "research, exhibitions and other shows", "case law by the Court of Justice", "the United States Census Bureau", "The Deadly Assassin", "Seventy percent", "Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code", "order", "a place for another non-European Union player in Frank Rijkaard's squad", "John McCain", "the \"cliff effect\"", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "1983", "Geraldine Margaret Agnew - Somerville", "Woodrow Wilson", "noli me tangere", "1993", "the Federal Reserve System", "General Motors", "Chicago", "polio", "The Spanish Armada", "Kinnairdy Castle", "the 2014\u201315 season", "Heathrow", "Robert Louis Stevenson", "County Executive", "The Making of Coca-Cola's \"I'd Like to Buy the World a Coke\" ad", "The Gettysburg Address", "the pull or the push", "lion", "Sammy Sosa", "tiger's milk"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6364898788227277}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9767441860465117, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2675", "mrqa_squad-validation-1877", "mrqa_squad-validation-8500", "mrqa_squad-validation-2408", "mrqa_squad-validation-1061", "mrqa_squad-validation-7288", "mrqa_squad-validation-2961", "mrqa_squad-validation-3971", "mrqa_squad-validation-1824", "mrqa_squad-validation-4260", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1425", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-1975", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3228", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-1203"], "SR": 0.578125, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.8307291666666666}, {"timecode": 6, "before_eval_results": {"predictions": ["Wahhabism or Salafism", "Del\u00fc\u00fcn Boldog, near Burkhan Khaldun mountain and the Onon and Kherlen rivers", "forceful taking of property", "220 miles (350 km)", "jiggle TV", "On the Councils and the Church", "a Western Union superintendent", "mid-Eocene", "\"do not disturb\" sign", "bounding", "Maling company", "sunlight", "avionics, telecommunications, and computers", "five", "individuals", "Robert Boyle", "new and enlarged bridges, a shuttle service and/or a tram", "1997", "Presiding Officer", "Michelle Gomez", "Laszlo Babai and Eugene Luks", "Wesleyan Holiness Consortium", "Aristotle", "1894", "average workers", "cholecalciferol", "1524\u201325", "religious", "original series serials", "the historical era", "2011", "1665", "closure temperature", "the light reactions", "Brown", "the ireport form", "Les Bleus", "Movahedi", "a man's lifeless, naked body", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "10 logarithm", "Howard Ashman", "Sanchez Navarro", "Vincent Price", "Article Two", "2001", "Popeye", "a temperature scale", "Donna Summer", "Vladimir Vladimirovich Putin", "Blue Ribband Trophy", "Vietnam", "Atlantic Coast Conference", "Prince of Cambodia Norodom Sihanouk", "Don Hahn", "Constance M. Burge", "400", "heavy metal", "drake", "Tokyo", "to When Corporations Rule the World", "naltrexone", "Weehawken", "the Chesapeake Bay"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7635130494505494}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9592", "mrqa_squad-validation-6156", "mrqa_squad-validation-3518", "mrqa_squad-validation-6968", "mrqa_squad-validation-7767", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1643", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6477", "mrqa_triviaqa-validation-4703", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-12747", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-748"], "SR": 0.703125, "CSR": 0.6674107142857143, "EFR": 0.9473684210526315, "Overall": 0.8073895676691729}, {"timecode": 7, "before_eval_results": {"predictions": ["10", "no French regular army troops were stationed in North America,", "Ed Mangan", "German", "c1750", "calcium phosphate", "central Europe", "since 1951", "immunoglobulins and T cell receptors", "average workers", "Brandon McManus", "Science", "33", "88", "monophyletic", "the Bible", "the smallest", "Sports Night", "superheaters", "Mercedes-Benz Superdome", "Wang Zhen", "dioxygen", "1953", "Capitol Hill, Washington, D.C.", "Parliament Square, High Street and George IV Bridge in Edinburgh", "organic solvents", "Mnemiopsis", "topographic", "the Tesla coil", "National Galleries of Scotland", "Old Town Hall, Gateshead", "Bright Automotive,", "$9,600", "wacko", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "more and more suspicious of the way their business books were being handled", "38", "the less tissue the X-rays have to go through", "the field is limited to drivers who meet more exclusive criteria", "the Philippines and Guam, then under Spanish sovereignty, to the U.S.", "Aristotle", "Peking", "Dylan Walters", "My Summer Story", "J. C. Stobart", "roast goose, turkey", "blue, green, red and yellow", "New Zealand", "Colorado", "Inigo Jones", "Brian Doyle- Murray", "Flushed Away", "Cuban", "Sam Waterston", "test pilot, and businessman", "Chief Strategy Officer", "Fernando Rey", "Honshu seaport", "the Black Stone", "Charles Francis \"Charlie\" Harper", "Sri Lanka", "The Mozambique Channel", "Kiss And Kill Killers", "The narwhal"], "metric_results": {"EM": 0.640625, "QA-F1": 0.67421875}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3628", "mrqa_squad-validation-2881", "mrqa_squad-validation-3657", "mrqa_squad-validation-5249", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-358", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-1689", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-720", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-4606", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15236", "mrqa_searchqa-validation-14908"], "SR": 0.640625, "CSR": 0.6640625, "EFR": 1.0, "Overall": 0.83203125}, {"timecode": 8, "before_eval_results": {"predictions": ["bars, caf\u00e9s and clubs", "T\u00f6regene Khatun", "Science Magazine", "3.6%", "Second Republic", "highly diversified", "Beijing", "screw stoking mechanism", "the type of reduction being used", "quickly", "chloroplast division", "historians", "stagnant", "Ali Shariati", "an algorithm", "Higher Real Gymnasium", "four", "events and festivals", "Apollo 1 backup crew", "Ikh Zasag", "1883", "independent schools", "Sophocles", "rare and desired", "One in five", "breaches of law in protest against international organizations and foreign governments", "dynamo electric machine commutators", "Christianized Yamasee", "six Africans dead", "two", "revelry", "Dan Brown", "tennis", "bickering and interrupting each other repeatedly as Trump looked on.", "Tim Clark, Matt Kuchar and Bubba Watson", "points", "18th century", "Erica Rivera", "Davos", "115", "1972", "a limited period of time", "Portugal", "SpongeBob", "photographer", "The Breakfast Club", "Thames Street", "christopher Columbus", "Amerigo Vespucci", "Reginald Engelbach", "Robert \"Bobby\" Germaine, Sr.", "\"Nip/Tuck\"", "Manchester United", "John Faso", "The Rite of Spring", "a pioneer", "The Devil's Dictionary", "dwts", "a fever", "Titan", "bbc", "9 to 5", "The Star-Spangled Banner", "Dennis Potter"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6647853708791209}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.16, 1.0, 0.16666666666666669, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3390", "mrqa_squad-validation-10303", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1585", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-431", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-1178", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-9818", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-3588"], "SR": 0.59375, "CSR": 0.65625, "EFR": 0.9615384615384616, "Overall": 0.8088942307692308}, {"timecode": 9, "before_eval_results": {"predictions": ["Ed Lee", "non-Mongol physicians", "northwestern Canada", "26", "cabin depressurization", "a restaurant situated at a Grade I-listed 16th century merchant's house at 28\u201330 Close", "Super Bowl 50", "private", "nineteenth-century cartographic techniques", "democracy", "ten minutes", "the balance of parties across Parliament", "very rare", "areas cleared of forest", "a lute", "Chagatai", "Tesla Electric Company", "E. W. Scripps Company", "Newcastle Beer Festival", "the western end of the second east-west shipping route", "land and housing", "Warsaw University of Technology building", "present somewhere in Europe in every year between 1346 and 1671", "three", "in herring barrels", "Donkey", "Judy Collins", "An aging group of outlaws look for one last big score as the \"traditional\" American West is disappearing around them", "the Arabica coffee bean", "Captain Nemo", "the Italian", "Paul McCartney", "William Frederick \"Buffalo Bill\" Cody", "Moton Field, the Tuskegee Army Air Field,", "120 m ( 390 ft )", "pan control setting", "Tito Jackson, and two long - shot candidates, Robert Cappucci and Joseph Wiley", "Andrew Gold", "Brooke Wexler", "Titanic earned $657.4 million in North America and $1.528 billion in other countries, for a worldwide total of $2.187 billion", "Caracas", "Vienna", "Wawrinka", "Bear Grylls", "Harry Shearer", "1879", "Dian Fossey", "E Street Band", "Cyclic Defrost", "Nathan Bedford Forrest", "Annales de chimie et de physique", "Faith", "more than 40 million", "Columbia Records", "last week", "September, bruna Bianchi Carneiro Ribeiro", "columbus", "Noida, located in the outskirts of the capital New Delhi", "Tuesday", "originated in a 1,700 year old Roman mosaic entitled Chamber of the Ten Maidens", "Nairobi, Kenya", "Tommy Burns", "christopher colppola", "Joe Torre"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6842310746762472}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.5882352941176471, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5137", "mrqa_squad-validation-477", "mrqa_squad-validation-6059", "mrqa_squad-validation-9310", "mrqa_squad-validation-4954", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-1072", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-2082", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-4698", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3503", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-7775"], "SR": 0.578125, "CSR": 0.6484375, "EFR": 1.0, "Overall": 0.82421875}, {"timecode": 10, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3259", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-6486", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7033", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9685", "mrqa_naturalquestions-validation-9866", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1072", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-1203", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8999", "mrqa_squad-validation-10027", "mrqa_squad-validation-10044", "mrqa_squad-validation-10090", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10125", "mrqa_squad-validation-10136", "mrqa_squad-validation-10192", "mrqa_squad-validation-10211", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10295", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10309", "mrqa_squad-validation-10338", "mrqa_squad-validation-10346", "mrqa_squad-validation-10428", "mrqa_squad-validation-10430", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1123", "mrqa_squad-validation-1146", "mrqa_squad-validation-1187", "mrqa_squad-validation-1211", "mrqa_squad-validation-1218", "mrqa_squad-validation-1226", "mrqa_squad-validation-1253", "mrqa_squad-validation-1277", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1367", "mrqa_squad-validation-1391", "mrqa_squad-validation-1411", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1530", "mrqa_squad-validation-1531", "mrqa_squad-validation-1539", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1664", "mrqa_squad-validation-1690", "mrqa_squad-validation-1695", "mrqa_squad-validation-1720", "mrqa_squad-validation-173", "mrqa_squad-validation-174", "mrqa_squad-validation-1766", "mrqa_squad-validation-1794", "mrqa_squad-validation-1824", "mrqa_squad-validation-1872", "mrqa_squad-validation-1877", "mrqa_squad-validation-1908", "mrqa_squad-validation-1913", "mrqa_squad-validation-1980", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2060", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2321", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2382", "mrqa_squad-validation-2402", "mrqa_squad-validation-2408", "mrqa_squad-validation-2439", "mrqa_squad-validation-2475", "mrqa_squad-validation-2478", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2675", "mrqa_squad-validation-2704", "mrqa_squad-validation-2724", "mrqa_squad-validation-2807", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-288", "mrqa_squad-validation-2881", "mrqa_squad-validation-2955", "mrqa_squad-validation-2961", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3255", "mrqa_squad-validation-3288", "mrqa_squad-validation-3355", "mrqa_squad-validation-3378", "mrqa_squad-validation-3388", "mrqa_squad-validation-3400", "mrqa_squad-validation-3447", "mrqa_squad-validation-3457", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3628", "mrqa_squad-validation-3657", "mrqa_squad-validation-3771", "mrqa_squad-validation-3799", "mrqa_squad-validation-38", "mrqa_squad-validation-3806", "mrqa_squad-validation-3813", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-3915", "mrqa_squad-validation-3916", "mrqa_squad-validation-3942", "mrqa_squad-validation-3971", "mrqa_squad-validation-3986", "mrqa_squad-validation-405", "mrqa_squad-validation-4054", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-409", "mrqa_squad-validation-4100", "mrqa_squad-validation-4127", "mrqa_squad-validation-4137", "mrqa_squad-validation-4149", "mrqa_squad-validation-4192", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-4320", "mrqa_squad-validation-4332", "mrqa_squad-validation-437", "mrqa_squad-validation-4425", "mrqa_squad-validation-4427", "mrqa_squad-validation-4439", "mrqa_squad-validation-4475", "mrqa_squad-validation-4488", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-453", "mrqa_squad-validation-4629", "mrqa_squad-validation-4642", "mrqa_squad-validation-4658", "mrqa_squad-validation-4675", "mrqa_squad-validation-4676", "mrqa_squad-validation-4701", "mrqa_squad-validation-4711", "mrqa_squad-validation-477", "mrqa_squad-validation-477", "mrqa_squad-validation-4795", "mrqa_squad-validation-4801", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4902", "mrqa_squad-validation-4930", "mrqa_squad-validation-5013", "mrqa_squad-validation-503", "mrqa_squad-validation-5063", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5208", "mrqa_squad-validation-5226", "mrqa_squad-validation-5234", "mrqa_squad-validation-5249", "mrqa_squad-validation-5260", "mrqa_squad-validation-5300", "mrqa_squad-validation-5320", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-545", "mrqa_squad-validation-551", "mrqa_squad-validation-5531", "mrqa_squad-validation-5535", "mrqa_squad-validation-5550", "mrqa_squad-validation-5597", "mrqa_squad-validation-5620", "mrqa_squad-validation-5631", "mrqa_squad-validation-5715", "mrqa_squad-validation-5721", "mrqa_squad-validation-5721", "mrqa_squad-validation-5736", "mrqa_squad-validation-5891", "mrqa_squad-validation-5908", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-5991", "mrqa_squad-validation-6059", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6156", "mrqa_squad-validation-6166", "mrqa_squad-validation-6191", "mrqa_squad-validation-6293", "mrqa_squad-validation-6326", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6404", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6471", "mrqa_squad-validation-6473", "mrqa_squad-validation-6610", "mrqa_squad-validation-6614", "mrqa_squad-validation-6639", "mrqa_squad-validation-6644", "mrqa_squad-validation-6650", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6753", "mrqa_squad-validation-6759", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6889", "mrqa_squad-validation-6896", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6978", "mrqa_squad-validation-6988", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-7086", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7189", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7288", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7446", "mrqa_squad-validation-7466", "mrqa_squad-validation-7490", "mrqa_squad-validation-7502", "mrqa_squad-validation-7504", "mrqa_squad-validation-7508", "mrqa_squad-validation-7526", "mrqa_squad-validation-754", "mrqa_squad-validation-7563", "mrqa_squad-validation-7609", "mrqa_squad-validation-7653", "mrqa_squad-validation-7707", "mrqa_squad-validation-7718", "mrqa_squad-validation-7726", "mrqa_squad-validation-7727", "mrqa_squad-validation-7731", "mrqa_squad-validation-7744", "mrqa_squad-validation-7751", "mrqa_squad-validation-7767", "mrqa_squad-validation-778", "mrqa_squad-validation-7789", "mrqa_squad-validation-7813", "mrqa_squad-validation-7926", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7954", "mrqa_squad-validation-7997", "mrqa_squad-validation-8107", "mrqa_squad-validation-811", "mrqa_squad-validation-8154", "mrqa_squad-validation-8204", "mrqa_squad-validation-8212", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8269", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8350", "mrqa_squad-validation-8409", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8575", "mrqa_squad-validation-8576", "mrqa_squad-validation-8617", "mrqa_squad-validation-8643", "mrqa_squad-validation-8649", "mrqa_squad-validation-8658", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-9038", "mrqa_squad-validation-9103", "mrqa_squad-validation-916", "mrqa_squad-validation-9189", "mrqa_squad-validation-930", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-934", "mrqa_squad-validation-9376", "mrqa_squad-validation-9378", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9476", "mrqa_squad-validation-9488", "mrqa_squad-validation-9498", "mrqa_squad-validation-9505", "mrqa_squad-validation-9525", "mrqa_squad-validation-9575", "mrqa_squad-validation-9590", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9717", "mrqa_squad-validation-9731", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9787", "mrqa_squad-validation-9810", "mrqa_squad-validation-9853", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_squad-validation-9920", "mrqa_squad-validation-9962", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5984", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-770", "mrqa_triviaqa-validation-7735", "mrqa_triviaqa-validation-7775"], "OKR": 0.904296875, "KG": 0.44375, "before_eval_results": {"predictions": ["Article 5", "increased by 0.3 to 0.6 \u00b0C", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Meredith Vieira", "extremely difficult, if not impossible", "most of the items in the collection, unless those were newly accessioned into the collection", "5,000 years of art", "Utopia", "hydrogen and helium", "Central Pacific Railroad", "equality", "Jan Andrzej Menich", "private conferences", "The View and The Chew", "Louis Adamic", "Roger Goodell", "lower bounds", "over 90", "the same message routing methodology as developed by Baran", "as soon as they enter into force", "Sakya", "1688\u20131692", "December 1922", "The United States is the only Western country currently applying the death penalty, one of 57 countries worldwide applying it, and was the first to develop lethal injection as a method of execution", "a major fall in stock prices", "Claims adjuster", "Valmiki", "Grand Inquisition", "a million", "disputes between two or more states", "soybean", "the shoulder", "Orson Welles", "Razor", "(B. b. bison)", "bitter almond", "show business", "Gloucestershire", "Wilhelmus Simon Petrus Fortuijn", "Mr. Tumnus", "Augusta Ada King-Noel, Countess of Lovelace", "703", "Mauritian", "Lee Sun-mi", "gehanna", "Hawaii", "Bill Gates", "56", "Jason Chaffetz", "gehland's", "Seminole", "girls around 11 or 12", "last week", "sculptures", "london", "geh Harrington", "King Louis XVI", "Siddhartha", "(J.A. Cuddon", "Vulpecula", "cosmopolitan", "Anthony trollope", "london", "Silence of the Lambs"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6040860615079364}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 0.6666666666666666, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.12500000000000003, 1.0, 0.2857142857142857, 0.5, 0.2222222222222222, 0.8, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6811", "mrqa_squad-validation-5505", "mrqa_squad-validation-5494", "mrqa_squad-validation-3667", "mrqa_squad-validation-4588", "mrqa_squad-validation-1568", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-8092", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-2213", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4133", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-7853", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3937"], "SR": 0.515625, "CSR": 0.6363636363636364, "EFR": 1.0, "Overall": 0.7531321022727273}, {"timecode": 11, "before_eval_results": {"predictions": ["the expulsion of the Acadians", "The E. W. Scripps Company", "1974", "the Uighurs", "15", "Invocavit Sermons", "1899", "microorganisms", "Ealy", "Von Miller", "10", "23.9%", "cattle and citrus", "to establish, equip, manage and maintain national and public libraries in the country", "In the 1060s", "1882", "William the Lion", "external combustion engines", "Ten", "a proper legal basis", "a Islamic shrine", "the utopian novels of H.G. Wells", "Albert Einstein", "drizzle, rain, sleet, snow, graupel and hail", "Richard Wright and non-lexical vocals by Clare Torry", "September 8, 2017", "Jehnna ( Olivia d'Abo )", "the 1920s", "Haiti", "Jackie Robinson", "James Brown", "a double dip recession", "Darby and Joan", "Bronx Mowgli", "the Basque separatist organization, the ETA (Euskadi Ta Askatasuna)", "(Jules) Verne", "the United Kingdom", "the Cheshire Phoenix", "Gweilo", "Nick on Sunset theater in Hollywood", "various", "2006", "1943", "The Design Inference", "1836", "John Lennon and George Harrison", "it would use it against Israel", "Osama bin Laden's sons", "the inspector-general", "seven", "through Greece, the birthplace of the Olympics, before being transported to Canada", "Jean Van de Velde", "a drug test after taking a medicine that contained the banned substance cortisone", "a barn", "(Ray Sahelian, M.D.", "Like a Rock", "Indiana", "Canada", "Edgar Rice Burroughs", "Frank Lloyd Wright", "a new class of ocean-going Navy tug, T-AGOS", "Russia", "James Lillywhite, Alfred Shaw and Arthur Shrewsbury", "Lesley Garrett"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6456015414235523}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.9565217391304348, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.16666666666666669, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2705", "mrqa_squad-validation-8498", "mrqa_squad-validation-8261", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-8937", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-2307", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-5891", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-1654", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-13198", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2684"], "SR": 0.546875, "CSR": 0.62890625, "EFR": 1.0, "Overall": 0.7516406250000001}, {"timecode": 12, "before_eval_results": {"predictions": ["The Knowledge School", "OneDrive", "\"Into your hand I commit my spirit; you have redeemed me, O Lord, faithful God\" (Ps. 31:5)", "September 5, 1985", "62", "Jean Ribault", "James Bryant Conant", "journalism", "Western", "a prasinophyte", "electromagnetic", "in the kingdom", "Chicago Bears", "observer", "requiring his arrest", "giving her brother Polynices a proper burial", "26-yard line", "against governmental entities", "Iowa ( 36.6 % )", "Justin Timberlake", "a harp of gold", "Curtis Armstrong", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "due to Parker's pregnancy", "Gaget, Gauthier & Co. workshop", "Afonso IV of Portugal", "Julie Gonzalo", "ostrich", "Jean Alexander", "John Enoch Powell", "the British Royal Air Force", "Alfred Wainwright", "McDonnell Douglas", "3000m", "John Masefield", "1982", "The Prodigy", "\u00c6thelstan", "Sean Yseult", "the Salzburg Festival", "Les Miles", "World War I", "County of York", "Charles Quinton Murphy", "Ais", "Haiti", "could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Wednesday", "of sumo", "Elena Kagan", "\"Two horses initially collapsed, and as vets and team officials scrambled to revive them, five others became dizzy,", "could see the lifeboat where pirates have been holding Capt. Richard Phillips", "Araceli Valencia", "Sabina Guzzanti", "Margot Kidder", "students", "Wild Bill Hickok", "Mario Puzo", "New Zealand", "baking soda", "bashing", "Peter Shaffer", "Rudyard Kipling", "Bangladesh"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6514574515396884}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.5, 0.46153846153846156, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.12121212121212123, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2550", "mrqa_squad-validation-249", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-1925", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-6151", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-59", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-633", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-1146"], "SR": 0.59375, "CSR": 0.6262019230769231, "EFR": 0.9615384615384616, "Overall": 0.7434074519230769}, {"timecode": 13, "before_eval_results": {"predictions": ["civil disobedience", "can produce both eggs and sperm at the same time.", "Necessity-based", "Tesla coil", "2008", "spinat", "Capability deprivation", "San Jose State", "1954", "Rhenus", "Xbox One", "the bishop has read the appointments at the session of the Annual Conference", "1996", "the Italian Plague of 1629\u20131631", "Northumbria University", "calcitriol", "2015", "Merrimen", "Toby Kebbell", "Robin", "President Andrew Johnson", "2005", "November 17, 1800", "1975", "Massachusetts", "enemy", "16.5 feet", "grey geese", "yellow", "second", "Nigel Short", "judoka", "Manchester", "leprosy", "john Bloor", "Ouse and Foss", "a Christian church", "Government of Ireland", "Mathieu Kassovitz", "Moonstruck", "Slaughterhouse-Five", "three", "December 24, 1973", "As of the 2010 census", "nearly $162 billion in war funding", "almost 100", "his mother", "second", "Jesus Mendez, 16,", "john McCain", "the composer of \"Phantom of the Opera\" and \"Cats\"", "strife in Somalia", "105-year", "fibula", "tinne", "Newton", "Trinity", "Brigham Young", "China", "the Roman Catholic Archdiocese of Los Angeles", "Korea", "Sideways", "Ladin", "Republican"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6122159090909092}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6735", "mrqa_squad-validation-4645", "mrqa_squad-validation-10449", "mrqa_squad-validation-9247", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-6585", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-5153", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1811", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2453", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-6636", "mrqa_searchqa-validation-12056", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-5808"], "SR": 0.546875, "CSR": 0.6205357142857143, "EFR": 1.0, "Overall": 0.7499665178571429}, {"timecode": 14, "before_eval_results": {"predictions": ["as far back as the early Cambrian, about 515 million years ago", "tentilla", "problem instance", "Knights Templar", "Algeria", "Edinburgh Pentlands", "828,000", "1206", "thylakoid", "WLS", "about half of Naples' 300,000 inhabitants", "Scotland", "Algeria", "Cadeby", "colonizing empires", "9 February 2018", "charbagh", "until the 1960s", "22 November 1914", "Skat", "Governor Al Smith", "During Hanna's recovery masquerade celebration", "John Young", "Reuben Kincaid", "lighter fluid", "\"Mr Loophole\"", "red deer", "mudflats and salt marshes", "1881", "NASCAR", "United Nations", "Roger Black", "Nova Scotia", "mule", "Tampa", "Mineola", "Sofia the First", "Hong Kong", "relationship with Apple co-founder Steve Jobs", "Hong Kong, New York City", "Chief of the Operations Staff", "Massapequa", "Kansas City crime family", "iPods", "death squad killings", "Kim Il Sung", "Dominic Adiyiah", "Garth Brooks", "1,700 year old Roman mosaic", "nuclear weapon", "five", "it has not intercepted any", "John Bunyan", "Steven Spielberg", "riddle", "Diogenes", "\"blood diamonds\"", "Robinson Crusoe", "National Gallery of Art", "Orlov diamond", "Fairness Doctrine", "Michoacan", "the abduction of minors", "an Italian and six Africans"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6489154942279942}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.7272727272727272, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.36363636363636365, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4474", "mrqa_squad-validation-9480", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-7754", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-5643", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-9954", "mrqa_newsqa-validation-2821"], "SR": 0.578125, "CSR": 0.6177083333333333, "EFR": 0.8888888888888888, "Overall": 0.7271788194444444}, {"timecode": 15, "before_eval_results": {"predictions": ["Since the 1980s", "Fresno", "net", "energize electrons", "if they arrest fully informed jury leafleters, the leaflets will have to be given to the leafleter's own jury as evidence.", "In the 20th century", "Lower Lorraine", "wave speeds", "three", "steady stream", "a system of many biological structures and processes within an organism that protects against disease.", "1606", "1951", "humber", "the Central Intelligence Agency", "pale ale", "George Bernard Shaw", "Clarence Thomas", "Washington Irving", "\"Rugrats\"", "Brooklyn", "David Lee Roth", "Abraham Lincoln", "Under normal conditions", "204,408 in 2013", "the same number of electron shells", "1986", "instructions", "in the Gospel of Matthew in the middle of the Sermon on the Mount", "the United States of America", "Staci Keanan", "egg through parthenogenesis", "Donna", "dice", "Elizabeth Taylor", "Western Australia", "Sergeant Claude Snudge", "thePackers", "perfume", "1882", "humberic", "the United States", "Dick Turpin", "Thomas Shaw", "Polo Grounds", "early 20th-century Europe.", "2,664", "Anna Clyne", "Caesars Entertainment Corporation", "13 May 2018", "the Maldives", "Sam the Sham", "Lalit", "elephants, and only a handful of media members are able to visit each year, in an effort to make the animals' lives as natural as possible.", "15", "Prince Harry.", "Kenneth Cole", "anaphylaxis,", "10 to 15 percent", "was back on the set at \"E! News\" on Tuesday.\"", "accused the charity of kidnapping the children and concealing their identities.", "Isaac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3,", "in the Angeles National Forest", "Jaime Andrade"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6201765682234432}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10504", "mrqa_squad-validation-3602", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-3026", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-1193", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-5920", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-4180", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-5502", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-1016", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-1286", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-3621"], "SR": 0.53125, "CSR": 0.6123046875, "EFR": 1.0, "Overall": 0.7483203125}, {"timecode": 16, "before_eval_results": {"predictions": ["Rhin", "4 vector equations", "chlorophyll", "neuronal dendrites", "evenly round the body", "Switzerland", "BSkyB", "British", "James E. Webb", "nitroaereus", "tidal delta", "DC traction motor", "Bangkok", "a (royal fork)", "(Emma Thompson)", "eggs", "Dren groan", "mass libraries", "a bath of potassium silver cyanide", "Cleveland", "Delaware", "The ship", "alveoli", "a kiss - off", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "moral", "Kanawha River", "a judge who lacks compassion is repeatedly approached by a poor widow", "Jojo and Ringo Garza", "Mohammad Reza Pahlavi", "Kenya and Scotland", "the utopian novels of H.G. Wells", "saccharides", "the Bank of England", "Brussels", "david hemery", "massachusetts", "(Ennio) Morricone", "Midnight Cowboy", "E in A-Level art", "The first summit was in 1975", "Vienna", "Rice University", "Mexico", "a fictional world", "March 23, 2017", "postmodern schools", "five", "2005", "the Salzburg Festival", "three different covers", "Saint Louis County", "200", "massachusetts, and they were featured on one side of a New Zealand five-cent coin that was phased out in 2006.", "more than 5,600", "the California Highway Patrol.", "a 57-year old male", "Citizens", "\"Empire of the Sun\"", "A New York appeals court", "Jacob", "the 3rd District of Utah", "Herbert Hoover", "two reservoirs in the eastern Catskill Mountains"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4942299836601307}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.35294117647058826, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10416", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-3152", "mrqa_searchqa-validation-2874", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-11438", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-7012", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-4744", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-4263", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-4859", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-3174"], "SR": 0.453125, "CSR": 0.6029411764705883, "EFR": 1.0, "Overall": 0.7464476102941177}, {"timecode": 17, "before_eval_results": {"predictions": ["teacher's colleges", "SAP Center in San Jose", "ash tree", "$5 million", "the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal", "mercuric oxide (HgO)", "the Pacific", "Esel (\"Donkey\")", "a modern canalized section", "John Fox", "Sava Kosanovi\u0107", "the Twin Towers", "Alaska", "Andorra, Portugal and Spain", "the nineteenth century", "klezmer", "Tiffany", "a locking pin", "Edith Wharton", "Norman Bates", "Indira Gandhi", "property", "brass band parades", "the base of the right ventricle", "Indian Standard Time", "Yugoslavia", "Road / Track", "1920", "the need to repent in time", "the intersection of Del Monte Blvd and Esplanade Street", "used as a pH indicator", "William Chatterton Dix", "fish", "slovakia", "The History Boys", "the Gambia", "the Kentucky Derby", "the Netherlands", "Robert Stroud", "William Lamb", "Malcolm Bradbury", "Pat Cash", "chalk quarry", "Jan Kazimierz", "200,167", "American", "The Social Network\" (2010)", "2001", "Abbey Road", "terrorist activity", "Dan Brandon Bilzerian", "YouTube", "jobs", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "\"peregruzka\"", "Nicole", "breast cancer.", "A Lion Among Men", "4 meters (13 feet) high", "it is not just $3 billion of new money into the economy", "1.2 million", "Thabo Mbeki", "George I", "wake"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6919977744885834}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.9411764705882353, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.2222222222222222, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-457", "mrqa_squad-validation-4634", "mrqa_squad-validation-3300", "mrqa_squad-validation-9190", "mrqa_searchqa-validation-8891", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-9094", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-6384", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3931", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1380"], "SR": 0.578125, "CSR": 0.6015625, "EFR": 0.9629629629629629, "Overall": 0.7387644675925926}, {"timecode": 18, "before_eval_results": {"predictions": ["if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional.", "ownership of private industries", "tentilla", "Levi's Stadium", "a drug treatment for an individual", "Charlesfort", "2000", "five", "24 September 2007", "Infrastructure", "various newspaper reporters, including Sylvia F. Porter in a column for the May 4, 1951, edition of the New York Post", "Missouri River", "the 1960s.", "a bow bridge", "more than 80 tank\u014dbon volumes", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Gregor Mendel", "United Nations Peacekeeping Operations", "Certificate of Release or Discharge from Active Duty", "Latitude", "20 locations all within the Pittsburgh metropolitan area", "Hyundai", "viscount austria", "the United States and Mexico", "congregregation", "viscount j john barbirolli", "Tahrir Square", "formic acid", "100 years", "Paul C\u00e9zanne", "oldpatricktoe-end", "Germany", "Philip Livingston", "The Hawai\u02bbi State Senate is the upper chamber of the Hawaii State Legislature.", "DreamWorks Animation", "DI Humphrey Goodman", "London", "1926", "She received an Olivier Award for Best Actress in 2013 for her West End performance in \"The Audience\", in which she also portrayed Elizabeth II,", "Nebraska Cornhuskers women's basketball team", "Wu-Tang Clan", "$19 million", "two", "Caylee, who was 2 when she vanished last summer.", "Shanghai", "11", "mental health treatment", "from Amsterdam, in the Netherlands, to Ankara, Turkey", "October 19", "an open window that fits neatly around him", "maintain an \"aesthetic environment\" and ensure public safety,", "Entertainment Tonight", "a cricket", "R2-D2", "The Press", "dampers", "Samuel Burl \"Sam\" Kinison", "The Vandellas", "Sisyphus", "Veep", "You have made my life complete", "Elizabeth Taylor", "fish", "a band of European sailors"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6939934197746698}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09999999999999999, 1.0, 1.0, 1.0, 0.7499999999999999, 0.22222222222222224, 1.0, 0.0, 0.46153846153846156, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6282", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3721", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-712", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-3773", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-3858", "mrqa_searchqa-validation-13223", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-10860", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-7646", "mrqa_triviaqa-validation-3215"], "SR": 0.59375, "CSR": 0.6011513157894737, "EFR": 0.9615384615384616, "Overall": 0.7383973304655871}, {"timecode": 19, "before_eval_results": {"predictions": ["Brandon McManus", "the Compromise of 1850", "1884", "Seven Stories", "1332", "Morgan", "Space", "neoclassical", "12.5", "Florence", "Canada goose", "the American Civil War", "King Edward III", "Blossom", "Star Trek", "the circulatory system", "Yitzhak Rabin", "The Letters of Lord Nelson", "four", "Providence", "around the world", "landowner", "July 10, 2017", "at birth", "Chris Rea", "Anna Faris", "outside cultivated areas", "the United States Committee for UNICEF", "senior enlisted sailor", "on the posterior aspect", "Ledger", "Mexico", "Funchal", "geena Davis", "Peter MacTaggart", "a web-based teaching aid", "the double-headed eagle", "a board that has lines and pads that connect various points together", "Gaston Leroux", "Whitney Elizabeth Houston", "seven", "Scotland", "local South Australian and Australian produced content", "basketball", "Animorphs", "MMA", "Vic Chesnutt", "William Douglas", "It opened on April 2, 2007 within the park's Tomorrowland section, where it replaced the Circle-Vision attraction The Timekeeper", "2013", "poetry, theater, art, music, the media, and books", "east\u2013west United States highway that runs for 640 mi in the Midwestern United States", "Native American tribes", "CNN's best ten golf movies ever made", "keeping our children from just this type of public exposure.", "onto the college campus.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "in Yemen", "what caused the collapse of the building which contained the city's historical archives,", "American icon's", "in Thessaloniki and Athens", "the use of torture and indefinite detention", "at least 25", "Fareed Zakaria"], "metric_results": {"EM": 0.5, "QA-F1": 0.5882421398046398}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.22222222222222224, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-969", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-15149", "mrqa_searchqa-validation-6804", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-4365", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-1838", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-796", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3891"], "SR": 0.5, "CSR": 0.59609375, "EFR": 0.9375, "Overall": 0.7325781250000001}, {"timecode": 20, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4685", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5891", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-97", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-13987", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14795", "mrqa_searchqa-validation-14908", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15552", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2996", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-410", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-4551", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5169", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-9748", "mrqa_searchqa-validation-9818", "mrqa_squad-validation-10087", "mrqa_squad-validation-10102", "mrqa_squad-validation-10103", "mrqa_squad-validation-10192", "mrqa_squad-validation-1021", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1176", "mrqa_squad-validation-1277", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1410", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1539", "mrqa_squad-validation-1577", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1688", "mrqa_squad-validation-1766", "mrqa_squad-validation-1767", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2382", "mrqa_squad-validation-2408", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2819", "mrqa_squad-validation-2854", "mrqa_squad-validation-2955", "mrqa_squad-validation-2956", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3447", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3628", "mrqa_squad-validation-3667", "mrqa_squad-validation-3806", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4054", "mrqa_squad-validation-4063", "mrqa_squad-validation-4074", "mrqa_squad-validation-409", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-4173", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-437", "mrqa_squad-validation-4439", "mrqa_squad-validation-453", "mrqa_squad-validation-457", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4642", "mrqa_squad-validation-4676", "mrqa_squad-validation-4772", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-49", "mrqa_squad-validation-4954", "mrqa_squad-validation-4957", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5211", "mrqa_squad-validation-5229", "mrqa_squad-validation-526", "mrqa_squad-validation-5272", "mrqa_squad-validation-5477", "mrqa_squad-validation-5492", "mrqa_squad-validation-5505", "mrqa_squad-validation-551", "mrqa_squad-validation-5550", "mrqa_squad-validation-5592", "mrqa_squad-validation-5631", "mrqa_squad-validation-5721", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6060", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6231", "mrqa_squad-validation-6254", "mrqa_squad-validation-6282", "mrqa_squad-validation-6404", "mrqa_squad-validation-6471", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6564", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6695", "mrqa_squad-validation-6750", "mrqa_squad-validation-6753", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-704", "mrqa_squad-validation-7086", "mrqa_squad-validation-7090", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7273", "mrqa_squad-validation-7288", "mrqa_squad-validation-7322", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7514", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7958", "mrqa_squad-validation-7988", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-823", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8342", "mrqa_squad-validation-8352", "mrqa_squad-validation-839", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8600", "mrqa_squad-validation-8643", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9365", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9643", "mrqa_squad-validation-9675", "mrqa_squad-validation-9680", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9859", "mrqa_squad-validation-9869", "mrqa_squad-validation-9920", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-2060", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-4634", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7337", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-839"], "OKR": 0.857421875, "KG": 0.446875, "before_eval_results": {"predictions": ["2004", "extremely old", "a liquid", "greater equality but not per capita income", "green algal derived chloroplast", "3.55 inches", "the Troika Design Group", "an Islamic rebellion against an allied Marxist regime in the Afghan Civil War", "a second lieutenant", "The Rolling Stones", "\"House\"", "toga", "a blow", "the Gretzky brothers", "C. S. Lewis", "furlong", "a baby goat", "\"Dejection: An Ode\"", "Ringo Starr", "Jenny and Eric", "parthenogenesis", "The Cornett family", "Zhu Yuanzhang", "Bob Dylan", "Sam Waterston", "a scythe", "the freezing point", "Eddie Murphy", "the band's logo in gold lettering over black sleeve", "The 1700 Cascadia earthquake", "\"Frenchie\"", "Manchester United", "people who live and work in rural areas and everything associated with that way of life", "kendo", "St. Ives", "300", "Toll House", "1951", "the Federal Reserve", "Marlon Brando", "Neighbours", "England", "Christianity Today", "five", "3,384,569", "Steve Coogan", "\"Marcella\"", "25", "Nashville", "2006", "Chris Anderson", "the Town of North Hempstead, Nassau County, New York, United States", "I, the chief executive officer, the one on the very top", "100", "the Dalai Lama's", "an unprecedented wave of buying amid the elections.", "Videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Body Tap", "general astonishment", "Saturday", "whites", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "The Real Housewives of Atlanta", "Renoir\u00b4s"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6278166812673687}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.5454545454545454, 0.6666666666666666, 0.5, 0.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 0.9565217391304348, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3689", "mrqa_squad-validation-9694", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-16866", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-7549", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-1038", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3605", "mrqa_triviaqa-validation-1423"], "SR": 0.484375, "CSR": 0.5907738095238095, "EFR": 0.9696969696969697, "Overall": 0.7245160308441558}, {"timecode": 21, "before_eval_results": {"predictions": ["non-violent", "$2 million", "specialised education and training", "Middle Miocene", "debased", "Robert Maynard Hutchins", "five", "Mark Anthony \"Baz\" Luhrmann", "Christine MacIntyre", "Michele Bachmann", "French", "beer and soft drinks", "Sim Theme Park", "Everything Is wrong", "Congo River", "the fictional city of Quahog, Rhode Island", "\"Grimjack\" (from First Comics) and \"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "December 5, 1991", "the efferent nerves that directly innervate muscles", "Mushnik", "stable, non-radioactive rubidium - 85", "Nancy Jean Cartwright", "seven", "the Indian Ocean", "Santa Fe, New Mexico, USA", "May 2017", "$19.8 trillion or about 106 % of the previous 12 months of GDP", "American musician George Harrison's statement of personal and artistic freedom from the Beatles", "Evan Spiliotopoulos", "Alexei Kosygin", "Saudi Arabia", "Ed Woodward", "Llanberis", "cyclops", "tobacco or alcohol addiction", "Oklahoma", "John Alec Entwistle", "Anna Mae Bullock", "Haiti", "Madonna", "2050,", "Graeme Smith", "three", "not grant full health-care coverage,", "The meter reader", "prostate cancer,", "Haiti", "A witness", "246", "the creation of a long-term plan to help Haiti recover from the devastating effects of the earthquake and Argentina's conflict with Great Britain", "NATO's Membership Action Plan, or MAP,", "the Way of St. James", "Wu-Tang Clan", "Coral Reef", "chain reaction", "Jean Valjean", "Botswana", "The Treasure of the Sierra Madre", "the daughter of Nokomis", "Moscow", "The New York Tribune", "Olympia", "present - day southeastern Texas", "The Republic of Tecala"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6556776915613123}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7272727272727272, 0.4, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2910", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-7685", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2885", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-8781", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-14446", "mrqa_naturalquestions-validation-9327"], "SR": 0.546875, "CSR": 0.5887784090909092, "EFR": 1.0, "Overall": 0.7301775568181819}, {"timecode": 22, "before_eval_results": {"predictions": ["three", "(currently Federica Mogherini)", "18 of 26 songs", "a great deal of utility", "the Scottish Parliament", "Maling company", "Johnny Cash, Waylon Jennings, Willie Kristofferson", "Humphrey Goodman", "ITV", "31 July 1975", "Edmonton, Alberta", "ONTV", "The Blackpool Gazette", "in 1877", "Tom Jones", "The Killer", "Kew Gardens", "\"Sully\" Sullenberger III", "Walter Brennan", "the 2010 World Series", "Freddie Highmore", "the end of 1066", "American colonies, then at war with the Kingdom of Great Britain, regarded themselves as thirteen independent sovereign states, no longer under British rule", "endocytosis", "Wales and 21st most common in England", "unknown origin", "the semilunar pulmonary valve", "30 October 1918", "360", "Addis Ababa", "t Tigger", "Persia", "\"nyah nyah,", "beans, onion, pepper, oil and spices", "Alberich", "david mulaitis", "Jeffrey Archer", "dinghy", "Montezuma", "Secretary of State", "job training", "the murders of his father and brother", "Pakistani Prime Minister Benazir Bhutto", "an Italian and six Africans", "in southwestern Mexico,", "Another high tide", "Camorra has been blamed for about 60 killings this year in Naples and its surrounding county.", "1-0", "the release of the four men", "November 1", "the Sodra nongovernmental organization,", "Chen Lu", "cow's milk", "physician", "whales", "Arthur Pitney", "ear", "John F. Kennedy", "British rock band", "Kilkenny cats", "Aleph", "shiatsu", "Daytona Pole Award winners", "BC Jean and Toby Gad"], "metric_results": {"EM": 0.5, "QA-F1": 0.5835500437062937}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6153846153846153, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4295", "mrqa_squad-validation-2386", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-509", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-12153"], "SR": 0.5, "CSR": 0.5849184782608696, "EFR": 0.9375, "Overall": 0.7169055706521739}, {"timecode": 23, "before_eval_results": {"predictions": ["antigens", "the courts of member states and the Court of Justice of the European Union", "sold two years later to satisfy a debt", "pastors", "south Wales", "\u00dcberseering BV v Nordic Construction GmbH", "9", "Graham Payn", "Democratic", "Charles L. Clifford", "Ludwig van Beethoven", "Jacking", "Japan", "Viscount Cranborne", "Kew, Melbourne, Australia", "Kim Bauer", "Citizens for a Sound Economy", "Gian Carlo Menotti", "in the 1820s", "The genome", "an address bar", "country", "1998", "Lex Luger and Rick Rude", "the biblical name of a Canaanite god associated with child sacrifice", "Presley Smith", "foreign investors", "Utah, Arizona, Wyoming, and Oroville, California", "Aalika Sheikh", "Dublin", "George H. W. Bush", "Spain", "Margaret Thatcher", "Ascot", "Tennis", "Brussels", "us", "Uruguay", "Carousel", "Apollo", "in that Lexus, Lincoln, Infiniti ororsche you always wanted, without laying out $70,000 or $80,000 for something you're not actually going to live in.", "the storm, five caused by falling trees, authorities said.", "Karen Floyd", "her death in the Holmby Hills, California, mansion he rented.", "Manny Pacquiao returned home to a hero's welcome in his native Philippines on Friday after wresting the WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.", "for security reasons and not because of their faith.", "in the form of tweets that alternated between raw descriptions and expressions of hope.\"", "$81,88010", "Ryder Russell", "and his attorney David Wymore.", "armed members of the radical Islamist group Jund Ansar Allah surround a group representative in Rafah on Friday.", "Sweden", "the King's Men", "Free Morrison Beloved Ghost Essays and Papers", "Canada", "George Orwell", "the Lincoln cent", "Princess Marie", "Pillsbury", "russia", "Northwestern University", "Angelina Jolie", "Sergei Rachmaninoff", "adrenaline and Stress / Fitness / Body"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6152214755730381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.15384615384615383, 0.1875, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-1406", "mrqa_hotpotqa-validation-5136", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-4047", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7265", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-2631", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-2736", "mrqa_searchqa-validation-9995", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-12618", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-2833"], "SR": 0.546875, "CSR": 0.5833333333333333, "EFR": 1.0, "Overall": 0.7290885416666666}, {"timecode": 24, "before_eval_results": {"predictions": ["38", "Inertia", "Edmonton, Canada", "Islam", "conservative Muslims", "Field Marshal Lord Gort", "north", "Ford Island", "Europe", "Amber Heard", "Matthieu Vaxivi\u00e8re", "the Rose Garden", "the House of Hohenstaufen", "Austrian Volksbanks", "2011", "Terry the Tomboy", "the NXT Tag Team Championship", "the Catholic Monarchs of Castile and Aragon", "Kelly Osbourne", "the right hand", "1804", "Hans Zimmer", "Lady Gaga", "James Hutton", "1901", "Games", "a god of the Ammonites", "1959", "Rose-Marie", "emilia", "The Ten Commandments", "Australia", "Kraft's", "Adolphe Adam", "Brooklyn", "Paris", "squall", "Today", "Lady Gaga", "the Joint Chiefs of Staff,", "Quebec", "an upper respiratory infection,", "ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "New York Times columnist David Brooks", "10 to 15 percent", "al-Shabaab", "the Oaxacan countryside of southern Mexico", "Collier County Sheriff Kevin Rambosk", "The European Commission", "Amanda Knox's", "jack Sprat", "Hollywood Canteen", "the Arabian Peninsula", "will lower the average life expectancy at birth", "Heathrow", "the THX surround sound system", "comedy series", "Everest", "a junkyard dog", "Napoleon", "the olfactory nerve", "Maine", "Ben Watson's", "Sapporo"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5653566919191919}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10328", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-4696", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-3050", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-8476", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-6214", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3213", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-7147", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-5890"], "SR": 0.484375, "CSR": 0.579375, "EFR": 1.0, "Overall": 0.728296875}, {"timecode": 25, "before_eval_results": {"predictions": ["Oirads", "electron microscopy", "a destination address, source address, and port numbers", "a lower bound", "We are beggars,", "South American country", "JackScanlon", "the coast of Guant\u00e1namo Bay in Cuba", "Western Australia", "non-ferrous", "the Veterans Committee", "1978", "Pakistan", "the National League ( NL ) champion Chicago Cubs", "94 by 50 feet", "Joe Spano", "autu", "Steve Miller Band", "a bacteria", "Aron Ralston", "watson", "John Poulson", "a giant warrior", "foods", "the Penguin", "Israel", "Southwest Airlines", "james hogg", "the People's Republic of China", "the International Conference on LGBT Human Rights", "Premier Division", "Terry Malloy", "Weare", "Rigel VII", "M Rookie Blaylock", "1988", "1959", "40 million", "Leonard Cohen", "President Bush", "Empire of the Sun", "Afghan lawmakers", "these cars are \"totaled,\"", "some of the best stunt ever pulled off", "a fair and independent manner and ratify successful efforts.", "five", "the two bodies out of the plant, which makes Slim Jim food products.", "9-week-old", "Australian officials", "a message that fight against terror will respect America's values.", "(Jack) London", "Philadelphia", "NYPD Shield", "chinook", "fog", "glass", "Jane Eyre", "a year", "Catholicism", "Heading south, in The Big Easy,", "Mikhail Gorbachev", "four months ago,", "Polo", "\"Rin Tin Tin: The Life and the Legend\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5588441506410255}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.625, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4677", "mrqa_squad-validation-1784", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-7353", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-2151", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1111", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-6579", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-1003"], "SR": 0.453125, "CSR": 0.5745192307692308, "EFR": 0.9714285714285714, "Overall": 0.7216114354395604}, {"timecode": 26, "before_eval_results": {"predictions": ["Disneyland", "Scandinavia", "back to an Earth ocean landing", "as soon as they enter into force,", "Charlene Holt", "25 years after the release of their first record", "the case of disputes between two or more states", "NIRA", "2010", "an epiphany song", "Atlanta", "The first feature film originally presented as a talkie was The Jazz Singer, released in October 1927", "every 23 hours, 56 minutes, and 4 seconds", "novella", "Javier Fern\u00e1ndez", "the Gospel of John", "hollywood", "Wild Thing", "Socrates", "170", "Nelson Rockefeller", "the Arizona Diamondbacks", "surf", "king of Cyprus", "Verdi", "jules Verne", "Wikipedia", "ligers", "1884", "a popular nursery rhyme", "Sully", "Thomas Christopher Ince", "quantum mechanics", "more than 26,000", "The entity", "John de Mol Jr.", "KBS2", "on the shore", "Harold", "Port Melbourne", "Islamic militants", "Dan Brown's", "Philip Markoff,", "killed", "citizenship", "pelvis", "\"illeg illegitimate.\"", "Friday,", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Isabella", "the Kurdish militant group in Turkey", "Brad Blauser,", "leeches", "16", "borealis", "the human breast", "the Mesozoic Era", "King Arthur", "commission", "coral", "one small step", "butter", "a counting frame", "elixir"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6286003486322759}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.09523809523809525, 0.9411764705882353, 1.0, 1.0, 0.19999999999999998, 1.0, 0.10526315789473682, 0.18181818181818182, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.6666666666666666, 1.0, 0.1212121212121212, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3879", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-3770", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-1858", "mrqa_triviaqa-validation-6070", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1143", "mrqa_searchqa-validation-11394", "mrqa_searchqa-validation-3290", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-7895"], "SR": 0.546875, "CSR": 0.5734953703703703, "EFR": 0.9310344827586207, "Overall": 0.7133278456257982}, {"timecode": 27, "before_eval_results": {"predictions": ["(8.880)", "13th", "probabilistic", "Donald Davies", "two tickets to Italy", "\"should be shot,\"", "outside influences in next month's run-off election,", "images of the small girl being sexually assaulted.", "allergies in general -- both food and inhalant -- are on the rise, but no one is sure why.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "150", "look-alikes", "made some power moves", "Miami Beach, Florida,", "more than 100", "police", "Dirk Benedict", "flawed democracy", "1986", "the adrenal medulla", "July 2, 1776", "parthenogenesis", "pilgrimages to Jerusalem", "Daniel A. Dailey", "the Allies", "74", "in Nashville, Tennessee", "Colman", "Sicily", "Albania", "jean", "e pluribus unum", "Eddie Cochran", "witch trials", "Ghana", "El Hiero", "Claudius", "Dos Equis", "fish", "Telstar", "Vaisakhi List", "West Africa", "Tranquebar", "Citizens for a Sound Economy", "Tie Domi", "the Rolls of Ol\u00e9ron", "bank of China Building", "Central University of India", "dice", "activist of tenant management of public housing properties", "raised in Laurel, Mississippi", "Interstate 22", "Dr. Seuss", "a cardinal", "Olympia", "ear infections", "Jacob and Wilhelm Grimm", "Sally Field", "platinum", "apples", "William Shakespeare", "rihanna", "President Herbert Hoover", "the Whermacht"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6993303571428571}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.8571428571428571, 0.0, 0.0, 0.5333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.2, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9274", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-828", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2600", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-351", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14772", "mrqa_searchqa-validation-11551", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-12694"], "SR": 0.578125, "CSR": 0.5736607142857143, "EFR": 1.0, "Overall": 0.7271540178571428}, {"timecode": 28, "before_eval_results": {"predictions": ["the terminal interface to X.29", "cyclic photophosphorylation", "without destroying historical legitimacy", "\"The Lost Symbol\"", "Gustav's top winds weakened to 110 mph,", "President Obama", "\"Walk -- Don't Run' and \"Diamond Head.\"", "her dancing against a stripper's pole.", "2006", "to put a lid on the marking of Ashura this year.", "Satsuma, Florida,", "her son has strong values.", "17 Again", "3-2", "Cameroon", "Atlantic Ocean", "Super Bowl LII", "a head, neck, a midpiece and a tail", "danzigarh", "July 25, 2017", "December 12, 2017", "St. Louis Blues", "increased productivity, trade, and secular economic trends", "c. 1000 AD", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "off the southernmost tip of the South American mainland", "1904", "yayin", "hieron", "Rugby School", "danzig", "David Paradine", "danzig", "Les Jolies Eaux", "Macbeth", "Morse code", "a symbol of position and title", "Turkey", "judoka", "a puppy", "Hee Haw", "2012", "Coyote Ugly", "Rowan Blanchard", "Brady Haran", "Squam Lake", "Sony Music and Syco Music", "Patrick Swayze", "Salford, Lancashire", "Washington, D.C.", "Burning Man", "jean", "Harriet Tubman", "Chuck Berry", "Colorado", "60 Minutes", "The Rio Hondo", "Spain", "if he... Winged Monkeys in the Land of the West, and he did not wish to meet them again", "femur", "Machiavelli", "Heracles", "fractured bone(s)", "the uvula"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5879178113553114}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true], "QA-F1": [0.6153846153846153, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.888888888888889, 1.0, 0.8, 0.28571428571428575, 0.7692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.923076923076923, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4834", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-1670", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-2042", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-1041", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-4828", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-6598", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-15667"], "SR": 0.484375, "CSR": 0.5705818965517242, "EFR": 1.0, "Overall": 0.7265382543103448}, {"timecode": 29, "before_eval_results": {"predictions": ["Christ and His salvation", "$2.50 per AC horsepower royalty", "recant his writings", "pregnancy.", "at least 15", "Dr. Death in Germany", "Matt Kuchar and Bubba Watson", "15 years ago", "new materials -- including ultra-high-strength steel and boron", "said he gave the victims \"assurances of the church's action\" after the April 18 meeting.", "Body Tap,", "whether health care can affect families.", "2-1", "The Human Rights Watch organization", "India", "Roman Reigns", "Nationalists", "April 10, 2018", "September 2017", "September 1972", "Sergeant Himmelstoss", "Burj Khalifa", "1932", "If These Dolls Could Talk ''", "a solitary figure who is not understood by others, but is actually wise", "T.S. Eliot", "Ludacris", "a docked yacht", "Midsomer Murders", "the Andesite Line", "sulfuric and nitric acids", "colorblindness", "dutch", "the first newspaper in London", "Director General of the Security Service", "david meadowbank", "Ghana", "Jan van Eyck", "the iris of the eye", "Wake Island", "emotion poetry", "1884", "the Bolts", "Madonna", "Bonkill", "Dan Rowan", "National Socialists", "Nikita Khrushchev", "Speedway World Championship", "47,818", "1903", "a compound compound", "a compound resource Identifier", "Fergie", "Prada", "the Third Crusade", "the Grapefruit", "Terminator 3: Rise of the Machines", "Lady Sings the Blues", "the zebra mussel", "the saltire", "the Philippines", "the heptathlon", "Asiana Town building"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6125417117604618}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 0.5714285714285715, 0.7272727272727273, 0.1111111111111111, 1.0, 0.8333333333333334, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5714285714285715, 0.25, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3935", "mrqa_newsqa-validation-3130", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-3633", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-824", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-4034", "mrqa_hotpotqa-validation-1851", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-14664"], "SR": 0.515625, "CSR": 0.56875, "EFR": 1.0, "Overall": 0.726171875}, {"timecode": 30, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1683", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3378", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3516", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-5502", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2479", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2960", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4174", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-99", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-10930", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6388", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-694", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-9797", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-10449", "mrqa_squad-validation-1126", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-143", "mrqa_squad-validation-1539", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1688", "mrqa_squad-validation-1695", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2382", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2854", "mrqa_squad-validation-288", "mrqa_squad-validation-2949", "mrqa_squad-validation-2955", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3516", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4063", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-415", "mrqa_squad-validation-42", "mrqa_squad-validation-4262", "mrqa_squad-validation-4439", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4676", "mrqa_squad-validation-4801", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4951", "mrqa_squad-validation-509", "mrqa_squad-validation-5156", "mrqa_squad-validation-5190", "mrqa_squad-validation-5229", "mrqa_squad-validation-5272", "mrqa_squad-validation-5505", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-612", "mrqa_squad-validation-6221", "mrqa_squad-validation-6254", "mrqa_squad-validation-6404", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-704", "mrqa_squad-validation-7133", "mrqa_squad-validation-718", "mrqa_squad-validation-7233", "mrqa_squad-validation-7273", "mrqa_squad-validation-7322", "mrqa_squad-validation-742", "mrqa_squad-validation-7427", "mrqa_squad-validation-7490", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7731", "mrqa_squad-validation-7767", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-8212", "mrqa_squad-validation-8278", "mrqa_squad-validation-8352", "mrqa_squad-validation-85", "mrqa_squad-validation-8600", "mrqa_squad-validation-8695", "mrqa_squad-validation-8792", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-9189", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9519", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9629", "mrqa_squad-validation-9639", "mrqa_squad-validation-9675", "mrqa_squad-validation-9698", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2757", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3163", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4047", "mrqa_triviaqa-validation-4077", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4275", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-960"], "OKR": 0.85546875, "KG": 0.48125, "before_eval_results": {"predictions": ["Sierra Sky Park", "second", "Recognized Student Organizations (RSOs)", "fractured pelvis and sacrum", "Saturday", "lower house of parliament,", "drama that pulls in the crowds", "Juan Martin Del Potro.", "40", "mi undergrad alma mater, Wake Forest,", "him to step down as majority leader.", "Christopher Savoie", "Missouri.", "Mitt Romney", "2-0", "Robin", "1986", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "embryo", "Bill Pullman", "St. Mary's County", "the fovea centralis", "Julie Stichbury", "Dorothy Gale", "John Prine and Roger Cook", "11 January 1923", "2015", "fur", "lignite", "Craggy Island", "tywin", "Sodor", "Munyon", "mutt", "Cadbury", "Wharton", "chlorine", "cabinet Office Briefing Rooms", "Big Brother", "Whoopi Goldberg", "Harlow Cuadra and Joseph Kerekes", "\"D Daredevil\"", "Lundbeck", "chronological collection of critical quotations", "tong-182", "2013", "Belarus", "\"Purple Heather\"", "Marvel Comics", "stein", "Gmail, Hangouts", "the stiletto", "Dune", "Jane Eyre", "Ron Paul", "The Wall Street Journal", "assassination conspiracy", "Tahiti", "V for Victory", "Bolshoi Ballet", "Debussy", "Florida State", "Armenia", "Don Garlits"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5137784090909091}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3300", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-3087", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-1340", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6254", "mrqa_triviaqa-validation-1465", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-4267", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2966", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-3241", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5514", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-3231", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-15451"], "SR": 0.453125, "CSR": 0.5650201612903225, "EFR": 1.0, "Overall": 0.7303477822580645}, {"timecode": 31, "before_eval_results": {"predictions": ["article 30", "the Seine", "a house party in Crandon, Wisconsin", "Jaime Andrade", "South Africa", "Goa", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "246", "consumer confidence", "Caylee Anthony,", "Siri", "Megan Lynn Touma", "blue booties", "Communist Party of Nepal", "John 6 : 67 -- 71", "Nazi Germany and Fascist Italy", "George Augustus", "Andrew Lloyd Webber", "seven", "gastrocnemius", "accomplish the objectives of the organization", "Narendra Modi", "1960", "Marie Fredriksson", "Theodore Roosevelt", "Jesus Christ", "lily allenis", "9", "Radio City Music Hall", "geyser", "Battle of Camlann", "Natty Bumppo", "France", "Maxwell", "piano", "florida", "180", "jack Kennedy", "Bill Walton", "mars", "Homer Hickam, Jr.", "1901", "250cc world championship", "Kinnairdy Castle", "1999 Odisha cyclone", "EBSCO Information Services", "Sylvia Pankhurst", "Bill Clinton", "Roseann O'Donnell", "Iranian government\u2019s propaganda channel", "Bessie Coleman", "They were marooned", "Hawks", "California, New York, Texas, Florida", "coral snake", "Green Lantern", "Hawaii", "The Greatest Show on Earth", "James V, King of Scotland", "blue state", "save the best for last", "Henry James", "Matt Monro", "Vital Records Office of the states"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7080357142857143}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.28571428571428575, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.4, 1.0, 0.5714285714285715, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-960", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-6084", "mrqa_triviaqa-validation-2300", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-821", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4995", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-4660", "mrqa_searchqa-validation-6423"], "SR": 0.609375, "CSR": 0.56640625, "EFR": 1.0, "Overall": 0.7306250000000001}, {"timecode": 32, "before_eval_results": {"predictions": ["banded iron formations", "multiplication", "five starting pitchers", "AMX - 30", "Captain Jones", "in rocks and minerals", "In 1988", "New Jersey Devils of the National Hockey League ( NHL )", "Manhattan Project", "Daytona Pole Award winners", "October 29, 2015", "Norman Pritchard", "Teddy Randazzo", "Kim Basinger", "Charles Manson", "Staraya Russa", "sebills", "Hindu", "The Green Mile", "Paris", "Labrador retrievevers", "having or seeing nosebleeds or bleeding to death", "pickled peppers", "Joan-Christophe Novelli", "Johann Baptist Strauss", "Pakistan", "Kentucky Music Hall of Fame", "3730 km", "Margiana", "\"It's a Small World (After All)", "the tropical terrestrial ecoregions of the Americas and the entire South American temperate zone", "the Moselle", "Prince Sung-won", "actor", "Drowning Pool", "\"Odorama\"", "Robin David Segal", "creeks,", "Anne Frank,", "speaking out about a cause", "the river will crest Saturday about 20 feet above flood stage.", "71 percent of Americans consider China an economic threat to the United States,", "Columbia,", "At least 33", "42 prostitutes", "Sri Lanka's Tamil rebels to \"release\" civilians,", "Gulf of Aden,", "Her two Manchester, England shows have been moved from Thursday and Friday to the end of her tour", "Afghanistan's restive provinces", "you're putting your child's safety and livelihood into other hands,\"", "Cyt p450 system", "John Hersey", "Solomon", "Lord of the Rings", "Thai", "a statue", "digamma", "Rome", "Dragons", "coral", "lyrebirds", "Joseph", "Siam Center", "MGM Resorts International"], "metric_results": {"EM": 0.46875, "QA-F1": 0.516729797979798}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.1818181818181818, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10213", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-1582", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3505", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-3300", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-3059", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3736", "mrqa_searchqa-validation-12171", "mrqa_searchqa-validation-3946", "mrqa_searchqa-validation-9569", "mrqa_searchqa-validation-13044", "mrqa_searchqa-validation-16682", "mrqa_hotpotqa-validation-5090"], "SR": 0.46875, "CSR": 0.5634469696969697, "EFR": 1.0, "Overall": 0.730033143939394}, {"timecode": 33, "before_eval_results": {"predictions": ["the foot of the mast", "Business Connect", "Arctic Ocean", "- ase", "Harry Potter and the Deathly Hallows", "Daniel A. Dailey", "Jeff Bezos", "Sharecropping", "Vasoepididymostomy", "Nodar Kumaritashvili", "the inner edge of the galaxy", "Archie Marries Betty", "marks locations in Google Maps", "Greek \u1f61\u03c3\u03b1\u03bd\u03bd\u03ac, h\u014dsann\u00e1", "\u201cMy dear, I don\u2019t give a damn\u201d", "australia", "Jackie Kennedy", "narcolepsy", "Bangladesh", "winton", "a fluid", "blancmengier", "linda evans", "eagles", "the Book of Esther", "linda evans", "the State House in Augusta", "Pan Am Railways", "a coaxial cable with RCA connectors or a fibre optic cable", "Division II", "David S. Goyer", "journal", "main east-west road", "\"Histoires ou contes du temps pass\u00e9\"", "a secularist and nationalist,", "Martin Joseph O'Malley", "Deep Purple", "Atlas ICBM", "Tuesday", "Joe Harn", "the Russian air force,", "nearly $2 billion in stimulus funds", "three", "Genocide Prevention Task Force", "NATO", "more than 30", "Republican Gov. Jan Brewer", "Muslim festival of Eid al-Adha.", "$81,88010", "david college students", "Hercules", "Connecticut", "elementary", "Nixon's next nominee, Judge Harrold Carswell of the Fifth Circuit,", "FDR", "the head swan", "Paul Vernon Hornung", "a fisheye lens", "The Blues Brothers", "lestat de Lioncourt", "polygons", "dishwasher", "Jet Republic", "Waseem Ahmad."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5439781093358396}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.31578947368421056, 0.25, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5714285714285715, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10442", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-5951", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-597", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-6480", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-6731", "mrqa_triviaqa-validation-7634", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1350", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-3051", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-6124", "mrqa_newsqa-validation-1095"], "SR": 0.4375, "CSR": 0.5597426470588236, "EFR": 1.0, "Overall": 0.7292922794117647}, {"timecode": 34, "before_eval_results": {"predictions": ["he did not want disloyal men in his army.", "\"world classic of epoch-making oratory.\"", "1960", "Ernest Hemingway", "Master Christopher Jones", "Pradyumna", "the most recent Super Bowl champions", "her husband Henry ( Lancaster ) is overdue and their servants have the night off, leaving her all alone in a Manhattan apartment", "Hirschman", "United States customary units are a system of measurements commonly used in the United States", "Zeus", "after initially peaking at number 41 in the UK", "an anembryonic gestation", "The Divergent Series : Ascendant was never made", "Wizard of Oz", "Andes", "united states of united states", "Barack Obama", "lord", "Super Bowl Sunday", "Elizabeth II,", "Republican", "baseball cards", "cuban cigars", "Charlie Chaplin", "Microsoft", "1955", "The Ones Who Walk Away from Omelas", "Province of New York", "historic buildings, arts, and published works", "Lamar Wyatt", "Amberley Village", "Colonial colleges", "\"Secrets and Lies\"", "Eisenhower Executive Office Building", "Oklahoma Sooners", "Mike Greenwell", "Kurt Vonnegut Jr.", "Bryant Purvis, 19, was arrested after the incident at Hebron High School in Carrollton, Texas.", "member states off the Somali coast.", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter", "is fighting for his life in a Buenos Aires hospital after being shot in the head", "wanted to change the music on the CD player and the 34-year-old McGee said the football star had acted aggressively in trying to grab the device.", "Iron Eyes Cody", "Monday night", "CNN's \"Piers Morgan Tonight\"", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "Louvre", "planned attacks", "housing, business and infrastructure repairs,", "Homo", "rob 'em", "Boston Red Sox", "a beehive", "Walter Cronkite", "Twenty", "skating comic duo", "Green olives", "Lincoln", "by Hermann Hesse", "Baby Gays", "Rick Springfield", "David Bowie", "\"Cruisin'\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.6636754833757325}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.5, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 0.25, 0.3076923076923077, 0.0, 0.35294117647058826, 0.0, 0.0, 1.0, 0.4, 0.3225806451612903, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-2333", "mrqa_triviaqa-validation-1230", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-4700", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3369", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-7948", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-1952", "mrqa_searchqa-validation-2411", "mrqa_searchqa-validation-12158", "mrqa_triviaqa-validation-6414"], "SR": 0.53125, "CSR": 0.5589285714285714, "EFR": 1.0, "Overall": 0.7291294642857143}, {"timecode": 35, "before_eval_results": {"predictions": ["Doctorin' the Tardis", "a religious basis", "punk rock", "Aaron Hall", "Biola", "Easter Rising", "A.S. Roma", "Arena of Khazan", "Dungeness crab", "the Man Booker Prize", "England", "Texas's 27th congressional district", "1969", "Mel Blanc", "Agnes Tjongarero", "Shalmaneser V", "warmth", "with the American Revolutionary War", "Gertrude Niesen", "Gibraltar", "Peking", "The Union's forces", "the nucleus", "the 1960s", "Patrick Swayze", "Michael Phelps", "Nikola Tesla", "relative humidity", "Uganda", "dutch East India Company", "Tennessee", "i second that emotion", "i second that emotion", "Pegida", "hastings", "rebecca", "the United States", "Peter Kay.", "Felipe Massa.", "supply vessel Damon Bankston", "his health", "Hamas rocket attacks", "three years", "Pope Benedict XVI", "twice", "Barack Obama", "a nuclear weapon", "2008", "natural gas", "Mehsud", "Henry Hudson", "geometria", "i second that emotion", "Barack Obama", "gold", "Happy Days", "anthrax", "America", "Michael Clayton", "the Waves", "William Jennings Bryan", "Hairspray", "Herman Cain", "Hakeemullah Mehsud"], "metric_results": {"EM": 0.5, "QA-F1": 0.58203125}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9597", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-10182", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-5556", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-3227", "mrqa_searchqa-validation-16347", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6783"], "SR": 0.5, "CSR": 0.5572916666666667, "EFR": 1.0, "Overall": 0.7288020833333334}, {"timecode": 36, "before_eval_results": {"predictions": ["5-cylinder", "Pat McCormick", "1984", "heads of federal executive departments", "reservoirs", "Tim Passmore", "a simple majority vote", "the court from its members for a three - year term", "bacteria", "NCIS Special Agent in Charge", "elected", "Mexican Seismic Alert System", "the root", "March 16, 2018", "madonna", "a hare or rabbit", "Indonesia", "Heston Blumenthal", "colorblind", "Jeffrey Archer", "madonna", "Cahaba", "Mercury", "Torres Strait", "crippen", "Mendip Hills", "Shamrock Rovers", "Tropical Storm Ana", "the Golden Globe Award for Best Actress in a Motion Picture \u2013 Drama", "ten", "Derry", "Koch Industries", "YouTube", "1978", "Arthur Miller", "over 1 million", "Allies of World War I, or Entente Powers", "city", "African National Congress Deputy President Kgalema Motlanthe", "one evening", "The Red Cross, UNHCR", "\"People of Palestine\"", "media", "Sudanese nor orphans,", "authorizing killings and kidnappings by paramilitary death squads", "forgery and flying without a valid license,", "Intensifying violence, food shortages and widespread drought", "more than 200", "\"Public Enemies\"", "MDC head Morgan Tsvangirai", "Bush", "Norah Jones", "king David", "Casablanca", "charlie parker", "macaroon", "Montego Bay", "Risky Business", "Alien", "New York Harbor", "charlie parker", "Russia", "Vichy", "American Idol"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4919650191532342}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.08695652173913045, 1.0, 0.8, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 0.0, 0.4444444444444445, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.2105263157894737, 0.2857142857142857, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-1682", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-7306", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-4679", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-641", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-5740", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-932", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3392", "mrqa_searchqa-validation-11138", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-15788", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2085"], "SR": 0.40625, "CSR": 0.5532094594594594, "EFR": 0.9736842105263158, "Overall": 0.7227224839971551}, {"timecode": 37, "before_eval_results": {"predictions": ["Wisdom, Compassion, Justice and Integrity", "Alex Ryan", "16,801", "1939", "Montreal Canadiens", "Dalveer Bhandari", "Payaya Indians", "Gibraltar", "216", "5,534", "Mirabilis", "ideology", "23 November 1996", "Tessa Virtue", "Pontiac Silverdome", "cricket", "2005", "Telstar", "Nikkei 225", "jack johnson", "Munich", "Tenerife", "Papua New Guinea", "bridge", "Idris I", "July 14th 1789", "Rural Electrification Act of 1936", "Thomas Allen", "Adolfo", "20 July 1981", "Peterhouse, Cambridge", "Visigoths", "34", "Skye McCole Bartusiak", "Christopher Nolan,", "David May", "April 8, 1943", "Leucippus", "two", "Nick Adenhart", "\"We're not going to forget you in Washington, D.C.\"", "forged credit cards", "Diprivan,", "Barack Obama", "\"I'm just getting started.\"", "Omar bin Laden", "five Texas A&M University crew mates", "Sharon Bialek", "Henry", "Antichrist", "inch", "Pentachlorophenol", "the Speaker", "Tom Cruise", "the Philippines", "beef", "Benjamin Siegel", "Oliver Twist", "a bow", "Wolfgang Johannes Puck", "Chile", "Hell Week", "Alien", "Septimus Luker"], "metric_results": {"EM": 0.5, "QA-F1": 0.6315104166666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-9163", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-633", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3997", "mrqa_hotpotqa-validation-2431", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4027", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-6565", "mrqa_searchqa-validation-3831", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-4180", "mrqa_triviaqa-validation-2605"], "SR": 0.5, "CSR": 0.5518092105263157, "EFR": 1.0, "Overall": 0.7277055921052632}, {"timecode": 38, "before_eval_results": {"predictions": ["311", "Sunset Publishing Corporation,", "25 August 1949", "Thrushcross Grange", "2005", "Harold Edward Holt", "their unusual behavior,", "Ray Romano", "Bundesliga", "Overijssel, Netherlands", "Budget Rent a Car", "Atlanta Athletic Club", "1986", "Sir Seretse Khama", "Richie Cunningham", "Amanda Bynes", "March 31, 2018", "March 1995", "September 21, 2017", "January 17, 1899", "Super Bowl LII,", "Heather Stebbins", "Arkansas", "1,228 km / h ( 763 mph )", "Andaman and Nicobar Islands -- Port Blair", "United States", "australia", "Nicolas Sarkozy", "kyu", "A quick brown fox", "the Silurian", "anesthetic agent", "90%", "South Africa", "Epiphany", "Emmy Awards", "Eleanor Roosevelt", "Charlotte's Web", "the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "17 children under 3 years old in America", "misdemeanor assault charges", "the United States will do its part to improve the environment by taking on greenhouse gas emissions.", "three full-length computer-generated animated film", "1994", "\"The United States can learn much from Turkey's expertise on Afghanistan and Pakistan", "ketamine", "Al Nisr Al Saudi", "French trimaran l'Hydroptere", "3 p.m. Wednesday", "Iran", "American Republic", "volcanic eruptions", "a small cap", "Rhizo-", "Popular Science", "the Xerox", "Mount Rushmore", "Michelle Pfeiffer", "HOV lane", "Louis Brandeis", "May", "Henry Morgan", "Setsuko Thurlow", "the President"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5662660256410257}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3076923076923077, 1.0, 0.846153846153846, 0.0, 0.5, 0.15384615384615383, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-972", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-1218", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3214", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-2270", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6002", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1451", "mrqa_searchqa-validation-1559", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-11590", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-9322", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-2980", "mrqa_naturalquestions-validation-7202"], "SR": 0.453125, "CSR": 0.5492788461538461, "EFR": 1.0, "Overall": 0.7271995192307693}, {"timecode": 39, "before_eval_results": {"predictions": ["Stephen Greenblatt", "Billy J. Kramer", "2.1 million", "British", "Happy Death Day", "ITV", "G\u00e9rard Depardieu", "Harvard", "\"novel with a key\"", "an Anglo-Saxon saint", "Animorphs", "Caesars Entertainment Corporation", "Juliet", "Autopia", "The balance sheet", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "Saint Alphonsa", "Sophia Akuffo", "John Travolta", "in late - 17th century New England", "The Chainsmoker", "Antigonon leptopus", "referee", "1959", "Nick Kroll", "Morgan Freeman", "Andy Warhol", "kabaddi", "Orion", "dasyurus geoffroii", "Canada", "Henry Addington", "Aunt Harriet", "Madagascar", "tempera", "Holly Johnson", "Rade Serbedzija", "Cape Blanc (Ra\u2019s al Abyad)", "Chinese", "school,", "\"momentous discovery\"", "for World War II in killings at a Nazi German death camp in Poland.", "Arsene Wenger", "Gov. Mark Sanford", "bronze medal in the women's figure skating final,", "seven", "$3 billion,", "Wilbert Gwashavanhu, political consul at Zimbabwe's embassy in Washington.", "Islamabad", "\"Oprah is an angel, she is God-sent,\"", "an 8.52-carat diamond", "The Jetsons", "the Somme", "a trumpet", "Riga", "Thelonious Monk", "Queen Mary II", "a tobacco case", "shrew", "Gangbusters", "to be admired by the woman he love", "some really good steaks, a fantastic bottle of red, and then kick back and let the", "HMS Amethyst", "Spearchucker"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6302083333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-3228", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6396", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1134", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-7000", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-15555", "mrqa_triviaqa-validation-7264"], "SR": 0.59375, "CSR": 0.550390625, "EFR": 0.9230769230769231, "Overall": 0.7120372596153847}, {"timecode": 40, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3828", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-4675", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10561", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12195", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12663", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12857", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15788", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-1604", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3797", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-65", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6637", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7589", "mrqa_searchqa-validation-7643", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9486", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-10416", "mrqa_squad-validation-1049", "mrqa_squad-validation-1176", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2402", "mrqa_squad-validation-2489", "mrqa_squad-validation-2840", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3300", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-4127", "mrqa_squad-validation-415", "mrqa_squad-validation-4320", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5129", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5597", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6282", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6610", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-677", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7788", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-7982", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8213", "mrqa_squad-validation-8269", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8744", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9498", "mrqa_squad-validation-9590", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9697", "mrqa_squad-validation-9717", "mrqa_squad-validation-972", "mrqa_squad-validation-9732", "mrqa_squad-validation-9776", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1158", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-1857", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4703", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839"], "OKR": 0.830078125, "KG": 0.475, "before_eval_results": {"predictions": ["within a few hundred feet of each other", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan:", "30", "upper respiratory infection,", "two Metro transit trains that crashed the day before, killing nine,", "Manmohan Singh", "Jaipur", "served in the military,", "The father of Haleigh Cummings,", "a renaissance of the game in the region.", "More than 15,000", "Fa'aleaga Young Yen", "seven", "glass shards", "1991", "the head of the Imperial Family", "a response to the sensation of food", "hydrological cycle", "Mike Czerwien", "eleven", "Iden Versio", "Victory gardens", "in Middlesex County, Province of Massachusetts Bay", "singer / songwriter Judy Collins", "defense against rain", "18", "two 1.6 moz (45.4 g) 100 per cent beef patties", "tunisia", "South Africa", "tunisia", "Mont Blanc", "lorne Greene", "lyon", "My favorite martian", "Portugal", "tunisia", "Dodge Ram cars", "Telstar", "Reverend Timothy \"Tim\" Lovejoy", "luchadora", "Girls' Generation", "Camille Saint-Sa\u00ebns", "6'5\"", "Martha Coolidge,", "16 November 1973", "Centennial Olympic Stadium", "Martin O'Neill", "the Beiro", "Mount Rainier, Washington", "Prince Antoni Radziwi\u0142\u0142", "epsilon", "Nebraska", "Madagascar", "Speed", "constellation", "champagne", "Como", "Sputnik", "Joseph Smith", "bylaws", "Hangman", "Crying of Lot 49", "Paris", "Best Buy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.605079265541222}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.17391304347826086, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.26666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.22222222222222224, 0.5714285714285715, 1.0, 1.0, 0.2857142857142857, 0.0, 0.5384615384615384, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1319", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-4437", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-2447", "mrqa_searchqa-validation-14412", "mrqa_searchqa-validation-15002", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-9355", "mrqa_searchqa-validation-44"], "SR": 0.53125, "CSR": 0.5499237804878049, "EFR": 0.9333333333333333, "Overall": 0.6998545477642277}, {"timecode": 41, "before_eval_results": {"predictions": ["Industry and manufacturing", "possible victims of physical and sexual abuse.", "Brian David Mitchell,", "$250,000 for Rivers' charity: God's Love We Deliver.", "Ferraris, a Lamborghini and an Acura NSX", "Larry Ellison,", "Nkepile M abuse", "Seminole Tribe", "18", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\" said Brinley.", "Ross Perot", "100% of its byproducts", "\"utterly baseless.\"", "Dolgorsuren Dagvadorj,", "The Spanish brought the European tradition to Mexico", "in 1898", "must receive the highest number of votes, and also greater than 50 % of the votes", "Isaiah Amir Mustafa", "political ideology", "John Locke", "crowned the dome", "In the 1979 -- 80 season,", "Issues of the American Civil War", "the breast or lower chest of beef or veal", "Teri Hatcher", "in capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "nine", "Alex Murphy", "the Wye", "Martin Luther King", "Rio de Janeiro", "piano", "Pyrrhus", "Nigel Short", "Nazi party", "georgia state", "georgia state", "brash", "Scott Mosier", "Leslie Knope", "between 1535\u201336", "The Food and Drug Administration (FDA) is a federal agency of the United States Department of Health and Human Services,", "University of Oxford", "Bruce Grobbelaar", "Rockhill Furnace, Pennsylvania", "the Chicago Bears", "Brookhaven", "Juventus of Italy", "'valley of the hazels'", "January 2016", "Mickey Spillane", "Thurman Munson", "Exodus", "The Filipino Academy of Movie Arts and Sciences Awards", "Monica Samille Lewinsky", "the Pacific and Atlantic oceans", "Amish", "coffee", "the foot", "Kansas State University", "Fettuccine", "baseball", "Benjam\u00edn Arellano", "pubs, bars and restaurants"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6512891433747412}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.7499999999999999, 0.6666666666666666, 1.0, 0.6086956521739131, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1122", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-5985", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1963", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-5472", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-10356", "mrqa_searchqa-validation-16878", "mrqa_hotpotqa-validation-655"], "SR": 0.53125, "CSR": 0.5494791666666667, "EFR": 1.0, "Overall": 0.7130989583333334}, {"timecode": 42, "before_eval_results": {"predictions": ["The Dornbirner Ach", "Cricket fighting", "XVideos", "William Harold \"Bill\" Ponsford", "Zero Mostel", "various bigfoot-like sightings, giant snakes and \"thunderbirds.\"", "The Grandmaster", "Miss Universe 2010", "Charles Hastings Judd", "1952", "Squam Lake", "MGM Resorts International", "Golden Gate National Recreation Area", "William Corcoran Eustis", "sedimentary", "ideology", "an idiom for the most direct path between two points", "October 19, 1961 -- January 19, 1963", "March 5, 2014", "The genome", "Kit Harington", "April 26, 2005", "March 14", "birth", "aiding the war effort", "Vincenzo Peruggia", "the Netherlands", "gold", "landmasses, mountainous regions, shallow seas, and deep ocean basins", "rudolf dassler", "rococo", "Friedrich Miescher", "Sergei Rachmaninoff", "british", "an", "Galileo Galilei", "Rocky Horror Show", "Abbey Theatre", "16", "Ralph Lauren,", "Sen. Evan Bayh", "2,700-acre", "Jeanne Tripplehorn's", "served in the military,", "Karen Floyd", "ties", "bard", "genocide", "5 1/2-year-old", "Arsene Wenger", "Boston", "Odysseus", "Nine to Five", "the polar bear", "French and Indian War", "Henry Hudson", "Double Jeopardy", "Calais", "Lois Lane", "Falafel", "Fiji", "an alsacian snail dish", "Haiti", "2.4 % of the world's land surface area"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6372023809523809}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3426", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-169", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-5163", "mrqa_triviaqa-validation-2777", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-436", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-9276", "mrqa_searchqa-validation-14554", "mrqa_naturalquestions-validation-8420"], "SR": 0.578125, "CSR": 0.5501453488372092, "EFR": 0.9259259259259259, "Overall": 0.698417379952627}, {"timecode": 43, "before_eval_results": {"predictions": ["November 2006", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "Broad Sands Bay", "Brigadier General Raden Panji Nugroho Notosusanto", "trio", "\"Lonely\"", "Anne Perry", "\"SOS\"", "920\u2013961", "Cleveland, Ohio", "Smoothie King Center", "Indooroopilly Shopping Centre", "1963\u201393", "\"Traumnovelle\"", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "1939", "the body - centered cubic ( BCC ) lattice", "a donor molecule", "Hank Williams", "Stax Records songwriters Homer Banks, Carl Hampton and Raymond Jackson", "2014", "Kyla Coleman", "the Hongwu Emperor of the Ming Dynasty", "Sebastian Vettel", "a candidate state", "John Goodman", "Verona", "The Jungle Book", "tunis", "the Dover", "gdansk", "The Magic Circle", "Anglo Saxon", "Leicester", "0-12", "John le Carr\u00e9", "The Virgin Spring", "Canada", "the Los Alamitos Joint Forces Training Base", "Leaders of more than 30 Latin American and Caribbean nations", "his enjoyment of sex and how he lost his virginity at age 14.", "warning about tendon problems.", "when daughter Sasha exhibited signs of potentially deadly meningitis", "bedrooms of their two-floor home", "12", "Nafees A. Syed,", "2002", "FDA's Office of Antimicrobial Products.", "iTunes,", "at a depth of about 1,300 meters in the Mediterranean Sea", "overwhelm", "savings", "Roget", "diesel", "an Iran-backed Shia government", "\"Bewitched\" Curse", "South African", "Steven Spielberg", "Pitney Bowes", "a microwave", "Agatha Christie", "Elvis Presley", "electron pairs", "The Hudson River"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5641804170792657}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.4210526315789474, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.125, 0.27272727272727276, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2954", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-3048", "mrqa_triviaqa-validation-1419", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-613", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-2617", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-11850", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-16638", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7107"], "SR": 0.484375, "CSR": 0.5486505681818181, "EFR": 1.0, "Overall": 0.7129332386363637}, {"timecode": 44, "before_eval_results": {"predictions": ["Warsaw Uprising Museum", "the Saddledome", "once", "Scandinavian design", "1896", "Juventus", "1968", "FX", "SKUM", "1998", "John Anthony \"Jack\" White", "Boyd Gaming", "Big John Studd", "Thon MarialMaker", "Acid rain", "Around 1200", "toys or doorbell installations", "China", "1992 to 2013", "Morgan Freeman", "a blind fury", "14 December", "foreign investors", "1837", "Bonnie Aarons", "Dan Stevens", "Pluto", "rugby school", "france", "pangram", "Gower Peninsula", "Narragansett Bay", "Zork", "Peshtigo, Wisconsin", "Today", "Home Alone 2: Lost in New York", "drag queens", "pink", "as he tried to throw a petrol bomb at the officers,", "Many members of Zoe's Ark describes itself as a nonprofit organization based in Paris that sends teams of physicians, nurses, firefighters and other specialists to care for children in war zones and place them with families in France,", "BMW A3 Audi A3", "July 4.", "Mark Fields of Ford,", "Spaniard Carlos Moya", "400 years ago and kept anonymously in an Irish home for much of the time", "National Archives", "Yusuf Saad Kamel", "September 2008,", "Kate Hudson's ex, Black Crowes rocker Chris Robinson,", "Democratic National Convention,", "A Few Good Men", "Brown University", "Fidelio", "bone", "Macaulay Culkin", "Neil Diamond", "Northern Exposure", "baseball movie", "Henry Cisneros", "Kennedy Onassis", "Marie Antoinette", "William Conrad", "Hobart", "reckless arson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5920498904024718}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.17391304347826084, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.045454545454545456, 0.0, 1.0, 0.6666666666666666, 0.0, 0.375, 0.0, 1.0, 0.0, 0.28571428571428575, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-839", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-4326", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-123", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-423", "mrqa_searchqa-validation-12912", "mrqa_searchqa-validation-10831", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-7791", "mrqa_triviaqa-validation-7370"], "SR": 0.4375, "CSR": 0.5461805555555556, "EFR": 0.9722222222222222, "Overall": 0.7068836805555556}, {"timecode": 45, "before_eval_results": {"predictions": ["Germany", "Djibouti", "city of atlantic", "Patrick Henry", "chancery", "Matlock", "Belgium", "the rent doesn't include additional costs such as insurance or business rates", "Peter Ackroyd", "chlorophyll", "even numbers", "obtaining and proper handling of human blood", "buch rugs", "city of atlantic", "one of the most internationally recognized symbols of San Francisco, California, and the United States", "April 7, 2016", "early 20th century", "the Eurasian Plate", "Michael Crawford", "the heart", "mathematical model", "111", "a theory", "President Lyndon Johnson", "electric potential generated", "Michael Moriarty", "Barbara Niven", "The Supremes", "1974", "Love the Way You Lie", "string musical instrument", "Tsung-Dao Lee", "1949", "Anne Erin \"Annie\" Clark", "2 March 1989", "Gal Gadot", "John Lennon", "2009", "Grayback forest-firefighters", "five female pastors", "Darrel Mohler", "natural gas", "Friday,", "protective shoes", "Ernesto Bertarelli", "Ryder Russell,", "Russian residents and worldwide viewers,", "issued his first military orders as leader of North Korea", "Brazil", "eight", "iconoclasm", "Tiger", "Aspen Ideas Festival", "the chancellor", "food combining", "General Andrew Jackson", "pumice", "megabytes", "eucalyptus", "Maria Montessori", "pot roast", "Magdalene Dietrich", "Brooke Hogan", "Over the Rainbow"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5478495896464646}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6421", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-943", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3552", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-6243", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-1507", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-961", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-4069", "mrqa_searchqa-validation-13958", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-8152"], "SR": 0.484375, "CSR": 0.5448369565217391, "EFR": 1.0, "Overall": 0.7121705163043479}, {"timecode": 46, "before_eval_results": {"predictions": ["two of Triton's daughters", "giant squid", "Sir Anthony Eden", "Glory", "Jackie Kennedy", "Tanzania", "Don Quixote", "Buddhism", "IBM", "Mercury", "Menninger", "a husband", "PEZ", "Hollaback Girl", "1986", "$2.187 billion", "Donna", "10 June 1940", "Americans who served in the armed forces and as civilians", "15 November 1996", "Andrew Michael Harrison", "Bob Dylan", "triglycerides", "the Ming", "Gustav Bauer", "1913", "biathlon", "decorate", "Nathuram Godse", "Big Ben", "a clock", "tomb raiser", "tiger", "the Dormouse", "toll house cookies", "commerce and trade", "tunis", "atlantic house", "Tainted Love", "\"18 Months\"", "1,382", "Everton", "Tom Kitt", "Ellie Kemper", "11,791", "pronghorn", "KULR-TV", "Big 12 Conference", "June 1975", "My Beautiful Dark Twisted Fantasy", "for death squad killings carried out during his rule in the 1990s.", "Switzerland", "in North Korea", "second-degree attempted murder and conspiracy,", "debris", "Pakistan's", "\"golden city,\"", "The Sopranos", "Paktika province in southeastern Afghanistan,", "16", "on the Ohio River near Warsaw, Kentucky,", "Adam Lambert and Kris Allen", "HMS Thunderbolt", "the War of the Austrian Succession"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6186992694805196}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818181, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.8, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-801", "mrqa_searchqa-validation-9659", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-15080", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-6632", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-7910", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-6264", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-6150", "mrqa_triviaqa-validation-6480", "mrqa_hotpotqa-validation-1103", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-3992", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3238", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-2573", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4334"], "SR": 0.515625, "CSR": 0.5442154255319149, "EFR": 1.0, "Overall": 0.712046210106383}, {"timecode": 47, "before_eval_results": {"predictions": ["The Seven Storey Mountain", "Morocco", "Walter Reed", "Okinawa", "Lake Victoria", "oil", "Walter Reed", "the gallbladder", "Out of Africa", "Dublin", "Marseille", "a light year", "archery", "awarded to the team that lost the pre-game coin toss", "The Massachusetts Compromise", "the southwestern part of the island", "Garfield Sobers", "Romance or Romantic languages", "Rouen Cathedral", "Thespis", "1908", "54 Mbit / s", "Jack Barry", "Bill Hayes", "Wisconsin", "france", "bicameral state", "mongoose", "a crochet in time", "Fred Gwynne", "Make Your Lives Extraordinary", "Vietnam", "Greece", "John McCarthy", "tobacco", "playing cards", "Steve Guttenberg", "Rana Daggubati", "Philadelphia", "Bart Conner", "200", "the Free and Sovereign State of Tamaulipas", "1986", "private", "The Dragon School in Oxford", "Aamir Khan", "Arrowhead Stadium", "Standard Oil", "Matt Groening", "2000.", "voluntary manslaughter", "10,000", "Tsvangirai", "plant-based diet", "women and breast cancer.", "338", "ultra-high-strength steel and boron", "Malawi,", "North Korea", "Belfast's Odyssey Arena.", "Symbionese Liberation Army", "a snowman", "Dick Van Dyke", "Pope Benedict XVI"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6301153273809523}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.375, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11470", "mrqa_searchqa-validation-11833", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-121", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-4488", "mrqa_naturalquestions-validation-9440", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-64", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3097", "mrqa_triviaqa-validation-7750"], "SR": 0.578125, "CSR": 0.544921875, "EFR": 1.0, "Overall": 0.7121875}, {"timecode": 48, "before_eval_results": {"predictions": ["bears", "John Tyler", "a roundabout", "gravity", "a Ouija", "sausages", "Larry King", "pipa", "Chief Joseph", "\"9 To 5\"", "Thor", "Mount Hood", "Smallville", "the Miracles", "Freddie Highmore", "March 16, 2018", "Nepal", "patients who don't have a functioning gastrointestinal tract or who have disorders requiring complete bowel rest", "Ferm\u00edn Francisco de Lasu\u00e9n", "about 15 metres ( 49 feet ) per year", "1933", "Humphrey Bogart", "biblical Book of Exodus", "23 % of GDP", "1930s", "tunisia", "onions", "Japan", "Francis Drake", "a karst cave", "Perkins Chapel", "Jean-Paul Gaultier", "Vienna", "Sheffield United", "\" Lost in Translation\"", "Dawn French", "murder/mystery", "Matt Groening", "Bulgarian-Canadian", "Valeri Bure", "Get Him to the Greek", "Balloon Street, Manchester", "the New York Islanders", "\" Training Day\"", "Daniel Boone", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements,", "Treaty of Trianon", "Santiago del Estero Province", "August 19, 2013", "held in a trust fund", "\"Nu au Plateau de Sculpteur,\"", "CNN's", "Amanda Knox's aunt", "aesthetic environment", "The attorney general will announce his decision early next week,", "Alan Graham.", "allegations that a dorm parent mistreated students at the school.", "1616.", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Fullerton,", "The two were separated in June 2004", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "Danish engineer Karl Kr\u00f8yer", "The patient, who prefers to be anonymous,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6328985760971055}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9690", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-4046", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-5483", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-4694", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-4862", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3197", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1092"], "SR": 0.53125, "CSR": 0.5446428571428572, "EFR": 1.0, "Overall": 0.7121316964285714}, {"timecode": 49, "before_eval_results": {"predictions": ["Some Like It Hot", "Margaret", "Bronchoconstriction", "Legoland", "William Henry Harrison", "Abraham Lincoln", "Floyd Mayweather Jr", "Isadora Duncan", "lewis", "Venice", "an exotoxin-mediated disease", "fog", "The British Broadcasting Corporation", "January 1, 2016", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "Andrew Lloyd Webber", "enabled business applications to be developed with Flash", "ecological regions", "Rashida Jones", "Representatives", "in the fascia surrounding skeletal muscle", "the Anglo - Saxon King Harold Godwinson", "divided into several successor polities", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "missionaries", "Howard Hoagland \"Hoagy\"", "Secretary of State", "1960", "a thin layer of gases", "leonard", "lewis", "lewis", "jackstones", "a horizontal desire", "Morgan Choir", "vascular bundles or ribs", "William Shakespeare", "the Mayor of the City of New York", "more than 110", "The Killer", "Dundalk, County Louth", "Lawton Mainor Chiles Jr.", "right-hand", "XVideos", "1997", "Mauthausen-Gusen", "French", "Aberdeen", "Joshua Rowley", "The Tupolev Tu-160 strategic bombers", "producing rock music with a country influence.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Buenos Aires.", "Heshmat Tehran Attarzadeh", "some of the most hostile war zones,", "Lifeway Christian Stores", "two years,", "\"it should stay that way.\"", "the \"face of the peace initiative has been attacked,\"", "a tanker", "Iraqi Prime Minister Nouri al-Maliki", "General Motors", "Bagel set", "graffiti"], "metric_results": {"EM": 0.5, "QA-F1": 0.6259472298534798}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6153846153846153, 1.0, 0.761904761904762, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.4, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.28571428571428575, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-11090", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-5802", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-162", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-5267", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2475", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2658", "mrqa_triviaqa-validation-1402"], "SR": 0.5, "CSR": 0.54375, "EFR": 1.0, "Overall": 0.7119531250000001}, {"timecode": 50, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4488", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11459", "mrqa_searchqa-validation-11470", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16588", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-5705", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4694", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7117", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-943"], "OKR": 0.837890625, "KG": 0.46796875, "before_eval_results": {"predictions": ["the skull & crossbones", "a swan", "the Swamp Fox", "Jelly Belly", "Zsa Zsa Gabor", "Eurydice", "the retina", "foot", "middle-aged", "Papua New Guinea", "William Jennings Bryan", "apocrypha", "Gallipoli", "Network - Protocol driver", "Sesel Zvidzai", "1773", "the Vice President of the United States", "Sebastian Lund ( Rob Kerkovich )", "Burbank, California", "the bank's own funds", "lumbar cistern", "Missouri River", "a line of committed and effective Sultans", "a moral tale", "The pour point", "Shotgun", "China", "blue", "Moon River", "Bronx Mowgli", "driving Miss Daisy", "Jeremy Bates", "lance-bombardier", "hanging Gardens", "Wimbledon", "zagreb", "a wedge-shaped microcar", "Michael Seater", "\"Sleeping Beauty\"", "ten years of probation", "\"Empire Falls\"", "Imagine", "Paul Avery", "Louis Silvie \"Louie\" Zamperini", "baeocystin", "Venice", "Ghanaian", "Kirk Humphreys", "the Mayor of the City of New York", "has been remixed in 5.1 surround sound.", "managing his time.", "Bobby Jindal", "Dr. Jennifer Arnold and husband Bill Klein,", "gasoline", "The station", "56", "Lieberman", "Roy Foster's motto has been \"No man left behind.\"", "Three teens", "Don Draper", "April 28", "Brenda", "Jean - Jacques Rousseau's Confessions,", "April 1979"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6143560779498279}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.0, 1.0, 0.08000000000000002, 0.6666666666666666, 0.3076923076923077, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-2548", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-689", "mrqa_triviaqa-validation-5385", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-443", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-974", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-879", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-493"], "SR": 0.515625, "CSR": 0.5431985294117647, "EFR": 0.967741935483871, "Overall": 0.7047662179791271}, {"timecode": 51, "before_eval_results": {"predictions": ["cherries", "Belgium", "benicio Del Toro Snchez", "the Netherlands", "Norse", "time", "crivisse", "time", "Jupiter's", "Shropshire (a county in)", "red", "a solar eclipse", "Jean Foucault", "1986", "Shreya Bhushan Pethewala", "Database - Protocol driver ( Pure Java driver )", "hydrogen", "1917", "in 2002", "Johnny Logan", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1955", "an instant messaging client", "France", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "\u2018Space Oddity\u2019", "cary park", "red", "Stereophonics", "georgia terrier", "matricide", "Monet", "curling", "Melpomene", "vitamin K", "Wat Tyler from Kent", "climatic", "USS \"Essex\"", "February 20, 1978", "Kirkcudbright", "dance partner and eventual confidante", "Port of Boston", "Lucas Grabeel", "first train robbery,", "Scarface", "evangelical Christian periodical", "Tallahassee City Commission", "The Rebirth", "press conference", "Chinese", "Henry Ford", "returning combat veterans", "permitted under Spanish Football Federation (RFEF) rules.", "sexual assault on a child.", "top designers, such as Stella McCartney,", "in the west African nation later this year.", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Pakistan's largest city and the capital of Sindh province.", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "in a public housing project,", "President Omar Bongo,", "port", "Mel Brooks", "feet"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6454969509657009}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.9, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3076923076923077, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-4975", "mrqa_searchqa-validation-9518", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-9522", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-7003", "mrqa_triviaqa-validation-7254", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-2980", "mrqa_hotpotqa-validation-3001", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-3076", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-3925", "mrqa_triviaqa-validation-6748"], "SR": 0.515625, "CSR": 0.5426682692307692, "EFR": 0.9354838709677419, "Overall": 0.6982085530397022}, {"timecode": 52, "before_eval_results": {"predictions": ["President George Washington", "Headless Horseman", "William Howard Taft", "deborah & the Family Stone", "Saudi Arabia", "Absalom", "Odysseus", "China's Xinjiang-Uygur Autonomous Region(TAR)", "England (Semenanjung Malaysia)", "Baltra", "drum", "Big Brown", "Patrick Henry", "the Naturalization Act of 1790", "the forex market", "6 January 793", "Camping World Stadium in Orlando, Florida", "Tandi, in Lahaul", "Daman and Diu ( 53.5 percent )", "1820s", "Vasoepididymostomy", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "self - closing flood barrier", "the nucleus", "irsten Simone Vangsness", "Rocky Horror Picture Show", "deborah", "Virginia", "China", "dmitri", "plays a big  time sports agent", "Wisconsin", "points based scoring system", "cosmology", "Mary Jane Grant", "Brett Favre", "per annum exclusive", "Stanis\u0142awa Biskupa i \u015bw", "Samuel Burl \"Sam\" Kinison", "Nova Scotia", "DI Humphrey Goodman", "Mika H\u00e4kkinen", "the top division of Mexican football, Liga MX", "Kevin Peter Hall", "a sailor coming home from a round trip", "Martha Wainwright", "Chicago", "Blue Origin", "Canada's first train robbery", "education about rainforests.", "taught a song about freedom of speech.", "reaching out and opening the door for the man who shot him,", "whether to close some entrances, bring in additional officers, and make security more visible.", "The poster boy of Indian action films", "Ketchum, Idaho.", "the return of a fallen U.S. service member", "an 13-week extension of unemployment benefits", "South Africa", "July 8", "Vernon Forrest,", "\"Larry King Live\"", "an embroidered cloth", "Harry Potter", "Jeremy Bates"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5885372899159663}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8235294117647058, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12283", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16958", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-5170", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-1569", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-1611", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-981", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-283", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1875", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-4128", "mrqa_triviaqa-validation-412"], "SR": 0.484375, "CSR": 0.5415683962264151, "EFR": 1.0, "Overall": 0.710891804245283}, {"timecode": 53, "before_eval_results": {"predictions": ["Salt Lake City", "the llama", "Jennifer Lopez", "Cheddar", "Rudy Giuliani", "Ramen", "Wilhelm II", "the House", "Arkansas", "the phi phenomenon", "Bowl Championship Series", "nomadic", "Wichita", "Ernest Rutherford", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "CBS Television City", "Donald Fauntleroy Duck", "1500 BC", "24", "gandhara", "an expression of unknown origin", "January 2017", "1989", "Peter Andrew Beardsley MBE", "The last twenty - five years", "1945", "gary", "david copperfield", "As You Like It", "bees", "british", "germania", "heart", "World War I", "gansbaai", "Harry Redknapp", "an earthquake", "wooden roller coaster", "Hawai\u02bb i state District of Ko\u02bb \u014d\u02bb io Point", "1786", "5 January 1921", "Miriam Margolyes", "Edward II", "Charles Eug\u00e8ne Jules Marie Nungesser", "film", "Valley Falls", "Lucas Stephen Grabeel", "1966", "Mark Sinclair", "Orlando Police Department.", "alert patients of possible tendon ruptures and tendonitis.", "eight", "Amir Zaki", "Iran's nuclear program.", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\"", "Mashhad", "Charlotte Gainsbourg and Willem Dafoe", "Michael Jackson's mother and his three children", "Rev. Alberto Cutie", "Jeannie Longo-Ciprelli", "tells stories of different women coping with breast cancer in five vignettes.", "Clio Awards", "Spanish", "Andes Mountains of Chile and Argentina"], "metric_results": {"EM": 0.5, "QA-F1": 0.6223292332667333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-10902", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-10248", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-3532", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-4129", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3807", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-153", "mrqa_triviaqa-validation-3547"], "SR": 0.5, "CSR": 0.5407986111111112, "EFR": 0.96875, "Overall": 0.7044878472222222}, {"timecode": 54, "before_eval_results": {"predictions": ["pilgrim", "Montague", "The Pirates of Penzance", "Goebbels", "berkshire", "the Blue", "Donald Trump", "Vipers", "Darren Star", "Catalina", "Alaska", "Cleveland", "Marilyn Monroe", "Nicole Gale Anderson", "James Watson and Francis Crick at the Cavendish Laboratory within the University of Cambridge in 1953", "The Live - Stock Dealers '", "when the forward reaction proceeds at the same rate as the reverse reaction", "over 74", "Asuka", "berkshire", "free floating", "100,000", "The anterior interventricular branch of left coronary artery, ( also left anterior descending artery ( LAD ), or anterior descending branch", "the thylakoid membranes", "1800", "Nigel Short", "cr\u00e8me", "tonsure", "1987", "copper", "1979", "robinson crusoe", "HMS Conqueror", "intravenously", "vinegar", "tarot", "four red stars with white borders to the right", "Orange County, California", "1941", "Larry Eustachy", "Oregon State Beavers", "Mexico", "elite cavalry", "The Books", "Cecily Strong", "1933", "Steve Carell", "MediaCityUK", "first baseman", "Three French journalists, a seven-member Spanish flight crew and one Belgian.", "near Garacad, Somalia,", "Adidas is now working with top designers, such as Stella McCartney, to create a distinctive genre of sportswear and lifestyle fashion products.", "Authorities in Fayetteville, North Carolina,", "military trials for some Guantanamo Bay detainees.", "6-4", "first", "Herman Thomas", "poems", "the British capital's other two airports,", "\"illegitimate.\"", "Arthur E. Morgan III,", "Utena", "$10\u201320 million", "September 25, 2017"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5454480917474338}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5263157894736842, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.3, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.2222222222222222, 0.5, 1.0, 0.8, 0.3333333333333333, 1.0, 0.0, 0.4, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0909090909090909, 0.888888888888889, 0.10256410256410256, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1877", "mrqa_searchqa-validation-15700", "mrqa_searchqa-validation-13612", "mrqa_searchqa-validation-874", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-5582", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-4915", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-678", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-891", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-1279", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-867", "mrqa_hotpotqa-validation-1196"], "SR": 0.421875, "CSR": 0.5386363636363636, "EFR": 1.0, "Overall": 0.7103053977272726}, {"timecode": 55, "before_eval_results": {"predictions": ["Caleb", "Ohio", "the person compelled to pay for reformist programs", "c. 3000 BC", "Donna Mills", "for the red - bed country of its watershed", "SIP ( Session Initiation Protocol )", "Thirty years after the Galactic Civil War", "pass grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ), 3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "Carol Ann Susi", "DeWayne Warren", ". The False Claims Act, also called the `` Lincoln Law ''", "Yugoslavia", "musical term Glossary", "Emily Davison", "Ellice Islands", "a nerve cell cluster", "Tripoli", "in the fortified grounds of an old mission known as the Alamo", "Oklahoma", "European Union (EU)-European Union", "peregrines", "a telephone service", "\"Stars on 45 Medley\"", "'Big Dipper'", "Dwarka", "Daniel Sturridge", "Texas Longhorns", "Omega SA", "\"The Gang\"", "Delilah Rene", "Italian", "WB", "Reese Witherspoon", "British Labour Party", "Christopher Tin", "Dutch", "President Barack Obama,", "bipartisan", "a woman", "a weight-loss show", "Mikkel Kessler", "$250,000 for Rivers' charity: God's Love We Deliver.", "Kindle Fire", "Golfer Tiger Woods", "800,000", "The 19-year-old woman", "101", "we will be back,\" Ali's wife Lonnie told Britain's Daily Telegraph newspaper.", "zinc", "birds", "a ship", "Florida", "the West Point", "Joe DiMaggio", "law", "Quasars", "Japan", "the 17th quadrennial presidential election", "resuscitation", "Jumbo", "Captain Jean-Luc Picard", "Gary Woolas", "Apollo 11"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6352511459129107}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false], "QA-F1": [0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-993", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-7521", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-679", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3759", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-9641", "mrqa_searchqa-validation-16358", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-125"], "SR": 0.53125, "CSR": 0.5385044642857143, "EFR": 0.9666666666666667, "Overall": 0.7036123511904762}, {"timecode": 56, "before_eval_results": {"predictions": ["michael Phelps", "scurvy", "isosceles", "herald of free enterprise", "1822", "willow", "perfume", "ovum", "can cause deformities such as spina bifida, fusions of the spine, and defects of the colon.", "woolchen Franklin", "Angus Robertson", "peter stuyvesant", "Botham", "Ed Sheeran", "Jackie Robinson", "erosion", "Kelly Osbourne", "T'Pau", "January 12, 2017", "more than 80 tank\u014dbon volumes", "Thomas Edison", "CBS", "Andrew Garfield", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "the ulnar nerve", "Kareena Kapoor", "Sim\u00f3n Bol\u00edvar", "ARY Digital Network", "1967", "a particular nation", "Lowestoft", "Antonio Salieri", "McKinsey & Company", "Labour Party", "Anna Pavlova", "Spiro Theodore \"Ted\" Agnew", "In a Better World", "Siemionow", "second time since the 1990s", "blind Majid Movahedi,", "weight-loss", "the home,", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "three different videos", "1995", "authorizing killings and kidnappings by paramilitary death squads.", "for his comments while Saudi authorities discuss whether he should be charged with a crime,", "make life a little easier for these families by organizing the distribution of wheelchair,", "Paul Schlesselman", "Worcestershire", "Isaac Newton", "titanium", "( Robert) Browning", "St. Augustine", "staff", "Sicilian pizza", "(Sam) Johnson", "quiet", "Dalmatian", "pitch", "Grover Cleveland.", "Jasenovac concentration camp", "Kohlberg & Co.", "John Robert Cocker"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6548619621459695}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 0.25, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.962962962962963, 0.375, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-7556", "mrqa_triviaqa-validation-405", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-993", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5049", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-5490", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-3512", "mrqa_newsqa-validation-1679", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-1146", "mrqa_searchqa-validation-1319", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14110", "mrqa_searchqa-validation-13085", "mrqa_searchqa-validation-4014", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4926"], "SR": 0.53125, "CSR": 0.5383771929824561, "EFR": 1.0, "Overall": 0.7102535635964913}, {"timecode": 57, "before_eval_results": {"predictions": ["scored a hat-trick", "its launch pad,", "bipartisan", "10", "The show went on without the self-proclaimed \"King of the South,\" whose car and home in the Atlanta suburb of College Park", "individual pieces.", "Himalayan kingdom", "Stratfor, a global intelligence company,", "tells stories of different women coping with breast cancer in five vignettes.", "Ayelet Zurer", "the Obama chief of staff", "it is provocative action,\" Clinton said.", "their culture, religion and national identity.", "March 11, 2018", "piety", "It plays a key role in chain elongation in fatty acid biosynthesis and polyketide biosynthesis", "the coffee shop Monk's", "Richard Carpenter", "`` Iggy '' DePerro", "October 2012", "Matthew Gregory Wise", "2003", "Noah Schnapp", "about 13,000 astronomical units ( 0.21 ly )", "May 26, 2017", "1973", "Toy Story 2", "athletics", "Charlie Sheen", "ian", "commander", "blue", "Prophet Joseph Smith,", "carbonated", "Ohio", "tauri", "Madagascar", "British", "Cartoon Network", "Ellesmere Port", "a large green dinosaur", "Sean", "the 45th Infantry Division", "Knoxville, Tennessee", "Lionsgate", "his superhero roles", "A123 Systems, LLC", "Rachel, Nevada", "Jenji Kohan", "\"O.T.\" Powell", "September 12, 1962", "Big Brown", "a torpedo", "Parkinson's disease", "California Highways", "potential energy", "Aunt Polly", "\"Coward of the County\"", "hockey", "Minnesota", "Clover Hill", "William Randolph Hearst", "Dublin", "Moses"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5598958333333334}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-177", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-481", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-7262", "mrqa_triviaqa-validation-3695", "mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-3137", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-881", "mrqa_searchqa-validation-11956", "mrqa_searchqa-validation-13976", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-13336", "mrqa_searchqa-validation-1462"], "SR": 0.46875, "CSR": 0.537176724137931, "EFR": 0.9705882352941176, "Overall": 0.7041311168864097}, {"timecode": 58, "before_eval_results": {"predictions": ["1994", "Dialogues of the Carmelites", "Dr. Alberto Taquini", "England", "Mary Harron", "Bank of China ( Hong Kong)", "Borgo San Donnino", "Japan Airlines Flight 123", "electric currents and magnetic fields", "Benjamin Andrew \"Ben\" Stokes", "Franconia, New Hampshire, United States", "2006", "Tianhe Stadium", "Mickey Rourke", "Gayla Peevey", "2001", "part - Samoyed terrier", "Otis Timson", "Richard Masur", "sperm and ova", "Paul Revere", "commemorating fealty and filial piety", "February 16, 2018", "West African traditions", "a pop ballad", "Taxonomic rank", "James Dean", "bodhisattva path", "eighty percent of Soviet males", "NOW Magazine", "Canada", "Guanabara bay", "Battle of Hastings", "adventureland", "basket", "Jews of Germany and Northern France", "USS Maine", "immediately appeal the ruling and seek a stay of the order with the U.S. Court of Appeals for the District of Columbia.", "German Chancellor Angela Merkel", "\"Quiet Nights,\"", "wildfires", "More than 15,000", "four", "London and Buenos Aires", "leftist Workers' Party.", "President Obama and Britain's Prince Charles", "Dolgorsuren Dagvadorj,", "$1.45 billion", "Dr. Cade", "Columbia River", "Brass", "comet", "Dolly Parton", "Terry Bradshaw", "United States of America", "Tunis", "Kunta Kinte", "Italy", "a cappella", "Daffodils, or Narcissi", "the FBI's Criminal Justice Information Services Division", "21-year-old", "$14.1 million", "their tour buses, as well as their road crew and traveling with their own equipment."], "metric_results": {"EM": 0.4375, "QA-F1": 0.561031351461039}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.0625, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.5, 0.30303030303030304, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-733", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-4925", "mrqa_triviaqa-validation-3907", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-7492", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-6023", "mrqa_searchqa-validation-7395", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-6939", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-13194", "mrqa_searchqa-validation-190", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1348"], "SR": 0.4375, "CSR": 0.5354872881355932, "EFR": 0.9722222222222222, "Overall": 0.7041200270715631}, {"timecode": 59, "before_eval_results": {"predictions": ["February 9, 1994", "Bank of China Building", "John Snow", "the Battelle Energy Alliance", "\"John of Gaunt\"", "European basketball", "1947", "35,402", "voice-work", "Mwabvi river", "\"Rich Girl\"", "California State University, Dominguez Hills", "Nickelodeon Animation Studio", "utopian novels of H.G. Wells", "President Gerald Ford", "September 1947", "160km / hour", "March 16, 2018", "Argentine composer Lalo Schifrin", "The Cornett family", "Baaghi", "a book of the Old Testament", "March 11, 2016", "Sino - Indian War of 1962", "Andaman and Nicobar Islands -- Port Blair", "6", "speedway", "furniture", "la traviata", "Cybill Shepherd", "Midsomer Murders", "Norman Mailer", "British yachtswoman Dee Caffari", "(Harry) Truman", "lakes and rivers", "mars", "bamboozled", "Sri Lanka", "\"tan lines\"", "Health Resources and Services Administration, and by the Centers for Disease Control and Prevention.", "Nigeria, Africa's largest producer.", "Fullerton, California,", "file for bankruptcy protection,", "the man was dead,", "back at work,\"", "Gavin de Becker", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "a review of state government practices completed in 100 days.", "Sunday.", "Crete", "Tartarus", "climbing", "Alaska", "Wings", "birds", "Mindanao", "stigma", "M&M's", "quicksand", "tanks", "\"9 To 5\"", "Eleven", "Elin Nordegren", "Japan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6074019909688013}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.28571428571428575, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-4905", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-2349", "mrqa_triviaqa-validation-3282", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-1828", "mrqa_triviaqa-validation-2374", "mrqa_triviaqa-validation-2709", "mrqa_newsqa-validation-2896", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-2401", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-10663", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-2826", "mrqa_newsqa-validation-2493"], "SR": 0.515625, "CSR": 0.53515625, "EFR": 1.0, "Overall": 0.709609375}, {"timecode": 60, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1196", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3008", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14834", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5058", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-7943", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9518", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-324", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5360", "mrqa_squad-validation-551", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1104", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1694", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3573", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7537", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-943"], "OKR": 0.802734375, "KG": 0.48046875, "before_eval_results": {"predictions": ["the British Catuvellauni tribe", "Timo Hildebrand (born 5 April 1979)", "Samuel Beckett's", "constant support from propaganda campaigns", "Marvel's The Defenders", "Philadelphia", "bacryopinax spathularia", "11 June 1959", "Anishinaabeg", "Marktown", "45%", "850 saloon", "1862", "bicameral Congress", "H.L. Hunley", "Human fertilization", "3D modeling ( or three - dimensional modeling )", "President Richard Nixon", "Germany", "- ase", "Andreas Vesalius", "Mary Elizabeth Patterson", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "William Jennings Bryan", "Minneapolis", "5", "arzan", "Sherlock Holmes", "Djibouti and Yemen", "bertrand", "hay fever", "bucatini", "Lincoln Logs", "dee caffe", "Styal", "Sicily", "Gorky", "Brian Mabry", "The opposition group, also known as the \"red shirts,\"", "Somali forces and Islamic insurgents.", "eight", "Nairobi, Kenya,", "Bill Clinton's wife -- U.S. Secretary of State Hillary Clinton", "only one", "Stratfor,", "183", "Sub-Saharan Africa", "jazz", "B-movie queen lauren Clarkson", "toothpaste", "Joe Paterno", "bacon", "the Shrew", "trees", "Emperor Heijo", "Universal Studios Hollywood", "genera", "Fairfax", "the Clark bar", "3.14159", "mammoth", "Ma Khin Khin Leh,", "protect ocean ecology, address climate change and promote sustainable ocean economies.", "Woods"], "metric_results": {"EM": 0.5, "QA-F1": 0.5988704004329004}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8181818181818181, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-126", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-3617", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-2799", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5674", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-5273", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-2879", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-2200", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10411", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-4850", "mrqa_newsqa-validation-4168"], "SR": 0.5, "CSR": 0.5345799180327868, "EFR": 1.0, "Overall": 0.7061347336065573}, {"timecode": 61, "before_eval_results": {"predictions": ["\"King of Cool\"", "Rhode Island", "Lake Placid, New York", "the Dominican Republic", "on the north bank of the North Esk", "Bolshoi", "Franconia, New Hampshire", "Tie Domi", "Charice", "Ian Fleming", "17 December 1998", "30", "James William McCutcheon", "Nodar Kumaritashvili", "Aaron Lewis", "the Western Bloc ( the United States, its NATO allies and others )", "Havana Harbor", "Far Away", "Help!", "British and French Canadian fur traders", "Georgia", "1997", "Moscow, Russia", "Mary Elizabeth Patterson", "Hank J. Deutschendorf II", "Catherine of Aragon", "Upstairs Downstairs", "Ruth Ellis", "Batman & Robin", "CAPTCHA", "Manifest Destiny", "Rio de Janeiro", "Gary Oldman", "United States of America (USA)", "Mar del Sur", "Rats", "Poem The Borough", "Chinese President Hu Jintao", "ties", "the vicious brutality which accompanied the murders of his father and brother.\"", "saying Chaudhary's death was warning to management.", "Tsvangirai's party and church groups", "Manmohan Singh's", "southern port city of Karachi,", "could be secretly working on a nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,", "Ashley \"A.J.\" Jewell,", "Seoul,", "Sri Lanka,", "10 percent", "Reuben", "John Deere", "Sarai", "a coefficient", "Victor Hugo", "a pram", "Get Back", "Herman Wouk", "Generation X: Tales for an", "Linkin Park", "Jupiter", "a plug in", "between 1765 and 1783", "Mesopotamia", "Agra Cantonment - H. Nizamuddin Gatimaan Express"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6879750122189638}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.4, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.1818181818181818]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-941", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1083", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5968", "mrqa_triviaqa-validation-2427", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-15104", "mrqa_searchqa-validation-6028", "mrqa_searchqa-validation-12461", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-3416"], "SR": 0.609375, "CSR": 0.5357862903225806, "EFR": 1.0, "Overall": 0.7063760080645161}, {"timecode": 62, "before_eval_results": {"predictions": ["Romania", "George White's Scandals of 1931", "Poems : Series 1", "Jacqueline Bouvier", "Kansas", "1937", "French Canadian", "1997", "2014 Winter Olympics in Sochi, Russia", "Guantanamo Bay Naval Base", "Thomas Mundy Peterson", "Lindsey Buckingham", "Alex Drake", "Sicily", "john Walsh", "12", "Ford Motor Company", "us", "someone named Godot", "Sony Interactive Entertainment", "hansa", "Gibraltar", "titan", "t Tosca", "lee love", "1936", "Tommy Cannon", "Charles Reed Bishop", "D\u00e2mbovi\u021ba River", "400", "supernatural psychological horror", "Martin Truex Jr.", "848", "Wiseman's View", "Blender", "My Boss, My Teacher", "Grave Digger", "an \"unnamed international terror group\"", "voluntary manslaughter", "AS", "Boys And Girls alone", "Mitt Romney", "CNN", "Iowa's critical presidential caucuses", "Yemen,", "137", "North Korea", "Mike Meehan", "Government Accountability Office", "John Glenn", "the Bering Strait", "Natalie", "Massoud Behrani", "molasses", "Knocked Up", "upright", "the Rhine", "turkey wing", "feet", "grain", "xenon", "Kentucky", "1892", "Nana Patekar"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6889434523809523}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 0.24000000000000002, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-4203", "mrqa_triviaqa-validation-7555", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-1371", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-3584", "mrqa_newsqa-validation-3151", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-11966", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9465", "mrqa_hotpotqa-validation-4624"], "SR": 0.609375, "CSR": 0.5369543650793651, "EFR": 0.96, "Overall": 0.698609623015873}, {"timecode": 63, "before_eval_results": {"predictions": ["Saint Michael, Barbados", "Danish", "Brickyard", "Richard Arthur", "Blue Grass Airport", "France", "1872", "Philip Livingston", "General Sir John Monash", "Margarine Unie", "July 8, 2014", "Dragons: Riders of Berk", "Trent Alexander-Arnold", "Humpty Dumpty and Kitty Softpaws", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "erosion", "assemble a stable, protective protein shell to protect the genome", "depending on the gender of the reigning monarch", "1923", "The weekly Torah portion", "Oklahoma native Major General Clarence L. Tinker", "Daren Maxwell Kagasoff", "the courts", "16", "Beorn", "red eye logo", "columbus", "9", "April", "d\u00fcsseldorf", "adventure", "Anne Boleyn", "Patrick Troughton", "isosceles", "tosca", "vienna", "umbrella", "The Drug Enforcement Administration said Wednesday it's considering tighter restrictions on propofol,", "scoliosis, or abnormal curving of the spine", "NASCAR", "the Defense of Marriage Act", "these deviant young men", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "The Tinkler.", "Ninety-two", "daily flights of its EC-130J Commando Solo aircraft", "sculptures", "as many as 250,000", "The two leaders also discussed the creation of a back channel for direct communication outside normal diplomatic protocols,", "The Masonic Manual", "poppy seeds", "charles", "Paganini", "The Untouchables", "Hinduism", "Maryland", "Don Quixote", "opera buffa", "Ezra Pound", "ponies", "Homicide: Life on the Street", "Baku", "About Eve", "carbohydrates"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6831717302305538}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.11764705882352941, 0.7058823529411764, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.23076923076923078, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-2577", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1640", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-6810", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-12690", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14562", "mrqa_searchqa-validation-8778", "mrqa_triviaqa-validation-6234"], "SR": 0.609375, "CSR": 0.5380859375, "EFR": 1.0, "Overall": 0.7068359375}, {"timecode": 64, "before_eval_results": {"predictions": ["srsy", "Anne Boleyn", "poland", "holliday", "Batan", "backgammon", "Denver", "judy holliday", "Adam\\'s apple", "Jemima Potts", "Brazil", "raw hides", "gift of the Gab", "Cheryl Campbell", "a crust of mashed potato", "colonists of the Thirteen Colonies who rebelled against British control during the American Revolution", "Donald Fauntleroy Duck", "Wednesday, May 24, 2017", "a mashed potato crust", "Matthew Gregory Wise", "Magnavox Odyssey", "a beer brewed", "Taron Egerton", "Mankombu Sambasivan Swaminathan", "Western Australia", "Oahu", "Kathleen O'Brien", "John \"John\" Alexander Florence", "Jean-Marie Pfaff", "John Lennon/Plastic Ono Band", "Parapsychologist Konstant\u012bns Raudive", "1894", "Ben R. Guttery", "Singapore", "A major avalanche", "Pensacola", "Christopher Lloyd Smalling", "$10 billion", "nearly 28 years", "500 feet down an embankment into heavy brush,", "Defense of Marriage", "19", "up three", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "45 minutes, five days a week.", "\"It got me thinking about what I would want to do when I got out of the game. Most people don't realize how fast the time goes, but I turned those questions on myself.\"", "The minister later apologized, telling CNN his comments had been taken out of context.", "Caylee Anthony's", "10 to 15 percent", "zero", "claymore", "an elk", "Solidarity", "Christopher Columbus", "Shalom", "Finnish", "the lead villain", "The Last Chance: Roads of Freedom", "a diamond", "a comedy", "a magnet", "Palatine Hill", "a cow", "a bassoon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6033482142857143}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.8, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-6752", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-6100", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-8298", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-4490", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-4883", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3561", "mrqa_searchqa-validation-780", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-16034", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-13757"], "SR": 0.453125, "CSR": 0.5367788461538461, "EFR": 1.0, "Overall": 0.7065745192307692}, {"timecode": 65, "before_eval_results": {"predictions": ["over 300,000", "DeWayne Warren", "Warren Hastings", "Colman", "Dougie MacLean", "Anna Muttathupadathu", "1960", "Carlos Alan Autry", "to the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "A simple majority", "Jenny Slate", "western Cuba", "It was held in England and Wales from 1 June to 18 June 2017", "cotswolds", "evonne goolagong Cawley", "ferisbee", "heavy bomber", "beetles", "gillis Grafstr\u00f6m", "cyclops", "John Constable", "lyon", "The Apprentice", "hippocampus", "scotland", "filibuster and scathing rhetoric", "Stephen Ireland", "The United States presidential election of 2016", "ten", "Kaley Cuoco", "lady", "The Washington Post", "Sippin' on Some Syrup", "film playback singer, director, writer and producer", "27 January 1974", "1848", "Patton Oswalt", "Michael Krane,", "Jaime Andrade", "Larry Ellison,", "15-year-old", "severe", "breast cancer.", "10,000 refugees,", "Marines fighting a dug-in Taliban force.", "drug cartels", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "16th grand Slam title.", "Katherine", "david thoreau", "Woo", "Afghanistan", "Bizarro", "Catherine", "violot", "Jawaharlal Nehru", "the Beringia", "crunching", "Rights of Man", "a witch", "frequency", "Toni Morrison", "Type A", "V Vampire"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5174148478835978}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.962962962962963, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.1111111111111111, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-8944", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-4955", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2525", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1154", "mrqa_searchqa-validation-15446", "mrqa_searchqa-validation-2935", "mrqa_searchqa-validation-11036", "mrqa_searchqa-validation-15729", "mrqa_searchqa-validation-14525", "mrqa_searchqa-validation-10278", "mrqa_searchqa-validation-16147", "mrqa_searchqa-validation-3848"], "SR": 0.421875, "CSR": 0.5350378787878788, "EFR": 0.972972972972973, "Overall": 0.7008209203521704}, {"timecode": 66, "before_eval_results": {"predictions": ["around 2011", "1956", "in a counter clockwise direction around the Sun", "2018", "Olivia Olson", "1987", "a humid subtropical climate, with hot summers and mild winters", "agriculture", "The 2017 -- 18 UEFA Champions League knockout phase", "eleven", "Meri", "Jesus'birth", "Kyla Pratt", "Nile", "orangutan", "Emeril Lagasse", "one", "solitaire", "Bono", "George Herbert Walker Bush", "cuckoo", "a region of SW Asia between the Tigris and Euphrates rivers", "Jasper Fforde", "Paris", "The Isle of Man", "United States Marine Corps", "Rhode Island School of Design", "Ector County", "Major League Soccer", "Gatwick Airport", "Sam Raimi", "J. K. Rowling", "a personalized certificate", "Colin Vaines", "31 October 1783", "Salvatore \"Salvie\" Testa", "fifth", "Amstetten,", "Tuesday afternoon.", "Sheik Mohammed Ali", "Marie-Therese Walter.", "Michael Jackson", "the reality he has seen is \"terrifying.\"", "a facility in Salt Lake City, Utah,", "his past and his future", "27-year-old's", "38,", "Ameneh Bahrami", "Spc. Megan Lynn Touma,", "the ecliptic", "33.4%", "Jacob", "a proverbial forest", "Turner Classic Movies", "Atlas", "Libya", "Adolf Hitler", "Brazil", "Donna Summer", "Ralph Lauren", "Colorado", "is God-sent,\" Masechaba Hine said Wednesday from her small home in gritty Soweto township.", "outlet mall", "natural disasters"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6253605769230769}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-7227", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-1805", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2289", "mrqa_hotpotqa-validation-1982", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-906", "mrqa_searchqa-validation-15537", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-536", "mrqa_searchqa-validation-8381", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-9653", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-268"], "SR": 0.53125, "CSR": 0.5349813432835822, "EFR": 1.0, "Overall": 0.7062150186567164}, {"timecode": 67, "before_eval_results": {"predictions": ["coercivity", "branch roots", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "a Midwestern theater owner named Glen W. Dickson", "James Corden", "often linked to high - ranking ( though not necessarily royalty ) in China", "four", "Hellenismos", "1878", "Randy Newman", "Game 1", "Pangaea", "1800s", "walker", "Orion", "joseph callaghan", "germany", "Brazil", "Tomorrow Never Dies", "Caracas", "magnetic", "Madagascar", "radiotelephony", "caucasus", "cats", "motor ships", "Arlo Looking Cloud", "Francis Egerton", "Max Kellerman", "Rothschild", "10 June 1921", "Scotiabank Saddledome", "\"Slaughterhouse-Five\"", "Indian", "Socrates", "1882", "National Lottery", "two remaining crew members", "a deceased organ donor,", "The Wall Street Journal Europe", "Alwin Landry's supply vessel Damon Bankston", "about 62,000", "Sgt. Barbara Jones", "Salafist", "\"Body Works\"", "Afghanistan,", "the body of the aircraft", "hank Moody", "between 1917 and 1924", "diamonds", "the Schoolyard", "chicken pot pie", "Sweet Home Alabama", "the black market", "the NHL", "Tara Reid", "Y", "Mickey Hargitay", "Germany", "the United States", "Moses", "Kafka", "compost", "Inferno"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6015625}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.3333333333333333, 1.0, 0.6, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-6811", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-4601", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-6541", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-5604", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2732", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-10433", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-9375", "mrqa_searchqa-validation-13487", "mrqa_searchqa-validation-11335", "mrqa_searchqa-validation-16220"], "SR": 0.515625, "CSR": 0.5346966911764706, "EFR": 0.967741935483871, "Overall": 0.6997064753320683}, {"timecode": 68, "before_eval_results": {"predictions": ["Fran\u00e7ois Hollande", "South Pacific", "soka gakkai", "UPS", "november", "east", "twenty-one", "Malawi", "adolphe (Adolphe) Adam", "cubed", "lamb", "patience", "Sandi Toksvig", "in the vascular bundles which are often arranged like beads on a necklace forming an interrupted ring inside the stem", "Coton in the Elms", "the government - owned Panama Canal Authority", "Sir Donald Bradman", "`` Killer Within ''", "The Vamps", "Profit maximization", "Nalini Negi", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "The vas deferens is connected to the epididymis above the point of blockage", "John Goodman", "Eddie Murphy", "Morocco", "Croatian", "Kurt Vonnegut", "\"Shqiptar\u00ebt e Malit t\u00eb Zi\"", "Comodoro Arturo Merino Ben\u00edtez International Airport,", "Mike Mills", "1620", "250 million", "\" Terry the Tomboy\"", "Umina Beach", "Taeko Ikeda", "Chevy", "are bartering -- trading goods and services without exchanging money,", "General Motors'", "The Kirchners", "undergoing a double mastectomy and reconstructive surgery,", "Robert Mugabe", "Val d'Isere, France.", "free laundry service.", "\"theoretically\"", "33-year-old", "was depressed", "federal ocean planning.", "one-shot", "(Daisy) Baker", "Katharine McPhee", "the Eiffel Tower", "a brick", "(Casey) Stengel", "Hodgkin\\'s disease", "( Marcia) Clark", "cement", "John Madden", "november", "hubris", "Tunisia", "Friday", "1967", "French Canadians"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6041742979242979}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2488", "mrqa_triviaqa-validation-5420", "mrqa_triviaqa-validation-2237", "mrqa_triviaqa-validation-1044", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-986", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5152", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1593", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-14255", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-15325", "mrqa_hotpotqa-validation-5518"], "SR": 0.546875, "CSR": 0.5348731884057971, "EFR": 1.0, "Overall": 0.7061933876811594}, {"timecode": 69, "before_eval_results": {"predictions": ["the Black Russian", "the San Francisco Chronicle", "Honolulu", "Thomas Paine", "the grey heron", "Universal Studios", "coal", "a (spinning) top", "a cave", "raisins", "Finding Nemo", "a catalog", "the Mariachi", "in 1837", "2010", "the South Pacific Ocean", "Brooklyn, New York", "Achal Kumar Jyoti", "dorsally on the forearm", "Darren McGavin", "orbit", "the Speaker of the House of Representatives", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "energy loss", "October 29, 2015", "wirral", "sugar baby love", "step by step", "Wisconsin", "stephen fry", "stephen Strauss", "islands in the south east", "18-litre", "January 1971", "French", "stephen fry", "King Edward III", "Four Weddings and a Funeral", "Los Angeles", "Karl Johan Schuster", "elderships", "Rose Byrne", "The Division of Cook", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "1894", "Walt Disney Productions", "KlingStubbins", "Bambi: Eine Lebensgeschichte aus dem Walde", "Australian", "Arsene Wenger", "two women", "two years,", "the child might still be alive,", "their take on several popular cars.", "a series of wildfires", "Jeddah, Saudi Arabia,", "$50", "root out terrorists within its borders.", "protective shoes", "J.G. Ballard,", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "Jerry Lee Lewis", "April 29, 2009", "Terry Reid"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6467731829573935}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.47619047619047616, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-12214", "mrqa_searchqa-validation-15232", "mrqa_searchqa-validation-11195", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-3440", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-913", "mrqa_triviaqa-validation-4594", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-4257", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-522"], "SR": 0.5625, "CSR": 0.5352678571428571, "EFR": 0.9642857142857143, "Overall": 0.6991294642857142}, {"timecode": 70, "UKR": 0.71875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7268", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15232", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7236", "mrqa_searchqa-validation-7369", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10102", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1794", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2133", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-5197", "mrqa_squad-validation-5410", "mrqa_squad-validation-551", "mrqa_squad-validation-5592", "mrqa_squad-validation-5721", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-6988", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7751", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8042", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8575", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8767", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9732", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2986", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4448", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5114", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.7890625, "KG": 0.46171875, "before_eval_results": {"predictions": ["Garden of Eden", "Quebec", "acceleration", "Roger Federer", "Roald Dahl", "Legally Blonde", "Voyager 1", "a glow worm", "the Naval Academy", "the Renaissance", "Halle Berry", "a prism schism", "the Jesuit", "a premalignant flat ( or sessile ) lesion of the colon", "John Joseph Patrick Ryan", "Actor Jason Lee", "February 14, 2015", "homicidal thoughts of a troubled youth", "Hodel", "1560s", "supervillains who pose catastrophic challenges to the world", "the Alamodome and city of San Antonio", "Bobby Beathard", "Gupta Empire", "caused by chlorine and bromine from manmade organohalogens", "oscar VIII", "ovid", "\"Ain't No Mountain High Enough\"", "Restless Leg Syndrome", "Hindenburg", "Sicily", "dirham", "henley royal Regatta", "six", "ellisia", "hand gun", "pickwick", "Sun Belt Conference", "47", "1994\u201395", "Manchester", "Ronnie Schell", "musicologist", "Lochaber, Highland, Scotland", "Motorised quadricycle", "field of science", "PlayStation 4", "a classification of cars that are larger than a subcompact car but smaller than a mid-size car,", "Elton John", "Steven Chu", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Jimi Hendrix and Janis Joplin,", "one", "22", "August 19, 2007.", "Russia", "southwestern", "Nigeria,", "an animal tranquilizer,", "Barnes & Noble", "American soldiers held as slaves by Nazi Germany during World War II.", "in a canyon in the path of the blaze", "75", "Angola"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6282271241830065}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.2222222222222222, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9530", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-8777", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-654", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2342", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-3993", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1959", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3610"], "SR": 0.53125, "CSR": 0.5352112676056338, "EFR": 0.9333333333333333, "Overall": 0.6876151701877935}, {"timecode": 71, "before_eval_results": {"predictions": ["28,", "him to step down as majority leader.", "\"card check\"", "July in the Philippines", "off the coast of Dubai", "45 minutes, five days a week.", "Monica Majumdar", "This will be the second", "ethnic Somalis", "Asashoryu", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "a national telephone survey", "Bridgestone Invitational", "Charles Dickens's novel Oliver Twist", "Longline fishing", "eight", "in the 1960s", "The eighth", "September of that year", "Lee Freedman", "the American League ( AL ) champion Cleveland Indians", "the North Atlantic Ocean", "September 1959", "a young girl", "May 2016", "stained glass", "axe", "whist", "hot peppers", "anteros", "2000", "Naomi Watts", "Patrick McGoohan", "The Passage of the Red Sea", "New Zealand", "Arctic Monkeys", "Peter Sellers", "Montagues and Capulets", "its variety of shops", "Italian", "Arizona", "Australia", "44", "LA Galaxy", "World Championship Wrestling", "Apatosaurus", "the Emancipation Proclamation", "raw materials", "June 1975", "the Nightingale", "deuterium", "the Oregon Trail", "asteroids", "a well", "The Aston Martin", "Earth shoes", "Dizzy", "Louise", "Richmond", "the Marathon", "Cairo", "H CO", "Effy", "the Korean Republic Won"], "metric_results": {"EM": 0.484375, "QA-F1": 0.58157693001443}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.2222222222222222, 0.8, 0.0, 0.7272727272727272, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-2809", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6987", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-1732", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-4644", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-1001", "mrqa_searchqa-validation-1545", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-4873", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-9260"], "SR": 0.484375, "CSR": 0.5345052083333333, "EFR": 0.9393939393939394, "Overall": 0.6886860795454546}, {"timecode": 72, "before_eval_results": {"predictions": ["Max Martin and Shellback", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "1976", "head of the Cabinet of Bluhme II", "Puli Alam", "The Kennedy Center", "Nevada", "Marshal of France", "Wiltshire, in the south west of England", "Polish Army", "Miami Gardens, Florida", "Captain", "Adam Karpel", "Paul", "Karen Gillan", "775", "Charles Lyell's Principles of Geology", "the band Cheap Trick", "Experimental neuropsychology", "card verification code ( CVC )", "Bill Ray", "2 %", "red lead primer and a lead - based topcoat", "Watson and Crick", "medieval realism", "Kyle Lafferty", "spiral", "apples", "baulk", "zanzibar", "cork", "(Margaret) Court", "henna", "Bugsy Malone", "Will Smith", "(Dans) Defoe", "1918", "(Rambosk)", "misdemeanor assault charges", "Brazil", "can play an important role in Afghanistan as a reliable NATO ally. The question is: How can", "he was released Friday and taken to the Australian embassy in Bangkok,", "one Iraqi soldier,", "12-1", "Del Potro.", "AS Roma beat Lecce 3-2", "Casalesi Camorra clan", "1969", "London", "a drink", "a skunk", "(Dan Morrison)", "(John) Lennon", "(Peter) Connolly", "an owl", "the Panama Canal", "the Washburn camp loop", "lace", "a saber-tooth cat", "Wall Street", "Versailles", "2009", "British", "Palm Sunday celebrations"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5144288003663003}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.09523809523809523, 0.0, 0.0, 0.25, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.9, 0.5, 1.0, 0.6666666666666666, 0.33333333333333337, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2578", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-2902", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-8654", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-4235", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-6859", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-4812", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-5302", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-3666", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-1927", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-259"], "SR": 0.359375, "CSR": 0.5321061643835616, "EFR": 0.975609756097561, "Overall": 0.6954494340962245}, {"timecode": 73, "before_eval_results": {"predictions": ["The Virgin Queen", "Sue Taylor Grafton", "4,972", "The 5 foot 9 inch tall twins", "July 8, 2014", "Julia Kathleen McKenzie", "the sarod", "Asif Kapadia", "Taylor Swift", "February 1914", "Jack St. Clair Kilby", "Scotiabank Saddledome", "Balvenie Castle", "the Bulgarian 1st Army", "pigs", "The ulnar collateral ligament of elbow joint", "outlaw motorcycle club", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Carlos Alan Autry Jr.", "Nick Kroll", "New York University", "lift", "the Ravi River", "James Madison", "at the hour of death or in the presence of the dying", "26 miles", "george british rock star", "Robert De Niro", "de Havilland moth", "dame kiri te Kanawa", "Hokkaido", "the Shard", "Episode III: Revenge of the Sith", "russia", "holly", "Jaguar Land Rover", "3DS", "Silvio Berlusconi.", "Mumbai", "the United States", "a level of autonomy that will allow them to protect and preserve their culture, religion and national identity.", "Isabella", "Monday and Tuesday", "15-year-old", "two", "Lana Clarkson", "customers are lining up for vitamin injections that promise", "Joan Rivers", "Mugabe's opponents", "the Dolphins", "Zombies", "Detroit", "December", "India", "3", "cable cars", "the Constitution", "Wings of Desire", "the Process Art", "World War I", "declension", "(Ken) Russell", "Sadat", "the Washington State Ferry"], "metric_results": {"EM": 0.5, "QA-F1": 0.5844618055555555}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 0.25, 1.0, 0.8, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.25, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-133", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3902", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-2730", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-2245", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-2984", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-8984", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-11000", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-11444"], "SR": 0.5, "CSR": 0.5316722972972974, "EFR": 1.0, "Overall": 0.7002407094594595}, {"timecode": 74, "before_eval_results": {"predictions": ["Walter Reed Army Medical Center", "102,984", "Nickelodeon", "848", "northwestern Georgia", "Cushman", "Mark Neveldine and Brian Taylor", "Harold Edward Holt", "Michael Crawford", "Fort Valley, Georgia", "Netflix", "Edward R. Murrow", "Chiltern Hills", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "to oversee the local church ( 1Tim 1 : 3 and Titus 1 : 5 )", "Michael Rooker", "the trinitarian formula", "Roman Reigns", "May 2010", "supported modern programming practices and enabled business applications to be developed with Flash", "South Asia", "Rajendra Prasad", "Ariana Clarice Richards", "President Yahya Khan", "Colon Street", "Roman legions", "Arkansas", "photographer", "peterloo massacre", "p\u00e9tain", "robinson's brewery", "gretzky", "mustowd", "Jim Peters", "sewing machines", "small faces", "Mike Tyson", "Michael Bloomberg", "Ross Perot", "depression", "the Employee Free Choice act", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.", "suppress the memories and to live as normal a life as possible;", "southern port city of Karachi,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "supermodel", "Bollywood", "16", "Steven Gerrard", "Dugong", "Buffalo", "Peter Sellers", "Herod", "American Samoa", "Chuck Berry", "Students for a Democratic Society", "petits fours", "Smallville", "Mike Nichols", "a volcano", "15", "the eye", "mustota", "benevento"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7350603070175439}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.06666666666666667, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-3484", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1703", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-3882", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-12306", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-6581"], "SR": 0.6875, "CSR": 0.53375, "EFR": 1.0, "Overall": 0.70065625}, {"timecode": 75, "before_eval_results": {"predictions": ["Bologna", "Big Ben", "The Bronx", "14", "beef", "a jump ball", "(Gerhard) Mercator", "not", "car", "natural selection", "the Communist Party", "forgive", "the University of Exeter", "the New York Yankees", "C\u03bc and C\u03b4", "his mother's side of the family, the Campbells, led by their grandfather Samuel", "orogenic belt", "butane", "James Hutton", "The Pittsburgh Steelers", "the Ming dynasty", "29 March 2009", "Daniel A. Dailey", "The Maginot Line", "the eurozone", "Cato", "david robinson", "carthaginian", "Andr\u00e9s Iniesta", "Joy Division", "Tony Manero", "canada", "henry november", "henry robinson", "12", "Dublin", "Barack Obama", "Major League Soccer", "$7.3 billion", "Michael Tippett", "Sutton Hoo helmet", "Art Bell", "Valhalla Highlands Historic District", "19th and early 20th centuries", "37", "Central Avenue", "Laura Jeanne Reese Witherspoon", "The 2015 Baylor Bears football team", "Alistair Grant", "sleet, freezing drizzle or rain.", "Saturday", "his native Philippines", "renew registration", "1975", "9-week-old", "raping and killing a 14-year-old Iraqi girl.", "Workers'", "two", "glamour", "246", "36", "carver", "henley royal regatta", "otto von Bismarck"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6590148133116882}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-10897", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-250", "mrqa_naturalquestions-validation-1162", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-497", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-4882", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-2049", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-334", "mrqa_triviaqa-validation-5331"], "SR": 0.59375, "CSR": 0.5345394736842105, "EFR": 0.9230769230769231, "Overall": 0.6854295293522268}, {"timecode": 76, "before_eval_results": {"predictions": ["Ben Johnston", "Tempo", "New Hampshire", "Francis Nethersole", "Bill Walton", "The Little Match Girl", "Finding Nemo", "Wu-Tang Clan", "Debbie Reynolds", "Mark \"Chopper\" Read", "a city in north-east Lithuania", "Aly Raisman", "Harry Booth", "to manage the characteristics of the beer's head", "American dystopian science fiction action thriller film written, directed, and produced by Andrew Niccol", "in the mid - to late 1920s", "Cee - Lo", "Vicente Fox", "Nebuchadnezzar", "Renhe Sports Management Ltd", "defense against rain rather than sun", "rubidium - 85", "in 1967, Celtic became the first British team to win the competition", "forested parts of the world", "the Warring States period", "My Fair Lady", "Basketball", "can be either collagen, elastic, or reticular fibers", "Lady Gaga", "hen", "Antarctica", "longchamp", "London Bridge", "Laura Solon", "James Warren Jones", "Ottorino Respighi", "Cyndi Lauper", "almost 100", "one", "one", "Peshawar", "the job bill's controversial millionaire's surtax,", "Muslims", "No 4, the highest ever position", "Matthew Fisher", "Laura Ling and Euna Lee", "snow,", "Basel", "he knew the owner of the home,", "kryptonite", "The Sixth Sense", "Mao Zedong", "tea", "Dang'rous", "D", "Andy Warhol", "Rocky", "the American Government", "the Vietnam War", "Australia", "frostbite", "he hosted a short - lived talk show in WCW called A Flair for the Gold", "helps scientists better understand the spread of pollution around the globe", "the Pir Panjal Range"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6248100857475858}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307692, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.7272727272727273, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.8, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.6]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3970", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-6109", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5068", "mrqa_triviaqa-validation-3615", "mrqa_triviaqa-validation-1368", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4264", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-14003", "mrqa_searchqa-validation-900", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-1848"], "SR": 0.515625, "CSR": 0.5342938311688312, "EFR": 0.9354838709677419, "Overall": 0.6878617904273147}, {"timecode": 77, "before_eval_results": {"predictions": ["the Code of Harry", "Mahalia Jackson", "WD-40", "a Bunsen burner", "King", "Cuba", "holiday", "Gatsby", "a cheese", "the Black Sea", "Prime Time", "New Mexico", "The Walking Dead", "Michael Rosen", "the notion that an English p Parsons may'have his nose up in the air ', upturned like the chicken's rear end", "Gamora", "Sir Hugh Beaver", "king Gautamiputra Satakarni", "Francisco Pizarro", "September 27, 2017", "Real Madrid", "Paradise, Nevada", "The flag of Hungary", "1857", "Richard Masur", "banjo", "Philippines", "cripples", "volcanoes", "South Africa", "wandlet of fire", "billowing skirt", "Wilhelm", "cattle", "Pierre Laval", "phone", "jim smith", "1935", "Premier League", "Orson Welles", "Washington metropolitan area", "Christopher Tin", "cruiserweight title", "the reigning monarch of the United Kingdom", "The Ethics of Ambiguity", "Kaep", "Brett Eldredge", "Benjam\u00edn", "Dolly Records", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics", "\"Draquila -- Italy Trembles.\"", "Michael Schumacher", "South Africa's", "\"He helped thousands to grow up and be productive citizens.\"", "new DNA evidence", "The nation's congress,", "Bill Stanton", "Natalie Cole", "11,", "strawberry", "public opinion in Turkey", "San Jose, California", "Audrey II", "the Iraq War"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6157169117647059}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7058823529411765, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-9577", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-346", "mrqa_triviaqa-validation-2316", "mrqa_triviaqa-validation-1704", "mrqa_triviaqa-validation-1851", "mrqa_triviaqa-validation-1941", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-5438", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-2016", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-655", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1281", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-5583"], "SR": 0.53125, "CSR": 0.5342548076923077, "EFR": 0.9333333333333333, "Overall": 0.6874238782051283}, {"timecode": 78, "before_eval_results": {"predictions": ["Copenhagen", "BraveStarr", "The 7 Habits of Highly Effective People", "Picric acid", "Quentin Coldwater", "1940s and 1950s", "Brickyard", "Hong Kong", "Rabies", "M. Night Shyamalan", "You Can Be a Star", "1996 NBA Slam Dunk Contest", "Currer Bell", "the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere", "the libretto", "Chuck Noland", "the wife of King Willem - Alexander", "an exultation of spirit", "the senior-most judge of the supreme court", "Cyrus", "Hon July Moyo", "Bonnie Aarons", "the right side of the heart", "1984", "home on Chesapeake Bay, south of Annapolis", "braille", "tanzania", "blue", "henry kramer", "Turin", "taka", "pagan", "Kenya", "silks", "hawaii", "the Colossus of Rhodes", "Albania", "Old Trafford", "The station", "Ben Freeth's", "South Africa.", "Les Bleus", "Canada.", "closed on 366 for eight wickets on the opening day.", "his sentencing was delayed three weeks so he could get some dental work done, including removal of his diamond-studded braces.", "Kurt Cobain's", "Pakistan's", "\"Body Works\"", "businessman", "thirteen", "English", "Ghost Recon", "Spanish", "Peter Shaffer", "Twin-lens reflex camera", "the Biggest Bird", "The Count of Monte Cristo", "Picasso", "David H. Petraeus", "Dreamgirls", "the Lincoln Memorial", "powerful anesthetic and sedative.", "the release of the four men", "Michael Schumacher"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7603794642857143}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1032", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-610", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-225", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-3510", "mrqa_searchqa-validation-766", "mrqa_searchqa-validation-13704", "mrqa_searchqa-validation-12666"], "SR": 0.703125, "CSR": 0.5363924050632911, "EFR": 1.0, "Overall": 0.7011847310126582}, {"timecode": 79, "before_eval_results": {"predictions": ["Ted Bundy", "2017", "David Naughton", "June 1975", "September 26, 2010", "The Prodigy", "Gal\u00e1pagos", "119 minutes", "Woizero Aselefech Wolde Hanna", "James Weldon Johnson", "Manhattan Project", "Green Chair", "17", "2013", "1961", "Andy Serkis", "January 1, 1976", "Alex Ryan", "Massachusetts", "neuropsychology", "10 May 1940", "Lewis Carroll", "Washington metropolitan area", "2017", "the Victoria Cross", "Peter Siddle", "six-pocket", "david mitchell", "Big Ears", "Hawaii", "Diego Velazquez", "the Comitium", "8", "magic", "giannina arangi-Lombardi", "24", "viscount mandeville", "\"The Rosie Show,\"", "Trevor Rees-Jones,", "in a remote part of northwestern Montana", "15-year-old's", "skeletal dysplasia,", "reached an agreement late Thursday to form a government of national reconciliation.", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Juan Manuel Santos,", "30", "\"procedure on her heart,\"", "Louela Binlac", "Chinese", "Tallahassee", "Punxsutawney", "Madonna", "the Prius", "Anja Prson", "the Cannoli", "Los Angeles", "Andrew Jackson", "Charlie Sheen", "Minginmanser", "Jeanette Rankin", "Theodore Roosevelt", "Trinity College", "Peppers", "Harvard"], "metric_results": {"EM": 0.5, "QA-F1": 0.5864281400966184}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913042, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0909090909090909, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-5882", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-5695", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1433", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-9856", "mrqa_searchqa-validation-15319", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-12923", "mrqa_searchqa-validation-510"], "SR": 0.5, "CSR": 0.5359375, "EFR": 1.0, "Overall": 0.7010937500000001}, {"timecode": 80, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1814", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7645", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.841796875, "KG": 0.48125, "before_eval_results": {"predictions": ["while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "the \u01c0Xam people", "Pakistan", "$1.06 trillion", "Tigris and Euphrates rivers", "December 1972", "18", "Scar's henchmen", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "Howard Caine", "Buddhism", "Roman Reigns", "Tennessee Whiskey", "Joan Rivers", "fish", "400 miles from London to Edinburgh", "the South Saskatchewan River", "placebo", "three", "gary atheron", "Hispaniola,", "flowers", "Pillar", "tartan", "University of Georgia", "the Qin dynasty", "CBS", "South America", "Milwaukee Bucks", "Prince Ioann Konstantinovich of Russia", "Selinsgrove,", "The Tempest", "Los Angeles", "Pacific Place", "Franconia, New Hampshire", "small family car", "\"We are a nation of Christians and Muslims, Jews and Hindus", "Sri Lanka's", "motor scooter", "United Arab Emirates", "Derek Mears", "President Obama's", "Hamas", "Sea World in San Antonio,", "four military officers", "Six members of Zoe's Ark", "15,000", "\"Quiet Nights,\"", "Eli Lilly", "Austen", "the zebra", "the Charleston", "turkey", "the eighth episode of the third season", "The Prince and the Pauper", "a taximeter", "Toby Keith", "metre", "Oahu", "Thor Heyerdahl", "lesser island", "Calcium carbonate", "nutty"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6371685606060606}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-4205", "mrqa_triviaqa-validation-6892", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-5714", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-4145", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-16239", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-143", "mrqa_triviaqa-validation-598", "mrqa_triviaqa-validation-1488"], "SR": 0.578125, "CSR": 0.5364583333333333, "EFR": 0.9259259259259259, "Overall": 0.6973206018518519}, {"timecode": 81, "before_eval_results": {"predictions": ["bowie atherian", "The Apprentice", "egrement", "mercury", "othello", "fungi", "wigan Warriors", "a book", "Pontiac Silverdome", "hogmanay", "acetone", "Virgil", "The Battle of the Three Emperors", "a Roman Catholic and fan of The Godfather Part II ( 1974 )", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "the Four Seasons", "ideology", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "a cross between Minecraft and Left 4 Dead", "Rent's script", "1773", "the St. Louis Cardinals", "Meri", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "1986", "Telugu", "Paris", "Tampa Bay Lightning", "Melville, NY, USA", "11", "Argentine", "Comodoro Arturo Merino Ben\u00edtez International Airport, Santiago, Chile", "\"Ready Player One\"", "\"Up There Down There\"", "\"R- Point\"", "a lack of any perceptible change in an adult female", "331", "two-state solution", "12 hours", "allegations that a dorm parent mistreated students at the school.", "any abuse that occurred in his diocese.", "\"I've come here at times and I've met people who were paying remembrances to their loved ones.", "Revolutionary Armed Forces of Colombia,", "two", "southern Bangladesh,", "a violent government crackdown", "Liverpool,", "education", "Illinois Reform Commission", "3 centimeters", "Gianni Versace", "a supper", "Wings", "Ivica Zubac", "salmon", "a gate", "Saskatchewan", "monkeys", "a monsoon", "Marie Antoinette", "Google", "Montreal", "left - sided heart failure", "the brain and spinal cord"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5930394324207919}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.29508196721311475, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.16666666666666666, 0.06896551724137931, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-1630", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-9487", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-1343", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-1652", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-4598", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-12769", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-6702", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7342"], "SR": 0.484375, "CSR": 0.5358231707317074, "EFR": 0.9696969696969697, "Overall": 0.7059477780857355}, {"timecode": 82, "before_eval_results": {"predictions": ["a heart", "Space Cadet", "anvil", "Judy Garland", "Miranda", "Absinthe", "time", "Belize", "Ban Ki-moon", "surface-to-air missile", "Babe Ruth", "improvisation", "the Sons of Liberty", "1834", "Ra\u00fal Eduardo Esparza", "Hellenism", "Leonardo da Vinci", "five", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "domestication of the wild mouflon in ancient Mesopotamia", "Panic! at the Disco", "John Smith", "Lakshwadweep -- Kavaratti", "American country music duo The Bellamy Brothers", "4.37 light - years", "my favorite Martian", "tunisia", "Colette", "Monopoly", "paulrhyn castle", "Margaret Beckett", "6", "goose green", "horseradish", "roman", "tepuis", "paul Krugman", "21st Century Fox", "at the Wanda Metropolitano", "67,575", "USS Essex", "Towards the Sun", "Japan", "7 February 14786", "1981 World Rowing Championships", "a valuation method detailed by Warren Buffett in 1986", "2 March 1972", "Angel Parrish", "Buck Owens", "Five of the injured", "Eintracht Frankfurt", "authorizing killings and kidnappings by paramilitary death squads.", "84-year-old", "serving its fast burgers in the Carrousel du Louvre, an underground shopping mall", "not", "Larry Ellison,", "Ameneh Bahrami", "France's famous Louvre", "Carol Browner", "CNN's", "10 years", "bart Mendoza and Steve Thorn", "1768", "antelopes"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7780731877771352}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8421052631578948, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10740", "mrqa_searchqa-validation-133", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7589", "mrqa_triviaqa-validation-3580", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-3700", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-3091", "mrqa_hotpotqa-validation-83", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-3264", "mrqa_triviaqa-validation-4939"], "SR": 0.71875, "CSR": 0.5380271084337349, "EFR": 1.0, "Overall": 0.712449171686747}, {"timecode": 83, "before_eval_results": {"predictions": ["Woods", "MDC head Morgan Tsvangirai.", "\"wildcat\" strikes,", "Lana Clarkson", "Wednesday.", "Russia", "two weeks after Black History Month", "Cain's", "Thirteen people", "more than 1.2 million people.", "Madeleine K. Albright", "travel four hours to reach a government-run health facility that provides her with free drug treatment.", "government", "$2.187 billion", "his guilt in killing the bird", "gastrocnemius", "August 15, 1971", "Tropical cyclones", "never made", "northernmost point on the Earth, lying diametrically opposite the South Pole", "1977", "the President", "on a sound stage in front of a live audience in Burbank, California", "a crown cutting of the fruit, possibly flowering in five to ten months and fruiting in the following six months", "HTTP / 1.1", "treaty of Waitangi", "basketball", "glucose", "Moldova", "Rosetta", "Tiananmen", "Ghana", "rhodwell", "archer", "Byker Grove", "malted barley", "stained glass", "energy trading", "English", "2006", "her translation of and commentary on Isaac Newton's book \"Principia\"", "Portsea", "two Nobel Peace Prizes", "Taeko Ikeda", "Salford", "15,000 people", "Christy Walton", "25", "Adrian Lyne", "Ted Turner", "\"What a joy to breathe the balmy air of Grosvenor Square\"", "tartar", "Cheddar", "Alexander Pope", "travel guide", "Herod", "bone marrow transplant", "Joe Montana", "Jewelry", "Duster", "The Daily Eye Hemorrhage", "March 15, 1945", "Parlement de Bretagne", "2"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5563727250957855}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0689655172413793, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-4068", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-3389", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-4148", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-5288", "mrqa_searchqa-validation-13464", "mrqa_searchqa-validation-5315", "mrqa_searchqa-validation-15829", "mrqa_searchqa-validation-7789", "mrqa_searchqa-validation-12451", "mrqa_searchqa-validation-15363", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-7021"], "SR": 0.453125, "CSR": 0.5370163690476191, "EFR": 1.0, "Overall": 0.7122470238095239}, {"timecode": 84, "before_eval_results": {"predictions": ["360", "Paul Monti, whose son, Medal of Honor recipient Jared, was killed in Afghanistan while trying to save a fellow soldier", "Geothermal gradient", "high rates of inflation", "Procol Harum", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "September 1973", "to prevent further offense", "American Indian allies", "Star Wars", "euro", "the right to be served in facilities which are open to the public", "243 days", "turkey", "peter Townsend", "19", "head", "aragonite", "jon perti Smith", "cogs", "rosetta", "South Africa", "helium", "Superman", "rosetta crow", "romantic drama", "Volvo 850", "Clara Petacci", "season three", "Critics' Choice Television Award", "the back portion", "\"the Docile Don\"", "North Dakota", "magnas", "Mark Dayton", "No. 60", "public", "fight back against Israel in Gaza.", "Mother's Garden of Verses", "Gov. Jan Brewer.", "Dube, 43, was killed", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place,", "lost his job as the supermarket chain he worked for cut staff.", "Nairobi, Kenya,", "taking on greenhouse gas emissions.", "Derek Mears", "the captain of a nearby ship", "84-year-old", "Texas and Oklahoma", "Three Mile Island", "rosetta", "Charles I Lake Vostok", "William Armstrong", "Milwaukee", "Darkwing Duck", "The Hunt for Bat Boy", "Tablecloth", "neurons", "fibre optics", "John Ford", "Latin", "Liverpool", "the sun", "Exodus"], "metric_results": {"EM": 0.34375, "QA-F1": 0.46411519240052673}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.8387096774193548, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5333333333333333, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.13793103448275862, 0.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1822", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-4919", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-406", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1016", "mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-5958", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-3091", "mrqa_searchqa-validation-15550", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-13197"], "SR": 0.34375, "CSR": 0.5347426470588235, "EFR": 1.0, "Overall": 0.7117922794117647}, {"timecode": 85, "before_eval_results": {"predictions": ["red", "seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "Mason Alan Dinehart", "red, white, and blue", "O'Meara", "12.65 m", "two", "methyl bromide ( MeBr )", "Super Bowl", "Bear McCreary", "Sir Henry Cole", "in the first RFC published on the UDP protocol ( RFC 675 : Internet Transmission Control Program, December 1974 )", "IX", "Dick Advocaat", "jean", "daughter", "Stieg Larsson", "marilla", "mulberry", "Tina Turner", "ad nausea", "cereal", "Operation Overlord", "York", "Angela", "Tom Shadyac", "pornographicstar", "The Weeknd", "Walldorf", "the Beatles", "Alfred Preis", "the Czech Kingdom", "third", "Elliot Fletcher", "over $1 million", "Marika Nicolette Green", "Javed Miandad", "Sunday,", "\"Nude, Green Leaves and Bust\"", "Africa", "Florida's Everglades,", "Olivia Newton-John", "Empire of the Sun", "1959,", "Marine", "the Southern California glam-rocker Adam Lambert", "\"both films, like the Brown novels they're based on, have been met with criticism for their melding of history and storytelling.", "immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "Bahrain", "Forrest Gump", "Achilles", "Bright, Precious Days", "Timex", "landfills", "Harvard", "James Bond", "Butterflies", "Sybil", "A Canticle for Leibowitz", "John Harvard", "Julia Roberts", "queens", "Johnny Mathis", "the Commonwealth Scientific and Industrial Research Organisation (CSIRO)"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6681164257262171}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.6086956521739131, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.8, 0.0, 1.0, 0.3, 0.07692307692307691, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.19354838709677416, 0.5405405405405405, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.923076923076923]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-1198", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-381", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-1932", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-4298", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-3227", "mrqa_triviaqa-validation-3052"], "SR": 0.5625, "CSR": 0.5350654069767442, "EFR": 1.0, "Overall": 0.7118568313953488}, {"timecode": 86, "before_eval_results": {"predictions": ["741 weeks", "the order of Vice President", "Exodus 20 : 1 -- 17", "Massachusetts", "ancient Athens", "Kevin Spacey", "6 March 1983", "innermost in the eye", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Hellenic Polytheistic Reconstructionism", "Shaquille O' Neal", "September 8, 2017", "1989 album Sleeping with the Past", "julia Andrews", "1875", "Theo Walcott", "bobby brown,", "nickel shrimpton", "wheel", "Philippines", "adventure game", "Virginia", "crossword puzzle clue", "a song", "nausea", "Kim Sung-su", "Mathew Sacks", "Sandusky, Ohio", "Amway", "Joseph I", "Reinhard Heydrich", "\"What's My Line?", "Woodsy owl", "Andr\u00e9 3000", "Big Bad Wolf", "Mandarin Airlines", "American pharmaceutical company", "autonomy.", "around 10:30 p.m. October 3,", "as soon as 2050,", "British Prime Minister Gordon Brown's", "Unseeded Frenchwoman Aravane Rezai", "humans", "businessman", "The son of Gabon's former president", "If a person can live here by myself, some of the late converts to digital depend on television for information and companionship.", "President Sheikh Sharif Sheikh Ahmed", "22", "\"There is no immediate decision pending on resources,", "Robert Anthony \" Tony\" Snow", "Frank Sinatra", "Auschwitz-Birkenau", "garlic", "radius", "Rogers & Hammerstein", "\"The One And Only Joe Carroll\"", "a quadrant", "Earhart", "a Halloween", "LADY BIRD JOHNSON", "a turban", "Times Square", "Immanuel Kant", "Australia"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5511916035353535}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 0.22222222222222224, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-2467", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-1473", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-3158", "mrqa_triviaqa-validation-4834", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-12", "mrqa_searchqa-validation-12870", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-5992", "mrqa_searchqa-validation-3710", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-13347"], "SR": 0.46875, "CSR": 0.5343031609195402, "EFR": 1.0, "Overall": 0.7117043821839081}, {"timecode": 87, "before_eval_results": {"predictions": ["Prince Igor", "conchita wurst", "Jessica Simpson", "is the third largest in the Netherlands,", "passeidon", "vito corleone", "London", "peter townshend", "Thailand", "sports agent", "Easter", "antelope", "role-playing", "non-voters", "the Gospel of Matthew", "cordate", "Service / Crown personnel serving in the UK or overseas in the British Armed Forces or with Her Majesty's Government", "Ron Harper", "a section of the Torah ( Five Books of Moses ) used in Jewish liturgy during a single week", "2014", "Haliaeetus", "Kevin McKidd", "usually in May", "Uralic", "$66.5 million", "12 countries", "Ice Princess", "1939", "Joanna No\u00eblle Levesque", "Paris", "The Division of Cook", "24 hours a day", "its air-cushioned sole (dubbed \"Bouncing Soles\")", "August 28, 1774", "alcoholic drinks", "twenty-three episodes", "TD Garden", "an older generation", "Diego Milito's", "Ricardo Valles de la Rosa,", "Summer", "a skilled hacker", "Manmohan Singh's", "more than 4,000 commercial farmers", "development of two courses on the Black Sea coast in Bulgaria.", "Christopher Columbus", "sovereignty over them.", "flipped and landed on its right side,", "183 people,", "\"Cold Mountain\"", "Texas A&M", "\"Fargo\"", "Charlotte", "Reno", "sostenuto", "Hammurabi", "Ohio State", "Hoosier", "the Rhine", "veterans", "San Francisco", "Teen Titans Go!", "Love", "number 5"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6405231829573934}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.7619047619047621, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 0.631578947368421, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3242", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2399", "mrqa_hotpotqa-validation-5868", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-2377", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-780", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-14831"], "SR": 0.5625, "CSR": 0.5346235795454546, "EFR": 0.9642857142857143, "Overall": 0.7046256087662338}, {"timecode": 88, "before_eval_results": {"predictions": ["almost entirely in Wake County, it lies just north of the state capital, Raleigh", "`` house edge ''", "John Adams", "Contemporary history is the span of historic events from approximately 1945 that are immediately relevant to the present time", "1995", "Kansas", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "September 9, 2010", "Isthmus of Corinth", "awarded to the team that lost the pre-game coin toss", "San Antonio, Texas", "Peter Andrew Beardsley MBE", "Steve Russell", "japan", "Atlanta", "Jan van Eyck", "mausoleum", "trade union", "catai", "From Russia with Love", "blackfriars", "Christian Dior", "my fair Lady", "human rights", "isle of man", "Pamela Chopra", "December 19, 1998", "Black Swan", "Westley Sissel Unseld", "a minor basilica", "Hampton University", "American", "Dr. Nefarious", "Interstate 95", "\"Nebo Zovyot\"", "1993", "James, Duke of York", "It takes place in an unlikely spot: a 2,700-acre sanctuary", "almost 9 million", "US Airways Flight 1549", "Kenneth Cole", "Hussein's Revolutionary Command Council.", "two bodies out of the plant,", "motor scooter", "an average of 25 percent", "Antonio Maria Costa,", "through a facility in Salt Lake City, Utah,", "Nineteen", "hardship for terminally ill patients and their caregivers,", "An out-of-print book", "repent", "Vespa", "\"NUN\"", "In 1869", "on or directly in front of the pitching rubber", "the Red Sea", "Dauphin", "Ulysses S. Grant", "Oakland Raiders", "shawl", "the tongue", "break up ice jams.", "Haiti.", "police dogs"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6552320075757576}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0909090909090909, 0.5, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.7272727272727273, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-1999", "mrqa_triviaqa-validation-5417", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3219", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-908", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-4967", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-414"], "SR": 0.546875, "CSR": 0.5347612359550562, "EFR": 1.0, "Overall": 0.7117959971910113}, {"timecode": 89, "before_eval_results": {"predictions": ["Ricardo Valles de la Rosa,", "Katherine Jackson,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "\"Let it Roll: Songs by George Harrison\"", "theft", "Kgalema Motlanthe,", "It's just going to become part of the fabric of the fashion imagery of pop culture,", "complicated", "Henrik Stenson", "executive director of the Americas Division of Human Rights Watch,", "the refusal or inability to \"turn it off\"", "12", "surgical anesthetic propofol", "Mandarin", "`` women and children are vulnerable to violence because of their unequal social, economic, and political status in society", "the Colony of Virginia", "April 1, 2016", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "Munich", "U + 002A * Asterisk", "Kirstjen Nielsen", "British R&B girl group Eternal", "Cyndi Grecco", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "about 3.5 mya", "Markus Aemilius Lepidus", "salsa", "mmorpgs", "palm sunday", "Nowhere Boy", "Cascade Range", "corvidae", "zulu", "Buddhist", "1943", "steel", "hones", "2012", "Nick Cassavetes", "Sam Phillips", "National Basketball Development League", "Frank Fertitta, Jr.", "2,627", "life insurance", "Kareena Kapoor Khan", "Hopeless Records", "1971", "Floyd Nathaniel \"Nate\" Hills", "the 2012 Summer Olympics", "Anzio", "Lawrence Taylor", "cvicus", "Mr. Fred Rogers", "Hillary Rodham Clinton", "Texas", "occipital lobe", "the divine right of kings", "Uranus", "cauliflower", "kettledrum", "the Reform Movement", "a pesticide", "The Office", "Elmore Leonard"], "metric_results": {"EM": 0.5, "QA-F1": 0.5980925324675325}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.5, 0.04761904761904762, 0.6, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-843", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-6968", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-6501", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-3067", "mrqa_hotpotqa-validation-3831", "mrqa_hotpotqa-validation-3787", "mrqa_searchqa-validation-4669", "mrqa_searchqa-validation-11910", "mrqa_searchqa-validation-14695", "mrqa_searchqa-validation-10083", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-5328", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-8375", "mrqa_searchqa-validation-926"], "SR": 0.5, "CSR": 0.534375, "EFR": 1.0, "Overall": 0.71171875}, {"timecode": 90, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.787109375, "KG": 0.44453125, "before_eval_results": {"predictions": ["David Joseph Madden", "Garbi\u00f1e Muguruza", "state", "1,000", "9.7 m", "Andy Serkis", "Andrew Garfield", "cut off close by the hip, and under the left shoulder", "Ali Daei", "star", "in February 2017 in Japan and in March 2018 in North America and Europe", "Nala", "Germany", "a dove", "james b Boyd", "copper", "Coke", "Joan Rivers", "croquet", "coelacanth", "mel Blanc", "germany kinnock", "Ken Platt", "peter Sellers", "shilie marie", "Edmonton, Alberta", "Chelsea", "Indiana", "Christina Ricci", "Treaty of Gandamak", "Newell Highway", "Sports Illustrated", "Lieutenant Colonel Iceal Hambleton", "1730", "1903", "Oregon Ducks football", "Nye County", "Mugabe's opponents", "28", "$81,880", "six", "Sabina Guzzanti", "It has never been, and never will be, the policy of Total to discriminate against British companies or British workers.", "400 farmers", "U.S. District Judge Ricardo Urbina", "heavy flannel or wool", "Chester Stiles,", "a nuclear weapon", "Malcolm X", "\"It's My Party\"", "the Haunted Mansion", "the Moon", "the White Mountains of California", "China", "the caldera", "the Book of Judges", "Thomas", "Edgar Rice Burroughs", "the Seine", "Toy Story", "(Samuel) Morse", "4", "a relic", "Kansas State"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7306175595238096}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-5299", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3657", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14567", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-2316", "mrqa_searchqa-validation-7599", "mrqa_searchqa-validation-13576"], "SR": 0.640625, "CSR": 0.5355425824175823, "EFR": 1.0, "Overall": 0.6897647664835164}, {"timecode": 91, "before_eval_results": {"predictions": ["16th century", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Wyatt and Dylan Walters", "the liver", "Coton", "1975", "2010", "12.65 m", "French Canadian", "counter clockwise", "272", "Narendra Modi", "The United States presidential line of succession", "Chesney Wold", "jimmy boyd", "tony blair", "Harmolodic", "Judges 16", "Islam", "Steve Biko", "lyon bd jr", "coldplay", "sow", "Donald Trump", "jimmy boyd", "King of Hanover", "Central Park", "over 281", "Craig William Macneill", "evangelical Christian periodical", "Prussia", "Dutch", "Bangkok, Thailand", "Springfield, Massachusetts", "1983", "Sam Kinison", "\"Race Through New York Starring Jimmy Fallon", "a hunting party of three men,", "Tetris,", "Mary Phagan,", "Brett Cummins,", "$40 and a bread.", "Al-Aqsa mosque", "Aung San Suu Kyi", "Former U.S. soldier Steven Green", "June 2004", "Michelle Obama", "Los Angeles.", "\"Oprah is an angel, she is God-sent,\"", "ice hockey", "the Birch-tree", "achates", "PachelbeJ", "Graceland", "Castle Rock", "North Korea", "Wrigley", "Daytona", "a general officer", "Inuit", "the optimist", "Panama Canal", "cobalt", "J.R. Tolkien"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5185031114718615}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.21428571428571427, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-5537", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-5512", "mrqa_triviaqa-validation-1438", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-6795", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-618", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-3803", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-12632"], "SR": 0.421875, "CSR": 0.5343070652173914, "EFR": 1.0, "Overall": 0.6895176630434783}, {"timecode": 92, "before_eval_results": {"predictions": ["mitosis", "Detective Superintendent Dave Kelly", "2015", "Lana Del Rey", "Wyatt `` Dusty '' Chandler ( George Strait )", "126", "Benzodiazepines", "U.S. service members who have died without their remains being identified", "due to Parker's pregnancy at the time of filming", "Malvolio", "James Corden", "in hopes of securing enough Southern votes in the Electoral College", "`` Psychomachia, '' an epic poem written in the fifth century", "Salman Rushdie", "robben island", "guo Verde", "john jimmy", "flowers", "Bjorn Borg", "Romania", "Baton Rouge", "Prince Edward", "john jarin", "December", "harrods", "Polka", "Ford Falcon", "Vernon Kay", "third baseman and shortstop", "1926", "Irish Chekhov", "Adrian Peter McLaren", "London", "bioelectromagnetics", "Dunlop", "Pakistan", "Hampton University", "in 1994", "leftist rebels", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Serie A", "a crocodile", "A New York City crackdown on suspects allegedly involved in forged credit cards and identity theft", "Bush administration", "Gen. Stanley McChrystal,", "Ignazio La Russa", "581 points", "near the George Washington Bridge,", "CNN's", "James Bond", "Wisconsin", "John Madden", "50 First Dates", "the Left Bank", "Khrushchev", "Newport", "South Carolina", "Groundhog Day", "1914", "Cairo", "E", "good character", "Stalybridge Celtic", "written for \"The New York Times\" and \"Popular Mechanics\", and is a regular contributor to various CNBC shows such as \"On the Money\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.7230610994397759}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.823529411764706, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.21428571428571425, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-191", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-5698", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-277", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1729", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-10237", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-5211"], "SR": 0.640625, "CSR": 0.5354502688172043, "EFR": 0.9130434782608695, "Overall": 0.6723549994156147}, {"timecode": 93, "before_eval_results": {"predictions": ["romantic comedy", "Richard Strauss", "Bow River", "is widely believed to have been the helmet of King R\u00e6dwald of East Anglia", "gull-wing", "12", "Dutch", "40 Days and 40 Nights", "Las Vegas", "commander", "Paulo Bartolo", "green and yellow", "Skegness", "season seven", "Speaker of the House of Representatives", "A 30 - something man ( XXXX )", "from the Anglo - Norman French waleis", "dense regions within molecular clouds in interstellar space", "chairman ( more usually now called the `` chair '' or `` chairperson '' )", "Harlem River", "Simon Callow", "Prafulla Chandra Ghosh", "May 5, 1904", "based on sovereign states", "portal tomb", "printed circuit", "mercury franklin", "cricket", "doe", "left book club", "giant", "baryon number", "gary", "Ghana", "Tony Blair", "freemen", "Camellia sinensis", "peanuts, nuts, shellfish and fish", "NASCAR.", "Asashoryu", "many Marines we talked to in this coastal, scrub pine-covered North Carolina base", "between Israel and Hezbollah.", "Thabo Mbeki,", "retired Navy F-14", "The Sopranos", "three empty vodka bottles,", "William Randolph Hearst.", "Four", "education", "A Beautiful Mind", "Reader's Digest", "John Wesley", "Okilly Dokilly", "40-Year-Old Virgin", "William Faulkner", "Chocolate", "President Clinton", "Canada", "The Sound of Silence", "Nadia Comneci", "General Lafayette", "one", "Melbourne", "\"Let it Roll:"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6710336538461539}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 0.7692307692307693, 0.11111111111111112, 1.0, 1.0, 0.6, 0.3333333333333333, 0.07692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-3056", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-2398", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6635", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3390", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-1090", "mrqa_searchqa-validation-11066", "mrqa_searchqa-validation-15122"], "SR": 0.5625, "CSR": 0.5357380319148937, "EFR": 0.8928571428571429, "Overall": 0.6683752849544072}, {"timecode": 94, "before_eval_results": {"predictions": ["Jello Biafra", "1910", "full-sized nameplates", "secondary school", "three", "Celtic", "Prince of Cambodia Norodom Sihanouk", "Kevin Smith", "Kansas", "My Cat from Hell", "February 12, 2014", "novelist", "Bergen County", "$2.187 billion", "Whiskey Shivers as Saddle Up", "India", "Super Bowl LII", "Ludacris", "1939", "Barbara Windsor", "Universal Pictures and Focus Features", "normally show IIII for four o'clock", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Guy Berryman", "The ladies'single figure skating competition of the 2018 Winter Olympics", "California", "cowles", "Leonardo da Vinci", "river Severn", "strychnine", "Hercule Poirot", "Colombia", "zina and Herzegovina", "Hans Lippershey", "Il Divo", "Omid Djalili", "the cow", "response by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Casa de Campo International Airport", "a depth of about 1,300 meters in the Mediterranean Sea.", "Michelle Rounds", "in the 1950s,", "the southern city of Naples", "entertainment venues", "Lafayette Square", "north-south highway", "a suicide bomber", "10.", "six months.", "the orbiter", "Morse code", "My Fair Lady", "Indiana Jones", "the ulnar nerve", "John Updike", "the Bay of Bengal", "lm", "(the) Boehner", "The Maltese Falcon", "William Blake", "Troy", "(the) Pringles can", "negligence", "a hat"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5753527586248175}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.29629629629629634, 0.7692307692307693, 0.9411764705882353, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-1269", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2743", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-3094", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2345", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-4133", "mrqa_newsqa-validation-273", "mrqa_searchqa-validation-9025", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-12343", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-11460", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-14791"], "SR": 0.46875, "CSR": 0.5350328947368421, "EFR": 1.0, "Overall": 0.6896628289473684}, {"timecode": 95, "before_eval_results": {"predictions": ["6,396", "23 July 1989", "a fictional character in the \"Star Wars\" franchise", "James Mitchum", "Hampton's hump and Hampton's line", "right-hand", "Mika H\u00e4kkinen", "Todd McFarlane", "Republic of Ireland", "1942", "Germanic", "Marc Bolan", "Realty Bites", "amphetamines", "Louis XV's", "Steve Goodman", "Saddle Up", "Thomas Jefferson's", "Miami Heat", "1920", "Dirk Benedict", "1983", "November 5, 2017", "The vascular cambium", "Neil Young", "Venezuela", "Let It Snow!", "Madrid", "Lithium", "Tuesday", "Rajasthan", "apron", "mustard", "zephyr", "Dick Fosbury", "jimmy", "Peter Nichols", "The Kirchners", "Friday,", "vegan bake sales", "Muslim revolutionary named Malcolm X", "The iconic Abbey Road music studios made famous by the Beatles are not for sale,", "10", "Pacific Ocean territory of Guam", "\"prostitute\"", "Colombia.", "Rihanna", "dancing With the Stars.", "Kyra and Violet,", "Python", "pi", "Rio de Janeiro", "Chuck Yeager", "the tsuba", "fats", "Central Park", "Monica Lewinsky", "whales", "the Battle of Fort Donelson", "Bech", "Aaron Burr", "Dougie MacLean", "victim blaming", "2010"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7364211309523809}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-988", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2374", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-6789", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-4649", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11026", "mrqa_naturalquestions-validation-1856"], "SR": 0.6875, "CSR": 0.53662109375, "EFR": 0.95, "Overall": 0.67998046875}, {"timecode": 96, "before_eval_results": {"predictions": ["Shropshire Union Canal", "\"Big Fucking German\"", "American", "1982", "Harlem neighborhood", "Jeff Meldrum", "private", "gorillas", "Henry Lau", "Beckstein\u2019s television version of \"Cind Cinderella\"", "private liberal arts college", "Marktown, Clayton Mark's", "North Kesteven", "the German and UK Kennel Clubs", "An elevator with a counterbalance", "a castle during a ball", "Laura Jane Haddock", "David Gahan", "J.J. Thomson", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "chromosome 21 attached to another chromosome", "mashed potato", "in London's West End in 1986, and on Broadway in 1988", "Nicole DuPort", "Driving Miss Daisy", "dogs", "Margaret Thatcher", "Leicester", "jimmaine Jackson", "Wash", "poland", "rugby", "cartilage", "France", "Jean-Paul Sartre", "branson", "rolled over", "finance", "Eleven", "strangled his wife in his sleep while dreaming that she was an intruder", "hiring of hundreds of foreign workers", "Wednesday's", "golf", "Roberto Micheletti,", "Kuranyi's", "40 militants and six Pakistan soldiers", "an engineering and construction company", "July 18, 1994,", "A Tale of Murder", "Marx's Dictatorship of the Proletariat", "the bees", "Israel", "a bit", "One Flew Over the Cuckoo's Nest", "Danny Elfman", "arsenic", "Tchaikovsky", "salmon", "pitch", "Senegal", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Muqtada al-Sadr,", "two"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5978793513230192}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.3157894736842105, 0.5, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6875000000000001, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1038", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-6407", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-3219", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-7020", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-14560", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2036"], "SR": 0.484375, "CSR": 0.5360824742268041, "EFR": 1.0, "Overall": 0.6898727448453608}, {"timecode": 97, "before_eval_results": {"predictions": ["11 to 12 year old", "bronze medal in the women's figure skating final,", "the remaining rebel strongholds in the north of Sri Lanka,", "Ryder Russell,", "three-time road race world champion,", "Pakistan", "137", "over a kilometer (3,281 feet)", "Frank,", "Gadahn,", "The auction, in Devizes, southwest England,", "President Bush", "Dennis Davern,", "The Sun", "Joseph Heller", "in all cases affecting ambassadors, other public ministers and consuls, and those in which a state shall be party", "A monocot related to lilies and grasses", "Shakespearean actresses and car salespeople", "IBM", "Kida", "the heart", "fermenting dietary fiber into short - chain fatty acids ( SCFAs )", "over a 20 - year period", "absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "Institute of Chartered Accountants of India ( ICAI )", "radium", "aircraft", "Cole Porter", "calf", "gaseous", "Clint Eastwood", "yichang", "Arthur Ashe", "cribbage", "stand-up comedian", "jimmy", "yorkshire", "Rhode Island", "February 20, 1978", "Paige O'Hara", "London", "the Dukes of Westminster", "Bentley Twins", "2.1 million", "Saint Michael, Barbados", "\"Baa, Baa, Black Sheep\"", "155 ft tall", "both ABC1 and ABC2", "Pontins", "multiplication", "the FBI", "tannins", "the black bear", "the Me Me Me Generation", "Venus", "French", "The first Chinese Emperor", "The Wall Street Journal", "Mowgli", "Cyrano de Bergerac", "India", "The Hunchback of Notre Dame", "Mississippi", "Blackbeard"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5418867807539682}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true], "QA-F1": [0.0, 0.22222222222222224, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5625, 0.8571428571428571, 0.9166666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-1590", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-2309", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-1086", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2961", "mrqa_searchqa-validation-2295", "mrqa_searchqa-validation-6936", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10743"], "SR": 0.421875, "CSR": 0.5349170918367347, "EFR": 1.0, "Overall": 0.6896396683673469}, {"timecode": 98, "before_eval_results": {"predictions": ["hawaii", "Los Angeles", "jimmy seymour", "terriers", "Francois mitterrand", "phrenology", "Dublin", "hard Times", "john travolta", "arson", "IKEA", "the Colossus of Rhodes", "the Goddess of Revenge", "The Walking Dead", "the euro", "quarterback Brad Johnson", "Georges Auguste Escoffier", "The Sheriff of Trapingus County", "either Tagalog or English", "altitude", "prevent any contaminants in the sink from flowing into the potable water system by siphonage and is the least expensive form of backflow prevention", "Watson", "Sohrai", "1923", "ten", "40 Acres and a Mule Filmworks", "Lake Wallace", "Captain Beefheart & His Magic Band", "Nevada", "Suffolk", "atomic bomb", "the Americas", "the Rose Garden", "Marvel Comics", "the Marx Brothers", "Ramzan A\u1e8bmat-khant Ramzan", "number 1", "$40 and a loaf of bread.", "expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "an auxiliary lock", "Yellow Polka Dot Bikini.", "UNICEF", "Ali Larijani", "The individual named is Thamer Bin Saeed Ahmed al-Shanfari.", "during his short time in office,", "coalition troops", "Daniel Wozniak,", "hardship for terminally ill patients and their caregivers,", "the Arctic north of Murmansk down to the southern climes of Sochi", "Rudy Giuliani", "the jaguar", "Punch", "fasab", "an apple-tree", "The Little Prince", "Garfield", "repent, for the", "the drum", "Scott Smith", "Moscow", "tabula rasa", "March 2016", "159", "in New York City"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6704521263273893}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 1.0, 1.0, 0.8064516129032258, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5297", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-1985", "mrqa_hotpotqa-validation-4662", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2178", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-15462", "mrqa_searchqa-validation-11428", "mrqa_searchqa-validation-2609", "mrqa_searchqa-validation-3863", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7880"], "SR": 0.5625, "CSR": 0.5351957070707071, "EFR": 1.0, "Overall": 0.6896953914141414}, {"timecode": 99, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-350", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13792", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-14843", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6990", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9515", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2498", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3358", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-425", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5375", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-723", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7310", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.869140625, "KG": 0.48515625, "before_eval_results": {"predictions": ["dharma", "Dublin", "Pilgrim's Progress", "seaweed", "wren", "Pakistan", "south bank", "lily elsie", "South Pacific", "Massachusetts", "black", "Melbourne", "La Boh\u00e8me", "International Orange", "warm and is considered to be the most comfortable climatic conditions of the year", "Claudia Grace Wells", "Jules Shear", "a compiler", "after releasing Xander from the obligation to be Sweet's `` bride ''", "Herod", "\u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Florida", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "after the Irish War of Independence and the Anglo - Irish Treaty", "two", "American musical theater", "Guangzhou", "lower Manhattan", "GE Appliances", "The Keeping Hours", "The Life of Charlotte Bront\u00eb", "\"Five Psychopaths\"", "2007", "40th United States president", "Girls' Generation", "The Herald Angels Sing", "183", "Venus Williams", "Lifeway's 100-plus stores nationwide", "number of calls, and those calls were intriguing, and we're chasing those down now,\"", "Marie-Therese Walter.", "alcohol", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "56,", "A Colorado prosecutor", "11th year in a row.", "C. Britt Beemer,", "Thursday", "King Arthur's court", "horse", "The Bravados", "Cessna", "China", "South Africa", "Alien", "Goose Gossage", "comic-Con", "Sephora", "Fletcher Christian", "Latter-day Saints", "Doctor Dolittle", "Over the Rainbow", "tea"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6649914963455017}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.8235294117647058, 1.0, 0.7368421052631579, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5690", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-1911", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-1749", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-716", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-3858", "mrqa_searchqa-validation-2348", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-9449", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-15555"], "SR": 0.546875, "CSR": 0.5353125000000001, "EFR": 1.0, "Overall": 0.716984375}]}