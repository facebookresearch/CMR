{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5390, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["positive divisors", "quietist/non-political", "Jonathan Stewart", "surface condensers", "Anglo-Saxons", "one of the first peer-to-peer network architectures", "Tanzania", "structure", "ABC Cable News", "Turner and Vernon", "$2 million", "German-language publications", "-40%", "BBC 1", "the \"blurring of theological and confessional differences in the interests of unity.\"", "pamphlets on Islam", "mad dogs", "Mnemiopsis", "both Kenia and Kegnia", "electricity", "student tuition, endowments, scholarship/voucher funds", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "European Council", "826", "1999", "Latin", "semantical problems", "$2 million", "committee", "(trunnion", "South Pacific", "Spanish moss", "1850s", "Abercrombie was recalled and replaced by Jeffery Amherst", "saturating them unconsciously with electricity", "slightly more than normal sea-level O2 partial pressure", "Associating forces with vectors", "showmanship", "social networking support", "Children of Earth", "Soviet", "Brock Osweiler", "San Diego", "Economist", "liquid", "Jerricho Cotchery", "suggested it for use in the ARPANET", "disrupting their plasma membrane", "Genghis Khan", "Robert Boyle", "feigned retreat", "Rotterdam", "the problem of multiplying two integers", "he was illiterate in Czech", "the Monarch", "4.7 yards per carry", "Sports Programs, Inc.", "only pharmacists", "ideological", "behavioral and demographic data", "Kuviasungnerk/Kangeiko", "94", "October 16, 2012", "transportation, sewer, hazardous waste and water"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7783752554812338}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.11111111111111112, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-4676", "mrqa_squad-validation-10430", "mrqa_squad-validation-5326", "mrqa_squad-validation-2291", "mrqa_squad-validation-1530", "mrqa_squad-validation-7086", "mrqa_squad-validation-8412", "mrqa_squad-validation-2478", "mrqa_squad-validation-3590", "mrqa_squad-validation-1913", "mrqa_squad-validation-3771", "mrqa_squad-validation-6293", "mrqa_squad-validation-1766", "mrqa_squad-validation-1187", "mrqa_squad-validation-288"], "SR": 0.734375, "CSR": 0.734375, "EFR": 0.9411764705882353, "Overall": 0.8377757352941176}, {"timecode": 1, "before_eval_results": {"predictions": ["1929", "the lack of a Parliament of Scotland", "small islands", "US$10 a week raise", "A fundamental error", "Horace Walpole", "any object can be, essentially uniquely, decomposed into its prime components", "1968", "straight", "complexity classes", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "80%", "leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "Informal rule", "seven", "petroleum", "1671", "the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "the headwaiter", "a comb jelly", "expositions", "1784", "terrorist organisation", "\"winds up\" the debate by speaking after all other participants.", "the Golden Gate Bridge", "Hulu", "National Galleries of Scotland", "Northern Rhodesia (today Zambia)", "Budapest", "Joanna Lumley", "Gateshead", "tentacles", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "East Smithfield burial site in England", "Jerome Schurf", "The arrival of satellite television", "we are neither making maximum effort nor achieving results necessary if this country is to reach a position of leadership.", "Isaac Newton", "dangerous enemies", "Robert Underwood Johnson", "the A13, Brenner Autobahn, en route to Italy", "kinetic friction", "X-rays", "Roger NFL", "Abu al-Rayhan al-Biruni", "British", "Spreading throughout the Mediterranean and Europe", "almost a month", "\"cellular\" and \"humoral\"", "traditional old boy network", "anti-Semitic policies", "the Scottish Government", "the Lisbon Treaty", "emerging market", "Bible", "24\u201310", "cellular respiration", "the lion, leopard, buffalo, rhinoceros, and elephant", "computer problems", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "From Russia", "Balvenie Castle", "Geological evidence shows that this 5,000-mile mountain chain may extend south into Antarctica", "Anne Noe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6966562757783831}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 1.0, 0.0, 0.0, 0.10810810810810811, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1290322580645161, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9525", "mrqa_squad-validation-2704", "mrqa_squad-validation-3113", "mrqa_squad-validation-6759", "mrqa_squad-validation-739", "mrqa_squad-validation-9465", "mrqa_squad-validation-8342", "mrqa_squad-validation-6614", "mrqa_squad-validation-4902", "mrqa_squad-validation-10410", "mrqa_squad-validation-4332", "mrqa_squad-validation-85", "mrqa_squad-validation-9732", "mrqa_squad-validation-4856", "mrqa_squad-validation-2497", "mrqa_squad-validation-9488", "mrqa_squad-validation-3516", "mrqa_squad-validation-8278", "mrqa_newsqa-validation-911", "mrqa_naturalquestions-validation-1802", "mrqa_triviaqa-validation-1415", "mrqa_searchqa-validation-187", "mrqa_hotpotqa-validation-3155"], "SR": 0.640625, "CSR": 0.6875, "EFR": 0.9130434782608695, "Overall": 0.8002717391304348}, {"timecode": 2, "before_eval_results": {"predictions": ["V", "estimated 16,000 to 35,000", "second", "phagocytes", "Jochi", "Alsace", "West Lothian question", "Wiesner", "representatives elected to either house of parliament", "become more integral within the health care system", "trial division", "August 2004", "(sworn brother or blood brother)", "warships", "if they were non-discriminatory, \"justified by imperative requirements in the general interest\" and proportionately applied", "lower sixth", "2002", "Organizational", "the number of social services that people can access wherever they move", "cytokines", "an individual in the form of a postman or tax collector whose hand hits the wood", "civil disobedience", "eighteenth century", "glaucophyte", "jellyfish", "existing level of inequality", "well before Braddock's departure for North America", "the means of production by a class of owners", "Hughes Hotel", "Golden Gate Bridge", "Annual Status of Education Report", "Six", "the Pauli exclusion principle", "1 September 1939", "Mexico", "Battle of Dalan Balzhut", "the relative units of force and mass then are fixed", "Russell T Davies", "Innate", "1903", "photosynthesis", "private research university", "article 49", "wine", "Arabic numerals", "application of electricity", "bilaterians", "risen with increased income inequality", "life on Tyneside", "student-teacher relationships", "external combustion engines", "that each side is capable of performing the obligations set out", "the Russian defense ministry said Wednesday.Russia's Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "\"wipe out\" the United States if provoked", "located almost entirely in Wake County, it lies just north of the state capital, Raleigh", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "the last 32 FA Cup places club by club for the inter-war period  1919-20 to 1938-39.", "communion", "Robert Noyce", "Eliot Spitzer", "a toast to this heir whose support of local pubs got him dubbed \"Beer Drinker of the Year 2002\"", "this Native American rite of passage or of spiritual renewal often includes fasting", "The chain on the big chain ring, going for it", "the Augustan Age"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7638558201058201}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.962962962962963, 0.4166666666666667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9697", "mrqa_squad-validation-6404", "mrqa_squad-validation-8909", "mrqa_squad-validation-3378", "mrqa_squad-validation-6970", "mrqa_squad-validation-7514", "mrqa_squad-validation-10428", "mrqa_squad-validation-7201", "mrqa_newsqa-validation-3489", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3648", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-2265", "mrqa_hotpotqa-validation-1174", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-12652", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-4178"], "SR": 0.71875, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["New Orangery", "parish churches", "Michael Mullett", "Tibetan Buddhism", "North American", "phagosome", "Smalcald Articles", "begging his son to return home", "1788", "The Rankine cycle", "Several thousand", "Get Carter", "source of most of the chemical energy released", "the college", "bones", "NFL Commissioner Roger Goodell", "purple skin patches", "Apollo 17", "1562", "cilia", "by qualified majority", "Blum complexity axioms", "the Diffie\u2013Hellman key exchange", "America's Funniest Home Videos", "16", "seven-layer OSI-compliant networking protocol", "the Vosges Mountains", "0 \u00b0C (32 \u00b0F)", "May 1754", "infected corpses", "2002", "Australia's first public packet-switched data network", "even greater inequality", "Association of American Universities", "economic utility in society from resources devoted on high-end consumption", "cut in half", "uncertain", "1835", "720p high definition", "mainline Protestant Methodist denomination", "comb-bearing", "Tate Britain", "CBS", "Treaty of Rome 1957 and the Maastricht Treaty 1992", "M. Theo Kearney", "offering a higher wage the best of their labor", "Lenin", "Prince Houston", "the Mayor of the City of New York", "Vishal Bhardwaj", "Congress passed the Chinese Exclusion Act in 1882 which targeted a single ethnic group by specifically limiting further Chinese immigration", "routing protocols", "The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "Antoine Lavoisier", "CeeLo Green", "Ted Stillwell", "Zelaya", "Chinese nationals", "Evan Bayh", "wandins", "Anthony Powell and John Betjeman", "a type of large cushion", "rural California", "World leaders"], "metric_results": {"EM": 0.671875, "QA-F1": 0.783649781679673}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.4, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4347826086956522, 0.5, 0.8666666666666666, 1.0, 1.0, 0.0, 0.4, 0.5, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1146", "mrqa_squad-validation-3530", "mrqa_squad-validation-83", "mrqa_squad-validation-4074", "mrqa_squad-validation-4675", "mrqa_squad-validation-2914", "mrqa_squad-validation-4840", "mrqa_squad-validation-7502", "mrqa_squad-validation-7300", "mrqa_squad-validation-545", "mrqa_hotpotqa-validation-5344", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-1676", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-1834", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-7746"], "SR": 0.671875, "CSR": 0.69140625, "retrieved_ids": ["mrqa_squad-train-12632", "mrqa_squad-train-19599", "mrqa_squad-train-78320", "mrqa_squad-train-15167", "mrqa_squad-train-31159", "mrqa_squad-train-44243", "mrqa_squad-train-36835", "mrqa_squad-train-66328", "mrqa_squad-train-8553", "mrqa_squad-train-32095", "mrqa_squad-train-61280", "mrqa_squad-train-37740", "mrqa_squad-train-67281", "mrqa_squad-train-60375", "mrqa_squad-train-11878", "mrqa_squad-train-57086", "mrqa_naturalquestions-validation-1802", "mrqa_squad-validation-1530", "mrqa_squad-validation-6970", "mrqa_squad-validation-4902", "mrqa_squad-validation-9575", "mrqa_triviaqa-validation-6170", "mrqa_squad-validation-10428", "mrqa_squad-validation-288", "mrqa_squad-validation-1187", "mrqa_squad-validation-739", "mrqa_triviaqa-validation-1415", "mrqa_squad-validation-6404", "mrqa_squad-validation-9488", "mrqa_squad-validation-6293", "mrqa_hotpotqa-validation-3155", "mrqa_squad-validation-3516"], "EFR": 1.0, "Overall": 0.845703125}, {"timecode": 4, "before_eval_results": {"predictions": ["9\u201318", "Norman Foster", "Battle of Hastings", "9 October 2006", "a \"lifeboat\" in the event of a failure of the command ship", "Robert R. Gilruth", "hermaphroditism and early reproduction", "Moscone Center", "generally poor French results in most theaters of the Seven Years' War in 1758", "1994", "patients' prescriptions and patient safety issues", "December 12", "June 6, 1951", "three", "John Wesley", "prohibitum", "21 January 1788", "Gryphon", "1859 and 1865", "ESPN Deportes", "1784", "LeGrande", "St. Lawrence", "technological superiority", "Golden Super Bowl", "TEU articles 4 and 5", "a Standard Model", "Westinghouse Electric", "Sayyid Abul Ala Maududi", "$5,000,000", "Mike Tolbert", "phagocytes", "glaucophyte", "the packets may be delivered according to a multiple access scheme", "Magnetophon tape recorder", "random access machines", "a trade magazine for the construction industry", "noisiest", "wars", "Ogr\u00f3d Saski", "Zygons", "Dorothy Skerrit", "Stratfor", "iCloud service will now be integrated into the iOS 5 operating system", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients", "0300", "1997", "they believed that it violated their rights as Englishmen to `` No taxation without representation ''", "Rent", "James Intveld", "buzzards", "Lorelei", "Salvador Dal\u00ed", "societies or amalgamations of persons", "Ricky Marco", "Frank Sinatra", "Little Richard", "Gerry Adams", "G Kessler", "Dobermann", "a hat", "An Osiris", "Frank Sinatra", "Amy & Chip"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7039615205056382}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.1111111111111111, 0.0, 1.0, 0.5490196078431372, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3806", "mrqa_squad-validation-10295", "mrqa_squad-validation-6877", "mrqa_squad-validation-5620", "mrqa_squad-validation-8643", "mrqa_squad-validation-718", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-3012", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-1403", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-265", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-6600"], "SR": 0.671875, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 5, "before_eval_results": {"predictions": ["atmospheric", "the constituting General Conference in Dallas, Texas", "central business district", "the third most abundant chemical element", "The mermaid", "Mick Mixon", "1992", "a course of study and lesson plan that teaches skills, knowledge and/or thinking skills", "Mansfeld", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "peptidoglycan", "the soul does not sleep (anima non sic dormit), but wakes (sed vigilat) and experiences visions", "15", "the port of Salerno", "84 hours", "Jordan Norwood", "Doritos", "four", "the American Revolutionary War", "the League of Nations", "the temperance movement", "Albany Congress", "the \"Eureka Stockade\"", "Giovanni Branca", "Tiffany & Co.", "NASA discontinued the manned Block I program", "Andy Warhol", "2011", "five", "The ability to make probabilistic decisions", "Divine Right of Kings", "teaching", "research, exhibitions and other shows", "case law by the Court of Justice", "the United States Census Bureau", "The Deadly Assassin", "91%", "Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code", "order", "a place for another non-European Union player in Frank Rijkaard's squad", "John McCain", "The National Telecommunications and Information Administration offered a program to help people buy converter boxes that make old TVs work in the new era", "a planned training exercise designed to help the prince learn to fly in combat situations", "1983", "Margaret Agnew - Somerville", "Woodrow Wilson", "noli me tangere", "during a game in 1988 while playing for the University of Pittsburgh ; Blue Edwards shattered a backboard during a Midnight Madness event while he was playing at East Carolina", "the Federal Reserve System", "General Motors", "the North Side", "polio", "The Spanish Armada", "Kinnairdy Castle", "the 2014\u201315 season", "Heathrow", "Wildhorn, Bricusse and Cuden", "the County Executive", "The Real Thing", "The Gettysburg Address", "a dandle board", "The Lion", "The Weekly Comebacker", "tiger's milk"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6264975733186251}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.08695652173913042, 0.8666666666666666, 1.0, 0.8571428571428571, 1.0, 0.0, 0.1764705882352941, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2675", "mrqa_squad-validation-3559", "mrqa_squad-validation-1877", "mrqa_squad-validation-2408", "mrqa_squad-validation-1061", "mrqa_squad-validation-7288", "mrqa_squad-validation-2961", "mrqa_squad-validation-3971", "mrqa_squad-validation-1824", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1283", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-1975", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3228", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-1203"], "SR": 0.546875, "CSR": 0.6640625, "EFR": 1.0, "Overall": 0.83203125}, {"timecode": 6, "before_eval_results": {"predictions": ["Wahhabism or Salafism", "Del\u00fc\u00fcn Boldog", "forceful taking of property", "220 miles", "jiggle TV", "On the Councils and the Church", "a Western Union superintendent", "mid-Eocene", "\"do not disturb\" sign", "bounding", "Maling", "water", "avionics, telecommunications, and computers.", "five", "not necessarily right", "Robert Boyle", "new and enlarged bridges, a shuttle service and/or a tram", "1997", "Presiding Officer", "Michelle Gomez.", "Laszlo Babai and Eugene Luks", "Wesleyan Holiness Consortium", "Aristotle", "1894", "average workers", "cholecalciferol", "1524\u201325", "religious", "Eight original series serials", "the historical era", "2011", "1665", "closure temperature", "light reactions", "Michael Krane", "the ireport form", "Les Bleus", "Movahedi", "a man's lifeless, naked body", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "10 logarithm of the molar concentration", "Emma Watson, Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "Sanchez Navarro", "Vincent Price", "Article Two", "2001", "Popeye", "the kelvin scale", "Giorgio Moroder", "Vladimir Putin", "Blue Riband", "Vietnam", "Atlantic Coast Conference", "Prince of Cambodia Norodom Sihanouk", "Walt Disney Feature Animation", "Constance M. Burge", "400", "heavy metal", "Gibbons v. Ogden", "Asakusa", "chicago", "drake", "Weehawken", "the Chesapeake Bay"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7751488095238095}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9592", "mrqa_squad-validation-4631", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1643", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6477", "mrqa_triviaqa-validation-3050", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1136", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-12205", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-12747", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-748"], "SR": 0.734375, "CSR": 0.6741071428571428, "retrieved_ids": ["mrqa_squad-train-56155", "mrqa_squad-train-32866", "mrqa_squad-train-53297", "mrqa_squad-train-30482", "mrqa_squad-train-8807", "mrqa_squad-train-16438", "mrqa_squad-train-37578", "mrqa_squad-train-69336", "mrqa_squad-train-56373", "mrqa_squad-train-174", "mrqa_squad-train-85039", "mrqa_squad-train-52237", "mrqa_squad-train-32987", "mrqa_squad-train-80775", "mrqa_squad-train-49987", "mrqa_squad-train-74352", "mrqa_hotpotqa-validation-3653", "mrqa_squad-validation-10410", "mrqa_hotpotqa-validation-3228", "mrqa_newsqa-validation-1834", "mrqa_triviaqa-validation-6170", "mrqa_squad-validation-2497", "mrqa_squad-validation-2675", "mrqa_squad-validation-1766", "mrqa_searchqa-validation-12652", "mrqa_squad-validation-3516", "mrqa_searchqa-validation-14936", "mrqa_squad-validation-1187", "mrqa_squad-validation-3530", "mrqa_searchqa-validation-4680", "mrqa_squad-validation-2914", "mrqa_squad-validation-7300"], "EFR": 0.9411764705882353, "Overall": 0.807641806722689}, {"timecode": 7, "before_eval_results": {"predictions": ["10 times", "French regular army troops were stationed in North America", "Ed Mangan", "German", "c1750", "bones", "central Europe", "since 1951", "immunoglobulins and T cell receptors", "average workers", "Brandon McManus", "Science", "33 feet (10.1 m)", "88", "monophyletic", "Bible", "second-largest", "Sports Night", "superheaters", "Mercedes-Benz Superdome", "Wang Zhen", "dioxygen", "1953", "Capitol Hill, Washington, D.C.", "Parliament Square, High Street and George IV Bridge in Edinburgh", "organic solvents", "Mnemiopsis", "topographic", "the Tesla coil", "National Galleries of Scotland", "The Pilgrim Street building", "Bright Automotive", "tax credits", "wacko", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "John and Elizabeth Calvert", "38", "mammograms are known to be uncomfortable", "the field is limited to drivers who meet more exclusive criteria", "Spain ceded Puerto Rico, along with the Philippines and Guam, then under Spanish sovereignty, to the U.S.", "Aristotle", "Peking", "Dylan Walters", "My Summer Story", "Mick Tully", "turkey", "Go", "donny Osmond", "Colorado", "Inigo Jones", "Brian Doyle- Murray", "Flushed Away", "Cuban descent", "Sam Waterston", "Moon Shot  Moon Shot: The Inside Story of America's Race to the Moon", "Chief Strategy Officer", "Fernando Rey", "Honshu seaport", "Kaaba", "Charles Francis \"Charlie\" Harper", "Sri Lanka", "the Mediterranean Sea", "Kiss La Kiss", "The narwhal"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7039953399122807}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.125, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10136", "mrqa_squad-validation-3909", "mrqa_squad-validation-3657", "mrqa_squad-validation-5249", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-358", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-1689", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-720", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-4606", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15236", "mrqa_searchqa-validation-14908"], "SR": 0.640625, "CSR": 0.669921875, "EFR": 1.0, "Overall": 0.8349609375}, {"timecode": 8, "before_eval_results": {"predictions": ["bars, caf\u00e9s and clubs", "T\u00f6regene Khatun", "Science Magazine", "3.6%", "the Second Republic", "highly diversified", "Beijing", "a chain", "the type of reduction being used", "quickly", "use in chloroplast division", "historians", "stagnant", "Ali Shariati", "an algorithm", "Higher Real Gymnasium", "four", "events and festivals", "the Apollo 1 backup crew", "the Ikh Zasag", "1883", "independent schools", "Sophocles", "rare and desired skills", "One in five", "breaches of law in protest against international organizations and foreign governments.", "electric lighting", "Christianized Yamasee", "six", "two", "revelry", "Dan Brown", "tennis", "$250,000", "Tim Clark, Matt Kuchar and Bubba Watson", "The ratio of the length s of the arc", "the 18th century", "Erica Rivera", "end of January in Davos", "breaking the single - season record", "In 1973", "a limited period of time", "Portugal", "SpongeBob", "photographer", "The Breakfast Club", "Thames Street", "Columbus Day", "Amerigo Vespucci", "Reginald Engelbach", "the Pierre Hotel", "Nip/Tuck", "Manchester United", "John Faso", "The Rite of Spring", "a pioneer", "The Devil's Dictionary", "christopher christopher", "a fever", "Titan", "the plague", "9 to 5", "The Star-Spangled Banner", "Dennis Potter"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6254934561965811}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.4, 0.4, 1.0, 0.5, 1.0, 0.2222222222222222, 1.0, 0.0, 0.16, 1.0, 0.37499999999999994, 0.0, 0.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3390", "mrqa_squad-validation-8649", "mrqa_squad-validation-1290", "mrqa_squad-validation-10303", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1585", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-431", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-1178", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-9818", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-3588"], "SR": 0.5625, "CSR": 0.6579861111111112, "EFR": 1.0, "Overall": 0.8289930555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["Ed Lee", "non-Mongol physicians", "the Acasta gneiss of the Slave craton in northwestern Canada", "26", "cabin depressurization", "a restaurant situated at a Grade I-listed 16th century merchant's house at 28\u201330 Close", "Super Bowl L", "private", "nineteenth-century cartographic techniques", "democracy", "ten minutes", "the balance of parties across Parliament", "very rare", "areas cleared of forest", "a lute", "Chagatai", "Tesla Electric Company", "WABM-DT2", "The Newcastle Beer Festival", "the Wesel-Datteln Canal", "land and housing", "Warsaw University of Technology building", "the plague was present somewhere in Europe in every year between 1346 and 1671", "three sources", "in herring barrels", "Donkey", "Judy Collins", "The Wild Bunch", "the Arabica coffee bean", "Captain Nemo", "the Italian", "Paul McCartney", "Buffalo Bill Cody's Wild West Show", "Moton Field, the Tuskegee Army Air Field", "120 m ( 390 ft )", "pan control", "Robert Cappucci and Joseph Wiley", "Andrew Gold", "Brooke Wexler", "Titanic earned $855.4 million in North America and $1.528 billion in other countries", "Caracas", "Vienna", "Wawrinka", "Bear Grylls", "Harry Shearer", "1879", "Dian Fossey", "the E Street Band", "Cyclic Defrost", "Nathan Bedford Forrest", "Annales de chimie et de physique", "Hurricane Faith", "more than 40 million", "Dolly Records", "last week", "September", "the piracy incident was discussed as one of the \"tests\" of President Obama that Joe Biden warned about during the campaign.", "Noida", "Tuesday", "1960", "Daryeel Bulasho Guud", "Tommy Burns", "Vito Corleone", "Mickey Mantle"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6977831196581197}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.13333333333333336, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5234", "mrqa_squad-validation-5137", "mrqa_squad-validation-477", "mrqa_squad-validation-6059", "mrqa_squad-validation-4054", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-1072", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-2082", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-2945", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-535", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-7775"], "SR": 0.609375, "CSR": 0.653125, "retrieved_ids": ["mrqa_squad-train-52116", "mrqa_squad-train-30775", "mrqa_squad-train-5038", "mrqa_squad-train-34870", "mrqa_squad-train-56584", "mrqa_squad-train-50916", "mrqa_squad-train-10261", "mrqa_squad-train-74924", "mrqa_squad-train-54982", "mrqa_squad-train-9838", "mrqa_squad-train-25932", "mrqa_squad-train-44623", "mrqa_squad-train-34471", "mrqa_squad-train-43340", "mrqa_squad-train-15182", "mrqa_squad-train-27898", "mrqa_squad-validation-3771", "mrqa_searchqa-validation-14908", "mrqa_squad-validation-9488", "mrqa_squad-validation-10136", "mrqa_squad-validation-2914", "mrqa_searchqa-validation-12652", "mrqa_naturalquestions-validation-3505", "mrqa_squad-validation-10410", "mrqa_searchqa-validation-9740", "mrqa_squad-validation-2675", "mrqa_squad-validation-4631", "mrqa_naturalquestions-validation-4359", "mrqa_hotpotqa-validation-661", "mrqa_squad-validation-545", "mrqa_squad-validation-3971", "mrqa_squad-validation-2291"], "EFR": 0.96, "Overall": 0.8065625}, {"timecode": 10, "UKR": 0.787109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3259", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-6486", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7033", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9685", "mrqa_naturalquestions-validation-9866", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1072", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-1203", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8999", "mrqa_squad-validation-10027", "mrqa_squad-validation-10044", "mrqa_squad-validation-10090", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10125", "mrqa_squad-validation-10136", "mrqa_squad-validation-10192", "mrqa_squad-validation-10211", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10295", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10309", "mrqa_squad-validation-10338", "mrqa_squad-validation-10346", "mrqa_squad-validation-10428", "mrqa_squad-validation-10430", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1123", "mrqa_squad-validation-1146", "mrqa_squad-validation-1187", "mrqa_squad-validation-1211", "mrqa_squad-validation-1218", "mrqa_squad-validation-1226", "mrqa_squad-validation-1253", "mrqa_squad-validation-1277", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1367", "mrqa_squad-validation-1391", "mrqa_squad-validation-1411", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1530", "mrqa_squad-validation-1531", "mrqa_squad-validation-1539", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1664", "mrqa_squad-validation-1690", "mrqa_squad-validation-1695", "mrqa_squad-validation-1720", "mrqa_squad-validation-173", "mrqa_squad-validation-174", "mrqa_squad-validation-1766", "mrqa_squad-validation-1794", "mrqa_squad-validation-1824", "mrqa_squad-validation-1872", "mrqa_squad-validation-1877", "mrqa_squad-validation-1908", "mrqa_squad-validation-1913", "mrqa_squad-validation-1980", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2060", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2321", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2382", "mrqa_squad-validation-2402", "mrqa_squad-validation-2408", "mrqa_squad-validation-2439", "mrqa_squad-validation-2475", "mrqa_squad-validation-2478", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2675", "mrqa_squad-validation-2704", "mrqa_squad-validation-2724", "mrqa_squad-validation-2807", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-288", "mrqa_squad-validation-2881", "mrqa_squad-validation-2955", "mrqa_squad-validation-2961", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3255", "mrqa_squad-validation-3288", "mrqa_squad-validation-3355", "mrqa_squad-validation-3378", "mrqa_squad-validation-3388", "mrqa_squad-validation-3400", "mrqa_squad-validation-3447", "mrqa_squad-validation-3457", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3628", "mrqa_squad-validation-3657", "mrqa_squad-validation-3771", "mrqa_squad-validation-3799", "mrqa_squad-validation-38", "mrqa_squad-validation-3806", "mrqa_squad-validation-3813", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-3915", "mrqa_squad-validation-3916", "mrqa_squad-validation-3942", "mrqa_squad-validation-3971", "mrqa_squad-validation-3986", "mrqa_squad-validation-405", "mrqa_squad-validation-4054", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-409", "mrqa_squad-validation-4100", "mrqa_squad-validation-4127", "mrqa_squad-validation-4137", "mrqa_squad-validation-4149", "mrqa_squad-validation-4192", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-4320", "mrqa_squad-validation-4332", "mrqa_squad-validation-437", "mrqa_squad-validation-4425", "mrqa_squad-validation-4427", "mrqa_squad-validation-4439", "mrqa_squad-validation-4475", "mrqa_squad-validation-4488", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-453", "mrqa_squad-validation-4629", "mrqa_squad-validation-4642", "mrqa_squad-validation-4658", "mrqa_squad-validation-4675", "mrqa_squad-validation-4676", "mrqa_squad-validation-4701", "mrqa_squad-validation-4711", "mrqa_squad-validation-477", "mrqa_squad-validation-477", "mrqa_squad-validation-4795", "mrqa_squad-validation-4801", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4902", "mrqa_squad-validation-4930", "mrqa_squad-validation-5013", "mrqa_squad-validation-503", "mrqa_squad-validation-5063", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5208", "mrqa_squad-validation-5226", "mrqa_squad-validation-5234", "mrqa_squad-validation-5249", "mrqa_squad-validation-5260", "mrqa_squad-validation-5300", "mrqa_squad-validation-5320", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-545", "mrqa_squad-validation-551", "mrqa_squad-validation-5531", "mrqa_squad-validation-5535", "mrqa_squad-validation-5550", "mrqa_squad-validation-5597", "mrqa_squad-validation-5620", "mrqa_squad-validation-5631", "mrqa_squad-validation-5715", "mrqa_squad-validation-5721", "mrqa_squad-validation-5721", "mrqa_squad-validation-5736", "mrqa_squad-validation-5891", "mrqa_squad-validation-5908", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-5991", "mrqa_squad-validation-6059", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6156", "mrqa_squad-validation-6166", "mrqa_squad-validation-6191", "mrqa_squad-validation-6293", "mrqa_squad-validation-6326", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6404", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6471", "mrqa_squad-validation-6473", "mrqa_squad-validation-6610", "mrqa_squad-validation-6614", "mrqa_squad-validation-6639", "mrqa_squad-validation-6644", "mrqa_squad-validation-6650", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6753", "mrqa_squad-validation-6759", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6889", "mrqa_squad-validation-6896", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6978", "mrqa_squad-validation-6988", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-7086", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7189", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7288", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7446", "mrqa_squad-validation-7466", "mrqa_squad-validation-7490", "mrqa_squad-validation-7502", "mrqa_squad-validation-7504", "mrqa_squad-validation-7508", "mrqa_squad-validation-7526", "mrqa_squad-validation-754", "mrqa_squad-validation-7563", "mrqa_squad-validation-7609", "mrqa_squad-validation-7653", "mrqa_squad-validation-7707", "mrqa_squad-validation-7718", "mrqa_squad-validation-7726", "mrqa_squad-validation-7727", "mrqa_squad-validation-7731", "mrqa_squad-validation-7744", "mrqa_squad-validation-7751", "mrqa_squad-validation-7767", "mrqa_squad-validation-778", "mrqa_squad-validation-7789", "mrqa_squad-validation-7813", "mrqa_squad-validation-7926", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7954", "mrqa_squad-validation-7997", "mrqa_squad-validation-8107", "mrqa_squad-validation-811", "mrqa_squad-validation-8154", "mrqa_squad-validation-8204", "mrqa_squad-validation-8212", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8269", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8350", "mrqa_squad-validation-8409", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8575", "mrqa_squad-validation-8576", "mrqa_squad-validation-8617", "mrqa_squad-validation-8643", "mrqa_squad-validation-8649", "mrqa_squad-validation-8658", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-9038", "mrqa_squad-validation-9103", "mrqa_squad-validation-916", "mrqa_squad-validation-9189", "mrqa_squad-validation-930", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-934", "mrqa_squad-validation-9376", "mrqa_squad-validation-9378", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9476", "mrqa_squad-validation-9488", "mrqa_squad-validation-9498", "mrqa_squad-validation-9505", "mrqa_squad-validation-9525", "mrqa_squad-validation-9575", "mrqa_squad-validation-9590", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9717", "mrqa_squad-validation-9731", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9787", "mrqa_squad-validation-9810", "mrqa_squad-validation-9853", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_squad-validation-9920", "mrqa_squad-validation-9962", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5984", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-770", "mrqa_triviaqa-validation-7735", "mrqa_triviaqa-validation-7775"], "OKR": 0.9140625, "KG": 0.4703125, "before_eval_results": {"predictions": ["Article 5", "0.3 to 0.6 \u00b0C", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Regis Philbin", "extremely difficult, if not impossible", "archival material", "5,000", "Utopia", "hydrogen and helium", "Central Pacific Railroad", "inequality", "Jan Andrzej Menich", "private conferences", "The View and The Chew", "Louis Adamic", "Roger Goodell", "lower bounds", "over 90", "the same message routing methodology as developed by Baran", "as soon as they enter into force, unless stated otherwise, and are generally concluded for an unlimited period", "Sakya", "the later decades of the 17th century", "December 1922", "The United States is the only Western country currently applying the death penalty, one of 57 countries worldwide applying it,", "a major fall in stock prices", "claims handler", "King Dasharatha", "Grand Inquisition", "more than a million", "disputes between two or more states", "soybean", "the shoulder", "Orson Welles", "\"Razor\"", "bison", "bitter almond", "she was the first woman to make a million dollars a movie,", "Gloucestershire", "Wilhelmus Simon Petrus Fortuijn", "Mr. Tumnus", "she was the first to recognise the full potential of a \"computing machine\"", "703", "Mauritian", "Lee Sun-mi", "music genres of electronic rock, electropop and R&B", "Hawaii", "Bill Gates", "56", "Jared Polis", "haute, bandeau-style little numbers", "Seminole", "girls around 11 or 12", "last week", "sculptures", "sheila marie ryan", "sheila Carter", "France", "Lumbini", "(J.A. Cuddon", "gehuanian Commonwealth", "the left side of the heart pumps oxygenated blood to the body", "(Anthony) trollope", "london", "The Third Woman"], "metric_results": {"EM": 0.46875, "QA-F1": 0.56976197991823}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7407407407407407, 1.0, 0.0, 0.0, 0.0, 0.12500000000000003, 0.5, 0.0, 0.5, 0.5454545454545454, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.125, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5758", "mrqa_squad-validation-6811", "mrqa_squad-validation-5505", "mrqa_squad-validation-3667", "mrqa_squad-validation-4588", "mrqa_squad-validation-7574", "mrqa_squad-validation-1568", "mrqa_squad-validation-4173", "mrqa_squad-validation-968", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-8092", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-2213", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4133", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-7853", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3937"], "SR": 0.46875, "CSR": 0.6363636363636364, "EFR": 1.0, "Overall": 0.7615696022727272}, {"timecode": 11, "before_eval_results": {"predictions": ["the expulsion of the Acadians", "The E. W. Scripps Company", "1974", "the Uighurs of the Kingdom of Qocho", "15", "Invocavit Sermons", "1899", "microorganisms", "Ealy", "Von Miller", "10", "23.9%", "cattle and citrus", "establish, equip, manage and maintain national and public libraries in the country", "In the 1060s", "1862", "William the Lion", "external combustion engines", "Ten", "a proper legal basis", "an Islamic shrine", "H.G. Wells", "Albert Einstein", "drizzle, rain, sleet, snow, graupel and hail", "Richard Wright", "September 8, 2017", "Jehnna ( Olivia d'Abo )", "In the 1920s", "Haiti", "jack johnson", "(Jerry) b.B. King, Dionne Warwick", "a double dip recession", "Darby and Joan", "Bronx Mowgli", "the lack of political means", "Jules Verne", "the United Kingdom", "Ellesmere Port, United Kingdom", "Gweilo", "Nick on Sunset", "various", "2006", "1943", "The Design Inference", "1836", "John Lennon and George Harrison", "Iran could develop a nuclear bomb within the next year.", "One of Osama bin Laden's sons", "Pelosi, a California Democrat, and Republican Minority Leader John Boehner of Ohio", "seven", "through Greece, the birthplace of the Olympics, before being transported to Canada", "Jean Van de Velde", "a drug test after taking a medicine that contained the banned substance cortisone", "Boston's historic Dorchester section", "Ray Sahelian, M.D.", "Like a Rock", "Indiana", "Canada", "Edgar Rice Burroughs", "Frank Lloyd Wright", "(SOund NAvigation and Ranging)", "the Soviet Union", "John Emburey", "The Little Prince"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6457104527417028}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.25, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 0.2222222222222222, 0.16666666666666669, 0.0, 0.16666666666666669, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2705", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-2307", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-5891", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-1654", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-13198", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-6419"], "SR": 0.5625, "CSR": 0.6302083333333333, "EFR": 1.0, "Overall": 0.7603385416666666}, {"timecode": 12, "before_eval_results": {"predictions": ["The Knowledge School", "OneDrive", "Ps. 31:5", "September 5, 1985", "62", "Jean Ribault", "James Bryant Conant", "journalism", "Western", "a green algal derived chloroplast", "electromagnetic", "in the kingdom", "the Chicago Bears", "observer", "requiring his arrest", "giving her brother Polynices a proper burial", "26", "against governmental entities.", "South Dakota", "Justin Timberlake", "a harp of gold", "Curtis Armstrong", "Hem Chandra Bose", "Parker's pregnancy", "the Gaget", "Afonso IV", "Julie Gonzalo", "ostrich", "Jean Alexander", "John Enoch Powell", "the British Royal Air Force", "Orrest", "McDonnell Douglas", "3000m", "John Masefield", "1966", "The Prodigy", "\u00c6thelstan", "Sean Yseult", "the Salzburg Festival", "les Miles", "World War I", "the County of York", "Charles Quinton Murphy", "Floridians", "estonia", "could be successful as a black candidate in part because of his \"light-skinned\" appearance", "Wednesday", "of sumo wrestling", "Elena Kagan", "seven", "could see the lifeboat where pirates have been holding Capt. Richard Phillips", "Araceli Valencia", "Sabina Guzzanti", "Margot Kidder", "free Expression", "Bill Hickok", "Mario Puzo", "New Zealand", "bicarbonate", "(A.G.Nauta", "Peter Shaffer", "Rudyard Kipling", "Bangladesh"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6299242424242424}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-249", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-1925", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-5120", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-59", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-633", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-1146"], "SR": 0.59375, "CSR": 0.6274038461538461, "retrieved_ids": ["mrqa_squad-train-67912", "mrqa_squad-train-48529", "mrqa_squad-train-75645", "mrqa_squad-train-82953", "mrqa_squad-train-42698", "mrqa_squad-train-73075", "mrqa_squad-train-81794", "mrqa_squad-train-54345", "mrqa_squad-train-16468", "mrqa_squad-train-17305", "mrqa_squad-train-58488", "mrqa_squad-train-18079", "mrqa_squad-train-29813", "mrqa_squad-train-5046", "mrqa_squad-train-30800", "mrqa_squad-train-61625", "mrqa_searchqa-validation-15657", "mrqa_squad-validation-9592", "mrqa_naturalquestions-validation-7595", "mrqa_newsqa-validation-3882", "mrqa_squad-validation-5234", "mrqa_squad-validation-718", "mrqa_newsqa-validation-1159", "mrqa_squad-validation-1824", "mrqa_newsqa-validation-2251", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-3588", "mrqa_squad-validation-739", "mrqa_newsqa-validation-169", "mrqa_hotpotqa-validation-4606", "mrqa_searchqa-validation-9818", "mrqa_hotpotqa-validation-3737"], "EFR": 0.9615384615384616, "Overall": 0.7520853365384614}, {"timecode": 13, "before_eval_results": {"predictions": ["\"the ones who are violating the greater law are the members of the Navy\"", "can produce both eggs and sperm at the same time", "Necessity-based entrepreneurship", "Tesla coil", "2008", "spin", "Capability deprivation", "San Jose State", "1954", "Rin", "Xbox One", "the bishop has read the appointments at the session of the Annual Conference", "1996", "Italian Plague of 1629\u20131631", "Northumbria University", "calcitriol", "2015", "Merrimen", "Andy Serkis", "Robin", "Andrew Johnson", "2005", "November 17, 1800", "1975", "Massachusetts", "from the Ute name for them", "16.5 feet", "geese on the ground", "yellow", "second", "Nigel Short", "judoka", "Manchester", "leprosy", "Harris", "Ouse and Foss", "a Christian church", "Government of Ireland", "Mathieu Kassovitz", "Moonstruck", "Slaughterhouse-Five", "three", "December 24, 1973", "1865", "the wars in Iraq and Afghanistan", "almost 100", "the poet's memories of his mother", "second", "and he plays it on a boom box during the showers, she said. \"He focuses on Ozzy, and he gets through his torture hour.", "Bob Dole", "composer of \"Phantom of the Opera\" and \"Cats\"", "strife in Somalia, where riots continued in the capital city of Mogadishu for the second day Tuesday.", "105-year", "fibula", "ruffled", "Sir Isaac Newton", "Trinity", "Brigham Young", "China", "Roman Catholic Archdiocese of Los Angeles", "South Korea", "Sideways", "Romance", "Hawaii Republican Party"], "metric_results": {"EM": 0.625, "QA-F1": 0.6909083104395604}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false], "QA-F1": [0.15384615384615383, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.08333333333333334, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-6735", "mrqa_squad-validation-4645", "mrqa_squad-validation-9247", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-6585", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-5153", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1811", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2453", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-6636", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-5808"], "SR": 0.625, "CSR": 0.6272321428571428, "EFR": 1.0, "Overall": 0.7597433035714285}, {"timecode": 14, "before_eval_results": {"predictions": ["515 million years ago", "tentilla", "an infinite collection of instances together with a solution for every instance", "Knights Templar", "Algeria", "Edinburgh Pentlands", "828,000", "1206", "thylakoid", "WLS", "about half", "Scotland", "Algeria", "Cadeby", "colonizing empires", "9 February 2018", "charbagh", "the 1960s", "22 November 1914", "Skat", "Secretary of Commerce Herbert Hoover", "during Hanna's recovery masquerade celebration", "John Young", "David Joseph Madden", "lighter fluid", "\"Mr Loophole\"", "the Denisovans.", "mudflats and salt marshes", "1881", "NASCAR", "Organisation", "Edward Toms", "Nova Scotia", "mule", "Tampa", "Mineola", "Sofia the First", "France", "was an American painter and writer who wrote the autobiography \"The Bite in the Apple\" about her relationship with Apple co-founder Steve Jobs.", "Japan and Hong Kong", "Chief of the Operations Staff", "Massapequa", "Kansas City crime family", "the iPods", "the conviction of their former president, Alberto Fujimori, for death squad killings carried out during his rule in the 1990s.", "Woosuk Ken Choi", "Dominic Adiyiah", "Garth Brooks", "1,700 year old Roman mosaic", "nuclear", "at least nine", "it has not intercepted any", "John Bunyan", "Steven Spielberg.", "riddle", "Diogenes", "\"white\" & \"blood\"", "Robinson Crusoe", "National Gallery of Art", "Orlov", "the Fairness Doctrine", "Michoacan state", "the abduction of minors.", "an Italian and six Africans"], "metric_results": {"EM": 0.625, "QA-F1": 0.6963567682317683}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.7272727272727272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1904761904761905, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4474", "mrqa_squad-validation-1767", "mrqa_squad-validation-4957", "mrqa_squad-validation-9480", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-9222", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-5643", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-9954", "mrqa_newsqa-validation-2821"], "SR": 0.625, "CSR": 0.6270833333333333, "EFR": 0.9166666666666666, "Overall": 0.743046875}, {"timecode": 15, "before_eval_results": {"predictions": ["Since the 1980s", "Fresno", "net", "energize electrons", "if they arrest fully informed jury leafleters, the leaflets will have to be given to the leafleter's own jury as evidence.", "20th century", "Lower Lorraine", "wave speeds", "three", "oxygen gas", "a system of many biological structures and processes within an organism that protects against disease", "1606", "1951", "humber", "the CIA", "beer", "George Bernard Shaw", "Clarence Thomas", "Washington Irving", "humber", "Brooklyn", "David Lee Roth", "Abraham Lincoln", "Under normal conditions", "200,564", "electron shells", "1986", "instructions that performs a specific task when executed by a computer", "Gospel of Matthew in the middle of the Sermon on the Mount", "U.S.", "Staci Keanan", "asexually", "Donna", "luck", "Elizabeth Taylor", "Australia", "Sergeant Claude Snudge", "Packers", "perfume", "1882", "the grant process", "the United States", "Dick Turpin", "\"Negro Cavalry\" by the Native American tribes they fought in the Indian Wars.", "Central Park", "early 20th-century Europe.", "2,664", "Anna Clyne", "Caesars Entertainment Corporation", "on 13 May 2018", "Maldives", "Samudio", "Lalit", "not a zoo.", "15", "the Ministry of Defense", "Kenneth Cole", "anaphylaxis,", "10 to 15 percent", "had the surgery December 13 after lumpectomies failed to eradicate her breast cancer.", "accused the charity of kidnapping the children and concealing their identities.", "husband, Chris.", "in the Angeles National Forest", "Jaime Andrade"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6516194309163059}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.125, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3602", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-3026", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-1193", "mrqa_triviaqa-validation-2308", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-4180", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1016", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-3621"], "SR": 0.578125, "CSR": 0.6240234375, "retrieved_ids": ["mrqa_squad-train-26849", "mrqa_squad-train-1253", "mrqa_squad-train-44945", "mrqa_squad-train-6418", "mrqa_squad-train-21021", "mrqa_squad-train-64044", "mrqa_squad-train-85525", "mrqa_squad-train-37899", "mrqa_squad-train-52189", "mrqa_squad-train-77893", "mrqa_squad-train-32172", "mrqa_squad-train-81843", "mrqa_squad-train-27495", "mrqa_squad-train-50429", "mrqa_squad-train-23511", "mrqa_squad-train-56245", "mrqa_hotpotqa-validation-5752", "mrqa_naturalquestions-validation-7309", "mrqa_squad-validation-8278", "mrqa_newsqa-validation-1989", "mrqa_squad-validation-8909", "mrqa_squad-validation-3667", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-1551", "mrqa_naturalquestions-validation-74", "mrqa_newsqa-validation-5", "mrqa_searchqa-validation-8451", "mrqa_triviaqa-validation-370", "mrqa_hotpotqa-validation-3826", "mrqa_newsqa-validation-3439", "mrqa_naturalquestions-validation-1802", "mrqa_triviaqa-validation-431"], "EFR": 1.0, "Overall": 0.7591015624999999}, {"timecode": 16, "before_eval_results": {"predictions": ["Rhin", "4", "chloroplasts", "neuronal dendrites", "evenly round the body", "Switzerland", "BSkyB", "British", "James E. Webb", "nitroaereus", "tidal delta", "DC traction motor", "the west", "a bishop", "(Emma Thompson)", "his toe", "Mort Sahl", "the Russian Collection", "an electrolyte", "Cleveland", "Delaware", "Da Vinci Code blue", "alveolar process", "a bipolar ex-lover", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "moral", "Kanawha River", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "brothers Henry, Jojo and Ringo Garza", "Mohammad Reza Pahlavi", "Pakistan", "the utopian novels of H.G. Wells", "saccharides", "the Bank of England", "l\u2019Etuve", "Gary Oldman", "algae", "Ennio Morricone", "Midnight Cowboy", "E in A-Level art", "two days", "london", "Rice University", "Mexico", "A campaign setting", "September 29, 2017", "postmodern", "five", "2006", "the Salzburg Festival", "three", "Saint Louis County,", "200", "The word \"tuatara\" is derived from a Maori word meaning \"spiny back.\"", "5,600", "The Los Angeles County Fire Department", "a 57-year old male", "Citizens", "Empire of the Sun", "al-Moayad", "Jacob", "Jason Chaffetz", "Herbert Hoover", "New Croton Reservoir in Westchester and Putnam counties"], "metric_results": {"EM": 0.5, "QA-F1": 0.5544730392156862}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.4, 0.3333333333333333, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10416", "mrqa_squad-validation-8792", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-3152", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-7115", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-2196", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-4263", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-4859", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-3174"], "SR": 0.5, "CSR": 0.6167279411764706, "EFR": 0.96875, "Overall": 0.7513924632352941}, {"timecode": 17, "before_eval_results": {"predictions": ["teacher's colleges,", "SAP Center in San Jose", "\"ash tree\"", "$5 million", "17", "mercuric oxide (HgO)", "the Pacific", "Isel", "a modern canalized section", "John Fox", "Sava Kosanovi\u0107", "the mid-19th century", "brussels", "Spain and Portugal", "the 19th century", "klezmer", "Tiffany", "a locking pin", "Newland Archer", "brussels", "Indira Gandhi", "the dwelling", "Louisiana", "the base of the right ventricle", "Indian Standard Time", "the League of Communists of Yugoslavia party", "Road / Track", "in 1900 to 1920", "their need to repent in time", "the intersection of Del Monte Blvd and Esplanade Street", "used as a pH indicator, a color marker, and a dye", "William Chatterton Dix", "brussels", "brussels", "The History Boys", "The Gambia", "the first race of the famed Triple Crown series", "the Netherlands", "Robert Stroud", "William Lamb", "Malcolm Bradbury", "Pat Cash", "chalk quarry", "Jan Kazimierz", "200,167", "American", "\"The Curious Case of Benjamin button\"", "2001", "Abbey Road", "the oil platforms in the North Sea", "Dan Brandon Bilzerian", "YouTube", "$2 billion", "183 times in a month,", "\"peregruzka\"", "Nicole", "breast cancer.", "the Cowardly Lion", "The remaining 240 patients will be taken to hospitals in the capital.", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "more than 1.2 million people.", "Thabo Mbeki", "King George I", "wake"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5916328578532862}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.4, 0.5217391304347826, 0.9411764705882353, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-457", "mrqa_squad-validation-4634", "mrqa_searchqa-validation-8891", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-9094", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-6384", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-3931", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1380"], "SR": 0.484375, "CSR": 0.609375, "EFR": 0.9696969696969697, "Overall": 0.750111268939394}, {"timecode": 18, "before_eval_results": {"predictions": ["if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "the control is spread more subtly through technological superiority,", "tentilla", "Levi's Stadium", "a drug treatment for an individual", "Charlesfort", "2000", "five", "24 September 2007", "Infrastructure", "William Strauss and Neil Howe", "Missouri River", "1960s", "a bow bridge with 16 arches shielded by ice guards", "more than 80 tank\u014dbon volumes", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Gregor Mendel", "United Nations Peacekeeping Operations", "Certificate of Release or Discharge from Active Duty", "Latitude", "Pittsburgh", "Hyundai", "austria melbourne", "western or southern border of the Texas counties of El Paso, Hudspeth, Presidio, Brewster", "Congregation", "John Barbirolli", "Tahrir Square", "histamine", "100 years", "Paul C\u00e9zanne", "oldpatricktoe-end", "Germany", "Philip Livingston", "Suzanne N.J. 'Susie' Chun Oakland is a Democratic member of the Hawaii Senate, representing the 13th District since 1996.", "DreamWorks Animation", "DI Humphrey Goodman", "London", "1926", "Helen Mirren", "Nebraska Cornhuskers women's basketball team", "Wu-Tang Clan", "$125.4 million", "two", "Caylee, who was 2", "Shanghai", "11", "behavioral health-care provider.", "from Amsterdam, in the Netherlands,", "October 19", "an open window that fits neatly around him", "maintain an \"aesthetic environment\" and ensure public safety,", "Entertainment Tonight", "a cricket", "R2-D2", "Meet the Press", "dampers", "Samuel Burl \"Sam\" Kinison", "The Vandellas", "Sisyphus", "Veep", "Love me tender", "Elizabeth Taylor", "fish", "Viking feet"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7096736596736597}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.7499999999999999, 1.0, 1.0, 0.0, 0.46153846153846156, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_squad-validation-6282", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3170", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-712", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-3773", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3858", "mrqa_searchqa-validation-13223", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-10860", "mrqa_searchqa-validation-2136", "mrqa_triviaqa-validation-3215"], "SR": 0.59375, "CSR": 0.6085526315789473, "retrieved_ids": ["mrqa_squad-train-28550", "mrqa_squad-train-73419", "mrqa_squad-train-39826", "mrqa_squad-train-44856", "mrqa_squad-train-14680", "mrqa_squad-train-54043", "mrqa_squad-train-81024", "mrqa_squad-train-32415", "mrqa_squad-train-25137", "mrqa_squad-train-44891", "mrqa_squad-train-62516", "mrqa_squad-train-30655", "mrqa_squad-train-50248", "mrqa_squad-train-26993", "mrqa_squad-train-65685", "mrqa_squad-train-58562", "mrqa_hotpotqa-validation-3258", "mrqa_searchqa-validation-1065", "mrqa_squad-validation-3667", "mrqa_searchqa-validation-15890", "mrqa_squad-validation-3806", "mrqa_searchqa-validation-13920", "mrqa_newsqa-validation-4208", "mrqa_squad-validation-4675", "mrqa_squad-validation-6735", "mrqa_triviaqa-validation-1567", "mrqa_squad-validation-3909", "mrqa_squad-validation-2914", "mrqa_hotpotqa-validation-3741", "mrqa_triviaqa-validation-431", "mrqa_hotpotqa-validation-3737", "mrqa_newsqa-validation-911"], "EFR": 0.9615384615384616, "Overall": 0.7483150936234818}, {"timecode": 19, "before_eval_results": {"predictions": ["Brandon McManus", "Compromise of 1850", "1884", "Seven Stories", "1332", "Morgan", "Space", "neoclassical", "12.5", "Florence", "Canada geese", "the American Civil War", "the Hundred Years' War", "come", "Star Trek", "the circulatory system", "Yitzhak Rabin", "Herman Melville", "4", "Providence", "around the world", "landowner", "July 10, 2017", "at birth", "Chris Rea", "Anna Faris", "outside cultivated areas", "the United States", "most junior enlisted sailor", "the buttock and down the lower limb", "Ledger", "Mexico", "Funchal", "Michael J. Fox", "Peter MacTaggart", "massively multiplayer online games", "the Holy Roman Empire", "a board that has lines and pads that connect various points together", "Gaston Leroux", "Whitney Houston", "seven", "Scotland", "local South Australian and Australian produced content", "Big 12 Conference", "Animorphs", "MMA", "Vic Chesnutt", "William Douglas", "The Timekeeper", "2013", "poetry, theater, art, music, the media, and books", "Shawnee Mission Parkway", "Native American tribes", "CNN's best ten golf movies ever made", "me", "onto the college campus.", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "the Yemeni port city of Aden", "the second missing person", "American icon's", "Thessaloniki and Athens,", "the use of torture and indefinite detention", "at least 25", "Fareed Zakaria"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5380809294871796}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-969", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-15149", "mrqa_searchqa-validation-6804", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-1838", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3891"], "SR": 0.484375, "CSR": 0.60234375, "EFR": 0.9696969696969697, "Overall": 0.748705018939394}, {"timecode": 20, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4685", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5891", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-97", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-13987", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14795", "mrqa_searchqa-validation-14908", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15552", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2996", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-410", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-4551", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5169", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-9748", "mrqa_searchqa-validation-9818", "mrqa_squad-validation-10087", "mrqa_squad-validation-10102", "mrqa_squad-validation-10103", "mrqa_squad-validation-10192", "mrqa_squad-validation-1021", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1176", "mrqa_squad-validation-1277", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1410", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1539", "mrqa_squad-validation-1577", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1688", "mrqa_squad-validation-1766", "mrqa_squad-validation-1767", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2382", "mrqa_squad-validation-2408", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2819", "mrqa_squad-validation-2854", "mrqa_squad-validation-2955", "mrqa_squad-validation-2956", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3447", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3628", "mrqa_squad-validation-3667", "mrqa_squad-validation-3806", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4054", "mrqa_squad-validation-4063", "mrqa_squad-validation-4074", "mrqa_squad-validation-409", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-4173", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-437", "mrqa_squad-validation-4439", "mrqa_squad-validation-453", "mrqa_squad-validation-457", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4642", "mrqa_squad-validation-4676", "mrqa_squad-validation-4772", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-49", "mrqa_squad-validation-4954", "mrqa_squad-validation-4957", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5211", "mrqa_squad-validation-5229", "mrqa_squad-validation-526", "mrqa_squad-validation-5272", "mrqa_squad-validation-5477", "mrqa_squad-validation-5492", "mrqa_squad-validation-5505", "mrqa_squad-validation-551", "mrqa_squad-validation-5550", "mrqa_squad-validation-5592", "mrqa_squad-validation-5631", "mrqa_squad-validation-5721", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6060", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6231", "mrqa_squad-validation-6254", "mrqa_squad-validation-6282", "mrqa_squad-validation-6404", "mrqa_squad-validation-6471", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6564", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6695", "mrqa_squad-validation-6750", "mrqa_squad-validation-6753", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-704", "mrqa_squad-validation-7086", "mrqa_squad-validation-7090", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7273", "mrqa_squad-validation-7288", "mrqa_squad-validation-7322", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7514", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7958", "mrqa_squad-validation-7988", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-823", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8342", "mrqa_squad-validation-8352", "mrqa_squad-validation-839", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8600", "mrqa_squad-validation-8643", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9365", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9643", "mrqa_squad-validation-9675", "mrqa_squad-validation-9680", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9859", "mrqa_squad-validation-9869", "mrqa_squad-validation-9920", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-2060", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-4634", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7337", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-839"], "OKR": 0.90625, "KG": 0.47109375, "before_eval_results": {"predictions": ["2004", "extremely old", "liquid oxygen storage containers", "greater equality but not per capita income", "a green algal derived chloroplast", "3.55 inches", "Troika Design Group", "an Islamic rebellion", "a second lieutenant", "The Rolling Stones", "'House'", "toga candida", "blow", "para handy", "C. S. Lewis", "furlong", "goat", "\"Dejection: An Ode\"", "Ringo Starr", "Jenny and Eric", "parthenogenesis", "The Cornett family", "Zhu Yuanzhang", "Baez", "Sam Waterston", "a scythe", "101.325 kPa", "Eddie Murphy", "art photography", "1700 Cascadia earthquake", "\"Frenchie\"", "Manchester City", "para handy", "kendo", "Bude", "300", "\"The Famous Toll House cookie\"", "1944", "the National Industrial Conference Board", "Marlon Brando", "Neighbours", "Australia", "\"mainstream evangelical magazine.\"", "five", "3,384,569", "Steve Coogan", "\"Marcella\"", "25", "Nashville", "September", "Chris Anderson", "The Great Neck School District", "I, the chief executive officer,", "almost 100", "the Dalai Lama's", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Body Tap,", "Kim Jong Il's", "Saturday", "blacks and Hispanics", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "The Real Housewives of Atlanta", "para handy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5809140587673196}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.5, 0.4, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.9565217391304348, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3689", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-16866", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4266", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-2242", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-1038", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-3605", "mrqa_triviaqa-validation-1423"], "SR": 0.484375, "CSR": 0.5967261904761905, "EFR": 0.9696969696969697, "Overall": 0.7450033820346321}, {"timecode": 21, "before_eval_results": {"predictions": ["non-violent", "$2 million", "specialised education and training", "Middle Miocene", "debased", "Robert Maynard Hutchins", "five", "Mark Anthony \"Baz\" Luhrmann", "Christine MacIntyre", "Michele Marie Bachmann", "French", "beer and soft drinks", "\"Theme Park\"", "Everything Is wrong", "Congo River", "the fictional city of Quahog, Rhode Island", "\"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "December 5, 1991", "the efferent nerves that directly innervate muscles", "Mushnik", "stable, non-radioactive rubidium - 85", "Nancy Jean Cartwright", "seven", "seawater pearls", "15 Bonanza Creek Lane, Santa Fe, New Mexico, USA", "May 2017", "$14.3 trillion", "his 1970 triple album All Things Must Pass", "Emma Watson", "Alexei Kosygin", "Saudi Arabia", "Ed Woodward", "llanberis", "Cyclops", "addiction and behavior change/issues", "Oklahoma", "John Alec Entwistle", "Anna Mae Bullock", "Canada", "Madonna", "2050,", "Graeme Smith", "three", "medical evacuations and housing allocations,", "The meter reader", "prostate cancer,", "Haiti.", "A witness", "246", "the creation of a long-term plan to help Haiti recover from the devastating effects of the earthquake", "NATO's Membership Action Plan, or MAP,", "the Way of St. James", "Wu-Tang Clan", "Coral Reefs", "uncontrolled", "Jean Valjean", "Botswana", "The Treasure of the Sierra Madre", "Nanabozho", "Moscow", "Whig", "Olympia", "present - day southeastern Texas", "The Republic of Tecala"], "metric_results": {"EM": 0.5, "QA-F1": 0.5987520551698939}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 0.4, 0.5, 0.11764705882352941, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.17391304347826086, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2910", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-7685", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-5062", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-8781", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-14446", "mrqa_naturalquestions-validation-9327"], "SR": 0.5, "CSR": 0.5923295454545454, "retrieved_ids": ["mrqa_squad-train-36725", "mrqa_squad-train-65151", "mrqa_squad-train-22554", "mrqa_squad-train-73725", "mrqa_squad-train-36519", "mrqa_squad-train-20558", "mrqa_squad-train-75821", "mrqa_squad-train-27075", "mrqa_squad-train-25091", "mrqa_squad-train-43544", "mrqa_squad-train-47071", "mrqa_squad-train-7924", "mrqa_squad-train-85683", "mrqa_squad-train-7533", "mrqa_squad-train-3268", "mrqa_squad-train-78547", "mrqa_triviaqa-validation-713", "mrqa_newsqa-validation-859", "mrqa_searchqa-validation-3937", "mrqa_triviaqa-validation-4843", "mrqa_searchqa-validation-14881", "mrqa_naturalquestions-validation-572", "mrqa_squad-validation-9488", "mrqa_squad-validation-2478", "mrqa_triviaqa-validation-1833", "mrqa_newsqa-validation-4144", "mrqa_squad-validation-10428", "mrqa_squad-validation-3667", "mrqa_searchqa-validation-11543", "mrqa_naturalquestions-validation-9939", "mrqa_squad-validation-1824", "mrqa_searchqa-validation-8775"], "EFR": 1.0, "Overall": 0.7501846590909091}, {"timecode": 22, "before_eval_results": {"predictions": ["three", "Federica Mogherini", "18", "luxury items", "the Scottish Parliament", "Maling", "Johnny Cash", "Humphrey Goodman", "Coronation Street", "31 July 1975", "Edmonton, Alberta", "ONTV", "Johnson Press", "in 1877", "Tom Jones", "The Killer", "Kew Gardens", "Chesley Sullenberger III", "Walter Brennan", "2007", "Freddie Highmore", "12th and 13th centuries", "Second Continental Congress", "newly formed vesicles", "son of Edward '", "unknown origin", "the semilunar pulmonary valve", "30 October 1918", "360", "Addis Ababa", "Tigger", "Persia", "yah nyah", "bofrot", "Alberich", "William Marshall", "Jeffrey Archer", "yachts", "Montezuma", "Secretary of State", "job training", "the murders of his father and brother,", "former Pakistani Prime Minister Benazir Bhutto", "an Italian and six Africans", "12 off-duty federal agents in southwestern Mexico,", "Another high tide", "Camorra has been blamed for about 60 killings this year in Naples and its surrounding county.", "1-0", "arrested, arraigned and jailed, with bail set at $500,000 each.", "November 1", "Sodra nongovernmental organization", "Chen Lu", "cow's", "medical", "Henry Hudson", "Arthur Pitney", "ear", "John F. Kennedy", "British rock band", "Kilkenny cats", "Aleph", "shiatsu", "former Clash race winners", "BC Jean"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5831802595049878}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.08695652173913043, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-7427", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-12153", "mrqa_naturalquestions-validation-6264"], "SR": 0.515625, "CSR": 0.5889945652173914, "EFR": 0.9354838709677419, "Overall": 0.7366144372370267}, {"timecode": 23, "before_eval_results": {"predictions": ["antigens", "the courts of member states and the Court of Justice of the European Union", "its contents were sold two years later to satisfy a debt", "pastors", "south Wales", "\u00dcberseering BV v Nordic Construction GmbH", "12", "Graham Payn", "Democratic", "Charles L. Clifford", "Ludwig van Beethoven", "Jacking", "Nippon Professional Baseball", "Lord Robert Cecil", "Kew", "Kim Bauer", "Citizens for a Sound Economy", "Gian Carlo Menotti", "in the 1820s", "The genome", "an address bar", "country", "1976", "Lex Luger and Rick Rude", "The name of this deity is also sometimes spelled Molech, Milcom, or Malcam", "Presley Smith", "foreign investors", "Utah, Arizona, Wyoming, and Oroville, California", "Aalika Sheikh", "Salford", "George H. W. Bush", "Spain", "Margaret Thatcher", "Ascot", "tennis", "Brussels", "rennes", "Host Uruguay", "Carousel", "Apollon", "used cars.", "the storm, five caused by falling trees,", "Karen Floyd", "Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "his sixth world title at a different weight by beating Cotto on Saturday night.", "for security reasons and not because of their faith.", "between raw descriptions and expressions of hope.\"", "$81,88010", "Ryder Russell", "A Colorado prosecutor", "the creation of an Islamic emirate in Gaza,", "Sweden", "the King's Men", "Beloved", "Canada", "George Orwell", "Lincoln Cent", "Queen Margrethe", "Pillsbury", "india", "Northwestern University", "Angelina Jolie", "Sergei Rachmaninoff", "the sympathetic nervous system"], "metric_results": {"EM": 0.5, "QA-F1": 0.5778459821428571}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.14285714285714288, 0.125, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1406", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-1886", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7265", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-1428", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2736", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-12618", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-2833"], "SR": 0.5, "CSR": 0.5852864583333333, "EFR": 1.0, "Overall": 0.7487760416666667}, {"timecode": 24, "before_eval_results": {"predictions": ["38", "at rest", "Edmonton, Canada", "Islam,", "conservative Muslims", "the 3rd Parachute Brigade", "south-north", "Ford Island", "Europe", "Amber Heard", "Matthieu Vaxivi\u00e8re", "the Rose Garden", "House of Hohenstaufen", "Austrian Volksbanks", "2011", "Terry the Tomboy", "The Vaudevillains", "The Portuguese", "Kelly Osbourne", "the right hand", "1804", "Hans Zimmer", "Lady Gaga", "James Hutton", "1901", "G -- Games", "a god of the Ammonites", "1959", "Rose-Marie", "Emilia", "The 20 Highest-Grossing Movies of All Time", "Australia and England", "Kraft", "Adolphe Adam", "Brooklyn", "phoebus", "the Snow Eater", "Today", "Lady Gaga", "the Afghanistan's restive provinces", "\"Get out, Mubarak!\"", "upper respiratory infection,", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "the Muslim Eid-ul-Adha", "10 to 15 percent", "the Somali border town of Afmado", "the Oaxacan countryside of southern Mexico", "Collier County Sheriff Kevin Rambosk", "The European Commission", "Amanda Knox's", "Old Mother Hubbard", "Hollywood Canteen", "the Arabian Peninsula", "the period between fertilization and birth", "Heathrow", "the T.H.X. System", "American television comedy series", "Everest", "a junkyard dog", "the Count of Monte Cristo", "the brain", "Maine", "Ben Watson", "Hokkaido"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6221173128342246}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5882352941176471, 0.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-4696", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4890", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-8476", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-6214", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3213", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-12372"], "SR": 0.5625, "CSR": 0.584375, "retrieved_ids": ["mrqa_squad-train-66529", "mrqa_squad-train-61155", "mrqa_squad-train-39571", "mrqa_squad-train-16440", "mrqa_squad-train-6994", "mrqa_squad-train-12675", "mrqa_squad-train-36989", "mrqa_squad-train-734", "mrqa_squad-train-38355", "mrqa_squad-train-56394", "mrqa_squad-train-78107", "mrqa_squad-train-47153", "mrqa_squad-train-25890", "mrqa_squad-train-186", "mrqa_squad-train-58632", "mrqa_squad-train-71624", "mrqa_searchqa-validation-1203", "mrqa_hotpotqa-validation-5800", "mrqa_naturalquestions-validation-7659", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-7988", "mrqa_naturalquestions-validation-7223", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-2866", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6723", "mrqa_searchqa-validation-1072", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-4466", "mrqa_squad-validation-5758", "mrqa_newsqa-validation-1120", "mrqa_squad-validation-1290"], "EFR": 1.0, "Overall": 0.7485937500000001}, {"timecode": 25, "before_eval_results": {"predictions": ["Oirads", "via electron microscopy", "complete addressing information", "a lower bound", "We are beggars", "South American country", "JackScanlon", "Cuba", "Western Australia", "non-ferrous", "the Veterans Committee", "1978", "Pakistan", "National League ( NL )", "94 by 50", "Joe Spano", "the passing of the year", "Eagles", "a bacteria", "Aron Ralston", "a Daffodil", "John Poulson", "four", "Skittles", "THE PENGUIN", "Israel", "low-cost", "James Hogg", "Macau", "World Outgames", "Premier Division", "Terry Malloy", "Weare, New Hampshire", "Rigel VII", "M Rookie Blaylock", "15 October 1988", "11 June 1959", "40 million", "Leonard Cohen", "President Bush of a failure of leadership at a critical moment in the nation's history.", "Empire of the Sun,", "Afghan lawmakers.", "that new Ford center represents an advancement over its previous paint and body tech operation in that it's larger, closer to the company's HQ, and now works more closely with design engineers", "some of the best stunt ever pulled off", "kite boards", "\"associate\" of the family,", "those missing", "9-week-old", "Australian officials", "Barack Obama sent a message that fight against terror will respect America's values.", "Jack London", "Philadelphia", "NYPD", "Blimpish", "clouds", "a vacuum", "Jane Eyre", "Tom Sennett", "Catholics", "The Big Easy", "Mikhail Gorbachev", "four months ago,", "Polo", "\"Rin Tin Tin: The Life and the Legend\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.5502533253565093}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 1.0, 0.2222222222222222, 0.8571428571428571, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.18867924528301885, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1784", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-7779", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-2151", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-6579", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-1003"], "SR": 0.4375, "CSR": 0.5787259615384616, "EFR": 0.9722222222222222, "Overall": 0.7419083867521368}, {"timecode": 26, "before_eval_results": {"predictions": ["Disneyland", "Scandinavia", "back to an Earth ocean landing", "Treaties apply as soon as they enter into force", "Charlene Holt", "25 years after the release of their first record", "the case of disputes between two or more states", "the NIRA", "20 April", "`` sick of keeping all these feelings inside and not speaking up for myself ''", "Atlanta", "1920s", "once every 23 hours, 56 minutes, and 4 seconds", "novella", "Yuzuru Hanyu", "the book of Acts", "Washington", "\"Wild Thing\"", "Plato", "170+", "Gerald R. Ford", "the Diamondbacks", "surf", "Pygmalion, king of Cyprus", "Verdi", "Passepartout", "Wikipedia", "ligers", "1884", "nursery rhyme", "Sully", "Thomas Christopher Ince", "quantum mechanics", "26,000", "The entity", "John de Mol Jr.", "KBS2", "on the shore", "james Fell", "Port Melbourne", "Islamic militants", "Dan Brown's", "murder in connection with the death of a woman who may have been contacted through a Craigslist ad,", "died almost instantly from his wounds.", "citizenship", "pelvis and sacrum", "\"illegitimate.\"", "Friday,", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Isabella", "Kurdish militant group in Turkey", "Brad Blauser, center,", "leeches", "The Rolling Stones", "Cheddar", "the human breast", "the Mesozoic Era", "King Arthur", "sales", "coral", "the moon", "butter", "a swan pan", "elixir of life"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6309893426762201}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 0.8750000000000001, 1.0, 0.09523809523809525, 0.9411764705882353, 1.0, 0.0, 0.1935483870967742, 1.0, 0.33333333333333337, 0.16666666666666666, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.1212121212121212, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-3879", "mrqa_squad-validation-4176", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-3770", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-1858", "mrqa_triviaqa-validation-6070", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1143", "mrqa_searchqa-validation-11394", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-7895", "mrqa_searchqa-validation-14095"], "SR": 0.515625, "CSR": 0.5763888888888888, "EFR": 0.967741935483871, "Overall": 0.740544914874552}, {"timecode": 27, "before_eval_results": {"predictions": ["Maurus Servius Honoratus", "13th", "A probabilistic Turing machine", "Donald Davies", "two tickets to Italy", "said the claims are unfounded,", "outside influences in next month's run-off election,", "the tape was given to authorities in September 2007 by a man who said he had found it in the desert five months before.", "allergies to peanuts, nuts, shellfish and fish,", "the Nazi war crimes suspect who had been ordered deported to Germany,", "150", "J. Crew outfits", "made digital music available on computers and digital phones and used it in commercials.", "Miami Beach, Florida,", "more than 100", "police", "Dirk Benedict", "flawed democracy ''", "1986", "adrenal medulla", "July 2, 1776", "parthenogenesis", "originated in pilgrimages to Jerusalem", "Daniel A. Dailey", "Gustav Bauer", "74", "in Nashville, Tennessee", "Billy Colman", "Sicily", "lek\u00eb Dukagjini", "stutthof", "\"e pluribus unum\"", "Eddie Cochran", "the witch trials", "Ghana", "El Hiero", "emperor Claudius", "Dos Equis", "fish", "Telstar", "Vaisakhi List", "Sierra Leone", "Tranquebar", "Citizens for a Sound Economy", "Tie Domi", "the Rolls of Ol\u00e9ron", "bank of China Building", "Central University of India,", "dice", "Property management", "Mississippi Institute of Arts and Letters", "Interstate 22", "Dr. Seuss", "a cardinal", "Paris", "ear infections", "Grimm's Fairy Tales", "Sally Field", "platinum is valuable for laboratory apparatus,", "apples", "Shakespeare", "Rihanna", "President Herbert Hoover", "Bulgaria"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7080605158730159}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.25, 0.5333333333333333, 1.0, 0.0, 0.11111111111111112, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-828", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-2782", "mrqa_triviaqa-validation-5892", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-351", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14772", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-11551", "mrqa_searchqa-validation-2074", "mrqa_searchqa-validation-1119"], "SR": 0.578125, "CSR": 0.5764508928571428, "retrieved_ids": ["mrqa_squad-train-20021", "mrqa_squad-train-39914", "mrqa_squad-train-75371", "mrqa_squad-train-59174", "mrqa_squad-train-61240", "mrqa_squad-train-18049", "mrqa_squad-train-83426", "mrqa_squad-train-7832", "mrqa_squad-train-70763", "mrqa_squad-train-25546", "mrqa_squad-train-67976", "mrqa_squad-train-1107", "mrqa_squad-train-67734", "mrqa_squad-train-37855", "mrqa_squad-train-61712", "mrqa_squad-train-20614", "mrqa_triviaqa-validation-370", "mrqa_searchqa-validation-12326", "mrqa_hotpotqa-validation-3806", "mrqa_squad-validation-545", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4368", "mrqa_triviaqa-validation-3101", "mrqa_naturalquestions-validation-6264", "mrqa_squad-validation-5137", "mrqa_hotpotqa-validation-2300", "mrqa_squad-validation-9247", "mrqa_hotpotqa-validation-64", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1053", "mrqa_triviaqa-validation-713", "mrqa_searchqa-validation-13117"], "EFR": 1.0, "Overall": 0.7470089285714285}, {"timecode": 28, "before_eval_results": {"predictions": ["ARPANET", "cyclic photophosphorylation", "without destroying historical legitimacy", "The Da Vinci Code", "Gustav's top winds weakened to 110 mph,", "President Obama", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "stripper pole photos", "2008,", "marking of Ashura", "Satsuma, Florida,", "her son's partying ways and the infamous bong photo.", "17 Again", "3-2", "Africa", "the Atlantic Ocean", "Super Bowl LII", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg", "Andaman and Nicobar Islands", "July 25, 2017", "December 12, 2017", "St. Louis Blues", "telecommunications, pharmaceuticals, aircraft, heavy machinery and other industries", "1000 AD", "very important in meat technology", "the southernmost tip of the South American mainland", "1904", "yayin", "Jupiter", "Rugby School", "Dutch", "David Frost", "Athens", "Les Jolies Eaux", "Macbeth", "the human voice", "a symbol of position and title", "Turkey", "judoka", "puppy", "Hee Haw", "2012", "oyote Ugly", "Rowan Blanchard", "Brady John Haran", "Squam Lake", "The Joshua Tree", "Donald Carl \"Don\" Swayze", "East Is East", "Washington, D.C.", "Burning Man", "Joseph", "Harriet Tubman", "Chuck Berry", "Colorado", "60 Minutes", "the Androscoggin", "Spain", "a brain", "the tibia", "Machiavelli", "Heracles", "bone fracture", "the uvula"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6462115575396825}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5, 0.28571428571428575, 0.0, 0.6666666666666666, 0.28571428571428575, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.4, 0.8, 0.25, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-2513", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-463", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-4148", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-15667"], "SR": 0.53125, "CSR": 0.5748922413793103, "EFR": 1.0, "Overall": 0.7466971982758621}, {"timecode": 29, "before_eval_results": {"predictions": ["Christ and His salvation", "$2.50 per AC horsepower royalty", "recant his writings", "her surrogate", "At least 15", "Dr. Death in Germany", "Tim Clark, Matt Kuchar and Bubba Watson", "15 years ago", "heavy steel and boron", "said he gave the victims \"assurances of the church's action\" after the April 18 meeting.", "Body Tap,", "putting a personal and human face on the issue", "2-1", "Jose Miguel Vivanco,", "India.", "Roman Reigns", "the Nationalists", "April 10, 2018", "September 2017", "December 19, 1971", "Sergeant Himmelstoss", "Burj Khalifa", "1932", "smacking a fly on her mirror", "a solitary figure who is not understood by others, but is actually wise", "T.S. Eliot", "Ludacris", "a docked yacht", "Midsomer Murders", "paria", "nitrogen oxides", "red-green colorblindness", "Nicky Morgan", "told both news and rumours", "Director General of the Security Service", "football", "Ghana", "Jan van Eyck", "achromatopsia", "Wake Island", "emotion poetry", "1884", "the Bolts", "Madonna Louise Ciccone", "Bonkill", "Dan Rowan", "Nazi Party", "Nikita Khrushchev", "the Speedway World Championship", "47,818", "1903", "a furrow", "a compound Resource Identifier", "Whitney Houston", "Prada", "the Lionheart", "the Redblush grapefruit", "G.I. Jane", "Lady Sings the Blues", "the zebra mussel", "St Andrew", "a king or Queen", "the long jump", "Asiana Town building"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5080605158730158}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.75, 0.1111111111111111, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3935", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-1049", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-824", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-1851", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-3974", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14664", "mrqa_searchqa-validation-7212"], "SR": 0.4375, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.7457812500000001}, {"timecode": 30, "UKR": 0.806640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1683", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3378", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3516", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-5502", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2479", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2960", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4174", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-99", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-10930", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6388", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-694", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-9797", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-10449", "mrqa_squad-validation-1126", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-143", "mrqa_squad-validation-1539", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1688", "mrqa_squad-validation-1695", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2382", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2854", "mrqa_squad-validation-288", "mrqa_squad-validation-2949", "mrqa_squad-validation-2955", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3516", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4063", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-415", "mrqa_squad-validation-42", "mrqa_squad-validation-4262", "mrqa_squad-validation-4439", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4676", "mrqa_squad-validation-4801", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4951", "mrqa_squad-validation-509", "mrqa_squad-validation-5156", "mrqa_squad-validation-5190", "mrqa_squad-validation-5229", "mrqa_squad-validation-5272", "mrqa_squad-validation-5505", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-612", "mrqa_squad-validation-6221", "mrqa_squad-validation-6254", "mrqa_squad-validation-6404", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-704", "mrqa_squad-validation-7133", "mrqa_squad-validation-718", "mrqa_squad-validation-7233", "mrqa_squad-validation-7273", "mrqa_squad-validation-7322", "mrqa_squad-validation-742", "mrqa_squad-validation-7427", "mrqa_squad-validation-7490", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7731", "mrqa_squad-validation-7767", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-8212", "mrqa_squad-validation-8278", "mrqa_squad-validation-8352", "mrqa_squad-validation-85", "mrqa_squad-validation-8600", "mrqa_squad-validation-8695", "mrqa_squad-validation-8792", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-9189", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9519", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9629", "mrqa_squad-validation-9639", "mrqa_squad-validation-9675", "mrqa_squad-validation-9698", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2757", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3163", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4047", "mrqa_triviaqa-validation-4077", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4275", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-960"], "OKR": 0.89453125, "KG": 0.49375, "before_eval_results": {"predictions": ["Sierra Sky Park", "second", "Recognized Student Organizations", "\"persistent pain.\"", "Saturday", "lower house of parliament,", "the same drama that pulls in the crowds", "Daniel Nestor,", "40", "livingston", "him to step down as majority leader.", "Christopher Savoie", "Missouri.", "Mitt Romney", "206", "Robin", "1986", "struck the left wing of the orbiter", "embryo", "Bill Pullman", "St. Mary's County", "the fovea centralis", "Julie Stichbury", "Dorothy Gale", "George Strait", "11 January 1923", "2015", "hair", "bituminous", "Craggy Island", "Jaime Jaime", "Sodor", "gove", "hybrid", "Cadbury", "Wharton", "fluorine", "cabinet Office Briefing Rooms", "Big Brother", "Whoopi Goldberg", "Harlow Cuadra and Joseph Kerekes", "\"D Daredevil\"", "Lundbeck", "William Shakespeare", "311", "2013", "Belarus", "\"The Braes o' Bowhether\"", "Marvel Comics", "stein", "Google Office", "stiletto", "Dune", "Jane Eyre", "Ron Paul", "The Wall Street Journal", "Lee Harvey Oswald", "Bora Bora", "V for Victory", "the Bolshoi Ballet", "Prelude to the Afterafter of a Faun", "Florida State", "Armenia", "Don Garlits"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4893939393939394}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.45454545454545453, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-4091", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2688", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-1340", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6254", "mrqa_triviaqa-validation-1465", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-4267", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2966", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3241", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5514", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-8649"], "SR": 0.421875, "CSR": 0.5655241935483871, "retrieved_ids": ["mrqa_squad-train-81199", "mrqa_squad-train-30194", "mrqa_squad-train-61283", "mrqa_squad-train-37996", "mrqa_squad-train-67079", "mrqa_squad-train-36077", "mrqa_squad-train-83243", "mrqa_squad-train-36209", "mrqa_squad-train-25177", "mrqa_squad-train-64354", "mrqa_squad-train-57199", "mrqa_squad-train-56446", "mrqa_squad-train-59149", "mrqa_squad-train-17051", "mrqa_squad-train-50653", "mrqa_squad-train-76995", "mrqa_squad-validation-9480", "mrqa_triviaqa-validation-1838", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-104", "mrqa_newsqa-validation-3858", "mrqa_squad-validation-9732", "mrqa_naturalquestions-validation-9530", "mrqa_hotpotqa-validation-4133", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-8541", "mrqa_searchqa-validation-187", "mrqa_newsqa-validation-3590", "mrqa_triviaqa-validation-1081", "mrqa_naturalquestions-validation-4746", "mrqa_newsqa-validation-2019"], "EFR": 1.0, "Overall": 0.7520892137096774}, {"timecode": 31, "before_eval_results": {"predictions": ["article 30", "the Seine", "a house party in Crandon, Wisconsin,", "Jaime Andrade", "South Africa", "Anjuna beach in Goa", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "246", "consumer confidence", "the skull", "Siri", "Lance Cpl. Maria Lauterbach", "protective shoes", "Communist Party of Nepal", "John 6 : 67 -- 71", "Nazi Germany and Fascist Italy", "George II", "Andrew Lloyd Webber", "five", "gastrocnemius", "accomplish the objectives of the organization", "Narendra Modi", "1960", "Marie Fredriksson", "John Findley Wallace", "Jesus Christ", "gebrselassie", "9", "Radio City Music Hall", "geyser", "the Battle of Camlann", "Natty Bumppo", "France", "Maxwell", "piano", "florida", "180", "john Fitzgerald Kennedy", "Bill Walton", "sphagnum", "Homer Hickam, Jr.", "1901", "motorsport world championship", "Kinnairdy Castle", "1999 Odisha", "EBSCO Information Services", "Sylvia Pankhurst", "Bill Clinton", "Roseann O'Donnell", "Iranian government\u2019s propaganda channel", "Bessie Coleman", "They were marooned", "Atlanta Hawks", "Arizona", "coral snake", "the Green Lantern", "Hawaii", "The Greatest Show", "Stephen King", "Blue state", "save the best", "Henry James", "Matt Monro", "the Vital Records Office of the states, capital district, territories and former territories"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6715959821428572}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.28571428571428575, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.625]}}, "before_error_ids": ["mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-960", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-6084", "mrqa_triviaqa-validation-2300", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-821", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4995", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-8819", "mrqa_searchqa-validation-4660", "mrqa_searchqa-validation-7674", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-10097", "mrqa_naturalquestions-validation-6998"], "SR": 0.5625, "CSR": 0.5654296875, "EFR": 1.0, "Overall": 0.7520703125}, {"timecode": 32, "before_eval_results": {"predictions": ["banded iron formations", "multiplication", "five starting pitchers", "heavy tank", "Captain Jones", "in rocks and minerals", "1988", "New Jersey Devils", "Manhattan Project", "former Clash race winners", "October 29, 2015", "Norman Pritchard", "Teddy Randazzo", "Kim Basinger", "Charles Manson", "Sorokoviny", "tall, thin legs", "Hindu Wisdom", "The Green Mile", "paris", "Labrador retrievevers", "blood or transfusions", "peppers", "sebastian govelli", "Johann Baptist Strauss", "Pakistan", "the Kentucky Music Hall of Fame", "3730 km", "Antiochia", "most performed song of all time", "the Neotropical realm", "Moselle", "Prince Sung-won", "actress", "Drowning Pool", "\"Odorama\"", "the title character", "creeks,", "Nazi concentration camps.", "speaking out about a cause", "the river will crest Saturday about 20 feet above flood stage.", "51 percent of the U.S. public consider China a military threat,", "his two-floor home in the St. Louis suburb of Columbia,", "At least 33", "two were in 1986, three in 1995, one in 1997 and one in 2007.", "Sri Lanka's Tamil rebels to \"release\" civilians,", "near Garacad, Somalia,", "June 17 and 18,", "Afghanistan's restive provinces", "\"When you're going into a restaurant environment, you're putting your child's safety and livelihood into other hands,\"", "DNA", "John Hersey", "Solomon", "The Lord of the Rings", "Thai", "a statue", "a solecism", "Rome", "Dragons", "coral", "the moult", "Lafayette", "Bangkok", "MGM Resorts International"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5241477272727273}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10213", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-1582", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3505", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-3300", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3059", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1313", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3736", "mrqa_searchqa-validation-12171", "mrqa_searchqa-validation-3946", "mrqa_searchqa-validation-13044", "mrqa_searchqa-validation-16682", "mrqa_searchqa-validation-10131"], "SR": 0.46875, "CSR": 0.5625, "EFR": 0.9705882352941176, "Overall": 0.7456020220588235}, {"timecode": 33, "before_eval_results": {"predictions": ["the foot of the mast", "Business Connect", "the Americas in the east", "ase", "Deathly Hallows", "Daniel A. Dailey", "Jeff Bezos", "Sharecropping", "a singleesthesidymal tubule ( luminal diameter. 15 -. 25 mm ) to the lumen of the vas deferens", "Nodar Kumaritashvili", "the disk", "Marries Betty", "inverted - drop - shaped icon", "Greek \u1f61\u03c3\u03b1\u03bd\u03bd\u03ac, h\u014dsann\u00e1", "\u201cMy dear, I don\u2019t give a damn\u201d", "kettle Falls, WA", "Jackie Kennedy", "narcolepsy", "Bangladesh", "winton", "fluid is a substance that continually deforms/flows", "blancmange", "tall city", "eagles", "Book of Esther", "ivan child's", "the State House in Augusta", "Pan Am Railways", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "NCAA Division II football season", "David S. Goyer,", "journal", "main east-west road", "Sleeping Beauty", "a secularist and nationalist,", "Martin Joseph O'Malley", "Whitesnake", "Atlas ICBM", "Tuesday", "Joe Harn of the Garland police said.", "the Russian air force,", "nearly $2 billion in stimulus funds", "three", "Genocide Prevention Task Force.", "NATO", "more than 30", "And we are aware that people are trying to convince other legislatures to go down this path, I think it's clear that both as a policy matter and the tough economic times as catalysts for introducing legislation.", "Muslim festival of Eid al-Adha.", "$81,880", "students to study, party, or sleep, only to be confronted with a massive mound of laundry.", "Zeus", "Connecticut", "elementary", "Nixon's next nominee, Judge Harrold Carswell of the Fifth Circuit,", "FDR", "cob", "36", "a fisheye lens", "The Blues Brothers", "Anne Rice", "polygons", "dishwasher", "Jet Republic,", "At least 13 suspects were arrested Sunday and Monday, including three people carrying suicide jackets and explosives inside a bus station,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5605919687950938}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 0.18181818181818182, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.13333333333333336, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0909090909090909, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10442", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-5951", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-597", "mrqa_triviaqa-validation-2378", "mrqa_triviaqa-validation-6480", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-6731", "mrqa_triviaqa-validation-7634", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-1350", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3051", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-6124", "mrqa_newsqa-validation-1095"], "SR": 0.46875, "CSR": 0.5597426470588236, "retrieved_ids": ["mrqa_squad-train-55537", "mrqa_squad-train-74091", "mrqa_squad-train-76502", "mrqa_squad-train-55099", "mrqa_squad-train-33639", "mrqa_squad-train-13162", "mrqa_squad-train-52709", "mrqa_squad-train-64038", "mrqa_squad-train-33615", "mrqa_squad-train-86222", "mrqa_squad-train-33870", "mrqa_squad-train-81975", "mrqa_squad-train-37830", "mrqa_squad-train-81727", "mrqa_squad-train-25107", "mrqa_squad-train-21706", "mrqa_hotpotqa-validation-3971", "mrqa_searchqa-validation-8775", "mrqa_naturalquestions-validation-8346", "mrqa_newsqa-validation-3053", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-9327", "mrqa_newsqa-validation-1834", "mrqa_naturalquestions-validation-9597", "mrqa_newsqa-validation-2821", "mrqa_triviaqa-validation-6728", "mrqa_newsqa-validation-477", "mrqa_hotpotqa-validation-2966", "mrqa_naturalquestions-validation-5961", "mrqa_searchqa-validation-14359", "mrqa_squad-validation-2675", "mrqa_naturalquestions-validation-7047"], "EFR": 1.0, "Overall": 0.7509329044117646}, {"timecode": 34, "before_eval_results": {"predictions": ["he did not want disloyal men in his army", "\"world classic of epoch-making oratory.\"", "1960", "Ernest Hemingway", "Master Christopher Jones", "Pradyumna", "National Football League Kickoff game", "11 : 15 p.m.", "Hirschman", "United States customary units", "Zeus", "1987", "anembryonic gestation", "never made", "Evita", "the Andes", "france", "Barack Obama", "byron", "Super Bowl Sunday", "Elizabeth II", "in Newspapers", "baseball cards", "cuban cigars", "linda usa", "Microsoft", "1955", "The Ones Who Walk Away from Omelas", "Province of New York", "historic buildings, arts, and published works", "Jim Jones", "Amberley Village", "Colonial colleges", "\"Secrets and Lies\"", "Eisenhower Executive Office Building", "Oklahoma Sooners", "Mike Greenwell", "Kurt Vonnegut Jr.", "Bryant Purvis,", "the United States, NATO member states, Russia and India,", "researchers of developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter", "he remains in a coma in a grave condition.", "he acted in self defense in punching businessman Marcus McGhee.", "Iron Eyes Cody", "Monday night", "Piers Morgan Tonight", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "Louvre", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "housing, business and infrastructure repairs,", "Homo", "hold close for warmth or comfort or in affection", "the Boston Red Sox", "a beehive", "Walter Cronkite", "Twenty", "Jon Heder", "green olives", "Abraham Lincoln could write shorter sermons but when I get started I'm too lazy to stop.", "Hermann Hesse", "Baby Gays", "Rick Springfield", "David Bowie", "cruisin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.63531470506091}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.3225806451612903, 0.5, 0.18181818181818182, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-6469", "mrqa_triviaqa-validation-1230", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-4700", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3369", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-7948", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-1952", "mrqa_searchqa-validation-7246", "mrqa_searchqa-validation-12158", "mrqa_triviaqa-validation-6414"], "SR": 0.5625, "CSR": 0.5598214285714286, "EFR": 1.0, "Overall": 0.7509486607142857}, {"timecode": 35, "before_eval_results": {"predictions": ["\"Doctorin' the Tardis\"", "religious", "punk rock", "Aaron Hall", "Biola University in La Mirada, California", "1916 Easter Rising", "A.S. Roma", "Arena of Khazan", "Dungeness crab", "the Man Booker Prize", "England", "Texas's 27th congressional district", "1969", "Mel Blanc", "Jerry Ekandjo", "the ruling city of the Northern Kingdom of Israel, Samaria", "warmth", "the American Revolutionary War", "Gertrude Niesen", "Spain", "Peking", "the Union", "the nucleus", "over 1,100 years ago", "Patrick Swayze", "Michael Phelps", "Nikola Tesla", "relative humidity", "us", "us", "Tennessee", "i i second that emotion", "i second that emotion", "far-right", "Jesse", "frottage", "the United States", "Peter Kay.", "Felipe Massa.", "supply vessel Damon Bankston", "his health", "the shelling of the compound", "1620", "President Paul Biya,", "2-1", "Barack Obama", "a nuclear weapon", "2008,", "natural gas", "Mehsud", "Henry Hudson", "geometry", "Ellen DeGeneres", "Viktor Yanokuvich", "diamonds", "Mork & Mindy", "anthrax", "the city of Indianapolis", "Michael Clayton", "the Waves", "William Jennings Bryan", "Hairspray", "Herman Cain,", "Hakeemullah Mehsud"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6214657738095238}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-10182", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-5957", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5556", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-1664", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6421", "mrqa_searchqa-validation-3697"], "SR": 0.546875, "CSR": 0.5594618055555556, "EFR": 1.0, "Overall": 0.750876736111111}, {"timecode": 36, "before_eval_results": {"predictions": ["5-cylinder engine", "Pat McCormick", "1980", "the heads of federal executive departments who form the Cabinet of the United States", "reservoirs", "Marty Stuart", "A simple majority vote", "the court from its members", "bacteria", "fictional character", "a single representative", "Mexican Seismic Alert System", "the root", "March 16, 2018", "charlie parker", "rabbit", "Indonesia", "Heston Blumenthal", "colorblind", "Archer", "Shirley Bassey", "Cahaba", "jonathan", "Torres Strait", "doctor", "Mendip", "Northampton", "Tropical Storm Ann", "Christina Ricci.", "ten", "Derry", "Koch Industries", "YouTube.", "1978", "Arthur Miller", "over 1 million", "Allies of World War I,", "Plymouth Regional High School", "South Africa's president", "Thursday night.", "The agencies", "Saddam Hussein", "censorship", "three international aid agencies", "authorizing killings and kidnappings by paramilitary death squads.", "forgery and flying without a valid license,", "Intensifying", "more than 200.", "\"Public Enemies\"", "MDC head Morgan Tsvangirai.", "Bush", "Norah Jones", "Absalom", "Casablanca", "\"Sympathy for the Devil\"", "macaroon", "the Flushing River", "Risky Business", "Alien", "the Brooklyn Bridge", "the telegraph office", "the Caucasus Mountains", "Vichy", "So You Think You Can Dance"], "metric_results": {"EM": 0.421875, "QA-F1": 0.491032097907813}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.5, 0.0, 0.08695652173913045, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.2105263157894737, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3347", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-1682", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-5686", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-5740", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-932", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3392", "mrqa_searchqa-validation-11138", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-171"], "SR": 0.421875, "CSR": 0.5557432432432432, "retrieved_ids": ["mrqa_squad-train-65173", "mrqa_squad-train-80390", "mrqa_squad-train-49716", "mrqa_squad-train-74888", "mrqa_squad-train-61300", "mrqa_squad-train-75892", "mrqa_squad-train-68894", "mrqa_squad-train-82694", "mrqa_squad-train-39350", "mrqa_squad-train-73578", "mrqa_squad-train-79078", "mrqa_squad-train-21997", "mrqa_squad-train-56270", "mrqa_squad-train-41243", "mrqa_squad-train-80429", "mrqa_squad-train-1402", "mrqa_searchqa-validation-6423", "mrqa_hotpotqa-validation-4395", "mrqa_triviaqa-validation-5052", "mrqa_hotpotqa-validation-171", "mrqa_newsqa-validation-2445", "mrqa_searchqa-validation-6636", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2148", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-4995", "mrqa_naturalquestions-validation-7837", "mrqa_triviaqa-validation-6854", "mrqa_newsqa-validation-2981", "mrqa_squad-validation-2408", "mrqa_searchqa-validation-3139", "mrqa_naturalquestions-validation-805"], "EFR": 1.0, "Overall": 0.7501330236486485}, {"timecode": 37, "before_eval_results": {"predictions": ["Wisdom, Compassion, Justice and Integrity", "Alex Ryan", "16,801", "1939", "Montreal Canadiens", "Dalveer Bhandari", "Payaya Indians", "Gibraltar", "over 38 million", "5,534", "Mirabilis to AOL in 1998, and from AOL to Mail.Ru Group in 2010", "ideology", "a Boeing 767 - 200ER", "Tessa Virtue and Scott Moir", "Silverdome", "cricket", "2005", "Telstar", "Nikkei index", "jack Nicholson", "Munich", "hierro", "Papua New Guinea", "bridge", "Idris", "July 14th 1789", "The Rural Electrification Act of 1936", "Thomas Allen \"Tom\" Coburn", "Adolfo Rodr\u00edguez Sa\u00e1", "20 July 1981", "Wycliffe Hall, Oxford", "Visigoths", "34", "\"Kill Your Darlings\"", "Christopher Nolan", "David May", "April 8, 1943", "Leucippus", "two", "Nick Adenhart", "\"We're not going to forget you in Washington, D.C.\"", "New York City", "a drug meant only for use in a medical setting by professionals trained in the provision of general anesthesia.", "Sen. Barack Obama", "\"I'm just getting started.\"", "Omar bin Laden,", "a missing sailor", "Sharon Bialek", "Henry,", "Antichrist", "an inch", "herbicides", "the Chief of the House", "Tom Cruise", "the Philippines", "chicken breast", "Ben Siegel", "Oliver", "a bolt", "Wolfgang Johannes Puck", "Chile", "Hell Week", "Alien", "christopher Dickens"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6630980998168499}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-9163", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-6486", "mrqa_triviaqa-validation-633", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3997", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4027", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-6565", "mrqa_searchqa-validation-3831", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-4180", "mrqa_triviaqa-validation-2605"], "SR": 0.546875, "CSR": 0.5555098684210527, "EFR": 1.0, "Overall": 0.7500863486842105}, {"timecode": 38, "before_eval_results": {"predictions": ["194", "Sunset Publishing Corporation", "25 August 1949", "Thrushcross Grange", "2005", "Harold Edward Holt", "their unusual behavior", "Ray Romano", "Bundesliga", "Overijssel, Netherlands", "Budget Rent a Car", "Highlands Course", "1986", "Seretse Khama", "Richie Cunningham", "Amanda Bynes", "April 13, 2018", "March 1995", "September 21, 2017", "January 17, 1899", "Super Bowl LII,", "Heather Stebbins", "Arkansas", "1,228 km / h ( 763 mph )", "Andaman and Nicobar Islands -- Port Blair", "Germany", "Lyon, France", "Nicolas Sarkozy", "kyu", "a sentence", "the diamond", "anesthetic agent", "90%", "South Africa", "Epiphany", "Emmy Awards", "i Eleanor Roosevelt", "diamond", "Dennis Davern,", "12 million", "misdemeanor assault charges", "improve the environment", "\"Toy Story\"", "1994", "can possibly make the greatest contribution by helping the United States", "ketamine.", "Al Nisr Al Saudi", "French trimaran l'Hydroptere", "3 p.m. Wednesday", "Iran", "Copier", "SO2", "a small cap", "Rhizo", "Popular Science magazine", "the Xerox model A", "Mount Rushmore", "Deepika Padukone", "a transit bus", "Louis Brandeis", "May", "Edward Morgan", "Setsuko Thurlow", "the President"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6354074593884376}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 0.5, 0.08695652173913043, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-3448", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-2270", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6002", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1451", "mrqa_searchqa-validation-1559", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-11590", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-14035", "mrqa_searchqa-validation-9322", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-2980", "mrqa_naturalquestions-validation-7202"], "SR": 0.53125, "CSR": 0.5548878205128205, "EFR": 1.0, "Overall": 0.749961939102564}, {"timecode": 39, "before_eval_results": {"predictions": ["Stephen Greenblatt", "Liverpudlian", "2.1 million", "British", "Happy Death Day", "ITV", "G\u00e9rard Depardieu", "Harvard", "\"novel with a key\"", "an Anglo-Saxon saint", "Animorphs", "Caesars Entertainment Corporation", "Juliet", "Autopia", "The balance sheet", "al - Mamlakah al - \u02bbArab\u012byah", "Saint Alphonsa", "Sophia Akuffo", "John Travolta", "New England", "American production duo The Chainsmoker", "Antigonon leptopus", "the referee", "1959", "Emily Rudolph", "Morgan Freeman", "Andy Warhol", "kabaddi", "roose", "eucalyptus", "Canada", "h. H. Asquith", "steve", "Madagascar", "tempera", "holly johnson", "rozzi", "danzig", "Chinese", "school, their books burned,", "\"momentous discovery\"", "Nazi war crimes suspect", "Arsene Wenger", "Gov. Mark Sanford", "bronze medal in the women's figure skating final,", "seven", "$3 billion,", "Zimbabwe Electoral Commission", "Islamabad.", "\"poorest of the poor\"", "Murfreesboro", "The Jetsons", "Vimy Ridge", "a trumpet", "St. Peter's", "Thelonious Monk", "Queen Mary II", "some", "shrew", "Gangbusters", "an 'unconscious' mental process or event, for Freud, is not one which merely", "some really good steaks, a fantastic bottle of red, and then kick back and let the", "HMS Amethyst", "hawkeye"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6120907738095238}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-5998", "mrqa_triviaqa-validation-7252", "mrqa_triviaqa-validation-3228", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6396", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-3801", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-7000", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-15555", "mrqa_triviaqa-validation-7264"], "SR": 0.53125, "CSR": 0.554296875, "retrieved_ids": ["mrqa_squad-train-53946", "mrqa_squad-train-50022", "mrqa_squad-train-1596", "mrqa_squad-train-85141", "mrqa_squad-train-54994", "mrqa_squad-train-75273", "mrqa_squad-train-79708", "mrqa_squad-train-25829", "mrqa_squad-train-48926", "mrqa_squad-train-45136", "mrqa_squad-train-6451", "mrqa_squad-train-67088", "mrqa_squad-train-56766", "mrqa_squad-train-75397", "mrqa_squad-train-10275", "mrqa_squad-train-75214", "mrqa_hotpotqa-validation-5521", "mrqa_naturalquestions-validation-3233", "mrqa_searchqa-validation-16043", "mrqa_naturalquestions-validation-4240", "mrqa_newsqa-validation-1382", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-2270", "mrqa_hotpotqa-validation-5094", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-3213", "mrqa_naturalquestions-validation-7715", "mrqa_hotpotqa-validation-246", "mrqa_searchqa-validation-4537", "mrqa_triviaqa-validation-1766", "mrqa_newsqa-validation-1425"], "EFR": 0.9666666666666667, "Overall": 0.7431770833333333}, {"timecode": 40, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3828", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-4675", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10561", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12195", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12663", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12857", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15788", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-1604", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3797", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-65", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6637", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7589", "mrqa_searchqa-validation-7643", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9486", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-10416", "mrqa_squad-validation-1049", "mrqa_squad-validation-1176", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2402", "mrqa_squad-validation-2489", "mrqa_squad-validation-2840", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3300", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-4127", "mrqa_squad-validation-415", "mrqa_squad-validation-4320", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5129", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5597", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6282", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6610", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-677", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7788", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-7982", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8213", "mrqa_squad-validation-8269", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8744", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9498", "mrqa_squad-validation-9590", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9697", "mrqa_squad-validation-9717", "mrqa_squad-validation-972", "mrqa_squad-validation-9732", "mrqa_squad-validation-9776", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1158", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-1857", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4703", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839"], "OKR": 0.87109375, "KG": 0.478125, "before_eval_results": {"predictions": ["within a few hundred feet of each other", "vitamin injections that promise to improve health and beauty.", "30", "upper respiratory infection,", "two Metro transit trains", "Manmohan Singh", "Jaipur", "served in the military,", "The father of Haleigh Cummings,", "has been a renaissance of the game in the region.", "More than 15,000", "Samoa", "seven", "discard", "1991", "head of the Imperial Family", "push the food down the esophagus", "the hydrological cycle or the hydrologic cycle", "Mike Czerwien", "18", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "Victory gardens", "in Middlesex County, Province of Massachusetts Bay", "Stephen Stills", "defense against rain rather than sun", "18", "beef patties", "s&DR", "South Africa", "tunie", "caucausus", "lorne Greene", "Georgia", "my favorite martian", "Portugal,", "conchita wurst", "Dodge Ram cars", "Telstar", "Reverend Lovejoy", "luchadora", "Girls' Generation", "Sergei Diaghilev", "6'5\"", "Martha Coolidge", "16 November 1973", "Centennial Olympic Stadium", "David Anthony O'Leary", "Sierra Nevada mountains", "Washington", "Prince Antoni Radziwi\u0142\u0142", "epsilon", "America", "Madagascar", "Speed", "a constellation", "champagne", "Like Water for Chocolate", "Sputnik I", "Joseph Smith", "DASB", "Hangman", "Keith Richards", "Paris", "Best Buy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.621638543237808}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.23529411764705882, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5384615384615384, 0.4, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-413", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-4437", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1492", "mrqa_searchqa-validation-14412", "mrqa_searchqa-validation-4155", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-9355", "mrqa_searchqa-validation-44"], "SR": 0.546875, "CSR": 0.5541158536585367, "EFR": 0.9655172413793104, "Overall": 0.7253328690075694}, {"timecode": 41, "before_eval_results": {"predictions": ["Industry and manufacturing", "possible victims of physical and sexual abuse.", "Brian David Mitchell,", "$250,000 for Rivers' charity: God's Love We Deliver.", "Ferraris, a Lamborghini and an Acura NSX", "Larry Ellison,", "Madonna", "Seminole Indian Tribe", "18", "\"drivers of the dependable Camry know what's important in life, and it's not your car.\"", "the Brundell family in Deene Park, England,", "byproducts emitted during the process of burning and melting raw materials.", "\"utterly baseless.\"", "Dolgorsuren Dagvadorj,", "in the town of Acolman, just north of Mexico City", "1898", "must receive the highest number of votes, and also greater than 50 % of the votes", "Isaiah Amir Mustafa", "political ideology", "John Locke", "crowned the dome", "in the 1979 -- 80 season", "issues of the American Civil War", "from the breast or lower chest of beef or veal", "Teri Hatcher as Mel Jones", "in capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "six", "robohunters", "nutshell", "Martin Luther King,", "Rio de Janeiro", "piano", "Pyrrhic War", "Thea Simpson,", "france", "datsun roosevelt i", "charlie miley columbus", "brash", "Scott Mosier", "Leslie Knope", "1535", "United States Food and Drug Administration (FDA)", "University of Oxford", "Bruce Grobbelaar", "Rockhill Furnace, Pennsylvania", "the Chicago Bears", "Farmingville", "Juventus of Italy", "'valley of the hazels'", "January 2016", "Mickey Spillane", "Thurman Munson", "Exodus", "the 65th annual Tony Awards", "Mary", "the Pacific Ocean", "Amish", "coffee", "the foot", "Kansas State University", "Fettuccine", "Field of Dreams", "Benjam\u00edn Arellano F\u00e9lix", "pubs, bars and restaurants"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5665592898948877}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.7499999999999999, 0.6153846153846153, 0.5714285714285715, 0.6086956521739131, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-1122", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-5985", "mrqa_triviaqa-validation-6660", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1963", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-5472", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-10356", "mrqa_searchqa-validation-16878", "mrqa_hotpotqa-validation-655"], "SR": 0.46875, "CSR": 0.5520833333333333, "EFR": 1.0, "Overall": 0.7318229166666665}, {"timecode": 42, "before_eval_results": {"predictions": ["The Dornbirner Ach", "cricket fighting", "XVideos", "William Harold \"Bill\" Ponsford", "Zero Mostel", "various bigfoot-like sightings", "The Grandmaster", "Miss Universe 2010", "King Kal\u0101kaua", "1952", "Squam Lake", "MGM Resorts International", "Golden Gate National Recreation Area", "William Corcoran Eustis", "sedimentary rock", "ideology", "an idiom for the most direct path between two points", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "March 5, 2014", "The genome", "Kit Harington", "April 26, 2005", "since 3, 1, and 4", "birth", "aiding the war effort", "Vincenzo Peruggia", "Netherlands", "gold", "british", "puma", "cabinetmaker", "Friedrich Miescher", "rachmaninoff", "british", "a capella", "Galileo Galilei", "Rocky Horror Show", "Abbey Theatre", "16", "Ralph Lauren,", "Barack Obama", "2,700-acre", "Jeanne Tripplehorn's", "served in the military,", "Karen Floyd", "ties", "an @", "a genocide", "5 1/2-year-old son, Ryder Russell,", "Arsene Wenger", "Boston", "Odysseus", "Nine to Five", "the polar bear", "the French", "Henry Hudson", "Double Jeopardy", "Calais", "Lois Lane", "Falafel", "an island", "an alsacian", "Haiti", "the world's second most populous country after the People's Republic of China"], "metric_results": {"EM": 0.625, "QA-F1": 0.6598958333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3426", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-169", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-5163", "mrqa_triviaqa-validation-2777", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-436", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-9276", "mrqa_searchqa-validation-14554", "mrqa_naturalquestions-validation-8420"], "SR": 0.625, "CSR": 0.5537790697674418, "retrieved_ids": ["mrqa_squad-train-70029", "mrqa_squad-train-41816", "mrqa_squad-train-82811", "mrqa_squad-train-78635", "mrqa_squad-train-52146", "mrqa_squad-train-49243", "mrqa_squad-train-11026", "mrqa_squad-train-25603", "mrqa_squad-train-25452", "mrqa_squad-train-29153", "mrqa_squad-train-25098", "mrqa_squad-train-9870", "mrqa_squad-train-48708", "mrqa_squad-train-47022", "mrqa_squad-train-24553", "mrqa_squad-train-61669", "mrqa_triviaqa-validation-5839", "mrqa_searchqa-validation-14148", "mrqa_newsqa-validation-1319", "mrqa_triviaqa-validation-1003", "mrqa_naturalquestions-validation-7705", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-4142", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-4457", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-6384", "mrqa_hotpotqa-validation-3241", "mrqa_newsqa-validation-2148", "mrqa_hotpotqa-validation-2177"], "EFR": 0.9166666666666666, "Overall": 0.7154953972868217}, {"timecode": 43, "before_eval_results": {"predictions": ["November 2006", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "North Berwick West Links golf course", "Brigadier General Raden Panji Nugroho Notosusanto", "duo", "\"Lonely\"", "Anne Perry", "\"SOS\"", "medieval", "Washington, D.C.", "Smoothie King Center", "Indooroopilly Shopping Centre", "1963\u201393", "\"Traumnovelle\"", "a brownstone in Brooklyn Heights, New York", "1939", "the body - centered cubic ( BCC ) lattice", "a donor molecule", "Hank Williams", "Luther Ingram", "2014", "Kyla Coleman", "the Hongwu Emperor of the Ming Dynasty", "Sebastian Vettel", "an EU state ( Italy", "John Goodman", "verona", "The Jungle Book", "Sardinian", "the Dover based ferry", "Poland", "maurice", "South Africa", "Leicester", "12", "John le Carr\u00e9", "The Virgin Spring", "Canada", "Los Alamitos Joint Forces Training Base", "Leaders of more than 30 Latin American and Caribbean nations", "his enjoyment of sex and how he lost his virginity at age 14.", "warning about tendon problems.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "bedrooms of their two-floor home", "12 brutal rounds", "President Obama", "2002", "U.S. Food and Drug Administration", "Apple", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Deluge", "savings", "Titianus", "diesel", "Yemen", "bewitched", "South African", "David Geffen", "Pitney Bowes", "microwave oven", "Agatha Christie", "Elvis Presley", "electron pairs", "the Harlem River"], "metric_results": {"EM": 0.515625, "QA-F1": 0.592355020253869}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.4210526315789474, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.125, 0.27272727272727276, 0.15384615384615383, 0.7499999999999999, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-2954", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-3048", "mrqa_triviaqa-validation-1419", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-613", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-306", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2617", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-14422", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-2017", "mrqa_naturalquestions-validation-7701"], "SR": 0.515625, "CSR": 0.5529119318181819, "EFR": 1.0, "Overall": 0.7319886363636363}, {"timecode": 44, "before_eval_results": {"predictions": ["Warsaw Uprising Museum", "Scotiabank Saddledome", "once", "Scandinavian design", "1896", "Juventus", "1968", "FX", "Palmolive and Viv Albertine", "1998", "John Anthony \"Jack\" White", "Boyd Gaming", "Big John Studd", "ThonMaker", "Acid rain", "Around 1200", "toys or doorbell installations", "the world's largest Economic Demonstrated Resources of iron ore", "1992 to 2013", "Morgan Freeman", "Lewis Carroll", "September 2000", "the public", "Sir Rowland Hill", "Bonnie Aarons", "Dan Stevens", "Titan", "rugby school", "france", "pangram", "Gower Peninsula", "Atlantic Ocean", "Zork", "Peshtigo, Wisconsin", "Today newspaper", "Home alone 2: Lost in New York", "drag club", "pink", "as he tried to throw a petrol bomb at the officers,", "If an NGO [nongovernmental organization] has done something wrong, it's impossible for us to know.", "used-luxury cars", "July 4.", "Ford is in a different position", "Spaniard Carlos Moya", "around 1610,", "Berga an der Elster", "Yusuf Saad Kamel", "July 1999,", "Chris Robinson,", "Democratic National Convention,", "A Few Good Men", "Brown University", "Fidelio", "porcelain", "Macaulay Culkin", "Neil Diamond", "Northern Exposure", "baseball movie", "Henry Cisneros", "Jacqueline Kennedy Onassis", "Marie Antoinette", "William Conrad", "Hobart", "arson"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6317406400966183}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.08695652173913042, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-839", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-123", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-423", "mrqa_searchqa-validation-12912", "mrqa_searchqa-validation-10831", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-7791"], "SR": 0.53125, "CSR": 0.5524305555555555, "EFR": 1.0, "Overall": 0.7318923611111111}, {"timecode": 45, "before_eval_results": {"predictions": ["Germany and Austria", "Djibouti and Yemen,", "zengo", "Patrick Henry", "vienna", "matlock", "Belgium", "can any one help me", "Peter Ackroyd", "oxygen", "number of even numbers", "obtaining and proper handling of human blood", "apartment", "viata", "one of the most internationally recognized symbols of San Francisco, California, and the United States", "March 11, 2018", "the early 20th century", "Eurasian Plate", "Michael Crawford ( in the title role )", "the heart", "Computer simulation", "111", "a theory", "President Lyndon Johnson", "electric potential generated", "Michael Sherlockarty", "Barbara Niven", "The Supremes", "1974", "Love the Way You Lie", "archery", "Tsung-Dao Lee", "1974", "Anne Erin \"Annie\" Clark", "40,400 members", "Gal Gadot", "John Lennon", "2009", "Grayback forest-firefighters", "a number of celebrities and ministers, ranging from Yolanda Adams to Bishop T.D. Jakes to Kirk Franklin.", "Darrel Mohler", "natural gas", "Friday,", "little blue booties.", "Swiss holders Alinghi", "Ryder Russell,", "France, Russia, India, South Korea, China, South Africa, Brazil, and Poland.", "issued his first military orders as leader of North Korea", "Brazil,", "eight", "iconoclasm", "Tiger", "Exxon", "the chancellor", "food combining", "the Second Seminole War", "pumice", "a megabytes", "eucalyptus", "Maria Montessori", "pot roast", "astrology", "Brooke", "Over the Rainbow"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5018455630358204}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.125, 0.0, 1.0, 0.0, 0.5714285714285715, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-943", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3552", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-6243", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-7976", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-3567", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-961", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-4069", "mrqa_searchqa-validation-13958", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-8152", "mrqa_searchqa-validation-3703"], "SR": 0.4375, "CSR": 0.5499320652173914, "retrieved_ids": ["mrqa_squad-train-63630", "mrqa_squad-train-28678", "mrqa_squad-train-72852", "mrqa_squad-train-57877", "mrqa_squad-train-56408", "mrqa_squad-train-24953", "mrqa_squad-train-45580", "mrqa_squad-train-58301", "mrqa_squad-train-64928", "mrqa_squad-train-67053", "mrqa_squad-train-83766", "mrqa_squad-train-4240", "mrqa_squad-train-50897", "mrqa_squad-train-37546", "mrqa_squad-train-9497", "mrqa_squad-train-62581", "mrqa_newsqa-validation-1664", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-5298", "mrqa_triviaqa-validation-7469", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-1174", "mrqa_squad-validation-6059", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1723", "mrqa_naturalquestions-validation-8582", "mrqa_hotpotqa-validation-3241", "mrqa_searchqa-validation-10939", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-10138", "mrqa_newsqa-validation-2617"], "EFR": 1.0, "Overall": 0.7313926630434783}, {"timecode": 46, "before_eval_results": {"predictions": ["two of Triton's daughters set out on a journey through the depths of the oceans and seas", "squid", "Sir Anthony Eden", "Glory", "Jackie Kennedy", "Tanzania", "Don Quixote", "Buddhism", "IBM", "Mercury", "Menninger", "a husband", "PEZ", "Wattpad", "1986", "$2.187 billion", "Donna", "10 June 1940", "Americans who served in the armed forces and as civilians during World War II", "23 November 1996", "Aaron Harrison", "Bob Dylan", "triglycerides", "the Sui", "Gustav Bauer", "1913", "biathlon", "embellish", "Nathuram Godse", "Big Ben", "the solar system", "Tomb Raider - Secret Of The Sword", "marsupial tiger", "the Mad Hatter", "toll house cookies", "trade clubs", "tunis", "tosca", "Tainted Love", "2012", "R\u00edo Grande", "Everton", "Tom Kitt", "Ellie Kemper", "11,791", "pronghorn", "KULR-TV", "Big 12 Conference", "June 1975", "My Beautiful Dark Twisted Fantasy", "authorizing killings and kidnappings by paramilitary death squads.", "Switzerland", "North Korea", "second-degree attempted murder and conspiracy,", "debris", "Pakistan", "it's historical, inspiring, creative, romantic and beautiful.", "The Sopranos", "Ketchum, Idaho.", "16", "on the Ohio River near Warsaw, Kentucky,", "Adam Lambert and Kris Allen", "HMS Thunderbolt", "the French & Indian War"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6549749729437229}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-801", "mrqa_searchqa-validation-10673", "mrqa_searchqa-validation-9659", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-15080", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-6632", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-7910", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-6264", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-6150", "mrqa_triviaqa-validation-6480", "mrqa_hotpotqa-validation-1103", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-3992", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2573", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4334"], "SR": 0.578125, "CSR": 0.550531914893617, "EFR": 1.0, "Overall": 0.7315126329787234}, {"timecode": 47, "before_eval_results": {"predictions": ["Thomas Merton", "Morocco", "Walter Reed", "Okinawa", "Lake Victoria", "the oil industry", "Walter Reed", "the gallbladder", "Out of Africa", "Dublin", "FRAU", "a light-year", "archery", "awarded to the team that lost the pre-game coin toss", "The Massachusetts Compromise", "Caparra", "Garfield Sobers", "the Old English pyrige ( pear tree )", "Burj Khalifa", "Aristotle", "1920", "54 Mbit / s, plus error correction code", "Jack Barry", "American daytime drama Days of Our Lives", "Wisconsin", "quebec", "hms the only state that has only one bordering, neighboring state", "mongoose", "quebec", "gwynne", "quebec", "Vietnam", "france", "John McCarthy", "tobacco", "playing cards", "Leonard Nimoy", "Rana Daggubati", "Philadelphia", "Nadia Com\u0103neci", "About 200", "Estado Libre y Soberano de Tamaulipas", "1986", "private university", "The Dragon School in Oxford", "Aamir Khan", "Arrowhead Stadium", "Standard Oil", "Matt Groening", "2000.", "voluntary manslaughter", "10,000", "Tsvangirai", "animal products.", "different women coping with breast cancer", "338", "ultra-high-strength steel and boron", "Malawi.", "North Korea", "Belfast's Odyssey Arena.", "Symbionese Liberation Army", "quebec", "Dick Van Dyke", "Pope Benedict XVI"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6544642857142857}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11833", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8233", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-6603", "mrqa_naturalquestions-validation-3246", "mrqa_triviaqa-validation-3680", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-6284", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-517", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-2261", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3097", "mrqa_triviaqa-validation-7750"], "SR": 0.578125, "CSR": 0.5511067708333333, "EFR": 1.0, "Overall": 0.7316276041666666}, {"timecode": 48, "before_eval_results": {"predictions": ["bears", "John Tyler", "roundabout", "gravity", "Ka-bala", "salsus", "Larry King", "pipa", "the Nez Perce", "\"9 To 5\"", "the Thundergod Thor", "Mount Hood", "Smallville", "the Miracles", "Graham Verchere", "March 16, 2018", "Nepal", "ulcerative colitis", "Ferm\u00edn Francisco", "about 15 metres ( 49 feet ) per year", "1933", "Humphrey Bogart", "Book of Exodus", "23 % of GDP", "1930s", "Uganda", "mushrooms", "Japan", "Francis Drake", "a karst cave", "Perkins Chapel", "Jean-Paul Gaultier", "Vienna", "Sheffield United", "\" Lost in Translation\"", "Dawn French", "maine", "Matt Groening", "Australian", "Valeri Vladimirovich \"Val\" Bure", "Get Him to the Greek", "Balloon Street, Manchester", "New York Islanders", "\" training Day\"", "his exploration and settlement of what is now Kentucky", "Burny Mattinson", "Treaty of Trianon", "Santiago del Estero Province", "October 15, 2013", "$41.1 million", "\"Nu au Plateau de Sculpteur\"", "Cirque du Soleil", "her niece", "a ban on inflatable or portable signs and banners on public property.", "new DNA evidence indicated someone else might have committed the crime.", "Alan Graham's", "allegations that a dorm parent mistreated students at the school.", "1616.", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "UC Irvine Medical Center.", "the boy's mother took him", "sent an e-mail to reporters", "the Dutch patent office,", "The patient, who prefers to be anonymous, is finally able to breathe through her nose, smell, eat solid foods and drink out of a cup,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5384454903756374}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.4615384615384615, 0.5, 1.0, 1.0, 0.35294117647058826, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9690", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-16588", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-5483", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-4694", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-1358", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1092"], "SR": 0.4375, "CSR": 0.5487882653061225, "retrieved_ids": ["mrqa_squad-train-62788", "mrqa_squad-train-14707", "mrqa_squad-train-24494", "mrqa_squad-train-27230", "mrqa_squad-train-4453", "mrqa_squad-train-20426", "mrqa_squad-train-43981", "mrqa_squad-train-49559", "mrqa_squad-train-52911", "mrqa_squad-train-24166", "mrqa_squad-train-56614", "mrqa_squad-train-49014", "mrqa_squad-train-27076", "mrqa_squad-train-48345", "mrqa_squad-train-33690", "mrqa_squad-train-44320", "mrqa_hotpotqa-validation-1595", "mrqa_newsqa-validation-0", "mrqa_triviaqa-validation-5559", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4743", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-2733", "mrqa_triviaqa-validation-2270", "mrqa_hotpotqa-validation-5808", "mrqa_newsqa-validation-1157", "mrqa_searchqa-validation-3697", "mrqa_hotpotqa-validation-1038", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-5098"], "EFR": 1.0, "Overall": 0.7311639030612245}, {"timecode": 49, "before_eval_results": {"predictions": ["driving Miss Daisy", "Margaret", "Bronchoconstriction", "Legoland", "William Henry Harrison", "Abraham Lincoln", "Floyd Mayweather Jr.", "Isadora Duncan", "Beth Israel Deaconess Medical Center", "Venice", "Purpura", "fog", "The British Broadcasting Corporation (BBC)", "January 1, 2016", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "Andrew Lloyd Webber and lyrics by Charles Hart and Richard Stilgoe", "enabled business applications to be developed with Flash", "ecological regions", "Rashida Jones", "Representatives", "in the fascia surrounding skeletal muscle", "Harold Godwinson", "lost the strengths that had allowed it to exercise effective control", "Phillip Schofield and Christine Bleakley", "Buddhism", "Howard Hoagland", "secretary of State", "1960", "78% nitrogen", "lewis", "Cold Comfort Farm", "france", "jackstones", "a horizontal desire", "Annie Lennox", "leaf", "William Shakespeare", "the Mayor of the City of New York", "more than 110 films", "The Killer", "Dundalk", "Lawton Mainor Chiles Jr.", "right-hand", "XVideos", "7 November 18672", "Mauthausen-Gusen", "French", "Peterborough", "Joshua Rowley", "Tupolev TU-160,", "producing rock music with a country influence.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Buenos Aires.", "Heshmat Tehran Attarzadeh", "Afghanistan's", "Teresa Hairston", "two years,", "\"one of the most magnificent expressions of freedom and free enterprise in history\"", "the \"face of the peace initiative has been attacked.\"", "a tanker", "Nouri al-Maliki", "GM", "Golden set", "graffiti"], "metric_results": {"EM": 0.5, "QA-F1": 0.5767284798534799}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6153846153846153, 0.42857142857142855, 0.761904761904762, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14051", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-11090", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-725", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-5267", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2475", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3671", "mrqa_triviaqa-validation-2662", "mrqa_triviaqa-validation-1402"], "SR": 0.5, "CSR": 0.5478125, "EFR": 1.0, "Overall": 0.73096875}, {"timecode": 50, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4488", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11459", "mrqa_searchqa-validation-11470", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16588", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-5705", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4694", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7117", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-943"], "OKR": 0.857421875, "KG": 0.5, "before_eval_results": {"predictions": ["the skull & crossbones", "a swan", "The Swamp Fox", "a gourmet jelly bean", "Hungarian-American actress", "Eurydice", "the retina", "foot", "middle-aged", "Papua New Guinea", "William Jennings Bryan", "Apocrypha", "Gallipoli", "Network - Protocol driver", "Hon July Moyo", "1773", "the Vice President", "Sebastian Lund ( Rob Kerkovich )", "a sound stage in front of a live audience in Burbank, California", "the bank's own funds", "the lumbar cistern", "Missouri River", "a line of committed and effective Sultans", "a moral tale", "semi solid", "shotguns", "China", "silver", "Moon River", "Bronx Mowgli", "driving Miss Daisy", "Jeremy Bates", "senior", "christ church", "wigan", "Zagreb", "motor-car.co.uk", "Michael Seater", "Charles Perrault's original fairy tale", "ten years of probation", "\"Empire Falls\"", "Imagine", "Paul Avery", "Louis Silvie \"Louie\" Zamperini", "baeocystin", "Venice, Florida", "Ghanaian", "Kirk Humphreys", "the Mayor of the City of New York", "\"Paul and I went to listen to the music in 5.1 and we go 'Whoa, listen to that,'", "managing his time.", "Bobby Jindal", "Klein and Arnold", "gasoline", "Brett thought it would be best if he resigned,\"", "56,", "Senator Lieberman", "Roy Foster's", "Three teens", "Don Draper", "April 28 -- one day before he was sworn in as ambassador,", "Ethel `` Edy '' Proctor", "Jean - Jacques Rousseau's Confessions, his autobiography ( whose first six books were written in 1765, when Marie Antoinette was nine years of age, and published in 1782 )", "April 1979"], "metric_results": {"EM": 0.5, "QA-F1": 0.6109292975941683}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.0, 0.6666666666666666, 0.9523809523809523, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0909090909090909, 0.5, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10065", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-2548", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5940", "mrqa_triviaqa-validation-689", "mrqa_triviaqa-validation-5385", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-443", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-974", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1122", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-879", "mrqa_naturalquestions-validation-493"], "SR": 0.5, "CSR": 0.546875, "EFR": 0.96875, "Overall": 0.729296875}, {"timecode": 51, "before_eval_results": {"predictions": ["a maraschino cherries", "Belgium", "Mark Antony", "the Dutch", "god", "a lento", "a Crayfish", "time", "Jupiter", "Shropshire", "red", "a solar eclipse", "Jean Foucault", "1986", "Tushar Dalvi", "Database - Protocol driver ( Pure Java driver )", "from atomic numbers 1 ( hydrogen ) to 118 ( oganesson )", "April 6, 1917", "2002", "Johnny Logan", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1955", "an instant messaging client", "France", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Space Oddity", "mildred cram", "red", "stereophonics", "greyfriars", "pater", "Monet", "curling", "Melpomene", "vitamin K", "Wat Tyler", "climatology", "USS \"Essex\" (CV/CVA/CVS-9)", "February 20, 1978", "Kirkcudbright", "dance partner", "Port of Boston", "Lucas Grabeel", "first used the polite \"Hands Up!\" in the course of the robbery", "Scarface", "evangelical Christian periodical", "the Tallahassee City Commission in February 2003", "The Rebirth", "a handheld game console", "Arizona.", "Henry Ford", "returning combat veterans", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "sexual assault on a child.", "top designers, such as Stella McCartney,", "in the west African nation later this year.", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "in the southern port city of Karachi,", "pelvis and sacrum", "in a public housing project,", "President Omar Bongo,", "port", "mel Brooks", "feet"], "metric_results": {"EM": 0.5, "QA-F1": 0.6506737186424687}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 0.9, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.8, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.8571428571428571, 1.0, 0.25, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13059", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-12020", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-9522", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-7003", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-1088", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-3001", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-3076", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-3925", "mrqa_triviaqa-validation-6748"], "SR": 0.5, "CSR": 0.5459735576923077, "retrieved_ids": ["mrqa_squad-train-31700", "mrqa_squad-train-84522", "mrqa_squad-train-5878", "mrqa_squad-train-26089", "mrqa_squad-train-32778", "mrqa_squad-train-68645", "mrqa_squad-train-6218", "mrqa_squad-train-38313", "mrqa_squad-train-8211", "mrqa_squad-train-78344", "mrqa_squad-train-79730", "mrqa_squad-train-57359", "mrqa_squad-train-10318", "mrqa_squad-train-24927", "mrqa_squad-train-42104", "mrqa_squad-train-32239", "mrqa_newsqa-validation-768", "mrqa_hotpotqa-validation-1989", "mrqa_squad-validation-9697", "mrqa_triviaqa-validation-5657", "mrqa_squad-validation-4902", "mrqa_hotpotqa-validation-3771", "mrqa_newsqa-validation-3242", "mrqa_searchqa-validation-3451", "mrqa_triviaqa-validation-5153", "mrqa_triviaqa-validation-1613", "mrqa_naturalquestions-validation-1226", "mrqa_newsqa-validation-3932", "mrqa_naturalquestions-validation-3048", "mrqa_triviaqa-validation-4798", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-9740"], "EFR": 0.96875, "Overall": 0.7291165865384615}, {"timecode": 52, "before_eval_results": {"predictions": ["President George Washington", "the Headless Horseman", "William Howard Taft", "Robbie Shakespeare", "Saudi Arabia", "David", "Odysseus", "the Himalayan", "England", "the Galapagos", "drum", "Big Brown", "Patrick Henry", "the Naturalization Act of 1790", "the forex market", "6 January 793", "Camping World Stadium in Orlando, Florida", "Tandi", "Dadra", "1820s", "Vasoepididymostomy", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "a flood defense system", "the nucleus", "Kirsten Simone Vangsness", "The Rocky Horror Picture Show", "novels", "Virginia", "China", "War and Peace", "sports agent", "Wisconsin", "points based scoring system", "astronomy", "Mary Seacole", "fred lewis", "doesn't include additional costs such as insurance or business rates", "Vilnius", "Sam Kinison", "Nova Scotia", "DI Humphrey Goodman", "Mika H\u00e4kkinen", "Liga MX", "Kevin Peter Hall", "a sailor coming home from a round trip", "Martha Wainwright", "Chicago", "Blue Origin", "Canada's first train robbery", "education about rainforests", "taught a song about freedom of speech.", "opened the door for the man police say was his killer.", "their emergency plans", "Akshay Kumar", "Paktika province in southeastern Afghanistan,", "the return of a fallen U.S. service member", "education", "South Africa", "July 8", "Vernon Forrest,", "\"Larry King Live\"", "embroidered cloth", "Harry Potter", "Jeremy Bates"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6254650297619048}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12283", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-10711", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-366", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-3005", "mrqa_triviaqa-validation-2506", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-981", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-283", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-4128", "mrqa_triviaqa-validation-412"], "SR": 0.578125, "CSR": 0.5465801886792453, "EFR": 1.0, "Overall": 0.735487912735849}, {"timecode": 53, "before_eval_results": {"predictions": ["the Mormon Tabernacle Choir", "the llama", "Jennifer Lopez", "cheddar", "Rudy Giuliani", "Ramen", "Wilhelm", "'House'", "Arkansas", "the phi phenomenon", "Bowl Championship Series", "nomadic", "Wichita", "Ernest Rutherford", "81.617 \u00b0 W \ufeff / 26.617", "CBS Television City, studios 41 and 43 in Hollywood", "Donald Fauntleroy Duck", "10,000 BC", "24", "Harishchandra", "unknown origin", "January 2017", "1989", "Peter Andrew Beardsley MBE", "the 1980s", "1951", "greepshire", "david Copperfield", "as You Like It", "bees", "Canada", "Buenos Aires", "heart", "gneisenau", "gansbaai", "harry redknapp", "an earthquake", "roller coaster", "Hawaii", "1826", "14 December 1990", "Mim", "Edward of Caernarfon", "Charles Eug\u00e8ne Jules Marie Nungesser", "film", "Valley Falls", "Lucas Stephen Grabeel", "1966", "Mark Sinclair", "\"We are still hoping for new tips that could lead us to finding Jennifer Kesse.\"", "alert patients of possible tendon ruptures and tendonitis.", "seven or eight", "Amir Zaki", "Iran's nuclear program.", "bipartisan", "Mashhad", "Willem Dafoe", "her mother, Katherine Jackson,", "Father Cutie", "Jeannie Longo-Ciprelli", "tells stories of different women coping with breast cancer in five vignettes.", "Clio Awards", "Spanish", "the Andes Mountains of Chile and Argentina"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6593743189495478}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3478260869565218, 0.9473684210526316, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-10902", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-3532", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-4129", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3807", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-153", "mrqa_triviaqa-validation-3547"], "SR": 0.578125, "CSR": 0.5471643518518519, "EFR": 0.9629629629629629, "Overall": 0.7281973379629629}, {"timecode": 54, "before_eval_results": {"predictions": ["pilgrim experience", "Montague", "The Pirates of Penzance", "Joseph Goebbels", "Yorkshire", "the Blue", "the GOP to unite behind Donald Trump", "vipers", "\"Dr. Fisher\"", "South Dakota", "Alaska", "Cleveland", "The Seven Year Itch (USA 1955)", "Nicole Gale Anderson", "James Watson and Francis Crick", "New Orleans", "when the forward reaction proceeds at the same rate as the reverse reaction", "over 74", "Sasha Banks", "berkson", "free floating", "100,000", "left coronary artery", "thylakoid membranes", "1800", "nigel short", "creme anglaise", "berks", "1990", "copper", "1979", "robinson crusoe", "HMS Conqueror", "injecting a 7 percent solution intravenously three times a day", "vinegar", "berks", "four red", "Virginia", "1941", "Larry Eustachy", "Oregon State Beavers", "Mexico", "shock cavalry", "The Books", "Cecily Legler Strong", "1933", "Steve Carell", "MediaCityUK", "broadcaster", "Three French journalists, a seven-member Spanish flight crew and one Belgian", "Gulf of Aden,", "top designers", "Authorities in Fayetteville, North Carolina,", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "6-4", "first", "Former Mobile County Circuit Judge Herman Thomas", "poems", "at London's Heathrow", "\"illeg illegitimate.\"", "Arthur E. Morgan III,", "north-east Lithuania", "$10\u201320 million", "September 25, 2017"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5636805555555555}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.3333333333333333, 1.0, 0.5, 0.4, 0.8, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.72, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-1877", "mrqa_searchqa-validation-15700", "mrqa_searchqa-validation-13612", "mrqa_searchqa-validation-98", "mrqa_searchqa-validation-11668", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2179", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2434", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-678", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-891", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-1279", "mrqa_hotpotqa-validation-1439", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-867", "mrqa_hotpotqa-validation-1196"], "SR": 0.421875, "CSR": 0.5448863636363637, "retrieved_ids": ["mrqa_squad-train-40598", "mrqa_squad-train-37807", "mrqa_squad-train-63237", "mrqa_squad-train-84915", "mrqa_squad-train-19778", "mrqa_squad-train-35340", "mrqa_squad-train-36984", "mrqa_squad-train-27515", "mrqa_squad-train-6326", "mrqa_squad-train-11676", "mrqa_squad-train-67693", "mrqa_squad-train-5351", "mrqa_squad-train-10581", "mrqa_squad-train-49322", "mrqa_squad-train-84226", "mrqa_squad-train-6928", "mrqa_searchqa-validation-13958", "mrqa_newsqa-validation-1808", "mrqa_searchqa-validation-10902", "mrqa_newsqa-validation-3041", "mrqa_naturalquestions-validation-5998", "mrqa_squad-validation-1146", "mrqa_searchqa-validation-11590", "mrqa_squad-validation-4332", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2315", "mrqa_hotpotqa-validation-5298", "mrqa_searchqa-validation-12438", "mrqa_newsqa-validation-1134", "mrqa_squad-validation-969", "mrqa_hotpotqa-validation-4995", "mrqa_newsqa-validation-595"], "EFR": 1.0, "Overall": 0.7351491477272727}, {"timecode": 55, "before_eval_results": {"predictions": ["Norma", "Ohio", "the person compelled to pay for reformist programs", "c. 3000 BC", "Donna Mills", "for the red - bed country of its watershed", "Session Initiation Protocol", "Thirty years after the Galactic Civil War", "a graded basis, consisting of pass grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ), 3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "Carol Ann Susi", "De Wayne Warren", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government", "Yugoslavia", "musical term Glossary", "Emily Davison", "Islands", "a nerve cell cluster", "Tripoli", "San Antonio", "Oklahoma", "European Union (EU)", "peregrines", "Wadsworth", "\"Stars on 45 Medley\"", "\"Big Dipper\"", "Dwarka", "Daniel Andre Sturridge", "Texas Longhorns", "Omega SA", "It's Always Sunny in Philadelphia", "Delilah Rene", "Italian", "WB Television Network", "Laura Jeanne Reese Witherspoon", "British Labour Party", "Christopher Tin", "Dutch", "President Barack Obama,", "bipartisan", "a woman", "effects of ABC's \"Dancing With The Stars\" and NBC's \"The Biggest Loser.\"", "Mikkel Kessler", "$250,000 for Rivers' charity: God's Love We Deliver.", "Nook", "Golfer Tiger Woods", "800,000", "The 19-year-old woman", "102", "we will be back,\" Ali's wife Lonnie told Britain's Daily Telegraph newspaper.", "Brass", "birds", "a ship or boat", "Xmas", "West Point", "Joe DiMaggio", "the Supreme Law", "Quasars", "Japan", "Presidential Election", "Cardiopulmonary resuscitation", "Jumbo", "Spock", "Debbie Abrahams", "Apollo 17"], "metric_results": {"EM": 0.5, "QA-F1": 0.616366341991342}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true], "QA-F1": [0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.4, 0.16, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-993", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-6436", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-7521", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-2049", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3759", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2803", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-9641", "mrqa_searchqa-validation-2882", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-5058", "mrqa_triviaqa-validation-4987"], "SR": 0.5, "CSR": 0.5440848214285714, "EFR": 0.9375, "Overall": 0.7224888392857143}, {"timecode": 56, "before_eval_results": {"predictions": ["Michael Phelps", "scurvy", "isosceles triangle", "lewis disaster", "1822", "willow", "perfume", "fertilization", "manx cat", "rasputin", "(John) Bercow", "Peter Stuyvesant", "Botham", "Ed Sheeran", "Jackie Robinson", "erosion", "Kelly Osbourne", "T'Pau", "January 12, 2017", "80", "Thomas Edison", "CBS", "Andrew Garfield", "is married to Bobby", "ulnar nerve", "Kareena Kapoor", "Francisco Antonio Zea", "ARY Films", "1967", "a particular nation", "Lowestoft", "Antonio Salieri", "McKinsey's offices in Silicon Valley and India", "British Labour Party", "Camille Saint-Sa\u00ebns", "( Gerald) Ford", "\"H\u00e6vnen\"", "Dr. Maria Siemionow,", "second time since the 1990s", "blind Majid Movahedi,", "weight-loss", "at the home, stopping to marvel at the stately main hall and gliding their hands along the same banister that supported the likes of the Marquis de Lafayette.", "attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "three different videos", "1995", "for death squad killings carried out during his rule in the 1990s.", "he discussed foreplay, sexual conquests and how he picks up women,", "to make life a little easier for these families", "Paul Schlesselman", "Worcestershire", "Isaac Newton", "titanium", "Robert Browning", "St. Augustine", "staff", "Sicilian pizza", "Samuel Johnson", "mum", "Dalmatian", "duck", "done", "Jasenovac concentration camp", "Kohlberg K Travis Roberts", "John Robert Cocker"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6306208228905598}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.2105263157894737, 0.08333333333333334, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-7556", "mrqa_triviaqa-validation-405", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-993", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-9741", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-5490", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-2354", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-1146", "mrqa_searchqa-validation-1319", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4014", "mrqa_searchqa-validation-5040", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4926"], "SR": 0.546875, "CSR": 0.5441337719298245, "EFR": 1.0, "Overall": 0.7349986293859649}, {"timecode": 57, "before_eval_results": {"predictions": ["scored a hat-trick", "a satellite", "bipartisan", "10", "\"The show went on without the self-proclaimed \"King of the South,\"", "individual pieces.", "Himalayan", "Stratfor,", "tells stories of different women coping with breast cancer in five vignettes.", "Ewan McGregor", "the Obama chief of staff", "the Security Council Resolution 1718,", "their culture, religion and national identity.", "early 2017", "eusebeia", "chain elongation", "the coffee shop Monk's", "Richard Carpenter", "Kate Flannery", "October 2012", "Matthew Gregory Wise", "2003", "a young girl Panic trying to cross a field with a dog barking in the background", "13,000 astronomical units ( 0.21 ly )", "2018", "1973", "Toy Story", "gymnastics", "Charlie Sheen", "Egypt", "herald", "blue", "the Prophet Joseph Smith,", "lewis", "Ohio", "bayer", "Madagascar", "British", "Cartoon Network", "Chester", "a large green dinosaur", "the role of Zander", "the 45th Infantry Division", "Knoxville", "Lionsgate", "his superhero roles", "A123 Systems, LLC", "Groom Lake Valley", "Jenji Kohan", "Charles Dickens", "November 22, 1963", "Big Brown", "the Navy Seahawksawk", "Parkinson's", "Coast Highway", "force", "Mary", "\"Kenny\" Rogers", "ice hockey", "Minnesota", "\"Clover Hill\"", "William Randolph Hearst", "Dublin", "Moses"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5508556547619048}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-481", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-8832", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-3137", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-2749", "mrqa_searchqa-validation-11956", "mrqa_searchqa-validation-13774", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-495", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-13336", "mrqa_searchqa-validation-1462"], "SR": 0.4375, "CSR": 0.5422952586206897, "retrieved_ids": ["mrqa_squad-train-49750", "mrqa_squad-train-65259", "mrqa_squad-train-24632", "mrqa_squad-train-50844", "mrqa_squad-train-58223", "mrqa_squad-train-2168", "mrqa_squad-train-66999", "mrqa_squad-train-58387", "mrqa_squad-train-69816", "mrqa_squad-train-42080", "mrqa_squad-train-72910", "mrqa_squad-train-63098", "mrqa_squad-train-30391", "mrqa_squad-train-73560", "mrqa_squad-train-27118", "mrqa_squad-train-21723", "mrqa_naturalquestions-validation-3784", "mrqa_newsqa-validation-4128", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5022", "mrqa_naturalquestions-validation-4556", "mrqa_hotpotqa-validation-5608", "mrqa_squad-validation-2291", "mrqa_triviaqa-validation-1718", "mrqa_searchqa-validation-2833", "mrqa_searchqa-validation-7674", "mrqa_hotpotqa-validation-4133", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-1562", "mrqa_newsqa-validation-1941", "mrqa_hotpotqa-validation-1811", "mrqa_newsqa-validation-3503"], "EFR": 0.9722222222222222, "Overall": 0.7290753711685823}, {"timecode": 58, "before_eval_results": {"predictions": ["June 2007", "Dialogues des Carmelites", "Dr. Alberto Taquini", "England", "Mary Harron", "Bank of China Tower", "Borgo San Donnino", "two Boeing 747 passenger jets, KLM Flight 4805 and Pan Am Flight 1736, collided on the runway at Los Rodeos Airport (now Tenerife North Airport)", "electric currents and magnetic fields", "Benjamin Andrew \"Ben\" Stokes", "Robert Frost's former home in Franconia, New Hampshire, United States", "2006", "Tianhe Stadium", "Mickey Rourke", "Gayla Peevey", "2001", "husky or other Nordic breed", "Otis Timson", "Richard Masur", "Gametes", "the church sexton Robert Newman and Captain John Pulling", "commemorating fealty and filial piety", "February 16, 2018", "West African traditions", "pop ballad", "taxonomic rank", "James Dean", "nirvana", "Russia,", "Daily Bugle", "Canada", "Sugarloaf Mountain", "Battle of Hastings", "madonna", "basket", "joseph", "maine", "Uighurs to appear in his Washington courtroom at 10 a.m. Friday and said he would hold a hearing next week to determine under what conditions they will be settled in the United States.", "German Chancellor Angela Merkel", "\"Quiet Nights,\"", "a series of wildfires from late summer through autumn in Bastrop County.", "15,000", "four", "British", "Workers' Party.", "President Obama and Britain's Prince Charles", "Asashoryu", "$1.45 billion", "Dr. Cade", "Columbia River", "brass", "HaleBopp", "Dolly Parton", "Terry Bradshaw", "Scotland", "Tunis", "Kunta Kinte", "Venetian", "a cappella", "Narcissus", "the FBI", "21-year-old", "$14.1 million", "work."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5989316645234198}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.923076923076923, 0.0, 0.6666666666666666, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0851063829787234, 1.0, 1.0, 0.42857142857142855, 0.5, 1.0, 1.0, 0.6666666666666666, 0.26666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-1763", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-733", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-4925", "mrqa_triviaqa-validation-3907", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-7492", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1128", "mrqa_searchqa-validation-6023", "mrqa_searchqa-validation-7395", "mrqa_searchqa-validation-6939", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-2458", "mrqa_searchqa-validation-13194", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1348"], "SR": 0.46875, "CSR": 0.5410487288135593, "EFR": 0.9705882352941176, "Overall": 0.7284992678215354}, {"timecode": 59, "before_eval_results": {"predictions": ["after the conclusion of the season", "Bank of China ( Hong Kong)", "John Snow (15 March 1813 \u2013 16 June 1858)", "the Battelle Energy Alliance", "John of Gaunt", "European basketball", "1946", "35,402", "voice-work", "Mwabvi", "\"Rich Girl\"", "California State University, Dominguez Hills", "DIC Entertainment", "the utopian novels of H.G. Wells", "President Gerald Ford", "1957", "160km / hour", "March 16, 2018", "Argentine composer Lalo Schifrin", "The Cornett family", "Baaghi ( English : Rebel )", "the Old Testament", "July 23, 2016", "The Third Five - year Plan", "Andaman and Nicobar Islands -- Port Blair", "eight", "speedway", "chairs", "la traviata", "dyan Cannon", "midsomer Murders", "Norman Mailer", "British yachtswoman Dee Caffari", "Harry S. Truman", "lakes and rivers", "Mars", "bamboozled", "Tillakaratne Dilshan", "\" Maria\"", "parents", "Nigeria, Africa's largest producer.", "Fullerton, California,", "if the airline doesn't perform, the credit card company still has your money and can give it right back to you.", "the man was dead,", "back at work", "Gavin de Becker", "cars know what's important in life,", "a review of state government practices completed in 100 days.", "Sunday.", "Rhodes", "Tartarus", "Mountaineering", "Alaska", "Wings", "birds", "Mindanao", "stizein", "candies", "quicksand", "tanks", "\"9 To 5\"", "Eleven", "Elin Nordegren,", "Beijing"], "metric_results": {"EM": 0.5, "QA-F1": 0.5750766594516594}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.6666666666666665, 0.3636363636363636, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-4905", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-5665", "mrqa_triviaqa-validation-3282", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-2374", "mrqa_triviaqa-validation-2709", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-2401", "mrqa_searchqa-validation-10663", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-14761", "mrqa_searchqa-validation-10352", "mrqa_searchqa-validation-2826", "mrqa_newsqa-validation-2493"], "SR": 0.5, "CSR": 0.5403645833333333, "EFR": 1.0, "Overall": 0.7342447916666666}, {"timecode": 60, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1196", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3008", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14834", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5058", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-7943", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9518", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-324", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5360", "mrqa_squad-validation-551", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1104", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1694", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3573", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7537", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-943"], "OKR": 0.78125, "KG": 0.48359375, "before_eval_results": {"predictions": ["British", "Timo Hildebrand", "Samuel Beckett", "constant support from propaganda campaigns", "Marvel Comics", "Philadelphia", "Buddha's delight", "1959", "Anishinaabeg", "Marktown", "45%", "850 saloon", "1942", "bicameral Congress", "Horace Lawson Hunley", "Human fertilization", "3D modeling", "Franklin D. Roosevelt", "Germany", "ase", "Andreas Vesalius", "Mary Elizabeth Patterson", "in the fictional town of West Egg on prosperous Long Island", "William Jennings Bryan", "Minneapolis, Minnesota", "5", "arthur", "Sherlock Holmes", "Djibouti and Yemen", "Pallenberg", "hay fever", "bacallini", "Lincoln Logs", "france", "Styal", "Sicily", "Gorky", "Brian Mabry", "The opposition group, also known as the \"red shirts,\" is demanding that the prime minister dissolve the parliament within 15 days.", "Islamic insurgents.", "eight", "Nairobi, Kenya,", "Sweden,", "one", "global intelligence company,", "183", "Sub-Saharan Africa", "jazz", "B-movie queen lauren Clarkson", "cavities", "Joe Paterno", "Bacon", "Shrew", "elms", "Heijo", "Universal Studios Hollywood", "visibilis", "Fairfax", "the Clark bar", "diameter", "Mastodonsaurus", "Ma Khin Khin Leh,", "protect ocean ecology, address climate change and promote sustainable ocean economies.", "Woods"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6284931077694236}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-126", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-3617", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-2799", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-2299", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-5273", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-2879", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-2200", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10411", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-4850", "mrqa_newsqa-validation-4168"], "SR": 0.5625, "CSR": 0.5407274590163935, "retrieved_ids": ["mrqa_squad-train-73635", "mrqa_squad-train-43618", "mrqa_squad-train-75446", "mrqa_squad-train-22305", "mrqa_squad-train-78474", "mrqa_squad-train-83416", "mrqa_squad-train-74546", "mrqa_squad-train-64480", "mrqa_squad-train-29554", "mrqa_squad-train-3084", "mrqa_squad-train-86184", "mrqa_squad-train-38563", "mrqa_squad-train-33942", "mrqa_squad-train-42424", "mrqa_squad-train-47656", "mrqa_squad-train-25348", "mrqa_hotpotqa-validation-151", "mrqa_searchqa-validation-187", "mrqa_triviaqa-validation-2709", "mrqa_searchqa-validation-10163", "mrqa_triviaqa-validation-2378", "mrqa_triviaqa-validation-7232", "mrqa_searchqa-validation-2136", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-3170", "mrqa_searchqa-validation-3831", "mrqa_squad-validation-4856", "mrqa_newsqa-validation-1092", "mrqa_triviaqa-validation-4266", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-8832"], "EFR": 1.0, "Overall": 0.7079892418032786}, {"timecode": 61, "before_eval_results": {"predictions": ["\"King of Cool\"", "Rhode Island", "Lake Placid, New York", "the Dominican Republic", "on the north bank of the North Esk", "Bolshoi", "Franconia, New Hampshire", "Tie Domi", "Charice", "Ian Fleming", "17 December 1998", "30", "James William McCutcheon", "Nodar Kumaritashvili", "Aaron Lewis", "the United States", "Havana Harbor", "`` Far Away ''", "Help!", "French Canadian fur traders", "Georgia", "1997", "Michigan", "Mary Elizabeth Patterson", "Oscar", "Catherine of aragon", "upstairs, Downstairs", "Ruth Ellis", "Batman", "CAPTCHA", "Manifest Destiny", "bacename", "Gary Oldman", "cardinal", "bacall", "Rats", "poets the Borough", "Chinese President Hu Jintao", "ties,", "the vicious brutality which accompanied the murders of his father and brother.\"", "his comments had been taken out of context.", "U.S. Assistant Secretary of State for African Affairs Jendayi Frazer", "Manmohan Singh's Congress", "southern port city of Karachi,", "could spur U.S. diplomacy to prevent Iran from developing nuclear weapons", "Ashley \"A.J.\" Jewell,", "Seoul,", "South Africa", "10 percent", "Reuben", "John Deere", "Sarai", "a number", "Victor Hugo", "a pram", "The Beatles", "Herman Wouk", "Frankenstein", "Linkin Park", "Jupiter", "a plug", "between 1765 and 1783", "Tigris and Euphrates", "12951 / 52 Mumbai Rajdhani Express"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6446428571428572}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-941", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1083", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-8762", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5968", "mrqa_triviaqa-validation-2427", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-15104", "mrqa_searchqa-validation-6699", "mrqa_searchqa-validation-6028", "mrqa_searchqa-validation-4816", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14344", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-9089"], "SR": 0.53125, "CSR": 0.5405745967741935, "EFR": 0.9666666666666667, "Overall": 0.701292002688172}, {"timecode": 62, "before_eval_results": {"predictions": ["Ukraine", "Ethel Merman", "in Poems : Series 1", "Julie Karen Kavner", "Kansas", "1937", "French Canadian", "1997", "2014 Winter Olympics in Sochi, Russia", "Guantanamo Bay Naval Base", "Thomas Mundy Peterson", "Walter Egan", "Alex Drake", "Sicily", "john Walsh", "12", "Ford Motor Company", "america", "a pair of men who divert themselves while they wait expectantly and in vain for someone named Godot", "Sony Interactive Entertainment", "hansa", "Gibraltar", "mercury", "tatyana", "kenny Everett", "1937", "Tommy Cannon", "Charles Reed Bishop", "D\u00e2mbovi\u021ba River", "400", "supernatural psychological horror", "Martin Truex Jr.", "848", "\"The Legend of the Brown Mountain Lights\"", "\"best of\" and \"worst of\" lists", "\"My Boss, My Hero\"", "\"Grave Digger\"", "Lashat-ul-Jihad al-Islami (HuJi)", "voluntary manslaughter", "AS", "oys And Girls alone", "Mitt Romney", "CNN", "Iowa's critical presidential caucuses", "Yemen,", "137", "U.N. Security Council", "Mike Meehan", "Government Accountability Office report", "Friendship 7", "the Bering Strait", "\"A Doll's House\"", "\"House of Sand and Fog\"", "molasses", "\"Knocked Up\"", "upright", "the Rhine", "a turkey wing", "leg", "grain", "xenon", "Kentucky", "1892", "\"Nana\" Patekar"], "metric_results": {"EM": 0.625, "QA-F1": 0.7160676129426129}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.3076923076923077, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-4203", "mrqa_triviaqa-validation-7555", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6243", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-5174", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3856", "mrqa_searchqa-validation-7364", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9465", "mrqa_hotpotqa-validation-4624"], "SR": 0.625, "CSR": 0.5419146825396826, "EFR": 0.9166666666666666, "Overall": 0.6915600198412698}, {"timecode": 63, "before_eval_results": {"predictions": ["Saint Michael, Barbados", "Danish", "\"Brickyard\"", "Richard Arthur", "Blue Grass Airport", "France", "1872", "The Livingston family", "General Sir John Monash", "Margarine Unie", "July 8, 2014", "Dragons: Riders of Berk", "Trent Alexander-Arnold", "his friends, Humpty Dumpty and Kitty Softpaws", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "erosion", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "depending on the gender of the reigning monarch", "1923", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "Oklahoma native Major General Clarence L. Tinker", "Daren Maxwell Kagasoff", "the courts", "16", "Beorn", "b - whale", "columbus", "9", "arthur", "German", "adventure", "Catherine of Aragon", "Patrick Troughton", "isosceles", "mercury", "vienna", "umbrella", "The Drug Enforcement Administration said Wednesday it's considering tighter restrictions on propofol,", "the spine.", "NASCAR", "Defense of Marriage Act", "the immorality of these deviant young men", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "The Tinkler.", "Ninety-two", "normal", "sculptures", "250,000", "his Brazilian counterpart, Emilio Medici, would be playing a bigger role in hemispheric affairs and seeking to fill whatever vacuum the U.S. leaves behind.", "the Corinthian", "poppy", "Salizan S. Sharipov", "Paganini", "The Untouchables", "Hinduism", "Maryland", "Don Quixote", "opera buffa", "Ezra Pound", "ponies", "Homicide: Life on the Street", "baku", "About Eve", "fats"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6520116842007826}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 0.0, 0.7058823529411764, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.23076923076923078, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.05405405405405406, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2577", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1640", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-6810", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-12690", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14562", "mrqa_searchqa-validation-8778", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-5176"], "SR": 0.5625, "CSR": 0.542236328125, "retrieved_ids": ["mrqa_squad-train-71564", "mrqa_squad-train-42823", "mrqa_squad-train-54649", "mrqa_squad-train-14418", "mrqa_squad-train-29461", "mrqa_squad-train-47817", "mrqa_squad-train-61116", "mrqa_squad-train-63505", "mrqa_squad-train-72928", "mrqa_squad-train-48452", "mrqa_squad-train-1860", "mrqa_squad-train-49637", "mrqa_squad-train-19444", "mrqa_squad-train-84581", "mrqa_squad-train-73540", "mrqa_squad-train-6301", "mrqa_squad-validation-2675", "mrqa_newsqa-validation-3614", "mrqa_hotpotqa-validation-4696", "mrqa_naturalquestions-validation-7848", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-10131", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-10118", "mrqa_newsqa-validation-3429", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-1833", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-3101", "mrqa_squad-validation-4902", "mrqa_hotpotqa-validation-5472", "mrqa_hotpotqa-validation-2369"], "EFR": 1.0, "Overall": 0.708291015625}, {"timecode": 64, "before_eval_results": {"predictions": ["germany", "anne boleyn", "poland", "Wellington", "judy", "backgammon", "Denver", "judy holliday", "an Adam\u2019s apple", "Jemima Potts", "germany", "raw hides", "germany", "Cheryl Campbell", "a crust of mashed potato", "Thomas Jefferson", "Duck", "Wednesday, September 21, 2016", "a mashed potato crust", "Matthew Gregory Wise", "Magnavox Odyssey", "Lager", "Taron Egerton", "Mankombu Sambasivan Swaminathan", "the majority coming from Western Australia", "Oahu", "Kathleen O'Brien", "John John Florence", "Jean-Marie Pfaff", "John Lennon/Plastic Ono Band", "Parapsychologist Konstant\u012bns Raudive", "1901", "Ben R. Guttery", "Singapore", "Galt\u00fcr", "Pensacola, Florida", "Christopher Lloyd Smalling", "$10 billion", "28 years", "heavy brush,", "Defense of Marriage", "19", "up three", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "20 minutes, five days a week.", "will be inducted into the Baseball Hall of Fame in July.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Caylee Anthony's", "10 to 15 percent", "On the Celsius scale, water freeze at this temperature", "a claymore", "an elk", "Solidarity", "Christopher Columbus", "shalom", "Finnish", "the lead villain", "Sartre", "a diamond", "Peter Shaffer", "a magnet", "Palatine Hill", "a cow", "a bassoon"], "metric_results": {"EM": 0.5, "QA-F1": 0.6368303571428572}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.8571428571428571, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8000000000000002, 0.0, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-6752", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-6100", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10617", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-4490", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-4883", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3561", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-780", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-13757"], "SR": 0.5, "CSR": 0.5415865384615385, "EFR": 1.0, "Overall": 0.7081610576923076}, {"timecode": 65, "before_eval_results": {"predictions": ["over 300,000", "DeWayne Warren", "Warren Hastings", "Colman", "Dougie MacLean", "Saint Alphonsa, F.C.C., ( born Anna Muttathupadathu ; 19 August 1910 -- 28 July 1946 )", "Super Bowl LII, their fourth NFL title", "Carroll O'Connor", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "A simple majority", "Jenny Slate", "Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Pakistan", "cotswolds", "evonne goolagong cawley", "tins", "b-24 Liberator", "beetles", "sonja Henie", "cyclops", "John Constable", "French", "BBC One's", "sea monster", "eva hanks", "filibuster and scathing rhetoric", "Stephen Ireland", "The United States presidential election of 2016", "ten", "Kaley Christine Cuoco", "lady", "The Washington Post", "Sippin' on Some Syrup", "film playback singer, director, writer and producer", "27 January 1974", "August 14, 1848", "Bill Paxton", "Michael Krane,", "Jaime Andrade", "Larry Ellison,", "teenage", "severe", "women", "10,000 refugees,", "Marine", "Vicente Carrillo Leyva,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "16th grand Slam title.", "Katherine", "Henry David Thoreau", "Two Guys in the Dark", "Afghanistan", "Panda", "Catherine the Great", "the bass viol", "Jawaharlal Nehru", "isthmus", "crunching", "Rights of Man", "crone", "frequency", "Toni Morrison", "Type A", "Dracula"], "metric_results": {"EM": 0.5, "QA-F1": 0.5787202380952381}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.6, 0.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-3562", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-4955", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-1666", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-4720", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1154", "mrqa_searchqa-validation-2935", "mrqa_searchqa-validation-11354", "mrqa_searchqa-validation-15729", "mrqa_searchqa-validation-14525", "mrqa_searchqa-validation-10278"], "SR": 0.5, "CSR": 0.5409564393939394, "EFR": 0.96875, "Overall": 0.7017850378787879}, {"timecode": 66, "before_eval_results": {"predictions": ["around 2011", "1956", "in a counter clockwise direction", "2018", "Olivia Olson", "1987", "a humid subtropical climate, with hot summers and mild winters", "agriculture", "The 2017 -- 18 UEFA Champions League knockout phase began on 13 February and will end on 26 May 2018 with the final at the NSC Olimpiyskiy Stadium in Kiev, Ukraine", "eleven", "Meri", "the winter solstice", "Kyla Pratt", "Nile", "orangutans", "Emeril Lagasse", "one", "solitaire", "Bono", "drake", "cuckoo", "a region of SW Asia between the Tigris and Euphrates rivers", "geoffrey bacall", "paris underground", "brackish", "United States Marine Corps", "Rhode Island School of Design", "Ector County", "Major League Soccer", "Gatwick Airport", "Sam Raimi", "J. K. Rowling", "a personalized certificate", "Colin Vaines", "11 November 1821", "The Crowned Prince of the Philadelphia Mob/M mafia", "fifth", "in the cellar with their mother, never seeing daylight.", "Tuesday afternoon.", "Sheik Mohammed Ali", "Marie-Therese Walter.", "Michael Jackson", "the reality he has seen is \"terrifying.\"", "through a facility in Salt Lake City, Utah,", "his past and his future", "27-year-old's", "38,", "Ameneh Bahrami", "Spc. Megan Lynn Touma,", "the zodiac", "33.4%", "Jacob", "the proverbial forest", "TCM", "Atlas", "Libya", "Bismarck", "Brazil", "Bad Girls", "Ralph Lauren", "Colorado", "she is an angel, she is God-sent,\"", "The Gibson Showcase and The Apple Barn Cider Bar & General Store", "natural disasters"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6301854754440961}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8333333333333334, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-7227", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-1805", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-2289", "mrqa_hotpotqa-validation-1982", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-447", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-536", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-7242", "mrqa_searchqa-validation-9653", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-268"], "SR": 0.515625, "CSR": 0.5405783582089552, "retrieved_ids": ["mrqa_squad-train-58385", "mrqa_squad-train-83993", "mrqa_squad-train-65516", "mrqa_squad-train-53869", "mrqa_squad-train-51473", "mrqa_squad-train-36276", "mrqa_squad-train-6308", "mrqa_squad-train-4261", "mrqa_squad-train-40517", "mrqa_squad-train-64", "mrqa_squad-train-42921", "mrqa_squad-train-32690", "mrqa_squad-train-80926", "mrqa_squad-train-32428", "mrqa_squad-train-72632", "mrqa_squad-train-63408", "mrqa_searchqa-validation-314", "mrqa_naturalquestions-validation-3562", "mrqa_hotpotqa-validation-1103", "mrqa_naturalquestions-validation-3236", "mrqa_hotpotqa-validation-97", "mrqa_searchqa-validation-1559", "mrqa_newsqa-validation-2550", "mrqa_searchqa-validation-14412", "mrqa_triviaqa-validation-1562", "mrqa_naturalquestions-validation-5000", "mrqa_triviaqa-validation-3132", "mrqa_squad-validation-10416", "mrqa_triviaqa-validation-1613", "mrqa_naturalquestions-validation-3757", "mrqa_newsqa-validation-2103", "mrqa_triviaqa-validation-7082"], "EFR": 1.0, "Overall": 0.7079594216417909}, {"timecode": 67, "before_eval_results": {"predictions": ["coercivity", "branch roots", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "Midwestern theater owner named Glen W. Dickson", "James Corden", "often linked to high - ranking ( though not necessarily royalty ) in China", "four", "Hellenismos", "1878", "Randy Newman", "2007", "Pangaea", "early 1800s", "Jack Douglas", "orangish tinge", "James Callaghan", "bismarck", "brazil", "Tomorrow Never Dies", "Caracas", "magnetic", "Madagascar", "radiotelephony Spelling Alphabet", "Krgystan", "cats", "motor ships", "Arlo Looking Cloud", "Francis Egerton", "Max Kellerman", "Rothschild banking dynasty", "10 June 1921", "Scotiabank Saddledome", "\"Slaughterhouse-Five\"", "Indian", "Socrates", "1942", "National Lottery", "two remaining crew members", "a deceased organ donor,", "Tennis Channel", "\"Alwin Landry's supply vessel Damon Bankston", "2,800", "Sgt. Barbara Jones", "Jund Ansar Allah,", "\"Body Works\"", "Afghanistan,", "the body of the aircraft", "Hank Moody", "between 1917 and 1924", "the Diamonds", "the school yard", "Turkey", "\"Sweet Home Alabama\"", "Arena", "the National Hockey League", "Tara Reid", "the Y", "Mariska Hargitay", "Germany", "the United States", "Moses", "Franz Kafka", "compost", "Dante"], "metric_results": {"EM": 0.578125, "QA-F1": 0.609375}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.3333333333333333, 1.0, 0.6, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-7387", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1492", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-6541", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-5604", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2732", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-13647", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-16443", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-13487", "mrqa_searchqa-validation-16220"], "SR": 0.578125, "CSR": 0.5411305147058824, "EFR": 1.0, "Overall": 0.7080698529411764}, {"timecode": 68, "before_eval_results": {"predictions": ["Fran\u00e7ois Hollande", "South Pacific", "bhutan", "UPS", "Kevin Painter", "east", "twenty", "Malawi", "Adolphe Adam", "cubed", "lamb", "november rosary", "Sandi Toksvig", "vascular cambium is the main growth layer in the stems and roots of many plants, specifically in dicots such as buttercups and oak trees, and gymnosperms", "Coton", "the government - owned Panama Canal Authority", "Australia's Sir Donald Bradman", "`` Killer Within ''", "The Vamps", "Profit maximization", "Nalini Negi", "Mockingjay -- Part 1 ( 2014 )", "Vasoepididymostomy", "John Goodman", "Eddie Murphy", "Morocco", "Croatian", "Kurt Vonnegut", "political party", "Comodoro Arturo Merino Ben\u00edtez International Airport", "Mike Mills", "1620", "250 million", "Terry the Tomboy", "Umina Beach", "Taeko Ikeda", "Chevy", "bartering", "\"Big Three\"", "The Kirchners", "undergoing a double mastectomy and reconstructive surgery,", "Robert Mugabe", "Val d'Isere, France", "laundromats", "\"theoretically\"", "33-year-old", "was depressed", "federal ocean planning.", "one-shot victory in the Bob Hope Classic", "Jay Gatsby", "Katharine McPhee", "the Eiffel tower", "a brick", "Casey Stengel", "Hodgkin\\'s disease", "Marcia Clark", "cement", "Madden", "foolish", "hubris", "Colombia", "Friday", "1967", "French Canadians"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6453926282051282}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.5, 0.3846153846153846, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2488", "mrqa_triviaqa-validation-2237", "mrqa_triviaqa-validation-1044", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-986", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-1427", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1593", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-14255", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7532", "mrqa_searchqa-validation-7179", "mrqa_hotpotqa-validation-5518"], "SR": 0.546875, "CSR": 0.541213768115942, "EFR": 1.0, "Overall": 0.7080865036231884}, {"timecode": 69, "before_eval_results": {"predictions": ["the Black Russian", "the San Francisco Chronicle", "Honolulu", "Robert Livingston", "the heron", "Universal Studios", "coal", "the Noh theatre", "potholing", "apples", "Finding Nemo", "a catalog", "the Mariachi", "in 1837", "2010", "Australia", "Brooklyn, New York", "Achal Kumar Jyoti", "dorsally on the forearm", "Darren McGavin", "orbit", "the Speaker of the House of Representatives", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "energy loss", "October 29, 2015", "Ellesmere Port", "\"Sugar Baby Love\"", "stephen melbourne", "Wisconsin", "emerald holliday", "Richard Strauss", "Hawaii islands", "6 Litres", "1971", "French", "Robert Guerrero", "king Edward III", "Four Weddings and a Funeral", "Los Angeles", "Karl Johan Schuster", "elderships", "\"Get Him to the Greek\"", "Division of Cook", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "1894", "Walt Disney Productions", "KlingStubbins", "\"Bambi, a Life in the Woods\"", "Australian", "Arsene Wenger", "two women", "two years,", "that the child might still be alive,", "cars", "a series of wildfires", "Jeddah, Saudi Arabia,", "$50 less,", "root out terrorists within its borders.\"", "protective shoes", "J.G. Ballard,", "that the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "Mickey Newbury", "March 23, 2013", "Terry Reid"], "metric_results": {"EM": 0.625, "QA-F1": 0.6892113095238095}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.47619047619047616, 1.0, 1.0, 0.5, 0.2, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-16849", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-15232", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-3440", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-4594", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-1506", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-522"], "SR": 0.625, "CSR": 0.5424107142857143, "retrieved_ids": ["mrqa_squad-train-475", "mrqa_squad-train-1753", "mrqa_squad-train-21364", "mrqa_squad-train-13814", "mrqa_squad-train-45391", "mrqa_squad-train-11784", "mrqa_squad-train-86082", "mrqa_squad-train-58591", "mrqa_squad-train-54357", "mrqa_squad-train-26023", "mrqa_squad-train-44198", "mrqa_squad-train-24090", "mrqa_squad-train-65335", "mrqa_squad-train-86507", "mrqa_squad-train-30624", "mrqa_squad-train-55033", "mrqa_squad-validation-1784", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-3588", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-2196", "mrqa_triviaqa-validation-394", "mrqa_hotpotqa-validation-189", "mrqa_squad-validation-10430", "mrqa_newsqa-validation-2544", "mrqa_hotpotqa-validation-3807", "mrqa_triviaqa-validation-1415", "mrqa_searchqa-validation-13647", "mrqa_hotpotqa-validation-4461", "mrqa_naturalquestions-validation-4609", "mrqa_hotpotqa-validation-2274", "mrqa_triviaqa-validation-943"], "EFR": 0.9583333333333334, "Overall": 0.6999925595238095}, {"timecode": 70, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7268", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15232", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7236", "mrqa_searchqa-validation-7369", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10102", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1794", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2133", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-5197", "mrqa_squad-validation-5410", "mrqa_squad-validation-551", "mrqa_squad-validation-5592", "mrqa_squad-validation-5721", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-6988", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7751", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8042", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8575", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8767", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9732", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2986", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4448", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5114", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.859375, "KG": 0.48984375, "before_eval_results": {"predictions": ["Garden of Eden", "Quebec", "gravity", "Roger Federer", "Roald Dahl", "Legally Blonde", "Voyager 1", "an Earthworm", "the Naval Academy", "the Renaissance", "Halle Berry", "a prism schism", "the Jesuit", "a premalignant flat ( or sessile ) lesion", "John Joseph Patrick Ryan", "Jason Lee", "February 14, 2015", "the homicidal thoughts of a troubled youth", "Hodel", "1560s", "supervillains who pose catastrophic challenges to the world", "Alamodome and city of San Antonio", "Bobby Beathard", "Gupta", "caused by chlorine and bromine from manmade organohalogens", "wis Warfield Simpson", "liriope", "\"Ain't No Mountain High Enough\"", "Restless Leg Syndrome", "Hindenburg", "s Sicily", "dinar", "Henley-on-Thames", "6", "eva", "gun", "Pickwick", "Sun Belt Conference", "47", "1 December 1948", "Manchester", "Ronnie Schell", "organist", "Loch Moidart", "Motorised quadricycle", "Engineering", "PlayStation 4", "small family car", "Elton John", "Carol Browner", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Jimi Hendrix and Janis Joplin,", "one", "22", "August 19, 2007.", "Russia", "in southwestern Mexico,", "Nigeria,", "an animal tranquilizer,", "Barnes & Noble", "350 U.S. soldiers", "wildfires", "75 percent", "Angola"], "metric_results": {"EM": 0.625, "QA-F1": 0.7068204365079365}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-8777", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-654", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-2615", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-2782", "mrqa_triviaqa-validation-2151", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-4322", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1959", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-1432"], "SR": 0.625, "CSR": 0.5435739436619718, "EFR": 0.9166666666666666, "Overall": 0.7185324970657276}, {"timecode": 71, "before_eval_results": {"predictions": ["133", "him to step down as majority leader.", "the union has sent his clients threatening letters for using his company, staged noisy protests, confronted employees, blocked building entrances and even released balloons in a client's building", "July in the Philippines", "off the coast of Dubai", "45 minutes, five days a week.", "Londoners", "This will be the second", "Press freedom groups", "Asashoryu", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "a national telephone survey", "Bridgestone Invitational in Ohio", "Charles Dickens's novel Oliver Twist", "long line, called the main line", "eight", "in the late 1970s", "The eighth and final season of the fantasy drama television series Game of Thrones", "September of that year", "Etienne de Mestre", "the American League ( AL ) champion Cleveland Indians", "North Atlantic Ocean", "September 1959", "a young girl", "May 2016", "stained glass", "axe handle", "whist", "peppers", "eros", "160", "sicily Watts", "Patrick McGoohan", "sc\u00e8nes de la Vie de Boh\u00e8me", "New Zealand", "Arctic Monkeys", "peter Sellers", "Montagues and Capulets", "its variety of shops", "Italian", "United States Auto Club", "Australian", "44", "The LA Galaxy", "World Championship Wrestling", "Appatosaurus", "the Emancipation Proclamation", "blood", "June 1975", "Florence Nightingale", "deuterium", "the Oregon Trail", "asteroids", "artesian", "The Aston Martin", "Eleanor and Raymond Jacobs", "Dizzy Gillespie", "Louise", "Virginia", "the battle of Marathon", "Cairo", "H CO ( equivalently OC (OH ) )", "Effy", "The won"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5945415060887622}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.04878048780487805, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.2222222222222222, 0.8, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-2809", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6987", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-4087", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-1732", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-372", "mrqa_hotpotqa-validation-1001", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-4914", "mrqa_naturalquestions-validation-9675"], "SR": 0.484375, "CSR": 0.5427517361111112, "EFR": 0.9696969696969697, "Overall": 0.7289741161616161}, {"timecode": 72, "before_eval_results": {"predictions": ["Max Martin and Shellback", "Hawaii", "1976", "head of the Cabinet of Bluhme II", "the town of Baraki Barak", "The Kennedy Center", "Kansas", "Marshal of France", "Wiltshire", "Austrian Landwehr", "Glendale", "Captain", "Adam Karpel", "the Apostle James", "the character in the Marvel Cinematic Universe", "775", "William Whewell", "Todd Griffin", "Experimental neuropsychology", "card verification code ( CVC )", "Katharine Hepburn", "2 %", "red", "Watson and Crick", "epic fantasy genre", "Kyle Lafferty", "spiral", "apple", "baulk", "zanzibar", "bohemian", "(Margaret) Court", "henna", "Bugsy Malone", "(Rosario Dawson)", "defoe", "1919", "(CNN)", "misdemeanor assault charges", "Brazil", "can play an important role in Afghanistan as a reliable NATO ally. The question is: How can", "he is obviously very relieved and grateful that the pardon was granted,\"", "one Iraqi soldier,", "12-1", "Del Potro.", "AS", "Casalesi Camorra", "1969", "London", "an orgeat", "an imaginary menagerie", "upside down", "(John) Lennon", "Apatosaurus", "the owl", "the Panama Canal", "the Washburn camp loop", "upside down", "the saber-tooth cat", "Wall Street", "Versailles", "2009", "the British", "the Gospel '' or the dictum `` Remember that you are dust, and to dust you shall return"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49497767857142855}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.09523809523809523, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-2902", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-4691", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-4235", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-6859", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-4812", "mrqa_triviaqa-validation-301", "mrqa_triviaqa-validation-1939", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1150", "mrqa_searchqa-validation-3666", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-1927", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-259"], "SR": 0.40625, "CSR": 0.5408818493150684, "retrieved_ids": ["mrqa_squad-train-30353", "mrqa_squad-train-49130", "mrqa_squad-train-57173", "mrqa_squad-train-80788", "mrqa_squad-train-1084", "mrqa_squad-train-23751", "mrqa_squad-train-74876", "mrqa_squad-train-30142", "mrqa_squad-train-23418", "mrqa_squad-train-64901", "mrqa_squad-train-74596", "mrqa_squad-train-66045", "mrqa_squad-train-54932", "mrqa_squad-train-11728", "mrqa_squad-train-29209", "mrqa_squad-train-28534", "mrqa_searchqa-validation-411", "mrqa_triviaqa-validation-2662", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6435", "mrqa_newsqa-validation-927", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-5958", "mrqa_naturalquestions-validation-39", "mrqa_searchqa-validation-12171", "mrqa_searchqa-validation-8152", "mrqa_naturalquestions-validation-1770", "mrqa_searchqa-validation-11668", "mrqa_squad-validation-7288", "mrqa_hotpotqa-validation-2853"], "EFR": 0.9473684210526315, "Overall": 0.72413442907354}, {"timecode": 73, "before_eval_results": {"predictions": ["The Virgin Queen, Gloriana", "C. W. Grafton", "4,972", "Hugh Hefner", "July 8, 2014", "Julia Kathleen McKenzie", "sarod", "Asif Kapadia", "\"Two Is Better Than One\"", "the first month of World War I", "Jack St. Clair Kilby", "Scotiabank Saddledome", "Balvenie Castle", "the Bulgarian commander - in - chief", "pigs", "The ulnar collateral ligament of elbow joint", "The Sons of Anarchy ( SOA )", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Carlos Alan Autry Jr.", "Nick Kroll", "New York University", "An elevator with a counterbalance", "Ravi River", "James Madison", "the hour of death", "26 miles", "davis", "Robert De Niro", "daniel hinkler", "daniel kiri Te Kanawa", "Sapporo", "The Shard", "sith", "russia", "roosevelt", "Jaguar Land Rover", "november", "Silvio Berlusconi.", "Mumbai", "United States", "Tibet's independence,", "Isabella", "Monday and Tuesday", "15-year-old's", "two", "Lana Clarkson", "customers are lining up for vitamin injections that promise", "Rivers", "Mugabe's opponents", "the (Miami) Dolphins", "Zombies", "(Jack) Robinson", "Christmas", "India", "3", "cable cars", "the People of the United States", "Wings of Desire", "the Process Art movement", "World War I", "Nominative", "Ken Russell", "Sadat", "the Washington State Ferry"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5886395676691729}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false], "QA-F1": [0.8, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.31578947368421056, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.25, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3357", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3902", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-1990", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-1843", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-2245", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1587", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-8984", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-11000", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-11444"], "SR": 0.484375, "CSR": 0.5401182432432432, "EFR": 0.9696969696969697, "Overall": 0.7284474175880425}, {"timecode": 74, "before_eval_results": {"predictions": ["hump and Hampton's line", "102,984", "Nickelodeon", "848", "northwestern Alabama", "Cushman", "Mark Neveldine and Brian Taylor", "Harold Edward Holt", "Michael Crawford", "Fort Valley, Georgia", "Netflix", "Edward R. Murrow", "Ashridge", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority '' ( Titus 2 : 15 )", "Michael Rooker", "Patris et Filii et Spiritus Sancti", "Roman Reigns", "May 2010", "supported modern programming practices and enabled business applications to be developed with Flash", "South Asia", "Rajendra Prasad", "Ariana Clarice Richards", "President Yahya Khan", "Colon Street", "major", "Arkansas", "photography", "h Henry Hunt", "peterain", "robinsons", "gretzky", "fools and Horses", "Jim Peters", "sewing machines", "humble pie", "Mike Tyson", "Michael Bloomberg", "Ross Perot.", "depression", "Employee Free Choice act", "they couldn't accept an offer", "suppress the memories and to live as normal a life", "southern port city of Karachi,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "supermodel", "Kambakkht Ishq,\"", "16", "Steven Gerrard", "the Dugong", "Buffalo", "Peter Sellers", "Herod", "a tuna", "Chuck Berry", "Students for a Democratic Society", "petits fours", "Smallville", "Mike Nichols", "a volcano", "15", "the eye", "wagen", "benevento"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7754682239057239}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4089", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-3484", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1703", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-5100", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1877", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-6581"], "SR": 0.6875, "CSR": 0.5420833333333333, "EFR": 1.0, "Overall": 0.7349010416666666}, {"timecode": 75, "before_eval_results": {"predictions": ["Bologna", "a watch", "The Bronx", "10", "chicken", "the opening jump ball", "Rutherford B. Hayes", "of", "a rhodesian ridgebacks", "natural selection", "the Communist Party", "God", "University of Exeter", "the New York / San Francisco Giants", "C\u03bc and C\u03b4 )", "using a baby as bait, allowing a child to go through a torturous treatment to gain information", "orogenic belt", "butane", "James Hutton", "The Pittsburgh Steelers", "the Ming dynasty", "20 March 2011", "Daniel A. Dailey", "The Maginot Line", "the eurozone", "Cato", "petie robinson", "carthaginian", "Andr\u00e9s Iniesta", "Joy Division", "vinnie Barbarino", "chamonix", "henry boelcke", "henry robinson", "12", "Dublin", "Barack Obama", "Major League Soccer", "$7.3 billion", "Sir Michael Kemp Tippett", "King R\u00e6dwald", "Art Bell", "Valhalla Highlands Historic District", "19th and early 20th centuries", "37", "Central Avenue", "Reese Witherspoon", "2015 Baylor Bears football team", "Sir Matthew Alistair Grant", "light snow or flurries", "Saturday", "his native Philippines", "renew registration", "1975", "9-week-old", "raping and killing a 14-year-old Iraqi girl.", "Workers'", "five Lebanese prisoners", "glamour and hedonism", "246", "Eleven", "carver", "henley royal regatta", "Otto von Bismarck"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5836055871212121}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2666666666666667, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-6320", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-665", "mrqa_searchqa-validation-10897", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-250", "mrqa_naturalquestions-validation-1162", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-497", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-2538", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-4882", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-137", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-334", "mrqa_triviaqa-validation-5331"], "SR": 0.484375, "CSR": 0.5413240131578947, "retrieved_ids": ["mrqa_squad-train-81828", "mrqa_squad-train-55698", "mrqa_squad-train-48612", "mrqa_squad-train-30785", "mrqa_squad-train-32380", "mrqa_squad-train-2370", "mrqa_squad-train-13500", "mrqa_squad-train-5594", "mrqa_squad-train-11600", "mrqa_squad-train-28970", "mrqa_squad-train-36897", "mrqa_squad-train-80095", "mrqa_squad-train-29089", "mrqa_squad-train-66020", "mrqa_squad-train-75378", "mrqa_squad-train-82255", "mrqa_newsqa-validation-1432", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-1003", "mrqa_newsqa-validation-3439", "mrqa_triviaqa-validation-1424", "mrqa_searchqa-validation-16137", "mrqa_naturalquestions-validation-9195", "mrqa_hotpotqa-validation-1395", "mrqa_naturalquestions-validation-3303", "mrqa_newsqa-validation-182", "mrqa_squad-validation-801", "mrqa_naturalquestions-validation-7507", "mrqa_newsqa-validation-3858", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-7297", "mrqa_searchqa-validation-187"], "EFR": 0.9696969696969697, "Overall": 0.7286885715709729}, {"timecode": 76, "before_eval_results": {"predictions": ["Ben Johnston", "Tempo", "Marlborough", "Francis Nethersole", "Bill Walton", "La vendedora de rosas", "Finding Nemo", "Wu-Tang Clan", "Debbie Reynolds", "Mark \"Chopper\" Read", "north-east Lithuania", "Aly Raisman", "Harry Booth", "to manage the characteristics of the beer's head", "dystopian science fiction action thriller film", "the mid - to late 1920s", "Cee - Lo", "Vicente Fox", "Nebuchadnezzar", "Renhe Sports Management Ltd", "often linked to high - ranking ( though not necessarily royalty ) in China", "rubidium - 85", "In 1967, Celtic became the first British team to win the competition", "many forested parts", "the Warring States period", "my fair Lady", "Basketball", "Loose connective tissue", "Lady Gaga", "peahen", "Antarctica", "longchamp", "fenchurch Street", "davis Mitchell", "Jim Jones", "Ottorino Respighi", "Cyndi Lauper", "100", "one", "four", "Peshawar", "millionaire's surtax,", "Muslims", "4, the highest ever position", "Matthew Fisher,", "Ling and Euna Lee,", "snow,", "Basel", "he knew the owner of the home,", "kryptonite", "The Sixth Sense", "Mao Zedong", "tea", "the Castalian Spring", "split", "Andy Warhol", "Rocky", "a \"rigid\" constitution", "the Vietnam War", "The Thorn Birds", "Frostbite", "he hosted a short - lived talk show in WCW", "helps scientists better understand the spread of pollution", "Pir Panjal Range"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6745389593045843}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7692307692307692, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.6, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.8750000000000001, 0.6]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-1196", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-6109", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5068", "mrqa_triviaqa-validation-1368", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4264", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-14003", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-1848"], "SR": 0.53125, "CSR": 0.5411931818181819, "EFR": 0.9666666666666667, "Overall": 0.7280563446969697}, {"timecode": 77, "before_eval_results": {"predictions": ["Dexter Morgan", "Mahalia Jackson", "the outer skin", "a Bunsen burner", "the King of the Blues", "Cuba", "Alexander Alexander", "Gatsby", "milk", "the Black Sea", "Prime Time", "New Mexico", "Black sheep", "Michael Rosen", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "an adopted daughter of Thanos", "Sir Hugh Beaver", "The Satavahanas", "Francisco Pizarro", "September 27, 2017", "Real Madrid", "Paradise, Nevada", "the official flag of Hungary", "1857", "Richard Masur", "banjo", "the Philippines", "cortisages", "volcanoes", "south african", "Moaning Myrtle", "seven year itch", "the third great empire", "forage", "vichy", "your phone", "david smith", "1935", "Premier League club Manchester City", "Orson Welles", "Washington metropolitan area", "Christopher Tin", "cruiserweight title", "the reigning monarch of the United Kingdom", "The New French Review", "Kaep", "Chris DeStefano", "Francisco Rafael Arellano F\u00e9lix", "Dolly Records", "in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "\"Draquila -- Italy Trembles.\"", "Michael Schumacher", "South Africa's", "former Alabama judge", "new DNA evidence", "reached an agreement late Thursday", "Bill Stanton", "Natalie Cole", "11,", "the strawberries,\"", "\"The American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic,", "the Cow Palace", "Audrey II", "the song's central theme of loss"], "metric_results": {"EM": 0.484375, "QA-F1": 0.636353689202954}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7878787878787877, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.5, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.23529411764705882, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7272727272727272]}}, "before_error_ids": ["mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-8530", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-5936", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-346", "mrqa_triviaqa-validation-2316", "mrqa_triviaqa-validation-1941", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-2665", "mrqa_triviaqa-validation-5438", "mrqa_triviaqa-validation-4079", "mrqa_hotpotqa-validation-1623", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-2016", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-4445", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-2474", "mrqa_newsqa-validation-3833", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-5583"], "SR": 0.484375, "CSR": 0.5404647435897436, "EFR": 0.9393939393939394, "Overall": 0.7224561115967366}, {"timecode": 78, "before_eval_results": {"predictions": ["Copenhagen", "BraveStarr", "The 7 Habits of Highly Effective Families", "Picric acid", "a college of magic in New York", "1940s and 1950s", "\"Brickyard\"", "Hong Kong", "Rabies", "M. Night Shyamalan", "\"You Can Be a Star\"", "1996 NBA Slam Dunk Contest", "Jane Eyre: An Autobiography", "autumn marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "libretto", "Chuck Noland", "the wife of King Willem - Alexander", "an exultation of spirit", "the senior-most judge of the supreme court", "Cyrus", "Hon July Moyo", "Bonnie Aarons", "the right side of the heart", "1984", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "braille", "republic of tonyika and zanzibar", "blue", "henry kramer", "Turin", "Bangladeshi taka", "gretner", "Kenya.", "fresh fruits", "hawaii", "the Colossus of Rhodes", "albania", "Old Trafford", "\"The station", "President Robert Mugabe's", "Malawi.", "Les Bleus", "Canada.", "closed on 366 for eight wickets on the opening day.", "removal of his diamond-studded braces.", "Kurt Cobain's", "Afghanistan,", "\"Body Works\"", "businessman", "13", "English", "Ghost Recon 2", "Spanish", "Peter Shaffer", "Twin lens reflex", "Madagascar", "The Count of Monte Cristo", "Picasso", "David H. Petraeus", "Dreamgirls", "the Hoover Dam", "powerful anesthetic and sedative.", "the release of the four men", "Michael Schumacher"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7696101641414141}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.7272727272727272, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-425", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-6344", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-3510", "mrqa_searchqa-validation-12666", "mrqa_searchqa-validation-1814"], "SR": 0.6875, "CSR": 0.5423259493670887, "retrieved_ids": ["mrqa_squad-train-34491", "mrqa_squad-train-38803", "mrqa_squad-train-32632", "mrqa_squad-train-29531", "mrqa_squad-train-26824", "mrqa_squad-train-14724", "mrqa_squad-train-32002", "mrqa_squad-train-59332", "mrqa_squad-train-73746", "mrqa_squad-train-6516", "mrqa_squad-train-58515", "mrqa_squad-train-34718", "mrqa_squad-train-25876", "mrqa_squad-train-15086", "mrqa_squad-train-65668", "mrqa_squad-train-14132", "mrqa_hotpotqa-validation-372", "mrqa_searchqa-validation-12283", "mrqa_triviaqa-validation-3048", "mrqa_searchqa-validation-7619", "mrqa_hotpotqa-validation-3780", "mrqa_newsqa-validation-2205", "mrqa_squad-validation-8649", "mrqa_squad-validation-9247", "mrqa_squad-validation-2478", "mrqa_searchqa-validation-1820", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-3620", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-2316"], "EFR": 1.0, "Overall": 0.7349495648734178}, {"timecode": 79, "before_eval_results": {"predictions": ["Ted Bundy", "in 2017", "David Naughton", "June 1975", "September 26, 2010", "The Prodigy", "Gal\u00e1pagos Islands", "119 minutes", "Empress Taitu Bitul", "James Weldon Johnson", "Manhattan Project", "Green Chair", "17", "2013", "1961", "Andy Serkis", "January 1, 1976", "Alex Ryan", "Massachusetts", "neuropsychology", "10 May 1940", "a foul - tempered monarch whom Carroll himself describes as `` a blind fury '', and who is quick to give death sentences at the slightest offense", "Washington metropolitan area", "2017", "U.S. service members who have died without their remains being identified", "johnson", "cue ball", "david mitchell", "billi in Iceland, Purzelknirps in Germany and Hilitos in Spain", "Honolulu, Hawaii", "Velazquez", "city of rome", "10", "magic", "Turkish", "24", "dukedom", "The Rosie Show,\"", "Trevor Rees,", "in a remote part of northwestern Montana", "15-year-old's", "skeletal dysplasia,", "reached an agreement late Thursday to form a government of national reconciliation.", "job training for all service members leaving the military.", "FARC,", "30", "heart,\"", "Louela Binlac", "Chinese", "Tallahassee", "Punxsutawney, Pennsylvania", "Madonna", "the Prius", "Picabo Street", "Cannoli", "Los Angeles", "William Henry Harrison", "Charlie Sheen", "Dante", "Jeanette Rankin", "Roosevelt", "Trinity", "peppers", "Harvard"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6201073232323233}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5504", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2448", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-5695", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1433", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-9856", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-15319", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-510"], "SR": 0.53125, "CSR": 0.5421875, "EFR": 0.9666666666666667, "Overall": 0.7282552083333333}, {"timecode": 80, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1814", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7645", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.84765625, "KG": 0.50078125, "before_eval_results": {"predictions": ["during a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "the \u01c0Xam people", "Pakistan", "$19.8 trillion", "Tigris and Euphrates rivers", "December 1972", "18", "Scar's henchmen", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "Howard Caine", "Buddhism", "Roman Reigns", "Tennessee whiskey", "Joan Rivers", "fish", "4-digit", "the South Saskatchewan River", "plac\u0113b\u014d", "three", "gretz Bajec-Lapajne", "Hispaniola", "S. longicarinata and S. molloyi", "balustrade", "tartan", "University of Georgia", "the Qin dynasty", "CBS", "South America", "the New York Knicks", "Prince Ioann Konstantinovich", "Selinsgrove", "The Tempest", "Los Angeles", "Pacific Place", "Franconia, New Hampshire", "small family car", "we seek a new way forward, based on mutual interest and mutual respect.\"", "Sri Lanka's", "motor scooter", "United Arab Emirates", "Derek Mears", "President Obama's", "Hamas", "Sea World in San Antonio,", "Mubarak,", "Six members of Zoe's Ark", "More than 15,000", "\"Quiet Nights,\"", "Eli Lilly", "Austen", "the zebra", "the Charleston", "Parrot", "the Sky", "The Prince and the Pauper", "a taximeter", "Toby Keith", "metre", "Oahu", "Thor Heyerdahl", "hawaii", "calcium carbonate", "green"], "metric_results": {"EM": 0.625, "QA-F1": 0.6711805555555556}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-4205", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-5714", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3620", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-143", "mrqa_triviaqa-validation-598"], "SR": 0.625, "CSR": 0.5432098765432098, "EFR": 0.9583333333333334, "Overall": 0.7266367669753085}, {"timecode": 81, "before_eval_results": {"predictions": ["mony Walters", "Apprentice", "egremont", "mercury", "othello", "fungi", "wigan Warriors", "the principal front of a building", "Pontiac Silverdome", "hogmanay", "acetone", "virgil", "the Battle of Austerlitz", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "reaction centres", "the Four Seasons", "ideology", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "in contemporary Earth", "script", "1773", "St. Louis Cardinals", "Robyn", "a role in synthesizing vitamin B and vitamin K as well as metabolizing bile acids", "before 1986", "Telugu", "Paris", "Tampa Bay Lightning", "Melville", "11", "Argentine cuisine", "Comodoro Arturo Merino Ben\u00edtez International Airport, Santiago, Chile", "Ready Player One", "Patti Smith", "R- Point", "Bonobo", "331", "two-state solution", "12 hours", "allegations that a dorm parent mistreated students at the school.", "any abuse that occurred in his diocese.", "flowers", "Revolutionary Armed Forces of Colombia,", "two", "southern Bangladesh,", "a violent government crackdown seeped out.\"", "Las Vegas.", "a new GI Bill that expands education benefits", "Illinois Reform Commission", "width", "Gianni Versace", "a lunch", "Wings", "Ivica Zubac", "salmon", "a place in front of a palace", "Saskatchewan", "Howler", "the monsoon", "Marie Antoinette", "Google", "Montreal", "left - sided heart failure", "brain and spinal cord"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5470576538805176}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.375, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8275862068965518, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-1630", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-9487", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-1652", "mrqa_hotpotqa-validation-4305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-4598", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-12769", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-3577", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7342"], "SR": 0.453125, "CSR": 0.5421112804878049, "retrieved_ids": ["mrqa_squad-train-29293", "mrqa_squad-train-8491", "mrqa_squad-train-35750", "mrqa_squad-train-10162", "mrqa_squad-train-12163", "mrqa_squad-train-20199", "mrqa_squad-train-16800", "mrqa_squad-train-68397", "mrqa_squad-train-77084", "mrqa_squad-train-30361", "mrqa_squad-train-66858", "mrqa_squad-train-63931", "mrqa_squad-train-74733", "mrqa_squad-train-66192", "mrqa_squad-train-71734", "mrqa_squad-train-80717", "mrqa_triviaqa-validation-3907", "mrqa_searchqa-validation-10860", "mrqa_naturalquestions-validation-8206", "mrqa_searchqa-validation-187", "mrqa_triviaqa-validation-2662", "mrqa_hotpotqa-validation-961", "mrqa_triviaqa-validation-2082", "mrqa_squad-validation-7300", "mrqa_searchqa-validation-9465", "mrqa_naturalquestions-validation-8439", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-2268", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-3211", "mrqa_hotpotqa-validation-5465", "mrqa_triviaqa-validation-558"], "EFR": 0.9714285714285714, "Overall": 0.7290360953832753}, {"timecode": 82, "before_eval_results": {"predictions": ["a heart", "Space Cadet", "anvil", "Judy Garland", "Miranda", "Williams pear", "time", "Guatemala", "Ban Ki-moon", "surface-to-air", "Babe Ruth", "comedy", "Sons of Liberty", "1834", "Ra\u00fal Eduardo Esparza", "Hellenism", "Leonardo da Vinci", "five", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "as a domesticated sheep goes back to between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "Panic! at the Disco", "John Smith", "Lakshwadweep -- Kavaratti", "The Bellamy Brothers", "4.37 light - years", "my favorite martian", "bhina", "colette", "monopoly", "tony ganol", "Margaret Beckett", "six", "goose green", "horseradish", "j Loki", "tepuis", "Forbes", "21st Century Fox", "Wanda Metropolitano", "67,575", "USS Essex", "Towards the Sun", "Japan", "7 February 14786", "1981 World Rowing Championships", "philanthropist", "2 March 1972", "Angel Parrish", "Buck Owens and the Buckaroos", "five victims", "Eintracht Frankfurt", "authorizing killings and kidnappings by paramilitary death squads.", "84-year-old", "serving its fast burgers in the Carrousel du Louvre,", "not", "Larry Ellison,", "Ameneh Bahrami", "France's", "Carol Browner", "CNN's", "10 years", "Union Gap", "1768", "dwarf antelopes"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7520432692307693}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-10740", "mrqa_searchqa-validation-16888", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-5602", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5511", "mrqa_triviaqa-validation-3580", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-3700", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-5743", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-3264", "mrqa_triviaqa-validation-589"], "SR": 0.703125, "CSR": 0.5440512048192772, "EFR": 1.0, "Overall": 0.7351383659638554}, {"timecode": 83, "before_eval_results": {"predictions": ["Woods", "MDC head Morgan Tsvangirai.", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery,", "B-movie queen Lana Clarkson", "Wednesday.", "Moscow.", "two weeks", "Republican presidential candidate", "90", "more than 1.2 million", "Madeleine K. Albright", "a government-run health facility that provides her with free drug treatment.", "government", "$2.187 billion", "a symbol for a burden to be carried as penance", "gastrocnemius", "August 15, 1971", "thunderstorms", "The Divergent Series : Ascendant", "the center", "1977", "the President", "on a sound stage in front of a live audience in Burbank, California", "a crown cutting of the fruit, possibly flowering in five to ten months and fruiting in the following six months", "png", "treaty of Waitangi", "Basketball", "lactic acid", "Moldova", "Rosetta", "Tiananmen", "Ghana", "roman ford", "stalls", "byker Grove", "wheat", "stained glass", "off-balance-sheet special purpose entities", "English", "2006", "Isaac Newton's book \"Principia\"", "Portsea", "two", "Tonde Burin", "Salford, Lancashire", "15,000 people for basketball matches and 15,500 for concerts", "Christy Walton", "25", "Adrian Lyne", "Ted Turner", "GILBERT & SULLIVAN", "root", "cheddar", "Alexander Pope", "travel", "Herod", "bone marrow", "Joe Montana", "Jewelry", "ursine", "Max Factor", "March 15, 1945", "The Parlement de Bretagne", "2"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6271861293859649}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.631578947368421, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.875, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.4, 1.0, 1.0, 0.19999999999999998, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-4068", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-3389", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-5288", "mrqa_searchqa-validation-6669", "mrqa_searchqa-validation-5315", "mrqa_searchqa-validation-15829", "mrqa_searchqa-validation-7789", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-7021"], "SR": 0.515625, "CSR": 0.5437127976190477, "EFR": 1.0, "Overall": 0.7350706845238095}, {"timecode": 84, "before_eval_results": {"predictions": ["360 members", "Paul Monti", "Geothermal gradient", "growing faster than the rate of economic growth", "British rock band Procol Harum", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "October 1976", "to prevent further offense", "American Indian allies", "as an apprentice of the fictional Jedi Order in the Star Wars franchise", "euro", "the right to vote", "243 days", "ethiopia", "peter Townsend", "20", "latitude and longitude coordinates", "aragonite", "jon perti smith", "cogs", "rosetta", "South Africa", "helium", "Superman", "rosetta crow", "Kal Ho Naa Ho", "Volvo 850", "Clara Petacci", "season three", "Critics' Choice Television Award for Best Supporting Actress in a Comedy Series", "the back", "the Gentle Don", "North Dakota", "great", "Mark Dayton", "No. 60", "public", "fight back against Israel in Gaza.", "\"A Child's Garden of Verses,\"", "Gov. Jan Brewer.", "Dube, 43, was killed", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "spiral into economic disaster.", "Nairobi, Kenya,", "its part to improve the environment by taking on greenhouse gas emissions.", "Dereks", "the captain of a nearby ship", "84-year-old", "from Texas and Oklahoma to points east,", "Three Mile Island", "Ragweed", "Charles I", "Daniel Boone", "Milwaukee", "Annie", "The Weekly World News", "Table cloth", "neurons", "fiber optics", "John Ford", "Latin", "Liverpool", "the sun", "Exodus"], "metric_results": {"EM": 0.5, "QA-F1": 0.6041991610539997}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.5714285714285715, 0.8387096774193548, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1822", "mrqa_hotpotqa-validation-4919", "mrqa_hotpotqa-validation-406", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3208", "mrqa_newsqa-validation-1016", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-13197"], "SR": 0.5, "CSR": 0.5431985294117647, "retrieved_ids": ["mrqa_squad-train-20756", "mrqa_squad-train-70986", "mrqa_squad-train-2145", "mrqa_squad-train-11671", "mrqa_squad-train-34554", "mrqa_squad-train-45233", "mrqa_squad-train-18266", "mrqa_squad-train-27992", "mrqa_squad-train-56613", "mrqa_squad-train-65864", "mrqa_squad-train-38255", "mrqa_squad-train-31754", "mrqa_squad-train-61981", "mrqa_squad-train-79200", "mrqa_squad-train-65006", "mrqa_squad-train-41676", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-3678", "mrqa_naturalquestions-validation-1640", "mrqa_squad-validation-288", "mrqa_naturalquestions-validation-5724", "mrqa_searchqa-validation-7989", "mrqa_hotpotqa-validation-2128", "mrqa_searchqa-validation-44", "mrqa_newsqa-validation-2627", "mrqa_hotpotqa-validation-2342", "mrqa_newsqa-validation-324", "mrqa_searchqa-validation-963", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-7342", "mrqa_triviaqa-validation-5153", "mrqa_searchqa-validation-246"], "EFR": 1.0, "Overall": 0.734967830882353}, {"timecode": 85, "before_eval_results": {"predictions": ["red", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "Mason Alan Dinehart III", "red, white, and blue", "O'Meara", "12.65 m", "two", "phaseout schedules were delayed for less developed ('Article 5 ( 1 )') countries", "2013", "Raya Yarbrough", "Sir Henry Cole", "the late 1980s", "IX", "Dick Advocaat", "joseph", "sister", "streek Larsson", "cuthbert", "mulberry", "$200 million", "ad nausea", "cereal", "Operation Overlord", "cumbria", "bowie", "Tom Shadyac", "pornographicstar", "The Weeknd", "Walldorf", "the Beatles", "Alfred Preis", "the Czech Kingdom", "third", "Elliot Fletcher", "$1 million", "Marika Nicolette Green", "Javed Miandad", "Sunday,", "\"Nude, Green Leaves and Bust\"", "Cameroon,", "Florida's Everglades.", "Olivia Newton-John", "\"Empire of the Sun,\"", "1959", "1,500 Marines will be part of the initial wave of President Obama's surge plan", "Adam Lambert", "\"I have a strong objection to the genre of mixing fact with fiction,\"", "police to question people if there's reason to suspect they're in the United States illegally.", "South African", "Forrest Gump", "Achilles", "Bright Lights, Big City", "Timex", "landfills", "Harvard", "James Bond", "butterflies", "Sybil", "A Canticle for Leibowitz", "John Harvard", "Ocean's Twelve", "queens", "Johnny Mathis", "Commonwealth Scientific and Industrial Research Organisation (CSIRO)"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6067813131552355}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.4827586206896552, 0.8571428571428571, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.3, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.25, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.08, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.923076923076923]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-1198", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-5193", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-2450", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-1932", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-4298", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-10809", "mrqa_triviaqa-validation-3052"], "SR": 0.484375, "CSR": 0.5425145348837209, "EFR": 1.0, "Overall": 0.7348310319767443}, {"timecode": 86, "before_eval_results": {"predictions": ["741 weeks", "Speaker of the House of Representatives", "on the two tablets", "Massachusetts", "ancient Athens", "Kevin Spacey", "6 March 1983", "innermost in the eye", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Hellenic Polytheistic Reconstructionism", "Bob Pettit", "September 8, 2017", "1989 album Sleeping with the Past", "lillian", "1875", "marx crystal", "pat holliday", "mod fashion", "wheel", "the Philippines", "adventure", "india", "tide-wise", "\" Happy Birthday to You\"", "travel sickness", "Kim Sung-su", "Lauren Lane", "Twisted Twister", "Amway", "Joseph I", "Reinhard Heydrich", "\"What's My Line?", "Woodsy owl", "Andr\u00e9 3000", "Big Bad Wolf", "Mandarin Airlines", "American pharmaceutical company", "autonomy.", "around 10:30 p.m. October 3,", "as soon as 2050,", "British Prime Minister Gordon Brown's", "Venus Williams", "humans", "businessman", "The son of Gabon's former president", "to digital.", "President Sheikh Sharif Sheikh Ahmed", "22", "President Obama", "Scott McClellan", "Frank Sinatra", "Auschwitz-Birkenau", "garlic", "the ulna", "Anna and the King of Siam", "Drogon", "a molar", "Earhart", "Punky Night", "LADY BIRD JOHNSON", "a turban", "Times Square", "Immanuel Kant", "Australia"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6412326388888889}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-2467", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-4741", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1473", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-7076", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3927", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-3710", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-1863"], "SR": 0.5625, "CSR": 0.5427442528735632, "EFR": 1.0, "Overall": 0.7348769755747127}, {"timecode": 87, "before_eval_results": {"predictions": ["prince Igor", "conchita wurst", "Jessica Simpson", "The port of Terneuzen", "paris", "bacino", "London", "ringo stork", "Thailand", "sports agent", "parable", "antelope", "mmorpgs", "as voters gathered as a tribe the members would be well known enough to each other that an outsider could be spotted", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "Antigonon leptopus", "British citizens", "Rick Marshall", "a section of the Torah ( Five Books of Moses ) used in Jewish liturgy during a single week", "2014", "Haliaeetus", "April", "annually ( usually in May )", "Uralic", "$66.5 million", "12 countries", "Ice Princess", "1939", "Joanna No\u00eblle Levesque", "Paris", "Division of Cook", "24 hours a day", "its air-cushioned sole", "August 28, 1774", "alcoholic drinks", "twenty-three episodes", "TD Garden", "American", "Diego Milito's", "Ricardo Valles de la Rosa,", "Summer", "a skilled hacker", "Manmohan Singh's", "400", "development of two courses on the Black Sea coast in Bulgaria.", "American Hugo Vihlen", "sovereignty over them.", "The bus, which was headed to Matamoros, Mexico, flipped and landed on its right side,", "183 people,", "The first time she meets Inman", "Texas A&M", "\"Fargo\"", "Charlotte", "Sacramento", "sostenuto", "Hammurabi", "Ohio State", "\"Land of the Indians\", or simply \"Indian Land\"", "the Weser River", "veterans", "San Francisco", "\" Teen Titans Go!", "Love", "number 2"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6069258432539683}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 0.375, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-1000", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2399", "mrqa_hotpotqa-validation-5868", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-2377", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-780", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-14831", "mrqa_searchqa-validation-10773", "mrqa_searchqa-validation-2453", "mrqa_hotpotqa-validation-1073"], "SR": 0.515625, "CSR": 0.5424360795454546, "retrieved_ids": ["mrqa_squad-train-65368", "mrqa_squad-train-43697", "mrqa_squad-train-57812", "mrqa_squad-train-6961", "mrqa_squad-train-77266", "mrqa_squad-train-11612", "mrqa_squad-train-51345", "mrqa_squad-train-16786", "mrqa_squad-train-48469", "mrqa_squad-train-55692", "mrqa_squad-train-16594", "mrqa_squad-train-25484", "mrqa_squad-train-55984", "mrqa_squad-train-79402", "mrqa_squad-train-82956", "mrqa_squad-train-79882", "mrqa_hotpotqa-validation-3314", "mrqa_squad-validation-5234", "mrqa_naturalquestions-validation-7017", "mrqa_newsqa-validation-3565", "mrqa_naturalquestions-validation-7201", "mrqa_triviaqa-validation-6384", "mrqa_naturalquestions-validation-7845", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-3880", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-4808", "mrqa_searchqa-validation-13161", "mrqa_naturalquestions-validation-124", "mrqa_hotpotqa-validation-3591", "mrqa_naturalquestions-validation-8417"], "EFR": 1.0, "Overall": 0.7348153409090908}, {"timecode": 88, "before_eval_results": {"predictions": ["almost entirely in Wake County, it lies just north of the state capital, Raleigh", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "John Adams", "the span of historic events from approximately 1945 that are immediately relevant to the present time", "1995 Dodge Stealth - Seen in The Turbo Charged Prelude for 2 Fast 2 Furious", "American Samoan", "Texas, Oklahoma, and the surrounding Great Plains", "September 9, 2010", "the Isthmus of Corinth", "awarded to the team that lost the pre-game coin toss", "the Alamodome in San Antonio, Texas", "Peter Andrew Beardsley MBE", "Steve Russell", "Japan", "city of Atlanta", "Jan van Eyck", "the Great Pyramid of giza", "trade union", "China", "from russia with love", "blackfriars", "Christian Dior", "My Fair Lady", "migration", "an island off the north-west coast of Wales", "Pamela Chopra", "December 19, 1998", "Black Swan", "Westley Sissel Unseld", "a minor basilica", "Hampton University", "United States", "Ferengi bartender Quark", "the junction with Interstate 95", "\" Mechanicalte Navstrechu\"", "1993", "John Churchill", "a 2,700-acre sanctuary", "almost 9 million", "The plane", "Kenneth Cole", "Hussein's Revolutionary Command Council.", "two bodies out of the plant,", "a motor scooter", "25 percent", "NATO forces", "a facility in Salt Lake City, Utah,", "Nineteen", "hardship for terminally ill patients and their caregivers,", "an out-of-print book", "repent", "Vespa", "Johnson", "Dmitri Mendeleev", "the windup", "the Red Sea", "the Dauphin", "Ulysses S. Grant", "the Oakland Raiders", "cashmere wool", "a cat's tongue", "\"It is very busy. They are filling sandbags as fast as they can.\"", "Haiti.", "police dogs"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6422139332706767}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.10526315789473682, 0.375, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-7409", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-7069", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-5417", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3219", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-908", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-4967", "mrqa_searchqa-validation-16107", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-414"], "SR": 0.515625, "CSR": 0.5421348314606742, "EFR": 0.9354838709677419, "Overall": 0.7218518654856831}, {"timecode": 89, "before_eval_results": {"predictions": ["Ricardo Valles de la Rosa,", "Katherine Jackson's", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "The Swiss art heist follows the recent theft in Switzerland", "Kgalema Motlanthe,", "black is beautiful,\"", "complicated", "Swedish golfer Henrik Stenson", "executive director of the Americas Division of Human Rights Watch,", "refusal or inability to \"turn it off\"", "12", "surgical anesthetic propofol", "English", "`` Power and Control Wheel, '' a graphic typically displayed as a poster in participating locations", "Virginia", "the evening", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "Munich", "division", "Secretary of Homeland Security", "British R&B girl group Eternal", "Cyndi Grecco", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "originally thought to be a part of the genus Plesippus", "Mark Antony", "salsa", "shealey", "Thanksgiving", "nowhere Boy", "Cascade Range", "corvidae", "The Lone Ranger", "buddha", "1943", "steel", "Sharpening stones", "2011", "Nick Cassavetes", "Sam Phillips", "National Basketball Development League", "Station Casinos", "2,627", "life insurance", "Kareena Kapoor Khan", "Hopeless Records", "1971", "Floyd Nathaniel \"Nate\" Hills", "2012 Summer Olympics", "Anzio", "Lawrence Taylor", "cvicus", "Fred Rogers", "Hillary Rodham Clinton", "Texas", "the occipital lobe", "the divine right of Kings", "Uranus", "cauliflower", "a kettledrum", "the Reform Movement", "a tuber", "The Office", "Elmore Leonard"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5475716991341991}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.04761904761904762, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-3899", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7027", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-6501", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-6027", "mrqa_triviaqa-validation-3067", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-3787", "mrqa_searchqa-validation-4669", "mrqa_searchqa-validation-11910", "mrqa_searchqa-validation-14695", "mrqa_searchqa-validation-10083", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-5328", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-8375", "mrqa_searchqa-validation-926"], "SR": 0.46875, "CSR": 0.5413194444444445, "EFR": 1.0, "Overall": 0.734592013888889}, {"timecode": 90, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.791015625, "KG": 0.44375, "before_eval_results": {"predictions": ["David Joseph Madden", "Garbi\u00f1e Muguruza", "the reactor core", "more than 1,000", "9.7 m", "Andy Serkis", "Andrew Garfield", "cut off close by the hip", "Ali Daei", "the star", "The Witch and the Hundred Knight 2", "Sarafina", "Germany", "a dove", "james b Boyd", "copper", "Coke", "Joan Rivers", "croquet", "coelacanth", "mel blanc", "glenys Kinnock", "Ken Platt", "charlie dreyfus", "steveland hardaway johnson", "Edmonton, Alberta", "Arsenal", "Indiana", "Christina Ricci", "Treaty of Gandamak", "Newell Highway", "Sports Illustrated", "Lieutenant Colonel Iceal Hambleton", "1730", "1903", "Oregon Ducks football", "Nye County", "Mugabe's opponents", "37th", "$81,88010", "40", "Sabina Guzzanti", "Lindsey", "400 farmers", "U.S. District Judge Ricardo Urbina", "heavy flannel or wool,", "Chester Arthur Stiles,", "a nuclear weapon", "Barack Obama, Jr.", "\"It's My Party\"", "the Haunted Mansion", "the moon", "the Patriarch Tree", "China", "the Grand Canyon", "Judges", "the Gospel of Judas", "Edgar Rice Burroughs", "the Marais", "Toy Story", "James Madison", "the denominator", "a relic", "Kansas State"], "metric_results": {"EM": 0.625, "QA-F1": 0.6714285714285714}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5288", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-2973", "mrqa_triviaqa-validation-5299", "mrqa_hotpotqa-validation-3738", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3657", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-2316", "mrqa_searchqa-validation-7599", "mrqa_searchqa-validation-12305", "mrqa_searchqa-validation-13576"], "SR": 0.625, "CSR": 0.542239010989011, "retrieved_ids": ["mrqa_squad-train-16221", "mrqa_squad-train-75957", "mrqa_squad-train-20949", "mrqa_squad-train-24128", "mrqa_squad-train-12541", "mrqa_squad-train-30895", "mrqa_squad-train-46647", "mrqa_squad-train-22233", "mrqa_squad-train-69547", "mrqa_squad-train-7269", "mrqa_squad-train-25994", "mrqa_squad-train-50459", "mrqa_squad-train-1958", "mrqa_squad-train-79953", "mrqa_squad-train-75258", "mrqa_squad-train-71549", "mrqa_newsqa-validation-2627", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-8891", "mrqa_triviaqa-validation-2885", "mrqa_searchqa-validation-5454", "mrqa_hotpotqa-validation-974", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-7818", "mrqa_searchqa-validation-5315", "mrqa_hotpotqa-validation-2474", "mrqa_triviaqa-validation-1545", "mrqa_squad-validation-1877", "mrqa_newsqa-validation-1054", "mrqa_searchqa-validation-10131", "mrqa_hotpotqa-validation-4883"], "EFR": 1.0, "Overall": 0.6983696771978022}, {"timecode": 91, "before_eval_results": {"predictions": ["1612", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Wyatt and Dylan Walters", "the liver", "Coton in the Elms", "2015", "2007", "12.65 m", "French Canadian", "counter clockwise", "332", "Narendra Modi", "Vice President", "Chesney Wold", "charlie boyd", "tony blair", "melodic", "Judges 16", "Islam", "michael biko", "LBJ", "Coldplay", "sow", "Donald Trump", "michael peggotty", "queen consort of Hanover", "Central Park", "over 281", "Craig William Macneill", "evangelical Christian periodical", "Prussia", "Dutch", "Bangkok, Thailand", "Springfield, Massachusetts", "1983", "Sam Kinison", "\"Race Through New York Starring Jimmy Fallon\"", "Mike Weland,", "tetris,", "Mary Phagan,", "Brett Cummins,", "$40 and a bread.", "Al-Aqsa mosque", "Aung San Suu Kyi", "Steven Green", "the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman -- to whom she was then married", "Michelle Obama", "Los Angeles.", "\"Oprah is an angel, she is God-sent,\"", "hockey", "the Birch Tree", "Garnet", "PachelbeJ", "August 16, 1977", "Castle Rock", "North Korea", "Wrigley", "Daytona", "Old", "Inuit", "Basil", "Panama Canal", "cobalt", "J.R. Tolkien"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5825520833333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.21428571428571427, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-5537", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-5512", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-1438", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-6795", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-618", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-3803", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-12632"], "SR": 0.46875, "CSR": 0.5414402173913043, "EFR": 0.9117647058823529, "Overall": 0.6805628596547314}, {"timecode": 92, "before_eval_results": {"predictions": ["mitosis", "Stephen Graham", "2015", "Lana Del Rey", "George Strait", "126", "Benzodiazepines", "U.S. service members who have died without their remains being identified", "due to Parker's pregnancy at the time of filming", "Malvolio", "James Corden", "1807", "`` Psychomachia, '' an epic poem written in the fifth century", "salem rushdie", "robben island", "poland", "john jon perthing", "fresh fruits", "Bjorn Borg", "maurice", "Baton Rouge", "Prince johnson", "john johnson", "December", "at the Coney Island Old Island Pier", "polka", "Ford Falcon", "Vernon Kay", "shortstop", "1926", "Irish Chekhov", "Adrian Peter McLaren", "London", "bioelectromagnetics", "Dunlop Tyres", "Pakistan", "Hampton University", "1994", "leftist rebels who often would not let him talk,", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Serie A", "a crocodile", "the largest and perhaps most sophisticated ring of its kind in U.S. history.", "Obama", "the coalition", "Ignazio La Russa", "581 points", "near the George Washington Bridge,", "CNN's", "septembre", "Massachusetts", "John Madden", "Miss You Already", "the Left Bank", "Nikita Khrushchev", "Newport", "South Carolina", "Harold Ramis", "1914", "Cairo", "a letter", "Chief Detective Maria Shvetsova", "Stalybridge Celtic", "written for \"The New York Times\" and \"Popular Mechanics\", and is a regular contributor to various CNBC shows such as \"On the Money\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6307349998318367}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823529411764706, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.5, 0.11320754716981131, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-191", "mrqa_triviaqa-validation-4332", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-5112", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-435", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-277", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-1729", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-11228", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-9478", "mrqa_searchqa-validation-10237", "mrqa_searchqa-validation-2094", "mrqa_searchqa-validation-11496", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-5211"], "SR": 0.546875, "CSR": 0.5414986559139785, "EFR": 0.9310344827586207, "Overall": 0.6844285027345198}, {"timecode": 93, "before_eval_results": {"predictions": ["erotic romantic comedy", "Richard Strauss", "Elbow River", "is widely believed to have been the helmet of King R\u00e6dwald of East Anglia", "gull-wing", "12", "Dutch", "40 Days and 40 Nights", "Las Vegas", "Ubba", "Josh", "green and yellow", "Skegness", "season seven", "Speaker of the House of Representatives", "A 30 - something man", "from the Anglo - Norman French waleis", "the study of the interstellar medium ( ISM ) and giant molecular clouds ( GMC )", "the chairman ( more usually now called the `` chair '' or `` chairperson '' )", "Hudson River", "Simon Callow", "Mamata Banerjee", "May 5, 1904", "sovereign states", "a chamber consisting of upright stones ( orthostats ) with one or more large flat capstones forming a roof", "printed circuit board", "Bulletin", "cricket", "doe", "left book club", "giant", "parity", "republica", "Ghana", "Tony Blair", "salman", "Camellia sinensis", "peanuts, nuts, shellfish and fish", "NASCAR.", "Asashoryu", "many Marines we talked to in this coastal, scrub pine-covered North Carolina base", "two Israeli soldiers,", "Mugabe's opponents", "F-14", "The Sopranos", "three empty vodka bottles,", "William Randolph Hearst.", "An Irish bishop", "education", "A Beautiful Mind", "Reader's Digest", "John Wesley", "metal band", "40 Year Old Virgin", "William Faulkner", "chocolate", "Clinton", "Canada", "The Sound of Silence", "Nadia Comneci", "Paul Yves Roch Gil- bert du Motier", "one", "Melbourne", "first solo greatest hits collection"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6428199404761905}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 0.5, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.4, 0.11111111111111112, 0.5, 1.0, 0.0, 0.3333333333333333, 0.08333333333333334, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-1313", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-7107", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-7898", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-2398", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6635", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-350", "mrqa_newsqa-validation-1209", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-1090", "mrqa_searchqa-validation-11066", "mrqa_searchqa-validation-15122", "mrqa_newsqa-validation-2850"], "SR": 0.53125, "CSR": 0.5413896276595744, "retrieved_ids": ["mrqa_squad-train-65087", "mrqa_squad-train-163", "mrqa_squad-train-73126", "mrqa_squad-train-17191", "mrqa_squad-train-23091", "mrqa_squad-train-47761", "mrqa_squad-train-19009", "mrqa_squad-train-74543", "mrqa_squad-train-74022", "mrqa_squad-train-27778", "mrqa_squad-train-65387", "mrqa_squad-train-62392", "mrqa_squad-train-73307", "mrqa_squad-train-66571", "mrqa_squad-train-81245", "mrqa_squad-train-1122", "mrqa_searchqa-validation-14009", "mrqa_squad-validation-4676", "mrqa_searchqa-validation-14095", "mrqa_triviaqa-validation-2212", "mrqa_searchqa-validation-16364", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-6027", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-8728", "mrqa_squad-validation-4332", "mrqa_newsqa-validation-3865", "mrqa_triviaqa-validation-1650", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-7395", "mrqa_searchqa-validation-9177"], "EFR": 0.9666666666666667, "Overall": 0.6915331338652482}, {"timecode": 94, "before_eval_results": {"predictions": ["rash", "1865", "full-sized nameplates", "secondary school", "three", "first team", "Prince of Cambodia Norodom Sihanouk", "Kevin Smith", "Kansas", "My Cat from Hell", "February 12, 2014", "novelist and poet", "Bergen", "$2.187 billion", "Alice Fairlight", "India", "Super Bowl LII", "Ludacris", "1939", "Barbara Windsor", "Universal Pictures and Focus Features", "normally show IIII for four o'clock", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Guy Berryman", "21 February", "Malta", "roosevelt", "petero da Vinci", "river severn", "strychnine", "Hercule Poirot", "columbia", "romania", "harriet lippershey", "Il Divo", "omid Djalili", "The little dog", "responded by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Casa de Campo International Airport", "a depth of about 1,300 meters in the Mediterranean Sea.", "Michelle Rounds", "in the 1950s,", "the southern city of Naples", "in Hong Kong and Shenzhen, a city in mainland China just across the border.", "Employee Free Choice act", "north-south", "a suicide bomber", "10", "at least 12 months.", "the orbiter", "the OSS", "My Fair Lady", "the Kingdom of the Crystal Skull", "the Funny Bone", "John Updike", "the Bay of Bengal", "lm", "(John) Boehner", "The Maltese Falcon", "William Blake", "a Trojan prince", "a Pringles can", "negligence", "kids"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6608173374487712}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.29629629629629634, 0.7692307692307693, 0.9411764705882353, 1.0, 0.6666666666666666, 1.0, 0.125, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-2068", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-3746", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-3094", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-4133", "mrqa_searchqa-validation-9025", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-11460", "mrqa_searchqa-validation-5274", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-6265"], "SR": 0.5625, "CSR": 0.5416118421052631, "EFR": 1.0, "Overall": 0.6982442434210527}, {"timecode": 95, "before_eval_results": {"predictions": ["6,396", "23 July 1989", "twin sister", "James Mitchum", "Hampton's hump and Hampton's line", "right-hand", "Mika H\u00e4kkinen", "Todd McFarlane", "Republic of Ireland", "1942", "Germanic", "Marc Bolan", "Realty Bites", "amphetamines", "Louis XV", "Steve Goodman", "Evermoist", "Thomas Jefferson's", "Miami Heat", "1920", "Dirk Benedict", "1983", "November 5, 2017", "The vascular cambium", "Nicolette Larson", "south America", "Let It Snow", "Madrid", "Lithium", "Wednesday's", "rajasthan", "a sweater", "mustard", "jon perturates", "Dick Fosbury", "sebecc", "peter smith", "The Kirchners", "Friday,", "vegan bake sales", "Muslim revolutionary named Malcolm X", "The iconic Abbey Road music studios made famous by the Beatles are not for sale,", "10", "Pacific Ocean territory of Guam", "\"prostitute\"", "Colombia.", "Rihanna", "\"Dancing With the Stars.\"", "Kyra and Violet,", "Python", "pi", "Rio de Janeiro", "Chuck Yeager", "the tsuba", "lipos", "Central Park", "Monica Lewinsky", "whales", "the Battle of Fort Donelson", "Bech", "Aaron Burr", "Dougie MacLean", "victims of rape", "2010"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6895461309523809}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-988", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2374", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-8859", "mrqa_triviaqa-validation-2690", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-6311", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11026", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-1856"], "SR": 0.640625, "CSR": 0.5426432291666667, "EFR": 0.9130434782608695, "Overall": 0.6810592164855073}, {"timecode": 96, "before_eval_results": {"predictions": ["Shropshire Union Canal", "\"the Defence Pole\"", "American", "1982", "Harlem", "Jeff Meldrum", "private Roman Catholic university located in Great Falls, Montana within the Diocese of Great Falls\u2013Billings", "gorillas", "Henry Lau", "most awarded female act of all-time", "private liberal arts college", "Marktown", "North Kesteven", "the variety b -- while short - haired type is listed as the variety a", "a counter-weighted elevator", "a castle", "Laura Jane Haddock", "David Gahan", "Ernest Rutherford", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "another chromosome", "mashed potato", "1986", "Nicole DuPort", "driving Miss Daisy", "public plane", "peter c.offical", "le Leicester", "jonet jackson", "john\u2019s lost treasure", "poland", "rugby", "cartilage", "France", "jon pertina flessibile Acquisto verificato", "branson, MO", "rolled over", "finance", "people", "murder", "hiring of hundreds of foreign workers", "Wednesday.", "golf", "Roberto Micheletti,", "Kuranyi's", "40 militants and six Pakistan soldiers", "engineering and construction company", "July 18, 1994,", "A Tale of Murder", "the Proletariat", "the waggle dance", "Israel", "a Byte Megabyte", "One Flew Over the Cuckoo's Nest", "Danny Elfman", "bromine", "Tchaikovsky", "salmon", "the staff", "Senegal", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Muqtada al-Sadr,", "three"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5666293513230191}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.13333333333333333, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 0.3157894736842105, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6875000000000001, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-793", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-7674", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-3219", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-7020", "mrqa_searchqa-validation-9706", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-15815", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2036"], "SR": 0.46875, "CSR": 0.5418814432989691, "retrieved_ids": ["mrqa_squad-train-79075", "mrqa_squad-train-20419", "mrqa_squad-train-17950", "mrqa_squad-train-38355", "mrqa_squad-train-52148", "mrqa_squad-train-40740", "mrqa_squad-train-22947", "mrqa_squad-train-33970", "mrqa_squad-train-51171", "mrqa_squad-train-50396", "mrqa_squad-train-59537", "mrqa_squad-train-80508", "mrqa_squad-train-78018", "mrqa_squad-train-82191", "mrqa_squad-train-77915", "mrqa_squad-train-9178", "mrqa_naturalquestions-validation-1798", "mrqa_searchqa-validation-6939", "mrqa_squad-validation-1824", "mrqa_searchqa-validation-12632", "mrqa_hotpotqa-validation-4743", "mrqa_triviaqa-validation-1660", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-3152", "mrqa_hotpotqa-validation-5829", "mrqa_naturalquestions-validation-10012", "mrqa_hotpotqa-validation-3785", "mrqa_triviaqa-validation-2612", "mrqa_searchqa-validation-175", "mrqa_naturalquestions-validation-2448", "mrqa_hotpotqa-validation-2475"], "EFR": 1.0, "Overall": 0.6982981636597938}, {"timecode": 97, "before_eval_results": {"predictions": ["11 to 12 year old", "bronze medal in the women's figure skating final,", "the remaining rebel strongholds in the north of Sri Lanka,", "Ryder Russell,", "three-time road race world champion,", "Pakistan's", "137", "23 million square meters (248 million square feet)", "Leo Frank, a northern Jew who'd moved to Atlanta to supervise the National Pencil Company factory.", "Azzam the American,", "Barbara Dainton-West,", "President Bush", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "The Sun", "Joseph Heller", "in the case of disputes between two or more states", "an edible tuber", "Elaine Stritch", "IBM", "Kida", "the heart", "by fermenting dietary fiber into short - chain fatty acids ( SCFAs )", "over a 20 - year period", "absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "Institute of Chartered Accountants of India ( ICAI )", "magnesium", "aircraft", "Cole peter", "calf", "gaseous", "Clint Eastwood", "Shanghai", "Arthur Ashe", "cribbage", "stand-up", "jonathan samedi", "poland", "Rhode Island", "February 20, 1978", "Donna Paige Helmintoller, better known as Paige O'Hara", "London", "Hugh Grosvenor, 3rd Marquess of Westminster", "Bentley Twins", "2.1 million", "Bridgetown", "\"Baa, Baa, Black sheep\"", "155 ft", "ABC1 and ABC2", "Pontins, Camber Sands, England", "subtraction", "the FBI", "acid", "the black bear", "Alcoholics Anonymous", "the Moon", "French", "Rajasinha II of Shal", "The Wall Street Journal", "Mowgli", "Cyrano de Bergerac", "India", "Frederic Chopin", "Kentucky", "Blackbeard"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6358104569042069}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 0.22222222222222224, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.25, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6060606060606061, 0.8571428571428571, 0.9166666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-3968", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-2309", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-1086", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2961", "mrqa_searchqa-validation-2295", "mrqa_searchqa-validation-6936", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-10743"], "SR": 0.53125, "CSR": 0.5417729591836735, "EFR": 1.0, "Overall": 0.6982764668367347}, {"timecode": 98, "before_eval_results": {"predictions": ["margaret thatah", "Los Angeles", "seymour", "illegal importation", "Francois Mitterrand", "charlatan", "london", "hard times", "john travolta", "leymour", "IKEA", "The Colossus of roman", "Furious Goddess of Revenge", "The Walking Dead ( franchise )", "the euro", "Tampa Bay", "Georges Auguste Escoffier", "Arthur `` The President '' Flanders -- An inmate on death row, convicted of killing his father in an insurance - fraud scheme", "The book is more commonly published and read in the Philippines in either Tagalog or English", "Latitude", "Water can easily flow from the faucet into the sink", "Watson and Crick", "Sohrai", "July 1, 1923", "ten", "40 Acres and a Mule Filmworks", "Lake Wallace", "the Magic Band", "Clark County, Nevada", "Brookhaven", "Hiroshima and Nagasaki", "the Americas", "the Rose Garden", "Marvel Comics", "the Marx Brothers film", "Umar S. Israilov", "number 1", "$40 and a loaf of bread.", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "a lock break", "Ity Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "UNICEF", "Obama", "opposition candidate Morgan", "the annual White House Correspondents' Association dinner Saturday,", "coalition troops", "Daniel Wozniak,", "hardship for terminally ill patients and their caregivers,", "St Petersburg and Moscow,", "Rudolph W. Giuliani", "Coyote", "Punch", "the Spark Ranger", "Narcissus", "The Little Prince", "Garfield", "Repent", "the drum", "Horatio Nelson", "Moscow", "tabula rasa", "March 2016", "159", "New York City"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6789338699494949}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.787878787878788, 0.5, 0.875, 1.0, 0.4, 0.8, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-2051", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1446", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4662", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-4118", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-15462", "mrqa_naturalquestions-validation-3558"], "SR": 0.5625, "CSR": 0.5419823232323233, "EFR": 0.9642857142857143, "Overall": 0.6911754825036075}, {"timecode": 99, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-350", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13792", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-14843", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6990", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9515", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2498", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3358", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-425", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5375", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-723", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7310", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.869140625, "KG": 0.47890625, "before_eval_results": {"predictions": ["dharma", "colombia", "paulim p Pilgrim's Progress", "seaweed", "christopher wren", "Pakistan", "columbus", "The Merry widow", "South Pacific", "Massachusetts", "black", "michael park", "la boheme", "International Orange", "tropical desert climate", "Claudia Grace Wells", "Jules Shear", "a programmer in a programming language", "Meeting Sweet at The Bronze", "Herod", "Jacques Cousteau", "Florida", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "1922", "two", "Broadway musicals", "Guangzhou, China", "lower Manhattan", "GE Appliances", "The Keeping Hours", "\"The Life of Charlotte Bront\u00eb\"", "\"The Jungle Book\" (2016)", "2007", "President of the United States", "Girls' Generation", "The Herald Angels Sing", "183", "Aravane Rezai", "Lifeway's 100-plus stores nationwide", "calls,", "Marie-Therese Walter.", "alcohol", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "56,", "A Colorado prosecutor", "11th year in a row.", "Michael Krane,", "Thursday", "a asylum", "horse", "The Bravados", "Cessna", "China", "South Africa", "Alien", "Goose Gossage", "The Scarlet Pimpernel", "Sephora", "Fletcher Christian", "Latter-day Saints", "Doctor Dolittle", "Over the Rainbow", "a touch of underbrush"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6767165309106098}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.7368421052631579, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-640", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-1911", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1749", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3562", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-3858", "mrqa_searchqa-validation-2348", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-9449", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-15555"], "SR": 0.59375, "CSR": 0.5425, "retrieved_ids": ["mrqa_squad-train-44730", "mrqa_squad-train-80163", "mrqa_squad-train-21542", "mrqa_squad-train-85522", "mrqa_squad-train-70669", "mrqa_squad-train-13977", "mrqa_squad-train-33035", "mrqa_squad-train-55859", "mrqa_squad-train-74876", "mrqa_squad-train-39223", "mrqa_squad-train-67546", "mrqa_squad-train-5386", "mrqa_squad-train-2969", "mrqa_squad-train-41799", "mrqa_squad-train-70332", "mrqa_squad-train-4093", "mrqa_hotpotqa-validation-2971", "mrqa_newsqa-validation-2731", "mrqa_naturalquestions-validation-5355", "mrqa_newsqa-validation-852", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-5153", "mrqa_squad-validation-7300", "mrqa_searchqa-validation-2114", "mrqa_triviaqa-validation-6550", "mrqa_searchqa-validation-4264", "mrqa_newsqa-validation-1150", "mrqa_naturalquestions-validation-8500", "mrqa_hotpotqa-validation-4828", "mrqa_newsqa-validation-3031", "mrqa_triviaqa-validation-6965", "mrqa_hotpotqa-validation-2771"], "EFR": 1.0, "Overall": 0.728890625}]}