{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4110, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "to synthesize fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters, and carry out part of the heme pathway", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology, specifically the rise of the internet over the past decade,", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments.", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\"", "to closing the achievement gap between financially comfortable white stu- dents...... before the Highland Lakes Democratic Women in Kingsland on Dec.", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound. These breeds are hunting dogs and are generally regarded as having some... Scent hounds specialize in following a scent without having to...", "Aeneid - Proper Names Flashcards, and more  for free.... Dardanus. son of Zeus and Electra, founder of Dardania in the Troad, and.... a mountain peak in northeast Greece", "the Alleged 9-11 Hijackers - Emerald  Within 24h of the attacks, CNN had this first FBI list of 19.", "City on the south side of the most congested U.S.-Mexico crossing; half the northbound cars wait 90 minutes", "Scandinavians", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7135416666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8761", "mrqa_squad-validation-3497", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1880", "mrqa_squad-validation-6244", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7135416666666667, "EFR": 1.0, "Overall": 0.8567708333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene welding", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "spreading \"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "\"The Book of Roger\"", "the object's mass", "Africa", "Pierre Bayle", "the strain that caused the Black Death is ancestral to most modern strains of the disease", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "The Shirehorses", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "to be identified as transgender", "672", "the Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "he is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8485835416182284}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.782608695652174, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.78125, "CSR": 0.73046875, "EFR": 0.8571428571428571, "Overall": 0.7938058035714286}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "March Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "domestic scale", "force model that is independent of any macroscale position vector", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Independence Day: Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "the ATP is synthesized there, in position to be used in the dark reactions", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry & June", "architecture", "arrows", "moles", "a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "\"caliper\"", "a \"nucleons\"", "James Hoban", "Amelia Earhart", "1963", "a cricket bat making process", "Sasha Banks", "The United States of America", "Tuesday's iPhone 4S news", "Charles M. Schulz Museum"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7198660714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m. weekdays", "ammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene", "he published his findings first", "Nurses", "time and space complexity", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "Hindu and Buddhist sculptures", "the mid-sixties", "Kuznets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0 out of phase", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "chloroplast's stroma", "cotton spinning", "10 November 2017", "baeocystin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Daniel Roebuck", "Kristine Leahy", "1999 Odisha cyclone", "Fat Albert", "Frontline", "shrews", "Tom Kartsotis", "modern genetics", "a person trained for travelling in space", "it appears that Ali Baashi was also specifically targeted by gunmen", "shrews", "independence"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6608225108225108}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.5, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-9176", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.59375, "CSR": 0.6979166666666667, "EFR": 0.9615384615384616, "Overall": 0.8297275641025641}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "an immunological memory", "Calvin cycle", "Zhenjin", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "an innate force of impetus", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "pianissimo", "the secant", "West Germany", "McKinney", "Spock", "Solomon", "Blackstar", "geomorphology", "Earth", "krokos", "Richmond", "Passenger Pigeon", "Richard Wagner", "a \"pair of horse-blinders\"", "Debbie Rowe", "Japan", "1973", "Duck Soup", "London", "Stephen Graham", "Southaven", "Indonesia", "\"Gold Digger\"", "Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6401538475891925}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_squad-validation-7013", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.5625, "CSR": 0.6785714285714286, "EFR": 0.9642857142857143, "Overall": 0.8214285714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "rheinkilometer", "14 reconstructions", "150", "North American Aviation", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "in the south", "Geordie", "gaseous and liquid oxygen", "US$10 a week", "harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds (64 kg)", "1806-07", "peters", "a shed", "Bill Clinton", "a police car", "dead man's curve", "paris", "the Chetniks", "paris", "paris", "Rookwood", "paris", "jonathan Murrow", "surrey", "rowe", "deborah dollar", "paris", "paris", "neptune", "Christopher Marlowe", "3GS", "congruent", "Genoa", "Jacob", "jonathan kiedis", "denmark", "Federal Reserve", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "the deputy", "cowardly lion", "March 22"], "metric_results": {"EM": 0.5, "QA-F1": 0.5693080357142857}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9109", "mrqa_squad-validation-8602", "mrqa_squad-validation-6324", "mrqa_squad-validation-2097", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-7961", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.5, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "1641\u20131679", "counterflow", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "MC Hammer", "Mao Zedong", "the Arroz con Leche", "Hawaii", "the Kiwanis Club", "the log cabin", "saxophones", "a tornado", "George Sand", "the Z", "Rome", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "a balloon", "The Princess Diaries", "the Konabar", "Massachusetts", "larynx", "John Galt", "Arbor Day", "cinnamomum", "the right angle", "Kentucky", "Henry Clay", "Chinese Exclusion Act", "a small tree with fragrant spring flowers", "1995", "Harry Nicolaides", "Mineola, New York", "Blender's \"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.65625, "QA-F1": 0.729203869047619}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.2857142857142857, 0.0, 1.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-3415", "mrqa_squad-validation-434", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.65625, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle", "William S. Paley", "anaerobic bacteria", "more sunlight in deep water", "eicosanoids and cytokines", "live", "50", "captured the mermaid", "1/6", "DC traction motor", "the \"richest 1 percent in the United States", "divinity of Jesus", "EastEnders", "Wolf Heintz", "highest", "a few drops", "1882", "Mel Jones", "North America", "Alastair Cook", "Swadlincote", "~ 1 kHz", "flytrap", "As sea levels rose, the river valley became flooded, and the chalk ridge line west of the Needles breached", "Allison Janney", "2026", "Georgia", "it showed such a disregard for the life and safety of others", "1984", "4 September 1936", "Andrew Moray and William Wallace", "Jane", "Pangaea", "Have I Told You Lately", "sinoatrial node", "September", "the 2013 non-fiction book of the same name by David Finkel", "prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "1977", "judges", "Lynda Carter", "100,000 writes", "X", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Meg Foster", "Tryphon Tournesol", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "around 3.5 percent of global greenhouse emissions", "\"Billy Budd, Billy Budd.", "hearing"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6602790265290266}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [0.4, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.8, 0.4, 0.0, 0.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-4460", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-7459", "mrqa_squad-validation-2451", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_triviaqa-validation-5948", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-15030"], "SR": 0.546875, "CSR": 0.6453125, "EFR": 0.9310344827586207, "Overall": 0.7881734913793104}, {"timecode": 10, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.845703125, "KG": 0.4609375, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "State Route 41", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "a secular \"guardians of the tradition\" (Salafis, such as those in the Wahhabi movement) and the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European countries", "RogerNFL", "events and festivals", "9 venues", "Adelaide", "once", "Around 200,000 passengers", "itty Hawk", "Nidal Malik Hasan", "National Collegiate Athletic Association", "priest Charles Coughlin, whose antisemitic broadcasts were popular with Boston's Irish Catholics", "Harry Hook in Disney's \"Descendants 2\"", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "James Gay-Rees, George Pank, and Paul Bell", "the State House in Augusta", "1970", "1978", "2005", "My Cat from Hell", "Mark Sinclair", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "gastrocnemius muscle", "John Roberts", "fishing out (to s.o. in difficulties)", "Carl Johan", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.609375, "QA-F1": 0.727096688034188}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.5, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-1491", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-85", "mrqa_squad-validation-680", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_triviaqa-validation-6332"], "SR": 0.609375, "CSR": 0.6420454545454546, "EFR": 1.0, "Overall": 0.7327059659090909}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Victoria", "huge compensation pools", "the main opposition party, the Orange Democratic Movement (ODM)", "Charlesfort", "rapidly evolve and adapt", "Battle of the Restigouche", "Boston", "forces", "head writer and executive producer", "tin tin tin", "Oswald Cobblepot", "every years", "Conan Doyle", "Batmitten", "Victoria Harbor", "ambidevous", "Robin", "a horse", "Irrawaddy River", "Ed White", "River Hull", "lunar new year", "Johnson", "Copenhagen", "Troy", "INTERNATIONAL", "John Gorman", "a buffalo", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "Bombe", "change of state", "Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "the Beatles", "floating ribs", "address international issues and tackle the most pressing global challenges", "golf", "The current Secretary of Homeland Security", "the Bee Gees", "Adelaide", "Eddie Izzard", "Sabina Guzzanti", "largest and perhaps most sophisticated ring of its kind in U.S. history", "a quark", "krypton", "Glory"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6197297265103869}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.8, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11320754716981131, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-9255", "mrqa_squad-validation-8421", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.53125, "CSR": 0.6328125, "EFR": 0.9333333333333333, "Overall": 0.7175260416666667}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "alternate", "Yasir Hussain", "Malayalam", "Kennedy Road", "2002", "31", "Atlanta", "Bill Boyd", "Jack Ryan", "Northern", "Buckingham Palace", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "1996", "Revolver", "Jack Nicklaus", "The term suggests repudiation, change of mind, repentance, and atonement", "Mussolini", "james boswell", "off the coast of Dubai", "1918", "butter", "Warwick", "an extended period of abundant rainfall lasting many thousands of years"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7269965277777778}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-2147"], "SR": 0.671875, "CSR": 0.6358173076923077, "EFR": 1.0, "Overall": 0.7314603365384615}, {"timecode": 13, "before_eval_results": {"predictions": ["riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing. I feel I have done no wrong.", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "$20.4 billion, or $109 billion in 2010 dollars", "twelve residential Houses", "Anglo-Saxons", "excerpts from the Doctor Who Confidential documentary", "stricter discipline", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law", "mutiny", "chipmunk", "james boswell", "Melbourne", "Albania", "brown trout", "Mayflower", "ali Lincoln", "lacrimal fluid", "George Best", "ali", "The Great British Bake Off", "Red Lion", "Fenn Street School", "Smiths", "ali Bronte", "The Nobel Prize in Literature", "Pakistan", "The Daily Universal Register", "United States", "Big Fat Gypsy Wedding", "hair that grows on the chin, upper lip, cheeks and neck of human beings and some non-human animals", "Andes", "Thor", "The Comitium", "james boswell", "ali Turner", "SW19", "Lancashire", "Pacific Ocean", "racing", "Oscar Wilde", "climatic types", "Charlie Brown", "james boswell", "avocado", "Black Sea", "glucose", "1933", "Mirror Image", "Abu Dhabi", "Craig William Macneill", "terminal brain cancer", "800,000", "pyroclastic debris", "giant slalom", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5700396825396825}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428564, 0.1111111111111111, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-3953", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-5760", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.484375, "CSR": 0.625, "EFR": 1.0, "Overall": 0.729296875}, {"timecode": 14, "before_eval_results": {"predictions": ["at the Cathedral of Saint John the Divine", "Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW study", "Magnetophon tape recorder", "evaded being drafted into the Austro-Hungarian Army", "Rotterdam", "If (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "wind", "go and save the best for last", "Hermitage Museum", "Portland", "water", "Solferino", "Troy", "10", "turkeys", "henry tamiris", "goldfish", "William", "henry kirchhoff", "a light-year", "henry h hilland", "Don Juan", "a major power broker", "Prince of Wales", "cocoa butter", "Violent Femmes", "oats", "a guardian angel", "laser", "James Fenimore Cooper", "Veep", "brilliant", "henry mother stole the ghi", "cape driver", "a henry", "roshi", "a control Freak", "james boswell", "investment money", "james boswell", "james boswell", "New Jersey", "a madhouse", "fertilization", "India is the world's second most populous country after the People's Republic of China", "Nissan", "a menorah", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "netherlands", "henry hudson"], "metric_results": {"EM": 0.40625, "QA-F1": 0.47858946608946606}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-1234", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.40625, "CSR": 0.6104166666666666, "EFR": 0.9736842105263158, "Overall": 0.7211170504385965}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhani Express", "President pro tempore of the Senate", "Hugo Weaving", "the passing of the year", "The Dursley family", "Brobee", "the nerves and ganglia", "Aman Gandotra", "Jethalal Gada", "Kevin Sumlin", "birch", "the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Emma Watson", "a United States military prison", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "the Sunni Muslim family", "Magnavox Odyssey", "the Internet", "Christianity", "the most by any side", "the nucleus", "The Occupation of the Ruhr", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "in the chloroplast stroma", "the 1964 Republican National Convention", "Hal Derwin", "oversee the local church", "2007", "55 - 75", "Robert Boyle", "the solar system", "Ascona community", "Ludwig van Beethoven", "former Boca Juniors teammate and national coach", "Akshay Kumar", "Harriet M. Welsch", "a political leader", "(temperature)", "a centerpiece"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6142728798978798}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 0.20000000000000004, 1.0, 0.7499999999999999, 0.0, 0.19999999999999998, 0.5454545454545454, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6, 1.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-8566", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-11316"], "SR": 0.46875, "CSR": 0.6015625, "EFR": 0.9705882352941176, "Overall": 0.7187270220588236}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n)", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "the symptoms of the Black Death are not unique", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies of '77", "Montmorency", "Nut & Honey Crunch", "Elton John", "beer", "Simon Moores", "a double dip recession", "Corfu", "the midrib", "Leopold II of Belgium", "8 minutes", "Federal Reserve System", "four red stars", "Cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "petroleum-derived clear, transparent liquid", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "Shooting Star", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret", "the Runaways", "Evonne Goolagong Cawley", "Max Bygraves", "the A38", "Nicola Walker", "Virgin Trains", "1949", "Port Talbot", "rain", "Epitaph", "Team GB", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "the third Viscount Barnewall", "federal ocean planning", "Two women and seven children were among the injured", "the Equator", "Aerosmith", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5447068191311613}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false], "QA-F1": [0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.2105263157894737, 0.3157894736842105, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-4622", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-1206", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4298"], "SR": 0.4375, "CSR": 0.5919117647058824, "EFR": 1.0, "Overall": 0.7226792279411764}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "the deportation of the French-speaking Acadian population from the area", "journalist", "Seventy percent", "the modern hatred of the Jews, cloaking it with the authority of the Reformer", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Mickey Mouse", "Rugby School", "Spain", "norway", "Google", "dance", "born into Brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "norway", "Arbor Day", "Countrywide Financial", "red light", "Conan O'Brien", "norway", "medical colleges", "nicita krushchev", "Other Rooms", "foll foll follies", "the Black Forest", "Robert Stempel", "boo", "sepoy", "best", "2009", "submarines", "joan la Pucelie", "turtles", "Trinidad and Tobago", "Vladimir Nabokov", "sugar frosting", "Peter Pan", "of the same or nearly the same meaning", "a laser beam", "Phi Beta Phi Society", "Joel", "Numbers 22 : 28", "Switzerland", "Prince Philip", "Cleopatra VII Philopator", "5.3 million", "pilot", "Lance Cpl. Maria Lauterbach", "Pandora", "Tears for Fears", "paper sales company"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5428571428571429}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4260", "mrqa_squad-validation-2609", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-6435"], "SR": 0.453125, "CSR": 0.5842013888888888, "EFR": 1.0, "Overall": 0.7211371527777778}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "Upper Lake", "Alfred Stevens", "domestic social reforms", "the difficulty of factoring large numbers into their prime factors", "eight", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher", "Dunlop India Ltd.", "David Anthony O'Leary", "a family member", "Tamil Nadu", "Attorney General and as Lord Chancellor of England", "Fort Berthold Reservation", "fennec", "Norwood, Massachusetts", "1993", "the 2009 FINA World Championionship", "liquidambar", "Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "\"The King of Hollywood\"", "Inklings", "paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Shakespeare", "2013", "Guthred", "Arizona Health Care Cost Containment System", "Australian", "1945", "1941", "La Scala", "\"How to Train Your Dragon\"", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "Monk's", "Sir Ernest Rutherford", "Arm", "(Ethel)", "France", "at least $20 million to $30 million", "Grant", "Virgil Tibbs", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6059027777777777}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9085", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.515625, "CSR": 0.580592105263158, "EFR": 0.967741935483871, "Overall": 0.7139636831494058}, {"timecode": 19, "before_eval_results": {"predictions": ["one in five", "swimming-plates", "MHC I", "Executive Vice President of Football Operations", "two", "France", "Time", "Nafzger", "Warszawa", "Troggs", "Sch schizophrenia", "hang yourself", "Tom Osborne", "Moses", "pekingese", "Golda Meir", "Fiddler on the Roof", "Monopoly", "Al Czervik", "Stanislaw Leszczyska", "masks", "Alien", "Tower of London", "reptiles", "Madonna", "onion", "Walter Alston", "Benazir Bhutto", "Coca-Cola", "Schussing", "Chaillot", "Ibrahim Petrovich", "butter", "In October 15, 1860,", "soup stand owner", "Pyrrhus", "Guatemala", "bonds", "Rue Morgue", "huevos rancheros", "August Strindberg", "Sacher Torte", "Palestine: Peace Not Apartheid", "Frank", "Agapornis", "Rhodesian", "watermelon, pumpkins", "Daisy Miller", "a calculators, typewriters, etc.", "In April 1775, Thomas Gage, the British governor of Massachusetts, ordered.... to accept his Academy Award for Best Director", "Frank Sinatra", "Sonnets", "South Africa", "In the United States Navy", "the Infamy Speech of US President Franklin D. Roosevelt", "M\u00e1laga-Costa del Sol", "Kidderminster", "\u00c9mile Verdet", "gull-wing doors", "In denying the show's producers the New Jersey Economic Development Authority's 20% tax credit", "sexual assault", "April 6, 1994", "state senators", "38"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5067708333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.4375, "CSR": 0.5734375, "EFR": 0.9722222222222222, "Overall": 0.7134288194444445}, {"timecode": 20, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.861328125, "KG": 0.4734375, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "The Muslims in the semu class", "The John W. Weeks Bridge", "9th", "an integral part of the interdisciplinary approach to patient care", "US$3 per barrel", "Trajan's Column", "river bottom", "Peter Anderson Lee", "Golda Meyerson", "xerophyte", "anions", "Uranus", "George III", "Mike Danger", "O'ahu, Hawaii", "Gandalf", "Mungo Park", "squash", "Pertwee", "magnetite", "Sam Mendes", "El Paso, Hudspeth, Presidio, Brewster", "Emeril Lagasse", "\"Shine\"", "Karl Marx", "an ornamental figure or illustration", "four and a half hours", "Denmark", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Zephryos", "Frobisher Bay", "Dumbo", "William Makepeace Thackeray", "Botany Bay", "Peterborough United", "Porto", "albedo", "11", "Washington, D.C.", "red", "a neutron star", "Groucho Marx", "Andrew Nicholson", "Prince Edward VII", "Algeria", "Spain", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "an 88-year-old white supremacists", "Veracruz, Mexico", "Ken Jennings", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6143229166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-6316", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.546875, "CSR": 0.5721726190476191, "EFR": 0.9655172413793104, "Overall": 0.7190223470853859}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "Panic of 1901", "modern programming practices and enabled business applications to be developed with Flash", "February 6, 2005", "the 1950s", "159", "an Easter egg", "the state of Odisha", "1975", "John Vincent Calipari", "winter solstice", "Billie Jean King", "Robert Hooke", "rocks and minerals", "October 2, 2017", "to avoid the inconvenienceiences of a pure barter system", "four", "Philadelphia", "Lykan", "in the pachytene stage of prophase I of meiosis", "Baltimore", "Taxation", "Hank J. Deutschendorf", "toasted wheat bun", "Dan Stevens", "Canterbury Tales", "May 5, 1904", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "solve its problem of lack of food self - sufficiency", "Bud '' Bergstein", "Portuguese", "bloodstream or surrounding tissue", "William Dawes", "Fox Ranch", "Gibraltar", "Dmitri Mendeleev", "war with the United States", "31", "the disputed 1824 presidential election", "12", "a form of business network", "control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Corinna and seven-time Formula One World Champion Michael Schumacher", "micronutrient-rich", "mexican", "top designers", "mexican", "heptathlon", "Lichfield Cathedral", "Lee Harvey Oswald", "liver"], "metric_results": {"EM": 0.5, "QA-F1": 0.6140748764227277}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.7058823529411764, 0.0, 1.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.16666666666666669, 0.8571428571428571, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-14780", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.5, "CSR": 0.5688920454545454, "EFR": 0.96875, "Overall": 0.7190127840909091}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "a bird of prey", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "a mustard based sauce", "royal Wives", "Harpers Ferry", "Confeitaria Colombo", "the \"Canterbury Tales\"", "Versailles", "Target", "a meadow katydid", "the land they worked on", "the \"Tom Terrific\" and \"The franchise\"", "magnesium", "the Swamp Fox", "The New York Times", "a German Shepherd", "peanuts", "Tibet", "Parker House", "Damascus", "Central Missouri", "a hologram", "Greg Montgomery", "the 1096 quake", "the Buonapartes", "the Scuppernong Grape", "Virginia Woolf", "apogee", "Cherry Garcia", "Africa", "Diamond Jim Brady", "a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T.S. Eliot", "Andes", "aurelie-dupont", "asteroids", "the Nutcracker", "an earthquake", "Conservative Party", "the 1996 World Cup of Hockey", "a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with grated cheese", "Falstaff", "red hair", "Republican", "Wojtek", "Bangor International", "1995", "breast cancer", "12-hour-plus shifts of backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards", "a meeting with the president to discuss her son"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5446428571428571}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.1, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061"], "SR": 0.484375, "CSR": 0.5652173913043479, "EFR": 0.9696969696969697, "Overall": 0.7184672472002636}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "Tim Kaine", "eight", "Adidas", "proud", "the body of the aircraft", "serfs", "the United States", "Michigan", "on the website", "two", "Russia", "the Dr. Octopus", "$106,482,500", "Tuesday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three", "tennis", "Toy Story", "Christmas parade", "90", "alleged that Arroyo and her husband were directly involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "Doral", "Jeffrey Jamaleldine", "an insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "more than 1.2 million", "Mitt Romney", "Egyptians", "forgiveness", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI", "anaphylaxis", "Martin Aloysius Culhane", "a model of sustainability", "Kenyan", "more use of nuclear, wind and solar power", "in a motor motorcycle accident", "the Isthmus of Corinth", "Needtobreathe", "Old Trafford", "Siddhartha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Floyd Nathaniel \"Nate\" Hills", "Hong Kong", "honey", "Henry Wadsworth", "Queen Charlotte Sound"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5173511904761905}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.72, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.390625, "CSR": 0.5579427083333333, "EFR": 0.9743589743589743, "Overall": 0.7179447115384615}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Deficiencies existed in Command Module design, workmanship and quality control.", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "Idisi", "Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records.", "Umar S. Israilov", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills,", "London", "1999", "2006", "Minnesota Timberwolves", "Zero Mostel", "October 13, 1980", "the Chechen Republic", "Orpington", "1926", "Nikolai Morozov", "1968", "Berthold Heinrich K\u00e4mpfert", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "the churches of Galatia '' ( Galatians 1 : 2 )", "narnia", "The 2018 Winter Olympics", "lamas", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "Jay Gillespie", "Glinda The Good Witch of the North", "Strait of Hormuz"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6735891712454213}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.59375, "CSR": 0.559375, "EFR": 1.0, "Overall": 0.723359375}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "manually suppress the fire", "Northern Rail", "South Korea", "botulism", "golf", "Romania", "Pocahontas", "Matlock", "Washington", "Chile and Argentina", "The Blue Boy", "The World Translation of the Holy Scriptures", "Liriope", "Creation", "Pennsylvania", "eastern Pyrenees mountains", "(See Important Quotations Explained )", "Dutch", "Salem witch trials", "Gryffendor", "Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Walter Jenkins", "Burkina Faso", "Billy Cox", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "(Kriss Akabusi", "Magi", "albult", "the Bachelor of Science", "Ash", "Ian Botham", "squash", "Leander Club", "Moss", "Miracles Do Happen", "Poland", "Play style", "Mexico", "Jesus", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "percipient", "Earhart", "Final Cut Pro"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6703125000000001}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5324"], "SR": 0.59375, "CSR": 0.5606971153846154, "EFR": 1.0, "Overall": 0.7236237980769231}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a nobleman who owned a village located at the modern-day site of Mariensztat neighbourhood", "between 100,000 and 180,000 light - years", "2016", "the Islamic prophet Muhammad", "Mel Tillis", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "two years", "Edd Kimber", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile", "Orange Juice", "a photodiode", "February 16, 2010", "Jesse Frederick James Conaway", "dromedary", "Dan Stevens", "chas chandler", "a warrior, Mage, or rogue", "1979", "October 27, 2016", "Authority", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Homer Banks", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "excessive growth", "1879", "British Columbia, Canada", "New York University", "Game 1", "2001", "the Washington Redskins", "Hebrew Bible", "September 14, 2008", "the Vital Records Office of the states", "Pasek & Paul", "the Chicago metropolitan area", "conquistador Francisco Pizarro", "1930s", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season of the fantasy drama television series Game of Throne", "1623", "neutrality", "he cheated on Miley", "banjo", "delitsch", "bing.com", "Taylor Alison Swift", "Keith Crofford", "Michael Crawford", "$22 million", "14-day mission", "flooding", "Angostura", "david", "delaware"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5342923518474989}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.16666666666666669, 0.19047619047619044, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.5, 1.0, 0.46153846153846156, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5333333333333333, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.8, 0.2666666666666667, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.421875, "CSR": 0.5555555555555556, "EFR": 0.972972972972973, "Overall": 0.7171900807057058}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "mathematical models of computation", "Best Supporting Actress", "around 300", "an anvil", "2007", "Robert G. Durant", "she is often regarded as the first to recognise the full potential of a \"computing machine\" and the first computer programmer", "London", "she is generally considered to have liberal political views", "Hanford Site", "the Thunderbird of Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "The Fugitive", "churros", "Christies Beach", "eastern", "Arsenal Football Club", "Don Bluth and Gary Goldman", "torpedo boats", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian movement", "defensive", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge", "USS Essex (CV-9)", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland", "Captain while retaining the substantive rank of Commodore", "Giuseppe Verdi", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Albert II, Prince of Monaco", "Minnesota", "The Rite of Spring", "practiced law", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "11 February 2012", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "fortieth", "Australia", "a brain injury", "denied the claim", "Amanda Knox's aunt", "100,000", "brandy", "The Beatles", "North Carolina"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5528254731379731}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.2222222222222222, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 0.28571428571428575, 0.15384615384615385, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.421875, "CSR": 0.55078125, "EFR": 1.0, "Overall": 0.721640625}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "common flagellated protists", "Juliet", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party", "Pope Benedict XVI", "8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "the Orange County Sheriff's Office", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm", "183", "American Civil Liberties Union", "helicopters and unmanned aerial vehicles", "40", "Liverpool Street Station", "137", "54-year-old", "Government Accountability Office", "Jacob", "South Africa", "Markland Locks and Dam", "4,000", "Baja California Language College", "provided Syria and Iraq 500 cubic meters of water a second", "Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Japan", "her children \"have no problems about the school, they are happy about everything.\"", "consumer confidence", "soldiers with the specific purpose of targeting military personnel", "six", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "southern Bangkok", "two", "an antihistamine and an epinephrine auto-injector", "4,000", "he discussed foreplay, sexual conquests and how he picks up women", "Jean F Kernel", "10 : 30am", "Johannes Gutenberg", "crossword", "Christian Wulff", "Gardiner", "Goetheanum", "the George Washington Bridge", "Atlanta Athletic Club", "junk", "Aristotle", "skipjack"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6408657177982835}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.8, 0.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.5, 0.9473684210526316, 0.6666666666666666, 1.0, 1.0, 1.0, 0.08333333333333334, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5484913793103448, "EFR": 1.0, "Overall": 0.721182650862069}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "July 7, 2015", "lovebirds", "Chicago", "monk seal", "bismarck", "british", "Take Me Out to the Ballgame", "English", "\"What hath God wrought\"", "Stewart Island", "St. Ermo", "bach", "1895", "milk", "illegible", "Scrabble", "Mussolini rose to power", "valkyries", "rain", "hindsaddles", "Jodie Foster", "Elysian Fields", "Five Easy pieces", "Thomas Edison", "Manhattan Project", "Charles I", "divorce", "Enchanted", "the Liberty Bell", "USB 2.0 Port", "Autobahn", "Destiny's Child", "Byron", "a robin", "corticosteroids", "Margot Fonteyn", "eels", "McMillan & wife", "(Whizzer) White", "\"77 Sunset Strip\"", "Galileo Galilei", "existentialism", "John Donne", "british", "A country girl", "human", "Charles Lindbergh", "a queen", "neurons", "Vatican City", "James W. Marshall", "a single, implicitly structured data item", "South Korea", "\"foreigner\"", "4077th MASH", "The Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "The e-mails", "HPV"], "metric_results": {"EM": 0.578125, "QA-F1": 0.657717803030303}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-14999", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-4268", "mrqa_newsqa-validation-1372"], "SR": 0.578125, "CSR": 0.5494791666666667, "EFR": 1.0, "Overall": 0.7213802083333334}, {"timecode": 30, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.84765625, "KG": 0.49765625, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Santiago", "the Baudot code", "Jacksonville, Florida", "DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Accokeek, Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "southwest Denver, Colorado", "Atlanta, Georgia", "the Atlanta Braves, and the Tampa Bay Devil Rays", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "Angus Brayshaw", "An impresario", "Sufism", "January 30, 1930", "Sulla", "the Matildas", "the design, development, manufacture and sale of vehicles bearing the Jaguar and Land Rover (including Range Rover) marques", "tempo", "Milk Barn Animation", "Jenson Alexander Lyons button", "Timothy Dowling", "London", "Jane", "Patricia Arquette", "Otto Hahn and Meitner", "AMC", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "Ben Campbell", "twenty-three", "Gararish", "A. R. Rahman", "Akosua Busia", "September 8, 2017", "volcanic activity", "Burbank, California", "a horse's life is often difficult.", "Heisenberg", "the Kiel Canal", "workers went on strike early Tuesday in Philadelphia, Pennsylvania, shutting down buses, subways and trolleys that carry almost a million people daily.", "Eintracht Frankfurt", "Republican", "poodle", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6660963556728359}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.19999999999999998, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.625, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 0.3076923076923077, 0.0, 0.6666666666666666, 1.0, 0.07692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032"], "SR": 0.53125, "CSR": 0.548891129032258, "EFR": 1.0, "Overall": 0.7218094758064516}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "video game", "Carson City", "The Nick Cannon Show", "\"Mickey's Christmas Carol\"", "ten", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann, Baron Cherwell", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don DeLillo", "The Seduction of Hillary Rodham", "balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Rosie O'Donnell", "Saturday in May", "Taylor Swift", "Miller Brewing", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Creech Air Force Base", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "An All-Colored Vaudeville Show", "Lutheranism", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Jewish", "JackScanlon", "Leonard Bernstein", "62", "France", "carbonic acid", "secretary", "eight", "a nuclear weapon", "2005", "the Beastie Boys", "Madison", "a Ford"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7213045634920635}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.609375, "CSR": 0.55078125, "EFR": 1.0, "Overall": 0.7221875}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Hammer", "LSD", "Matilda", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "football", "a web-based teaching aid", "a hearth", "Greece", "(1932-1934)", "Steve Coogan", "Renard", "Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "The Rescuers", "checkers", "Les Dawson", "Arthur, Prince of Wales", "the Grail", "Ronald Reagan", "Jonathan Creek", "climate", "Harrods", "Hammer", "liver", "Guildford Dudley", "Amoco Cadiz", "John Howard", "Adonijah", "His Holiness", "12th", "Cornell University", "Flybe", "The Altamont Speedway Free Festival", "a fat like oil or lard", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "Christians", "Representatives", "Machine Gun Kelly", "Central Avenue", "middleweight", "Jacob Zuma", "digging ditches.", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6768914473684211}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.640625, "CSR": 0.5535037878787878, "EFR": 1.0, "Overall": 0.7227320075757576}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "diversity", "Ennis", "Stratfor", "in the last few months,", "Jaime Andrade", "children ages 3 to 17", "girls", "possible victims of physical and sexual abuse.", "island's dining scene", "gasoline", "vote wozniak", "The plane, an Airbus A320-214", "two Emmys for work on the 'Columbo' series starring Peter Falk.", "an ice jam", "ozzy", "abduction of minors.", "Brazil", "j. Crew", "jenny Sanford", "Florida", "southern Bangladesh", "Clifford Harris,", "Pew Research Center", "nirvana", "doctors", "james polis", "race or its understanding of what the law required it to do.", "he won it after facing various challenges and turning them to his advantage.", "between June 20 and July 20", "lama Clarkson", "misdemeanor failure to appear in court warrant", "1.2 million", "pythons", "Heshmatollah Attarzadeh", "crossfire by insurgent small arms fire,", "2002", "Noriko Savoie", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "when people gathered outside as the conference in the building ended.", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "Majid Movahedi", "former Himalayan kingdom", "Jiverly Wong,", "sexual assault with a minor", "the Louvre", "September 21.", "Wilderness- Firefighter", "Entonox", "Prince Bao", "Narendra Modi", "Steve Davis", "aged 74", "Justin Bieber", "musical research", "Randall Boggs", "Mick Jackson", "West Virginia", "Gary Oldman", "Paris"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5589657738095237}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.8, 1.0, 0.0, 0.8, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5238095238095238, 1.0, 0.5, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-3907", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-44"], "SR": 0.46875, "CSR": 0.5510110294117647, "EFR": 1.0, "Overall": 0.7222334558823529}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins,", "dennis taylor", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "taylor taylor", "Westminster Abbey", "Battle of Agincourt", "white spirit", "dennis taylor", "Kent", "Miss prism", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "Hong Kong", "Mercury", "Torchwood", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "Oil of Olay", "hair", "Collage", "Bathsheba", "Ennio Morricone", "DitaVon Teese", "a collapsible support assembly", "Republican", "Argentina", "French", "dennis taylor", "the internal kidney structures ( Glomerular disease)", "dennis taylor", "Rocky Marciano", "The Benedictine Order", "M69. Coventry to Leicester Motorway", "June Brae", "Jack Klugman", "four", "2000", "2018", "Qutab Ud - Din - Aibak", "Danny Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds", "Democrats", "31 meters (102 feet)", "dennis taylor", "Sacramento, California,", "Hawaii"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6395833333333334}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-2181", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.578125, "CSR": 0.5517857142857143, "EFR": 0.9629629629629629, "Overall": 0.7149809854497354}, {"timecode": 35, "before_eval_results": {"predictions": ["1970s", "Aristotle's", "joe mercer", "calvary", "armadillos", "roosevelt", "Danielle Steel", "Absalom", "molly", "The Goonies", "joe mercer", "Cape Town", "Seine", "alcohol", "Alyssa Milano", "bites a dog", "the Star Spangled Banner", "The Rolling Stones", "Lincoln's", "a chessboard", "Benjamin Franklin", "peter peter wanda", "a urinal", "lunar mare", "Portugal", "Cadillac", "Matt Damon", "dennis roosevelt", "sholom", "white", "arthur james balfour", "dictum", "Easton", "scrabble", "Great Britain", "peter merk", "cumbere", "wolff", "Stephen Vincent Bent", "Brooke Ellen Bollea", "pukka", "Nancy Sinatra", "David", "pinot noir", "Robert Lowell", "\"Bob ate the pie\"", "Richmond", "March", "Amy Tan", "Florence", "pithos", "Grenada", "Mahalangur Himal", "Kusha", "`` Heroes and Villains ''", "cami de Repos", "aluminium", "1", "2015", "October 20, 2017,", "Columbus", "110 mph,", "piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5412202380952381}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4706", "mrqa_searchqa-validation-11147", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-9007", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-16289", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_naturalquestions-validation-10026", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212"], "SR": 0.484375, "CSR": 0.5499131944444444, "EFR": 1.0, "Overall": 0.722013888888889}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Pfc. Bowe Bergdahl", "fans of the MJ Fan Club", "a Columbian mammoth", "her church", "a floating National Historic Landmark,", "a mechanism at the federal level to ensure that drivers comply.", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75", "prisoners at the South Dakota State Penitentiary", "women", "CNN/Opinion Research Corporation", "Kingdom City", "a former CEO of an engineering and construction company with a vast personal fortune", "Ku Klux Klan", "President Felipe Calderon", "137", "3-3", "Dancing With the Stars", "a traditional form of lounge music that flourished in 1940's Japan.", "Michael Jackson", "military commissions are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust.", "Venezuela", "they were growing more and more suspicious of the way their business books were being handled.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "a number of calls,", "Mandi Hamlin", "Iraq", "the American Legion", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Oklahoma", "\"Dance Your Ass Off\"", "Malawi", "246", "the skull's", "six", "Izzat Ibrahim al-Douri,", "Nearly eight in 10", "a one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "Muslim north of Sudan", "21st", "Clifford Harris,", "she is survived by her sons Matthew and Daniel and grandchildren Kyra and Violet,", "Susan Boyle", "Florida", "UNICEF", "United States, NATO member states, Russia and India", "27-year-old", "45", "Justinardo de Stinky", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Allhallowtide", "Gregor Mendel", "(George) Jones"], "metric_results": {"EM": 0.5, "QA-F1": 0.5902463159176714}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.17391304347826086, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5217391304347826, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.5, "CSR": 0.5485641891891893, "EFR": 0.96875, "Overall": 0.7154940878378379}, {"timecode": 37, "before_eval_results": {"predictions": ["over $20 billion.", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "All the Way", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "a chronological collection of critical quotations about William Shakespeare", "Terrence Jones", "\"S&M\"", "one", "Commander Stowe", "O", "The Grandmaster", "highland regions of Scotland", "1940", "half of the Nobel Prize in Physics", "Russian Empire", "West Point", "Hilary Erhard Duff", "Ogallala", "October 21, 2016,", "fifth studio album, \"My Beautiful Dark Twisted Fantasy\" (2010)", "Everything Iswrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Spitsbergen", "1967", "residential", "Andrea Maffei", "band director", "1837", "$10\u201320 million", "Mandarin", "Fester Addams", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "`` Wah - Wah ''", "The First Battle of Bull Run ( the name used by Union forces )", "Alison Krauss", "Steve Hansen", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars in Chinese products each year,", "Diamond", "Simon Legree", "Sideways", "pindar"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5858693389943389}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.8, 0.6666666666666666, 1.0, 0.7142857142857143, 0.4, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5404", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-6464", "mrqa_newsqa-validation-1312", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.453125, "CSR": 0.5460526315789473, "EFR": 1.0, "Overall": 0.7212417763157895}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "a gastropod shell", "Thailand", "Abraham Lincoln", "Alex Hamilton", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "horse", "Mussolini", "Southern California", "Fort Leavenworth", "INXS", "Flat", "graham graham", "Extra-Terrestrial Intelligence", "Arthur", "Pablo Picasso", "Clara Barton", "Nine to Five", "a snake", "a moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Margaret Armstrong-Jones", "1937", "an algae", "feminism", "Space Coast Convention Center", "the gallbladder", "graham Gilman", "midway", "Liechtenstein", "Custer", "Gilead", "salt", "Gloria Steinem", "Catherine de' Medici", "Tonga", "Minos", "Gulliver", "rum", "SeaWorld", "coup de grce", "Tyra Banks", "Richard Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "negative", "an inch", "Polish", "Canterbury", "Lowndes County", "Northern Rhodesia", "his son-in-law Cleve Landsberg", "Goa", "stuck to with remarkably little internal drama. He won it after facing various challenges and turning them to", "1,500"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6265514184397163}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.7659574468085107, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_triviaqa-validation-1183", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3579"], "SR": 0.515625, "CSR": 0.5452724358974359, "EFR": 1.0, "Overall": 0.7210857371794872}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "Sunday", "The 1980 Summer Olympics", "an international educational foundation", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Angus Young", "Floyd", "late as the 1890s", "studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "Big Ten Conference Champions", "Franklin and Wake counties", "60", "RMS Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "Texas - style chili con carne", "6 March 1983", "Kevin Michael Richardson", "James Arthur", "James Watson and Francis Crick", "Antarctica", "during the American Civil War", "Thomas Middleditch", "secession", "Sir Ernest Rutherford", "Buddhist", "1889", "diurnal and insectivorous", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy", "1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "2013", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` central '' or `` middle ''", "metamorphic rock", "Carmen", "a whelp", "glass", "Rikki Farr's", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "in the Gaslight Theater", "\"M*A*S*H\"", "The Cure", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6229491136630605}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.18181818181818182, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.2857142857142857, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-269", "mrqa_searchqa-validation-14218"], "SR": 0.546875, "CSR": 0.5453125, "EFR": 0.9310344827586207, "Overall": 0.7073006465517241}, {"timecode": 40, "UKR": 0.591796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.765625, "KG": 0.45625, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "malaria, dengue fever, yellow fever, and... Vectors of human disease", "Jefferson Davis", "Rubik's Cube", "a kettledrum", "meringue", "let (department manager) go, but can't do it until I have someone to replace him.", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "National Motorcycle Museum Anamosa, IA. The book Iowans of Impact", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "comedy", "Sardinia", "air (atmo), water (hydro), and life (bio)", "Winston Churchill", "Popcorn", "Madonna", "Light Flyweight", "yoyo", "Winston-Salem", "A Streetcar Named Desire", "Scotland", "antibiotics", "goalkeeper, forward, midfielder", "Columbine Columbine", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Seeing Stone", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a", "Chicago", "the Great Pyramid", "Herod", "Alaska", "\"more likely to be killed by a terrorist\" than to ever...", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "1.o", "The Day of the Locust", "diamond", "Emilio Estevez", "The Call of the Wild", "Spain", "the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy,", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6275958110516935}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-16454", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-13067", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-5758", "mrqa_naturalquestions-validation-3961", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.53125, "CSR": 0.5449695121951219, "EFR": 1.0, "Overall": 0.6717282774390243}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "Prince Rainier", "Walden", "Fred Astaire", "Humphrey Bogart", "honda", "Alan Bartlett Shepard Jr.", "by burning nitrates and mercuric oxides", "(Gollancz)", "jacks", "the Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "(Prunella) Scales", "South Sudan", "Tuesday", "the Dannebrog", "Lincoln", "the east coast", "Lavoisier", "NOW Magazine", "Tuscany", "Battle of San Jacinto", "Beaujolais Nouveau", "Edmund Cartwright", "Stern", "(born) 1577, Siegen, Nassau, Westphalia [Germany]\u2014died May 30, 1640, Antwerp, Spanish Netherlands (now in Belgium)", "the popes", "kautta", "(Gordon) Ramsay", "Wisconsin", "John Barbirolli", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "General Joseph W. Stilwell", "a leaf", "sternum", "Portuguese", "Mexico", "Greece", "Ed Miliband", "marriage", "an iron lung", "The Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Major Molineux", "the Distinguished Service Cross", "Indian classical", "1998", "11", "\"an eye for an eye,\"", "Arabic, French and English", "the Schwalbe", "the runcible spoon", "Seinfeld", "Cress"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6936011904761905}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1676", "mrqa_naturalquestions-validation-7945", "mrqa_hotpotqa-validation-1596", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.640625, "CSR": 0.5472470238095238, "EFR": 0.9565217391304348, "Overall": 0.6634881275879917}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "George Strait", "Andrew Gold", "1983", "a virtual reality simulator", "Banquo", "Pakistan", "fall 2010", "MFSK", "Isaiah Amir Mustafa", "Commander in Chief of the United States Armed Forces", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "1898", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "after 5 years, it was earning $300,000,000 a year. Despite strong sales into the 90s", "Gibraltar", "electron pairs", "by the hip, and under the left shoulder", "Lulu", "ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Tokyo", "1972", "Virgil Tibbs", "Ethel Merman", "1961", "passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Kate Flannery", "Lake Wales, Florida", "1923", "Gutenberg", "Wichita", "tina turner", "st Laurent", "Henry John Kaiser", "Marilyn Martin", "SARS", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Siemionow", "over a kilometer (3,281 feet) high.", "neon", "gag bennett", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6119879201680672}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.28571428571428575, 0.2222222222222222, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.1111111111111111, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.515625, "CSR": 0.5465116279069768, "EFR": 0.967741935483871, "Overall": 0.6655850876781695}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a Chocolate macchiato", "Sheffield United", "Apple and Google", "Wat Tyler", "john Wayne", "Dutch", "the Earth", "James Hogg", "Texas", "Rhino", "soap", "Austria", "Louis XVI", "john connans", "fifty-six", "Uranus", "Plato", "the chord", "Chuck Berry", "Separate Tables", "Wilson", "coal", "Stephen", "bulgarth saloner", "eukariste", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "a tittle", "E. T. A. Hoffmann", "the Republic of Upper Volta", "Robert Wright", "an elephant", "German", "New Zealand", "Mendip Hills", "stencils, and other art pieces", "JMW Turner", "God bless America, My home sweet home", "Trade Mark Registration Act 1875", "boxing", "Benjamin Disraeli, 1st Earl of Beaconsfield", "The Jungle Book", "YouTube", "Jan van Eyck", "Rabin", "Shania Twain", "John Nash", "electron donors", "a mid-tempo ballad, musically inspired by Motown, Philly soul, and Earth, Wind & Fire", "a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "Robert Park", "plus or minus 3 percentage points", "Cairo", "Jackson Pollock", "an elk", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.5, "QA-F1": 0.5755666208791208}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.19047619047619047, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1551"], "SR": 0.5, "CSR": 0.5454545454545454, "EFR": 0.96875, "Overall": 0.665575284090909}, {"timecode": 44, "before_eval_results": {"predictions": ["Between 1975 and 1990", "tariq Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "the end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "Premier League", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "1995 to 2012", "Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "Soviet Union", "smith", "Gwyneth Paltrow, Ewan McGregor", "uniform that a sports team wear in games instead of its home outfit or its away outfit", "1874", "acid", "North Dakota and Minnesota", "David Edward Williams, OBE", "Zambia", "The Sun", "Christopher Tin", "Saint Louis", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2015", "19th", "faby Apache", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn Arellano F\u00e9lix", "Bank of China Tower", "first Spanish conquistadors in the region of North America now known as Texas", "Battle of Etowah", "12", "Antiochia", "Gatwick Airport", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County", "honey bees", "squash", "Chicago", "soybean", "Nineteen", "How I Met Your Mother", "collapsed ConAgra Foods plant", "Everest", "I.M. Pei", "Florence Nightingale", "the Citadel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.619500248015873}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9, 0.5, 0.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.515625, "CSR": 0.5447916666666667, "EFR": 0.967741935483871, "Overall": 0.6652410954301076}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "Morocco", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "Sir John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "Duke of Buccleuch and Queensbury", "The Lion King", "licensing deals", "Wyoming", "\u201cblessed\u201d", "Cast", "Javier Bardem", "8", "Lee Harvey Oswald", "virtual image", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine River", "Confucius", "Japan", "stewardi(i)", "London", "Christian Dior", "Phoenicia", "(C) Bobby Moore", "Ventress", "Jerez de la Frontera", "plac\u0113b\u014d", "\"ISM MUTUAL FRIend\"", "FC Porto", "men", "argument form", "Manchester", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "myxoma virus", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "mid-size four - wheel drive luxury vehicle", "La T\u00e8ne culture", "eastern India", "World Famous Gold & Silver Pawn Shop", "Little Rock Central High School", "\"in the interest of justice.\"", "South Africa", "teeth", "ABBA", "Phoenicia", "Tom Coughlin"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5693548387096774}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [0.5, 1.0, 0.5, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_hotpotqa-validation-3195"], "SR": 0.453125, "CSR": 0.5427989130434783, "EFR": 0.9714285714285714, "Overall": 0.6655798718944099}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "rugby", "a modem", "president", "George Herbert Walker Bush", "Penn State", "Luxor", "Berlusconi", "leviathan", "Mending Wall", "wombat", "a crystal", "thunder", "josephine", "The Three Musketeers", "the iTunes Store", "triton", "sartell", "The Comedy of Humours", "KLM", "Superman", "in X-Men: The Last", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Leon Trotsky", "cacique enchilado cheese", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "jesse", "Charles M. Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "Rikki- tikki-Tavi", "mutual fund", "polygons", "Tennessee", "lm", "a ferry", "New York Times", "The Oresteia", "cereal", "edich rommel", "the Miami Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "In 1946", "george terrier", "crocodiles", "Hindi", "St Mary the Virgin and All Saints church", "John Snow", "Ghana's Asamoah Gyan", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday", "1955"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6016648065476191}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6875000000000001, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8988", "mrqa_searchqa-validation-3091", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-15513", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-201", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1216"], "SR": 0.546875, "CSR": 0.5428856382978724, "EFR": 1.0, "Overall": 0.6713115026595744}, {"timecode": 47, "before_eval_results": {"predictions": ["Hawaii", "The Lord Mayor", "Shel Silverstein", "beers", "trolley", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "George Harrison", "Logan's Run", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "Darfur", "bicentennial", "midway", "George Gershwin", "alpacas", "the Atlantic Ocean", "heredity", "Bicentennial Man", "rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fidel Castro", "The Indianapolis 500", "the Twist", "beryl", "cuckoos", "London", "beetle", "Joan of Arc", "palindrome", "quid", "The Big One", "A Night at the Roxbury", "John Steinbeck", "Eric Knight", "Heroes", "the Ganges", "Thomas Mann", "Samuel", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Bedfordshire", "Charles V", "The Lion, The Witch", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "The government late Tuesday afternoon announced it would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "Fama and French's research is period dependent."], "metric_results": {"EM": 0.625, "QA-F1": 0.7006240287490287}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.22222222222222218, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-10425", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.625, "CSR": 0.5445963541666667, "EFR": 0.9583333333333334, "Overall": 0.6633203125}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "the Dorchester Hotel", "smith", "JIMMY Patterson", "Incredibles", "a cheetah", "Charlie Brown", "the god Odin", "Japan", "Sea-Monkeys", "a host", "\"24\"", "Neil Simon", "Voyager 2", "a gulls", "Nez Perce", "Eva Peron", "a solip", "the Hawkeyes", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "The Trojan Horse", "the Chagos", "the Colosseum", "Cambodia", "Dr. Hook & the Medicine Show", "Innocence", "Uvula", "the sacrament of mystic imction", "Jacob", "Scrubs", "Cheyenne", "the Black Sea", "The Madness of King George", "Frank Sinatra", "the Zambezi river", "tea", "Exodus To Exile", "The Police", "Jamestown", "American funk rock band", "crittenden", "St. Francis of Assisi", "the Alaska dish", "Melville", "Tarzan & Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "marriage", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6806344696969697}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-9800", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-4739", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-8195", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-13972", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.609375, "CSR": 0.5459183673469388, "EFR": 1.0, "Overall": 0.6719180484693877}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "Canaletto", "Carmen", "Isles of Scilly", "the Temple Mount", "feminist", "fourteen", "the kidney", "an apple", "Athina Onassis", "nadal", "Apollo 11", "five", "Kirk Douglas", "John Ford", "tin", "Longchamp", "nihon", "Forbes", "a joey", "Maine", "USS Missouri", "the Pyrenees", "basketball", "Janis Joplin", "Stringer", "basketball", "South Africa", "Pet Sounds", "Ed Miliband", "Scotland", "a pianoforte", "Margaret Mitchell", "Burkinab\u00e9", "Virginia Wade", "40", "75", "Winston Churchill", "John Masefield", "Rio", "\"Party of God\"", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Frank Saul", "radishes", "martin bimmer", "Downton Abbey", "a knife", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Ford", "\"diversity\"", "one of 10 gunmen who attacked several targets in Mumbai", "a rat", "a tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7326073232323232}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-1657", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.65625, "CSR": 0.548125, "EFR": 0.8636363636363636, "Overall": 0.6450866477272726}, {"timecode": 50, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.814453125, "KG": 0.44765625, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "in the human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "2018 Winter Olympics", "Robert Kirkman, Tony Moore, and Charlie Adlard", "in Christian eschatology", "1962", "non-ferrous", "the state sector", "the sacroiliac joint or SI joint ( SIJ )", "Joudeh Al - Goudia family", "after World War II", "a typical child's friend", "to ensure its ratification and lead to the adoption of the first ten amendments, the Bill of Rights", "L.K. Advani", "a reserve unit in accordance with the military's needs", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "`` brilliant celebrity with praise ''", "early 1960s", "Phoenix city proper", "the beginning of the American colonies", "2013", "Diego Tinoco", "in two variables is equal to the Pearson correlation between the rank values of those two variables", "2004", "Glenn Close", "Romanesque ancestors of the Gothic rib vault can be found in early Gothic constructions at Cefal\u00f9, Caen, Durham, and elsewhere.", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Kathleen Erin Walsh", "Carolyn Sue Jones", "De pictura", "a mark that reminds of the Omnipotent Lord, which is formless", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "jhelum river", "a response to the sensation of food within the esophagus itself", "Dolly Parton", "narrowboats", "jane and wear", "Jack Murphy Stadium", "Black Abbots", "Prince Amedeo, 5th Duke of Aosta", "a real person to talk to,\"", "Suba Kampong township", "in the first near-total face transplant in the United States,", "laryngitis", "Pequod", "\"Silent Cal\"", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5979674459674305}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.5, 0.28571428571428575, 0.0, 0.0, 0.6956521739130436, 0.14814814814814814, 0.0, 1.0, 0.0, 1.0, 0.47058823529411764, 0.4, 0.0, 0.4, 1.0, 1.0, 0.12903225806451615, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_searchqa-validation-7913", "mrqa_newsqa-validation-2294"], "SR": 0.484375, "CSR": 0.546875, "EFR": 0.9696969696969697, "Overall": 0.690501893939394}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander as Lara Croft, who embarks on a perilous journey to her father's last - known destination, hoping to solve the mystery of his disappearance", "Franklin Roosevelt", "Scottish post-punk band Orange Juice", "1837", "Louisa Johnson", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "the mid - to late 1920s", "near Camarillo, California", "birmingham nambahu", "2018 and 2019", "in the ark of the covenant", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew )", "Mirror Image ''", "two senators, regardless of its population, serving staggered terms of six years ; with fifty states presently in the Union, there are 100 U.S. Senators", "E-2s and E-3s", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata,", "Jesse McCartney", "the Veterans Committee", "Herman Hollerith", "the ulnar nerve is trapped between the bone and the overlying skin at this point", "December 18, 2017", "Brooklyn, New York", "2008, 2009", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "21 June 2007", "the president of the organization and the president becomes the chair of the board", "Great Britain and the other Allied powers", "Ego", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "douglas", "Vito Corleone", "supply chain management", "Baugur Group", "Venice", "Hyundai Steel's", "birmingham", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5432292995119602}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.3571428571428571, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.8, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.13793103448275862, 0.0, 1.0, 0.0, 1.0, 1.0, 0.375, 0.4444444444444445, 0.3076923076923077, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 0.0, 0.0, 1.0, 0.8205128205128205, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.40625, "CSR": 0.5441706730769231, "EFR": 0.9210526315789473, "Overall": 0.6802321609311741}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Qu'est-ce qu'on a fait au Bon Dieu", "created the American Land-Grant universities and colleges", "Asia-Pacific War", "1949", "\"gunslinger\"", "John Samuel Waters Jr.", "April 31, 1912", "Sacramento Kings", "S6", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "macbeth", "Brad Silberling", "1976", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca Hopkins", "Robert Moses", "Godiva", "Manchester United and the England national team", "futurama", "Manhattan Project", "land area", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University", "Restoration Hardware", "1942", "Arrowhead Stadium", "Luis Edgardo Resto", "C. H. Greenblatt", "Stephen Graham", "Congress", "introverted feeling ( Fi ) and Extroverted Intuition ( Ne )", "Belgium", "jape", "james pollock", "alwin Landry's supply vessel Damon Bankston", "3,000 kilometers (1,900 miles)", "Casalesi clan", "Wyatt Earp", "Scrabble", "Wendell, North Carolina", "an intercalary year"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6822916666666667}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-4933", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-2103"], "SR": 0.578125, "CSR": 0.5448113207547169, "EFR": 1.0, "Overall": 0.6961497641509434}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day journey that has also taken him to Japan and Singapore,", "She's in a residential nursing home. She's 97 years of age.", "a delegation of American Muslim and Christian leaders", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express.", "admitting they learned of the death from TV news coverage,", "a house party in Crandon, Wisconsin", "full Senate Sotomayor,", "in every port, the catamaran and its message has been warmly received.", "Grand Ronde, Oregon.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.", "Conway", "no consensus as to the causes of genocide and mass atrocities,", "rwanda", "Premier Game Match Officials Board", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro", "Genocide Prevention Task Force.", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "\"black box\"", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group, also known as the \"red shirts,\"", "Saturday", "Jezebel.com's Crap E-mail From A guy", "40 years", "Democratic VP candidate", "jackson bergman", "Spanish giants Barcelona and Real Madrid", "three", "between June 20 and July 20", "President Richard M. Nixon,", "Piedad Cordoba,", "Buddhism", "most high-profile amalgamation of Indian and western talent yet,", "Pakistani territory", "a fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "Swamp Soccer", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "four appearances", "surfer", "arthropods", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "a nest", "a crust of mashed potato"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6270701762332955}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 0.28571428571428575, 0.4, 0.11764705882352942, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.21052631578947367, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.3333333333333333, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-16162", "mrqa_naturalquestions-validation-10616"], "SR": 0.53125, "CSR": 0.5445601851851851, "EFR": 1.0, "Overall": 0.6960995370370371}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004 American biblical drama film directed by Mel Gibson, written by Gibson and Benedict Fitzgerald, and starring Jim Caviezel as Jesus Christ", "in Turkey", "illegitimate son of Ned Stark, the honorable lord of Winterfell,", "Sam L. Jackson as Lucius Best / Frozone,", "Latavious Williams", "Hans Raffert", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in the pancreas", "2018", "on a beach in Malibu, California", "desublimation", "eight", "Anglo - Norman French waleis", "The three wise monkeys ( Japanese :  \u4e09\u733f, Hepburn : san'en or sanzaru,", "white blood cell", "mitochondrial inner membrane", "Kansas", "New England Patriots", "Chesapeake Bay", "Fred Ott", "in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "Christian dispensationalist End Times", "the topography and the dominant wind direction", "Development of Substitute Materials", "pagan", "in various submucosal membrane sites of the body", "2013", "John Ridgely as Jim Merchant", "ummat al - Islamiyah", "Arthur Froom, Nawab Ali Khan, Shivdev Singh Taxioi, Zulfiqar Ali Khan", "the volume", "Article 300 - A", "Robert Gillespie Adamson IV", "the 18th century", "1998", "the left atrium of the heart", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the temporal lobes of the brain and the pituitary gland", "1858", "delivering appliances and other goods for department stores", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's", "misdemeanor assault charges", "$106,482,500 to an unidentified telephone bidder,", "improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5646605163804652}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.08695652173913045, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5294117647058824, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-9759", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-3624", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-3606"], "SR": 0.421875, "CSR": 0.5423295454545455, "EFR": 0.9459459459459459, "Overall": 0.6848425982800983}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "North Rhine-Westphalia", "incense", "Jericho", "swab", "asteroids", "\"plankton\"", "Al Gore", "Eleanor Roosevelt", "the War of 1812", "Bangladesh", "success", "Sudan", "Judd Apatow", "a laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "Aladdin Hotel", "9 to 5", "Jan and Dean", "force his", "ice cream", "Mike Huckabee", "Count Grigory Orlov", "Texas", "constellations", "Darl Bundren", "Emily Nussbaum", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "plutonium", "an antelope", "Anne Boleyn", "Q'umarkaj", "Dizzy", "\"from beginning to end\"", "the ACT", "Enrico Fermi", "Icarus", "suspension bridge", "Tigger", "songs", "marathon", "QWERTY", "Deuteronomy", "collect menstrual flow", "13 May 1787", "cartilage", "Triumph", "Victoria", "instruments", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "Tibetan exile leaders,", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5806576236263736}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-12353", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-3845", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-3905", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.46875, "CSR": 0.541015625, "EFR": 1.0, "Overall": 0.695390625}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "bowie british", "british", "The Potteries", "british", "iron", "Little arrows", "the founder of modern nursing", "cats", "Reanne Evans", "republic of the Congo", "Battle of Camlann", "German mathematician David Hilbert", "1905", "Strasbourg", "british", "Jack London", "Book 1: Sowing", "Muhammad Ali", "carbon", "Sierra Oscar", "M65", "Boxing Day", "cheers", "Taliban", "alpestrine", "a mole", "to make or become better", "bokm\u00e5l", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "a black Ferrari", "river Hull", "the Canary Islands", "South Africa", "tendon", "Annie Mae Bullock", "John le Mesurier", "Shinto", "Cleckheaton", "the Greater Antilles", "malt", "Pluto", "Jim Branning (John Bardon)", "cryonic suspension", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "iOS, watchOS, and tvOS", "Ub Iwerks", "American Idol", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World tabloid", "propofol", "Emmett Kelly", "Jesse Malin", "Shakespeare", "Chris cornell"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5764136904761905}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-4669", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.53125, "CSR": 0.5408442982456141, "EFR": 0.9666666666666667, "Overall": 0.6886896929824562}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election,", "Monday,", "eight-week long", "to make gnocchi with Mario Batali", "coalition forces in Afghanistan", "to fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor", "to free Bergdahl,", "ninth", "murder in the beating death of", "kaka", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "kite surfers and wind surfers", "\"Brazil will be playing a bigger role in hemispheric affairs and seeking to fill whatever vacuum the U.S. leaves behind.", "opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Saturday", "foxes", "around 8 p.m. local time Thursday", "11 healthy eggs", "florence will smash their victims' fingers with bricks, snip their backs open with wire cutters, carve them up with knives or simply shoot them.", "\"The missile defense system is not aimed at Russia,\"", "Citizens", "refusal to \"turn it off\"", "Department of Homeland Security Secretary Janet Napolitano", "bicycles", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Pakistan from Afghanistan, and used rocket launchers and machine guns in their attacks.", "the fact that the teens were charged as adults.", "Siri", "small blue booties.", "10 to 15 percent", "Israel", "british", "The security breach", "Landry", "President Bush of a failure of leadership at a critical moment in the nation's history.", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "the Golden Gate Yacht Club of San Francisco", "Veracruz", "Grease", "in some of the worst fighting since the start of the war in Afghanistan.", "2002 for British broadcaster Channel 4", "because its facilities are full.", "the job bill's controversial millionaire's surtax,", "seven", "One of Osama bin Laden's sons", "Africa", "Britain of Florida", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "2004", "scotch", "Ambassador Bridge", "The University of Liverpool", "Alfred von Schlieffen", "Chillingham Castle", "400th anniversary", "the Liffey", "scrabble", "jeremy ley"], "metric_results": {"EM": 0.375, "QA-F1": 0.5205700129843275}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.1212121212121212, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.5, 0.06451612903225806, 0.4, 1.0, 0.8333333333333333, 0.4444444444444445, 1.0, 0.0606060606060606, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.2857142857142857, 0.0, 0.5714285714285715, 1.0, 0.2222222222222222, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3166", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-844", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_hotpotqa-validation-2798"], "SR": 0.375, "CSR": 0.5379849137931034, "EFR": 0.975, "Overall": 0.6897844827586207}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted 2", "1,467", "1911", "The Hours", "14", "The NBA G League", "Charlie Wilson's War", "involuntary euthanasia", "Moon Shot: The Inside Story of America's Race to the Moon", "a diving duck", "The Summer Olympic Games", "Glendale", "National Football League and Major League Baseball", "November 23, 1992", "1993", "University of Vienna", "Jack Ridley", "The Pennsylvania State University", "the Willis (Sears) Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia", "German immigrants", "Flex-fuel", "The Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "the Mach number (M or Ma)", "James Gay-Rees", "1872", "Bhavageeth", "Who's That Girl", "musicologist", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "coca wine", "a small card, often made of thin paper-based card for competitions and plastic to conceal PINs,", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance", "the Carrousel du Louvre,", "A Tale of Two Cities", "The Angel Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6392299107142857}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.28571428571428575, 0.4, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2632", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955"], "SR": 0.546875, "CSR": 0.538135593220339, "EFR": 1.0, "Overall": 0.6948146186440678}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "John Barry", "Aristotle", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus", "Ewan McGregor", "1952", "iron", "Jesse Frederick James Conaway", "autopistas, or tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Anne Murray", "1957", "actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "a four - page pamphlet", "Have I Told You Lately", "the world's second most populous country", "the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "the Roman Empire", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "Pittsburgh Pirates", "to condense the steam coming out of the cylinders or turbines", "Jacques Cousteau", "Felix Baumgartner", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Hal Derwin", "Pyeongchang", "in the 1970s", "1919", "23 September 1889", "carbon, chlorine, and fluorine", "October 27, 2017", "three levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "Wet Wet", "republic", "One Direction", "Delacorte Press", "Drifting", "1927", "Bollywood", "Iran", "\"wipe out\" the United States if provoked.", "Fahrenheit", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6986595002220002}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.7333333333333334, 0.0, 1.0, 0.7272727272727272, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_searchqa-validation-2403", "mrqa_hotpotqa-validation-4642"], "SR": 0.609375, "CSR": 0.5393229166666667, "EFR": 0.92, "Overall": 0.6790520833333333}, {"timecode": 60, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.83984375, "KG": 0.49765625, "before_eval_results": {"predictions": ["Batista", "St. Vincent", "5,042", "Mandalay Entertainment", "Tricia Leigh Fisher", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Frederick Louis, Prince of Wales", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "High Knob", "Miss Universe 2010 Ximena Navarrete", "Maryland", "2010", "democracy and personal freedom", "Sami Brady", "French Canadians", "1964 to 1974", "Vanarama National League", "City Mazda Stadium", "Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "The Panther", "British", "Fainaru Fantaj\u012b Tuerubu", "The University of California", "City of Onkaparinga", "31 October 1999", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950", "Raza Jaffrey", "David Letterman", "\u201cFor Gallantry;\u201d", "ArcelorMittal Orbit", "Congressional auditors", "Joe Harn", "$50", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.625, "QA-F1": 0.6939112103174603}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-2096", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315"], "SR": 0.625, "CSR": 0.5407274590163935, "EFR": 1.0, "Overall": 0.7209579918032787}, {"timecode": 61, "before_eval_results": {"predictions": ["The Blades", "George Blake", "jon stewart", "trout", "jon stewart", "borneo", "France", "Manchester", "sky", "jon stewart", "Angel Cabrera", "November", "jon stewart", "Alan Ladd", "Genghis Khan", "Kofi Annan", "jon stewart", "left", "\u0130skenderun", "lamb", "Space Oddity", "collie", "14", "shark", "jalopy of kinfolk", "jon stewart", "jon stewart", "Dame Evelyn Glennie", "a heart", "jota", "David Bowie", "Billy Wilder", "Mr Loophole", "palla", "4.4 million", "jon stewart", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock,", "piled peaches and cream", "Todd Fisher", "Barry Humphries", "cations", "George Santayana", "Rudolf Nureyev", "Brian Gabbitas", "cat", "apple", "Argos", "Rodgers & Hammerstein", "in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater ; Richard Nixon gave that nomination speech", "By 1770 BC", "The United States Secretary of State", "5", "Amal Clooney", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "Roanoke Island", "the Louvre", "Minneapolis", "YIVO"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5523437499999999}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-7044", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-1385", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-600", "mrqa_triviaqa-validation-6901", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.46875, "CSR": 0.5395665322580645, "EFR": 0.9705882352941176, "Overall": 0.7148434535104364}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Jesus Iglesias", "The Snowman", "bushan Patel", "Helsinki, Finland", "Nayvadius DeMun Wilburn (born November 20, 1983), known professionally as Future, is an American rapper, singer, songwriter, and record producer.", "Tommy Cannon", "Scottish national team", "203 people", "Ward Bond", "Illinois's 15 congressional district", "Rochester", "between 7,500 and 40,000", "5,112", "UTH Russia", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\" and film \"King Jack\"", "perjury and obstruction of justice", "Michael Redgrave", "Sturt", "Taylor Swift", "singer and teacher", "Europe", "Trilochanapala", "deadpan sketch group", "small family car", "Spanish", "Algernod Lanier Washington", "14,000 people", "in photographs, film and television", "37", "Taoiseach of Ireland", "137th", "Nick of Time", "Japan Airlines Flight 123", "professional wrestling", "in Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "pressure-sensitive film products", "survival horror", "The United States of America (USA), commonly known as the United States (U.S.) or America (", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Swiss Confederation", "New Jersey", "Sophie Monk (born 14 December 1979) is an English-born Australian singer, songwriter, actress, model and radio personality", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers ; and the Levant, the eastern coast of the Mediterranean Sea", "September 2000", "Woodrow Wilson", "our mutual friend", "zebra", "Volkswagen", "l Larry Flynt.", "Pope Benedict XVI", "St. Louis, Missouri.", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5915198631535947}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.6666666666666666, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0625, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.47058823529411764, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5442", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2290", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_searchqa-validation-6948"], "SR": 0.453125, "CSR": 0.5381944444444444, "EFR": 0.9714285714285714, "Overall": 0.7147371031746032}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil", "Mokotedi Mpshe,", "apartment building in Cologne, Germany,", "in July for A Country Christmas,", "2005 & 2006 Acura MDX", "Ryan Adams.", "The forehead and chin", "in a ceremony at the ancient Greek site of Olympia on Thursday,", "27-year-old's", "next week.", "1913,", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "his son, Isaac, and daughter, Rebecca.", "The Falklands, known as Las Malvinas in Argentina,", "we can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "Three", "in the 1950s", "Gary Player", "12 million", "The Orchid thief", "The Lost Trailers have also partnered with Keep America Beautiful, a national organization dedicated to litter reduction and recycling.", "President George Bush", "an average of 25 percent", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his decision to join the U.S. military,", "Johan Persson and Martin Schibbye", "Israel", "Sunday's", "between government soldiers and Taliban militants in the Swat Valley.", "Jamaleldine", "The Rev. Alberto Cutie", "all day starting at 10 a.m.", "The TNT series", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "in the head", "350 U.S. soldiers", "neck", "dining scene", "Andrew Garfield", "The National Football Conference ( NFC ) champion Philadelphia Eagles", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "Mutiny on the Bounty", "Melbourne", "1998", "23 July", "Tuesday", "Evian", "(four weeks ago) for dinner last night", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6137824824949396}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 0.16666666666666666, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.45454545454545453, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.6666666666666666, 1.0, 1.0, 0.0, 0.2666666666666667, 0.9473684210526316, 0.13333333333333333, 0.6666666666666666, 0.4, 1.0, 0.8, 1.0, 0.4444444444444445, 0.4347826086956522, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3848", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3434", "mrqa_naturalquestions-validation-8963", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5662", "mrqa_searchqa-validation-5963"], "SR": 0.453125, "CSR": 0.536865234375, "EFR": 0.9714285714285714, "Overall": 0.7144712611607142}, {"timecode": 64, "before_eval_results": {"predictions": ["The theatre's first production was Holberg's comedy \"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "The father of Queen Victoria", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million albums", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "House of Ivar", "more than 265 million", "January 2004", "Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Dan Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "on the town of El Nacimiento in M\u00fazquiz Municipality,", "1968", "Holston River", "September 5, 2017", "Peterborough", "jazz homeland section of New Orleans and on that part of the South", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "Samoa", "The single became Lamar's second number-one single on the US \"Billboard\" Hot 100 after \"Bad Blood\"", "Timo Hildebrand", "Netflix", "first flume ride in Ireland", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "organic vodka", "The Bridges of Madison County", "Thomas Jefferson", "a derivative financial instrument"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6746527777777778}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2030", "mrqa_naturalquestions-validation-7286", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.609375, "CSR": 0.5379807692307692, "EFR": 1.0, "Overall": 0.7204086538461538}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Gustav Bauer", "Ward Productions", "July 31, 2010", "T - Bone Walker", "the entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "four", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "The phrase tippecanoe and Tyler Too ''", "Robert Irsay", "runoff will usually occur unless there is some physical barrier", "1940", "the pulmonary trunk or main pulmonary artery", "Puente Hills Mall", "1977", "An optional message body", "June 1992", "general taxation", "28 July 1914", "Richard Stallman", "525", "October 27, 1904", "December 25", "lizard", "Tom Burlinson, Red Symons and Dannii Minogue", "the fourth season", "Auburn Tigers football team", "during meiosis", "an everyday story of country folk", "Yuzuru Hanyu", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant matter", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "Article Seven", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "Real Life:", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "New York Post's Page 6 gossip column.", "fifth", "timothy branson", "Bingo SOLO", "Amsterdam", "salve"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5734696293290042}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.37499999999999994, 1.0, 0.18181818181818182, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.8, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.46875, "CSR": 0.5369318181818181, "EFR": 1.0, "Overall": 0.7201988636363637}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille", "huggins", "720\u00b0", "Steely Dan", "glimmerball", "hanry asquith", "about a mile north of the village of Dunvegan", "i", "Rebecca", "The Stone Age", "ciao", "Tallinn", "The Great Gatsby", "The Gunpowder Plot of 1605", "Moldova", "Tasmania", "Edwina Currie", "sprite", "IKEA", "Pablo Picasso", "Some Like It Hot", "bach", "Tony Blair", "Pickwick", "360", "Caracas", "Ireland", "The Donington Grand Prix Collection", "Jim Peters", "horse racing", "onion", "bobby brown", "1948", "narwhal", "India", "giraffa camelopardalis", "kabuki", "website", "Zachary Taylor", "indigo", "Thursday", "for gallantry", "Swindon Town", "cricket", "Jordan", "Burma", "Tottenham Court Road", "hongi", "basketball", "Snow White", "Italy", "Zane Lowe's show on BBC Radio 1 in June 2010, at the Rockstar offices in New York in July 2010, and at the Spike Video Game Awards in December 2010", "Buddhism", "caveolae internalization", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano,", "Robert Barnett,", "Get Smart", "The Bridges of Madison County", "Paraguay", "WikiLeaks"], "metric_results": {"EM": 0.625, "QA-F1": 0.663764880952381}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1142857142857143, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-7706", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_hotpotqa-validation-1714"], "SR": 0.625, "CSR": 0.5382462686567164, "EFR": 0.7916666666666666, "Overall": 0.6787950870646766}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "Tiffany and Co.", "niger", "Cambridge", "france", "1825", "Lorraine", "parisitosis", "Geoffrey Chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james will", "grey squirrels", "Richard Lester", "Buick", "mania", "gooseberry", "\"Rarely is the question asked,", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Obama", "1984", "scapa flow", "China", "nairobi", "Charles I", "rye plomley", "Leon Baptiste", "360", "james brahms", "1123", "Mitford", "Sparta", "Hyundai", "30th anniversary", "Julian Fellowes", "haddock", "Yemen", "George Miller", "mainland China", "nowhere Boy", "wieden", "head and neck", "quant pole", "Edward Lear", "35", "back", "wales", "Meri", "South Asia", "Uralic languages", "the world's fourth-largest media group", "1942", "a reward for ability or finding an easy way out of an unpleasant situation by dishonest means", "bobby darin", "murder in the beating death of a company boss who fired them.", "Ron Howard", "Oakland Raiders", "Leonardo", "Isabella", "Turing"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6109375}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.515625, "CSR": 0.5379136029411764, "EFR": 0.967741935483871, "Overall": 0.7139436076850095}, {"timecode": 68, "before_eval_results": {"predictions": ["natural-language requests", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark, Matt Kuchar and Bubba Watson", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "\"E! News\" on Tuesday.\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "the foyer of the BBC building in Glasgow, Scotland", "his father", "Iran", "South Africa", "the insurgency,", "near the grave.", "Rosie O'Donnell's new nightly talk show, \"The Rosie Show,\"", "Ricardo Valles de la Rosa,", "March 24,", "exciting and simple and effortless and fun.", "mouth.", "100", "Frank,", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio,", "robert may", "off the coast of Dubai", "near the Somali coast", "Arnoldo Rueda Medina.", "hiring veterans as well as job training for all service members leaving the military.", "shock, quickly followed by speculation about what was going to happen next,\"", "northwestern Montana", "Iran test-launched a rocket capable of carrying a satellite,", "without bail", "February 12", "general astonishment", "a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004", "Democratic VP candidate", "martial arts,", "poor, older than 55, rural residents or racial minorities,", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "devastating impact on the city's population causing enormous suffering and massive displacement,\"", "sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horses", "Samuel H. Rumph", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Bongoyo island", "dualism", "William Wordsworth"], "metric_results": {"EM": 0.5, "QA-F1": 0.5909892313568784}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.8, 0.0, 0.3529411764705882, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.787878787878788, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-885", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-6786"], "SR": 0.5, "CSR": 0.5373641304347826, "EFR": 0.96875, "Overall": 0.7140353260869565}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "high-end premium open-air shopping center, the Americana Manhasset", "Mayfair", "Minister for Health from 2014 to 2016 and Minister for Transport, Tourism and Sport from 2011 to 2014", "28 January 1864, Halifax, Yorkshire, England \u2013 19 February 1927,", "Arab", "the southern North Sea", "Larry Richard Drake", "\"The Bad Hemingway Contest,\"", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\"", "\"Back to December\"", "Heather Langenkamp", "two Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "A novel", "\"The Krypto Report\"", "Fort Saint Anthony", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Nippon Professional Baseball", "1919", "\"Tak and the Power of Juju\"", "the western end of the National Mall in Washington, D.C.", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\" in Jacksonville, Florida,", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Tuesday", "Anabolic steroids, also known more properly as anabolic\u2013androgenic steroids", "North West England", "I", "\"Kismet\" (1953)", "Kentucky", "1961", "1896", "2000", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu's most successful ever grand champion,", "Russian bombers", "Juilliard Opera Center", "lizard hips", "Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7319362289757028}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.1818181818181818, 0.4615384615384615, 0.5, 0.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.609375, "CSR": 0.5383928571428571, "EFR": 1.0, "Overall": 0.7204910714285714}, {"timecode": 70, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.81640625, "KG": 0.48359375, "before_eval_results": {"predictions": ["Arkansas", "during the early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "the port city of Aden", "Scott Eastwood", "Springfield, Massachusetts", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland", "\"master builder\" of mid-20th century New York City", "Waialua District", "St.Louis", "Badfinger", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "the UNLV Rebels", "mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "\"Realty Bites\"", "Hanna", "Manchester Victoria station", "Mark Lockheart", "My Love from the Star", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "hard rock", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "beloved religious leaders", "Bill Russell", "Andre Agassi", "Vienna", "Braves", "fill a million sandbags and place 700,000 around our city", "Caster Semenya", "to stop manufacturing 14 unapproved narcotics that are widely used to treat pain.", "Cuyahoga River", "uranium", "Peter Sellers", "river Elbe"], "metric_results": {"EM": 0.625, "QA-F1": 0.7208741359447004}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.06451612903225808, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-247", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.625, "CSR": 0.539612676056338, "EFR": 1.0, "Overall": 0.7108912852112675}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "help at-risk youth, victims of violent crimes and homeless children.", "Russian air force", "female soldier", "Nearly eight in 10", "Goa", "nuclear weapons", "100 percent", "Kenyan and Somali governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones,", "National September 11 Memorial Museum", "Harlem", "\"I don't think I'll be particularly extravagant.\"", "during last year's Gaza campaign", "woman", "1959.", "his", "269,000", "issued his first military orders as leader of North Korea", "music", "group of teenagers.", "at least 300", "kase Ng", "27-year-old's", "Zimbabwe", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "\"A Whiter Shade of Pale\"", "The security breach happened about 5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "$250,000", "combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "NeNe Leakes", "scooter", "Israeli forces were responding to militant fire near the complex.", "Guinea, Myanmar, Sudan and Venezuela", "pine beetles", "the 103 children that a French charity attempted to take to France from Chad for adoption", "Aspirin", "March 1", "Indo - Pacific", "ironworkers", "quetzalcoatl", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxygen", "the Bird of Prey", "Truman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.599678554074403}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.13793103448275862, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.53125, "CSR": 0.5394965277777778, "EFR": 0.9, "Overall": 0.6908680555555555}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997 ( Act No. 33 of 1997 )", "Sharyans Resources", "drives on all four wheels, but may not be designed for off - road use", "Lawrence John Wargrave, a retired judge, known as a `` hanging judge '' for liberally awarding the death penalty in murder cases", "Texas A&M University", "stromal connective tissue", "the last section of the Tanakh, known as the Ketuvim ( or `` Writings '' ), and a book of the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Cloris Leachman", "Eukarya -- called eukaryotes", "Mara Jade, who has learned to better her Force knowledge since her training at Luke's Jedi Academy", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "to determine the tenderness of meat", "Richard of Shrewsbury, Duke of York", "Ashrita Furman ( born Keith Furman, September 16, 1954 )", "30 - something man ( XXXX ), is a London underworld criminal who has established himself as one of the biggest cocaine suppliers in the city, with effective legitimate cover", "Jean Fernel ( 1497 -- 1558 ), a French physician, introduced the term `` physiology ''", "2007 and 2008", "May 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Ronald Reagan, who was 73 years, 274 days of age at the time of his election to a second term", "Johnny Logan", "revenge and karma", "a prohibition of blasphemy, specifically, the misuse or `` taking in vain '' of the name of the God of Israel, or using His name to commit evil", "England and Wales", "1996", "1000 BC", "Idaho", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "eight hours ( UTC \u2212 08 : 00 )", "Rajendra Prasad", "Alan Autry Jr. ( also known for a period of time as Carlos Brown ; born July 31, 1952 ),", "Jay Baruchel", "Ann Doran", "merengue and bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "toward the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "secure communication over a computer network", "3", "1939", "BT Sport", "Ticket to Ride", "all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "John of Gaunt", "75 or older", "j29", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "a bathing suit", "a nurse who tried to treat Jackson's insomnia with natural remedies testified that Jackson told her that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "a staff member at the Iswahyudi hospital in nearby Madiun, told local media that 19 people were brought to the hospital -- several with serious injuries, including multiple fracture.", "the Amazon", "Upromise", "The Crow", "Spain,"], "metric_results": {"EM": 0.328125, "QA-F1": 0.5376066347820896}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.2857142857142857, 0.5, 0.0, 0.2857142857142857, 1.0, 0.4, 0.11764705882352941, 0.3333333333333333, 1.0, 1.0, 0.13333333333333333, 1.0, 0.5, 0.21052631578947367, 0.1904761904761905, 0.7499999999999999, 0.2, 0.4, 0.07999999999999999, 0.3636363636363636, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.2758620689655173, 0.6, 0.8, 0.4, 1.0, 1.0, 0.4, 1.0, 0.9302325581395349, 0.47058823529411764, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 0.05405405405405406, 0.14634146341463414, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-284", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3544", "mrqa_searchqa-validation-3477", "mrqa_newsqa-validation-646"], "SR": 0.328125, "CSR": 0.5366010273972603, "EFR": 0.9534883720930233, "Overall": 0.7009866298980567}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "Fall Guy", "crown", "Maria Montessori", "Kinsey Millhone", "Alexander Hamilton", "Rendezvous with Rama", "March of the Penguin", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liqueur", "Texas", "a Condors", "John James Audubon", "Pontius Pilate", "Goldwater", "synapse", "halfpipe", "Jackie Collins", "\"carnaval\"", "Freakonomics", "George Washington Carver", "Devon", "Champagne", "Red Heat", "New Orleans", "Haiti", "a carrel", "Love potions", "Prince William", "Sherlock Holmes", "a conchiglioni", "Orion", "Bangladesh", "carbon monoxide", "John", "plug in", "loathsome", "Thailand", "manslaughter", "programming techniques", "the Tennessee River", "Hipparchus", "Billy Idol", "Missouri Compromise", "a Rat", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$689.4 million in North America and $1.528 billion in other countries", "the left hand ring finger", "Conrad Murray", "Gryffindor", "Czech Republic", "in Sochi, Russia, from 7 to 23 February 2014", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "American Civil Liberties Union", "month"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6929814976689976}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-10889", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_hotpotqa-validation-4076", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.59375, "CSR": 0.5373733108108107, "EFR": 1.0, "Overall": 0.7104434121621621}, {"timecode": 74, "before_eval_results": {"predictions": ["glucose", "Angela Rippon", "Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "400 meter event", "British Airways", "Bachelor of Science", "B4425", "Pete Best", "Bonnie and Clyde", "Avatar", "Concepcion", "St Moritz", "Edmund Cartwright", "par", "Pandora", "Japanese silvergrass", "April", "Sir Arthur", "Wolfgang Amadeus Mozart", "bees", "Sun Hill", "The Nutcracker", "Lightweight", "Adare", "muppet", "photography", "kirsty Young", "Samuel Johnson", "Sports & Leisure", "a bear", "Ganga", "tabloid", "car door", "Kolkata", "the Temple of Artemis", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "Up stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "CareMore Health System", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International.", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.65625, "QA-F1": 0.6870510057471264}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.65625, "CSR": 0.5389583333333333, "EFR": 0.9545454545454546, "Overall": 0.7016695075757575}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Massachusetts", "Japan", "hiphop", "erotic thriller", "Pylos and Thebes", "Eddie Vedder", "Arabella Churchill", "Sir William McMahon", "Hopi", "North Kesteven", "Australian", "Annie Ida Jenny No\u00eb Haesendonck", "sixth year", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Eunice Kennedy Shriver", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Adelaide", "4145 ft above mean sea level", "University of Georgia", "just over 1 million", "Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "A.P. M\u00f8ller", "J. Cole", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "1945", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Liverpudlian", "Mindy Kaling", "3 October 1990", "September 21, 2016", "state - of - the - art photography", "earache", "Diego Garcia", "cuckoo", "$2 billion", "127 acres.", "inject them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "Patrick", "the Tomb of the Unknown", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6812356913919414}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.25, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.15384615384615385, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-4030", "mrqa_searchqa-validation-13410"], "SR": 0.5625, "CSR": 0.5392680921052632, "EFR": 0.9285714285714286, "Overall": 0.6965366541353383}, {"timecode": 76, "before_eval_results": {"predictions": ["pet Sounds", "on Scottish soil took place on a nearby moor at Culloden", "\u201cArs Gratia Artis\u201d", "Johann Strauss II", "James Callaghan", "Cypressaceae", "(Tokyo Interbank Offered Rate)", "Dublin", "Pyrenees", "leprosy", "left", "Bill Kerr", "avocado", "Anne of Cleves", "The Double", "United States", "Supertramp", "rhea", "Octavian", "one Night / I Got Stung", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "\"Mr Loophole\"", "Ken Purdy", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti art", "Friedrich Nietzsche", "Dee Caffari", "cheese", "Annie", "Kristiania", "piano", "Moby Dick", "moss path", "Sacred Wonders of Britain", "Changing Places", "pea", "Dr Tamseel", "Sea of Galilee", "4.4 million", "Menelaus", "Alzheimer's disease", "The Firm", "1966", "a fair chance of winning", "31536000", "Jordan", "arthropods", "desperation", "2018", "Colorado Rockies", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "place", "American Red Cross"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5045454545454545}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.18181818181818182, 0.3333333333333333, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-7746", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.453125, "CSR": 0.5381493506493507, "EFR": 0.9142857142857143, "Overall": 0.693455762987013}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "The Kirchners", "iPods", "45 minutes, five days a week.", "killed wife Christine while they were on vacation in 2008, be dropped due to a \"unique set of circumstances.\"", "Kris Allen,", "Jared Polis", "ore Gold,", "Zimbabwe", "Harry Nicolaides", "a couple of those who managed to survive the incident hid in a boiler room and storage closets during the massacre.", "April 2010.", "Zed", "to be one of his four children and know that is there for the world to see,", "environmental", "his father's parenting skills.", "Iran", "head injury.", "\"Antichrist.\"", "African National Congress Deputy President Kgalema Motlanthe,", "Hugo Chavez", "Fourteen", "Anne Frank,", "The Da Vinci Code", "Gary Brooker", "Rawalpindi", "Colorado prosecutor", "Helmand province, Afghanistan.", "Jonathan Breeze,", "removal of his diamond-studded braces.", "Ennis, County Clare", "United States", "to eavesdrop on a series of conversations in Arabic, Russian and Mandarin that led police to 86 suspects in a series to raids that started Tuesday,", "Hamas,", "Two pages -- usually high school juniors who serve Congress as messengers -- have been dismissed for allegedly having oral sex in public areas of their Capitol Hill dormitory.", "At least 40", "four", "Courtney Love,", "28 years", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "is undergoing renovation.", "Naples home.", "Hanford nuclear site,", "November 26,", "sportswear,", "Shanghai", "hopes the journalists and the flight crew will be freed,", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "arousal regulation", "katrughan", "India and Pakistan", "allergic reaction", "a lie detector", "chords from the second half of this song", "1963", "\"Black Abbots\"", "a nurse bag", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.5625, "QA-F1": 0.677007260684999}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.11538461538461539, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.07142857142857142, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615383, 0.8823529411764706, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.5625, "CSR": 0.5384615384615384, "EFR": 1.0, "Overall": 0.7106610576923076}, {"timecode": 78, "before_eval_results": {"predictions": ["\"blind\" spot", "the Silk Road", "norway", "George Rogers Clark", "an amu", "a coach dog", "Sweden", "volleyball", "John Alden", "Ghost World", "Deuteronomy", "a map", "Japan", "Madison Avenue", "Job", "hertz, 1000, May 26, 2008.", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "colonel", "the National Archives Building", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "Southern Christian Leadership Conference", "Moscow", "a Ford car", "\"Cecilia\" Beaux", "Mormon Tabernacle Choir", "\"Do you have it in something/ other than 'A'?\"", "Griffith", "Bangkok", "St. Paul", "positrons", "Ted Kennedy Vice President Run 1968", "Jefferson", "Jerusalem", "Pushing Daisies", "cranberry", "tzatziki sauce", "a souffle glaze", "United Healthcare Workers East", "charlotte russe", "canals", "Abraham", "a self-appointed or mob-operated tribunal", "in Iran, and cultures such as the Persians relied on sheep's wool for trading", "Rachel Kelly Tucker", "works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "London", "Kermadec Islands", "Julius Caesar's", "\"the mother of gods\"", "\"The Danny Kaye Show\"", "2012", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "Victor Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5742673992673992}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.6666666666666665, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.2666666666666667, 0.07692307692307693, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11290", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-11682", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-14242", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.46875, "CSR": 0.5375791139240507, "EFR": 1.0, "Overall": 0.7104845727848101}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "is actually wise", "Doug Pruzan", "a simple majority vote", "byte - level operations", "Rich Mullins", "September 19, 2017", "marriage officiant", "1661", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit", "soft ( low coercivity ) iron", "Incumbent Democratic mayor Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "40.5 metres ( 133 ft )", "Los Angeles Dodgers", "Gugu Mbatha - Raw", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "FaZe Rug", "10 June 1940", "the citizens", "the highest number of votes", "Amanda Fuller", "`` The Forever People ''", "1997", "mitochondrial membrane", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "American swimmer Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Josie ( Gabrielle Elyse )", "2002", "John Michael Higgins as John Smith", "Pangaea or Pangea", "Selena Gomez", "Leslie and Ben", "the dress shop", "6,259 km ( 3,889 mi )", "February 27, 2007", "1960", "March 2, 2016", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "the internal reproductive anatomy ( such as the uterus in females )", "Brundisium", "France", "Georgia", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Woods", "try and reduce the cost of auto repairs and insurance Premium for consumers"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7135409372082167}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.05128205128205128, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.2857142857142857, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9166666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-5639", "mrqa_newsqa-validation-454"], "SR": 0.59375, "CSR": 0.53828125, "EFR": 0.9615384615384616, "Overall": 0.7029326923076923}, {"timecode": 80, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.818359375, "KG": 0.475, "before_eval_results": {"predictions": ["Richard Attenborough", "Miranda v. Arizona", "Oscar Wilde", "Vancouver Island", "violin", "Utrecht", "Vietnam", "Austen", "George Fox", "Deputy Lifeboat Press Officer", "Leadbetter", "Gorbachev", "CBS", "jazz", "Earthquake", "The Jungle Book", "bran doyle", "neoclassic designs of Robert Adam", "a gallon", "Great Dane", "sacred", "Cambodia", "jujitsu", "the 74th Hunger Games", "the head and neck", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "kitty in Boots", "Whisky Galore", "Tunisia", "13", "Sen. Edward M. Kennedy", "Egremont", "head", "Google", "shoulder", "Iran", "Downton Abbey", "bird", "Rudyard Kipling", "backgammon", "dorrit", "Albert Einstein", "Germany", "Beethoven", "exploits on the Island", "ear", "tree", "Imola Circuit", "trout", "Thyllis Applegate", "Lathrop `` Doc '' Brown, Ph. D.", "in the North Atlantic Ocean", "1961", "\"Boston Herald\" Rumor Clinic", "Lord Chancellor of England", "\"Britain's Got Talent,\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "the right doctor for me", "the library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6565972222222223}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666665, 0.05555555555555555, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-550", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.578125, "CSR": 0.5387731481481481, "EFR": 0.9259259259259259, "Overall": 0.6867679398148148}, {"timecode": 81, "before_eval_results": {"predictions": ["Maggie Smith", "worcester", "Dal\u00ef\u00bf\u00bd", "van rijn", "Illinois", "monaco", "paul Maskey", "wawrinka", "tartar sauce", "the three Graces", "satyrs", "Gustave III, ou Le bal masque", "non-Orthodox synagogues", "martin van buren", "leeds", "jennifer taylor", "Operation", "white", "Jay-Z", "(University of) Munich", "honda", "runcorn", "Vietnam", "monaco", "van gogh", "sakhalin", "Croatia", "NBA", "steel", "dolittle", "teddy rees-Jones", "kentle", "bird", "Samuel Johnson", "martini", "belgium", "Victor Hugo", "endosperm", "the Adriatic Sea", "heartburn", "Facebook Stories", "HMS Conqueror", "(University of) Britain", "print letters", "Standard Oil Company", "cynthia Nixon", "hamlet", "Wat Tyler", "Patrick Henry", "126 mph", "kochkino gnezdo", "Eddie Murphy", "Pakistan", "geoffrey tina", "Thorgan Hazard", "Lithuanian national team", "geology", "almost 100", "accusations of improper or criminal conduct.", "in critical condition", "Superman", "Leif Erikson", "Towering Inferno", "member states"], "metric_results": {"EM": 0.5, "QA-F1": 0.5845486111111111}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4207", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.5, "CSR": 0.5383003048780488, "EFR": 1.0, "Overall": 0.7014881859756097}, {"timecode": 82, "before_eval_results": {"predictions": ["laul daguerre", "ukraine", "tarn", "GM", "Sheffield", "eryx", "piano", "Louis XVIII", "prat cash", "santiago", "Wild Atlantic Way", "Kyoto Protocol", "imperial camera case", "repechage", "Donald Woods", "charleston", "peacock", "rita", "Miss Trunchbull", "imola", "ukraine", "antelope", "all animals, no matter how non dangerous or harmless they are", "boreas", "Ivan Basso", "bullfighting", "two", "Playboy", "ukraine", "peter Ackroyd", "bromley-by- Bow", "ganan eriksson", "Thierry Roussel", "mungo park", "the death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "toea", "Lady Gaga", "mark", "raging bull", "ars gratia artis", "baloney", "all things Must Pass", "fire signs", "tet", "Arabah", "mark", "gerry adams", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team owner", "Isabella II", "Mexico", "U.S. Marines training base", "yFZ", "Frdric Chopin", "Indiana Jones", "Jakarta", "The Cosmopolitan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5671875}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_hotpotqa-validation-668"], "SR": 0.515625, "CSR": 0.5380271084337349, "EFR": 0.967741935483871, "Overall": 0.6949819337835212}, {"timecode": 83, "before_eval_results": {"predictions": ["Ricky Gervais", "Wonder Woman", "parable", "Romeo & Juliet", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "the United States", "the pyramid of giza", "Ruth Bader Ginsburg", "the Boer War", "touch", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "penaeus monodon Brackishwater", "College of William and Mary", "a chimp", "Indian reservations", "John Updike", "the Ganges", "vision", "bright and sunset", "would be the Bayou State's next governor", "coelacanth", "Northanger Abbey", "Cheers", "foxes", "Crosby, Stills & Nash", "Matt Leinart", "a blood cell", "Charles Edward Stuart", "an albatross", "Falklands", "taro", "a quip", "a lighthouse", "white", "Dan Rather", "georgia-pacific corp", "Buffalo Bill", "the Big Bang Theory of Creation", "pig", "Harvard", "neurons", "Hawaii", "Pierian spring", "a hobo code", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "pentecost", "humble pie", "36 Hours", "City and County of Honolulu", "Australian coast", "1992", "had publicly criticized his father's parenting skills.", "Barack Obama", "top designers, such as Stella McCartney,", "Rwanda"], "metric_results": {"EM": 0.53125, "QA-F1": 0.648623511904762}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-3660"], "SR": 0.53125, "CSR": 0.5379464285714286, "EFR": 1.0, "Overall": 0.7014174107142858}, {"timecode": 84, "before_eval_results": {"predictions": ["in the 1970s", "Vancouver's fair, the PNE", "sometime during the 1930s", "Lenny Jacobson", "the status line", "each team is given a position in the drafting order in reverse order relative to its record in the previous year, which means that the last place team is positioned first", "resulted in integration and was a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "the weeks before the release of Xscape", "biscuit - sized", "687 ( Earth ) days", "a jazz funeral", "the previous year's Palm Sunday celebrations", "Castleford", "the fourth C key from left on a standard 88 - key piano keyboard )", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "wintertime", "Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "Robber Barons", "2001", "Lucius Verus", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians during World War II", "Michael Crawford", "200 to 500 mg up to 7 ml", "gastrocnemius muscle", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "the mayor's home", "1945", "Pebble Beach", "Andaman and Nicobar Islands", "uterus and uterine tubes", "The highest architectural element, tip and occupied floor, and is indeed the tallest structure of any kind ever built, surpassing the ( now destroyed ) 646.38 metres ( 2,120.7 ft ) Warsaw Radio Mast", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash & Willie Nelson", "Rick Grimes ( Andrew Lincoln )", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing ( NLP )", "10 years", "2026", "eleven", "The record for most non-consecutive weeks at number one is 18 by Frankie Laine's `` I Believe '' in 1953", "After World War I", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "Lingerie Football League", "a griffin", "Mick Jackson", "Lake Huron", "15", "Michelle Obama", "Consumer Product Safety Commission", "\"This are the products we created from them,\"", "a trailgator bars and attachment on 2 adult and 2 kids bikes", "Stones from the River", "Dwight D. Eisenhower", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5414404173975675}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.13793103448275862, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.6153846153846153, 0.6666666666666666, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2727272727272727, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132"], "SR": 0.421875, "CSR": 0.5365808823529412, "EFR": 0.9459459459459459, "Overall": 0.6903334906597774}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "sprint", "ganga", "gerry adams", "purple", "Roy Rogers", "Steve Jobs", "Tommy Lee Jones", "Nirvana", "Donna Summer", "heel", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "oregon", "9801", "Franklin Delano Roosevelt", "neurons", "Porridge", "Mario Segale", "Swordfish", "eardrum", "Best", "pork", "11", "a bluebird", "Australia", "pascal", "British Airways", "five", "Columbia", "The World is Not Enough", "Giglio Island", "Vienna", "glee", "Laura and Kenneth Hockney", "iron", "Japan", "bayern muni", "Lisa Richards", "Italy", "Ciudad Ju\u00e1rez", "May Day", "jalape\u00f1o", "Madagascar", "Beaujolais", "Angus Robertson", "kolkata, India", "strictly come dancing", "David Bowie", "Candace", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam,", "propofol,", "concerns expressed this week about a certain carrier based in Texas.", "Treaty of Versailles", "Zinedine Zidane", "Macduff", "a newt"], "metric_results": {"EM": 0.5, "QA-F1": 0.5977678571428571}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-5241", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-2517", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4938", "mrqa_naturalquestions-validation-6711", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261"], "SR": 0.5, "CSR": 0.536155523255814, "EFR": 1.0, "Overall": 0.7010592296511627}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "\"Runaways\"", "\"50 best cities to live in.\"", "La Liga", "best prom ever", "June 13, 1960", "Iran", "ribosomes", "crimes such as murder, treason, espionage, war crimes, crimes against humanity and genocide", "London", "SBS", "quantum mechanics", "king Duncan", "January 16, 2013", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "\"The Curious Case of Benjamin button\"", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "Numb", "Shropshire Union Canal", "1670", "A skerry", "Oliver Parker", "The Strain", "kamehameha I", "Pac-12 Conference", "Roots: The Saga of an American Family", "five", "William Scott Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "1907", "The Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "The Witch and the Hundred Knight 2", "nathan leopold Jr.", "Bill Haley & His Comets", "george Carey", "Amanda Knox's niece", "\"Leave This Town\"", "Jeddah, Saudi Arabia,", "Charlotte's Web", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6696811868686869}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-1530"], "SR": 0.609375, "CSR": 0.5369971264367817, "EFR": 1.0, "Overall": 0.7012275502873563}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "1754", "8 November 1978", "Hamlet", "Marty Ingels", "New York Knicks", "McLaren-Honda", "Ferengi bartender Quark", "The Spiderwick Chronicles", "American reality television series", "a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "Qualcomm", "water sprite", "10-metre platform event", "Cincinnati Bengals", "on the shore, associated with \"the Waters of Death\"", "\"Nanny McPhee\"", "November 15, 1903", "Bury St Edmunds, Suffolk, England", "Rothschild banking dynasty", "Mr. Church", "\"Billboard\" Hot 100, \"Rich Girl\"", "Thomas Christopher Ince", "Peter 'Drago' Sell", "public", "Los Angeles", "\"The Future\"", "Prussian Lithuanian poet and philosopher Vyd\u016bnas", "al-Qaeda", "Darling", "Baldwin, New York", "2 April 1977", "House of Commons", "William Finn", "\"Love Letter\"", "Indian", "German", "Barnoldswick", "late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann,", "Robert Jenrick", "Somerset County, Pennsylvania", "in a mixed-ethnicity British household headed by Pakistani father George (Om Puri)", "British Conservative", "The Division of Cook", "Bajirao Ballal", "Prafulla Chandra Ghosh", "retina", "Confederate", "Western Samoa", "DeLorean", "comets", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "hooked up with Mildred, a younger woman of about 80, in March.", "a snowmobile", "a scorpion", "bone", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6683636675824176}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.8, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-4148", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.546875, "CSR": 0.537109375, "EFR": 0.9655172413793104, "Overall": 0.6943534482758621}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La Mome Piaf,\" or \"The Little Sparrow.\"", "Ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Dr. StrangeJob", "copper and zinc", "Hammertone", "Dunfermline", "bison bison", "Edmund Cartwright", "Mary Poppins", "Hampshire", "mary mary", "Kiribati", "John Gorman", "The Daily Mirror", "copper", "Olympus Mons on Mars", "Poland", "Dee Caffari", "the Labyrinth", "Belize", "mary rushton", "Ellesmere", "graphical applications programs", "James Hogg", "mmor multiplayer", "Fermanagh", "Colombia", "Kevin Painter", "Llyn Padarn", "Catherine Parr", "Muhammad Ali", "Carmen Miranda", "maryal Husain", "John McEnroe", "August 10, 1960", "Estonia", "Sarajevo", "gluten", "entirely enclosed", "Arthur Ransome", "muthia murali", "Ridley Scott", "four years", "Futurama", "adrian edmondson", "63 to 144 inches", "1951", "September 29, 2017", "Walter Brennan", "13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the surge", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "George Jetsons"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6442708333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-5053", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-6143", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.59375, "CSR": 0.537745786516854, "EFR": 0.9615384615384616, "Overall": 0.6936849746110632}, {"timecode": 89, "before_eval_results": {"predictions": ["the denominator", "Graphical", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "Large", "Nelly", "gladiators", "Finding Nemo", "the hyoid horns", "the Kite Runner", "a shark", "nairobi", "Oprah Winfrey", "Dixie Chicks", "apple tart", "California", "appliances", "the Mediterranean", "Pope John Paul II", "fish", "Yemen", "David Geffen", "chariots", "Pablo Neruda", "The due process clause of the Fifth Amendment", "a mite", "Orbital Sciences Corporation", "Nanny Diaries", "liquid crystal", "Robert Frost", "an authoritative pronouncement", "Butternut Squash Tortellini", "Crete Greece", "Father Brown", "reuben", "The Outsiders", "the waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q's assistant", "Harry Met Sally", "mEXICO", "a pumice", "John Molson", "Jan & Dean", "Robin Hood", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "Andorra", "Michael Faraday", "Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection,\"", "Fernando Gonzalez", "At least 14", "as spies for more than two years,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7518398268398268}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.8, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7272727272727273]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-1594", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-12893", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.640625, "CSR": 0.5388888888888889, "EFR": 1.0, "Overall": 0.7016059027777778}, {"timecode": 90, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.80859375, "KG": 0.47578125, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "\"Loch Lomond\"", "wives and girlfriend of high-profile sportspersons", "Gweilo", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "was the daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "The Los Angeles Dance Theater", "Lola Dee", "Hampton University", "The Clash of Triton", "Jenji Kohan for Netflix", "1", "the second line", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "Hard rock", "Louis Mountbatten of Burma", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University", "Bigfoot (also known as Sasquatch)", "Cyclic Defrost", "England, Scotland, and Ireland", "Coal Miner's daughter", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "mid-ninth-century Viking chieftain", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "the Harlem River", "Turkish", "go!", "sulfuric and nitric acids", "1913.", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "The Lord of the Rings", "Jaguar", "smut", "semi-autonomous organisational units"], "metric_results": {"EM": 0.6875, "QA-F1": 0.771155753968254}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.6875, "CSR": 0.5405219780219781, "EFR": 0.95, "Overall": 0.6967762706043956}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Phillip Schofield and Christine Bleakley", "describe the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds,", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "membranes of the body's cells", "USS Chesapeake", "1977", "official residence of the President of the Russian Federation", "Darwin's On the Origin of Species", "marks locations in Google Maps", "Richard Stallman", "January 2004", "1940", "an armed conflict without the consent of the U.S. Congress", "a evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "heat", "Spanish", "peptide bonds", "New England Patriots", "used their knowledge of Native American languages as a basis to transmit coded messages", "Zhu Yuanzhang", "1980", "American rock band Panic! at the Disco", "within a dorsal root ganglion", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Jonathan Tunick", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "roughly 2,500 quadrillion liters", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt, Robert M. La Follette, Sr., and Charles Evans Hughes", "August 5, 1937", "voters gathered as a tribe the members would be well known enough to each other that an outsider could be spotted,", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings,", "Colleen", "2015, 2016", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson", "playing cards", "Sparta", "Las Vegas", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "death of cardiac arrest", "brush it off.", "Madison", "Babel", "20th century", "Ponce de Len"], "metric_results": {"EM": 0.375, "QA-F1": 0.5400148276149729}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6363636363636364, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.6086956521739131, 0.16666666666666669, 0.0, 0.0, 0.5, 1.0, 0.06666666666666667, 1.0, 0.5, 0.6666666666666666, 0.5714285714285715, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.25, 1.0, 0.5945945945945945, 1.0, 0.0, 0.375, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.375, "CSR": 0.5387228260869565, "EFR": 0.825, "Overall": 0.6714164402173912}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the Saskatchewan River", "Carlisle", "electronic junk mail or junk newsgroup", "Tahrir Square", "David Frost", "Newbury Racecourse", "torture", "clanford", "republic of bando Verde", "Spongebob", "Farthings", "China", "Maine", "Thomas Cranmer", "George H. W. Bush", "Washington, D.C.", "jack Sprat", "Ronnie", "conclave", "Dublin", "The Mayor of Casterbridge", "heel", "amsterdam", "John Lennon", "Lusitania", "anne boleyn", "australia", "antelope", "Portugal", "republic of Botswana", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "modified First World War Vickers Vimy", "Jinnah International Airport", "republic of india", "king anne", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "republic of republic republic", "Charles Taylor", "Pancho Villa", "changing display or audio settings quickly, such as brightness, contrast, or volume", "Jerry Leiber and Mike Stoller", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Brown Mountain Overlook", "South Africa's most famous musicians,", "Middle East and North Africa,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "king mursili", "leukotomy"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6598958333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-2892", "mrqa_naturalquestions-validation-1587", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-594", "mrqa_searchqa-validation-14470", "mrqa_searchqa-validation-16895"], "SR": 0.609375, "CSR": 0.5394825268817205, "EFR": 0.92, "Overall": 0.690568380376344}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "Keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "British former racing driver, commentator, and journalist from Scotland", "Coahuila, Mexico", "Atomic Kitten", "methylenedioxy meth", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "the D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle on Ice", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Mudvayne", "1947", "Easter Rising", "November 23, 2012", "John Monash", "edward the Elder", "Middlesbrough Football Club", "leftwing", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "future AC/DC founders Angus Young and Malcolm Young", "Cher", "125 lb (57 kg)", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gull", "chariot", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "a cat", "Gibraltar", "the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7112847222222223}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_naturalquestions-validation-8652"], "SR": 0.59375, "CSR": 0.5400598404255319, "EFR": 1.0, "Overall": 0.7066838430851063}, {"timecode": 94, "before_eval_results": {"predictions": ["Aston Villa", "Guinea", "Mayflower", "four", "Guardian News & Media", "tartan", "Toy Story", "GM Korea", "lungs", "the Periodic System", "Left Book Club", "portugal", "Columba", "Donald Sutherland", "Boston", "nekemte", "england", "sternum", "Differential pressure", "James Murdoch", "Chicago", "clay", "Ambroz Bajec-Lapajne", "Squeeze", "The Rolling Stones", "Robert Plant", "The Pen", "propeller", "kia", "mouse", "Sir Robert Walpole", "eight", "Andorra", "crab apple", "king of england", "king", "Great Paul", "27", "Formula One", "squash", "Mary Decker", "Godwin Austen", "France", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "Lady Godiva", "king Britannia", "feet", "a palla", "1940s", "7.6 mm", "the amino acid N - terminal to the bond is a tryptophan, tyrosine, phenylalanine, or leucine", "Neymar", "supporters of King Charles II and supporters of the Rump Parliament", "5.3 million", "6-4", "al Qaeda", "UNICEF", "The B", "Lady of the Lamp", "Saturn", "a global village"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5880208333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-2378", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-5055", "mrqa_triviaqa-validation-1469", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2959", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.53125, "CSR": 0.5399671052631578, "EFR": 0.9666666666666667, "Overall": 0.6999986293859648}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "its air-cushioned sole (dubbed \"Bouncing Soles\") upper shape, welted construction and yellow sewing", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Tasmania", "Jim Kelly", "Oderturm", "Lieutenant Martin", "Girls' Generation", "June 12, 2017", "two or three", "davovi\u0107", "prussia", "David Wells", "the north bank of the North Esk", "two", "Argentine cuisine", "between about the 5th century and early 7th century", "Pru Goward", "Premier League club Manchester United", "Matt Groening", "Hazel Keech", "Wake Island", "1993", "Jesus", "Sulla", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "ninth season", "bay Munich", "Deftones", "Gangsta's Paradise", "clitheroe Football Club", "green Lantern", "cleopatra", "The Fault in Our Stars", "Liesl", "how the Grinch Stole Christmas", "sheepskin", "White Horse", "banjo player", "Yellow fever", "Elise Marie Stefanik", "Francis Schaeffer", "extends 2,000 kilometres ( 1,200 mi ) down the Australian northeast coast", "between 3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "david stockwell", "capture of Quebec", "cold comfort farm", "red", "lightning strikes", "final moments of the late Libyan strongman's life.", "Guernsey", "the Southern Christian Leadership Conference", "bermany", "brain and spinal cord"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5965474323470648}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.9230769230769231, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.9523809523809523, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-4302", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.484375, "CSR": 0.5393880208333333, "EFR": 1.0, "Overall": 0.7065494791666665}, {"timecode": 96, "before_eval_results": {"predictions": ["12951 / 52 Mumbai Rajdhani Express", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "the pyloric valve,", "Ben Fransham", "Ephesus", "the Gaither Vocal Band", "Phillip Paley", "Germany", "Einstein's `` light quanta ''", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi ( English : Rebel )", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "the epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "swine", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract )", "1595", "The musical premiered on October 16, 2012, at Ars Nova ; directed by Rachel Chavkin the show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "in over seven years", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75", "Miller Lite", "Oona Castilla Chaplin", "The speech compares the world to a stage and life, sometimes referred to as the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age, facing imminent death", "Lulu", "the NFL", "Spacewar", "The Star - Spangled Banner", "Profit maximization", "Melbourne", "April 1, 2016", "the Alamodome and city of San Antonio", "801,200", "Michael Phelps", "royal oak", "The Krankies", "France", "Province of Syracuse", "1986", "Alan Kardec scored a spectacular second-half winner for Brazil against Costa Rica to set up a replay of the 1993 Under-20 World Cup final against Ghana in Egypt.", "200", "Republican Gov. Bobby Jindal", "reshit", "Deere", "shear", "curfew"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7126565561720227}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.48275862068965514, 0.0, 0.3137254901960785, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.058823529411764705, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-848", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334", "mrqa_searchqa-validation-16855"], "SR": 0.640625, "CSR": 0.5404317010309279, "EFR": 0.9130434782608695, "Overall": 0.6893669108583594}, {"timecode": 97, "before_eval_results": {"predictions": ["Rachmaninoff", "dark places", "a candy bar", "a boll weevil", "touchpad", "Wikipedia", "Sundance Kid", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "Sergey Brin", "Joe Lieberman", "The Smashing Pumpkins", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "you created the fresco walls in the Stanza della Segnatura", "an ant", "birkenstock", "Firebird", "Hafnium", "flax", "the Muse", "The Wachowski brothers", "Rumpole of the Bailey", "Sam Tilden", "Steve Austin", "Kurt Warner", "40", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "a brown bear", "The Office", "Allison", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Vietnamese", "Crayola", "the Man in the Gray Flannel Suit", "Assimilation", "orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "mike mckinane", "The Crow", "Hartley", "Tifinagh", "European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "may indeed help people with irritable bowel syndrome,", "Prada"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6735733695652175}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.608695652173913, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.609375, "CSR": 0.5411352040816326, "EFR": 1.0, "Overall": 0.7068989158163265}, {"timecode": 98, "before_eval_results": {"predictions": ["Filippo Brunelleschi", "Pierre Trudeau", "Red", "a chargeback", "The Little Puffer", "the cornea", "Crystal Light", "Rumpole", "the guillotine", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "James A. Van Allen", "beer", "Zen", "El", "Zenith", "baboon", "wine", "The Sopranos", "the q- tip", "natural selection", "Massachusetts", "Battle of the Bulge", "a crossword clue", "W. Somerset Maugham", "the Two Sicilies", "Trafalgar", "a republic", "Sir Francis Drake", "the college of Charles", "Albert Einstein", "Nokia", "the pituitary", "Alfred Hitchcock", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Joseph Haydn", "Meringue", "Babe Zaharias", "Thought Police", "calcium", "four", "geologist Charles Lyell", "961", "William", "a curse", "Mary Seacole", "Orchard Central", "Fort Hood", "Andr\u00e9 3000", "the iPods", "suspend all aid operations in Somalia for the time being.", "Wednesday", "Nick Sager"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6483901515151516}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.4, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-6821", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-2040"], "SR": 0.546875, "CSR": 0.5411931818181819, "EFR": 0.9655172413793104, "Overall": 0.7000139596394984}, {"timecode": 99, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.798828125, "KG": 0.4734375, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2018", "neuropsychology", "Hem Chandra Bose", "potential of hydrogen", "Peking", "Bart Howard", "2013 ( XLVIII )", "Ozzie Smith", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "Persian", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "March 14, 1988", "Natural - language processing", "six", "HTTP / 1.1", "$2 million in 2011", "three high fantasy adventure films", "Sohrai", "Jos Plateau", "celebrity alumna Cecil Lockhart", "James Long", "257,083", "April 13, 2018", "quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "the episode `` Killer Within ''", "Disha Vakani", "Nickelback", "1999 to 2001", "King Willem - Alexander", "is a song recorded, written, and produced by American musician Lenny Kravitz for his second studio album, Mama Said ( 1991 )", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1996", "Manley", "chinaplin", "Francis Matthews", "god of Weddings", "August 17, 1907", "1776", "Stapleton Cotton", "transit bombings", "eight-day", "101", "Spain", "the Korean War", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.578125, "QA-F1": 0.68769954004329}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5025", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894"], "SR": 0.578125, "CSR": 0.5415625, "EFR": 0.9259259259259259, "Overall": 0.6870133101851852}]}