{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4390, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["polynomial algebra", "Sydney", "Von Miller", "Kalenjin", "thymus and bone marrow", "Black's Law Dictionary", "building construction, heavy and civil engineering construction, and specialty trade contractors", "3.5 billion", "John Elway", "1798", "Albert Einstein", "monophyletic", "Xingu", "the New York Times", "white flight", "males", "uncivilized", "Greenland", "the equality of forces between two objects", "to encourage investment", "Grissom, White, and Chaffee", "N\u2013S", "Tolui", "18 million volumes", "since the Sui and Tang dynasties", "Ps. 31:5", "a new magma", "in an adult plant's apical meristems", "the Lisbon Treaty", "cysteine and methionine", "12", "17", "Honorary freemen", "Seine", "1072", "Academy of the Pavilion of the Star of Literature", "1969", "cilia", "main porch", "Denver Broncos", "electric eels", "San Jose Marriott", "Half of Paris's population of 100,000 people died. In Italy, the population of Florence was reduced from 110\u2013120 thousand inhabitants in 1338 down to 50 thousand in 1351", "Richard Leakey", "\u00dcberseering BV v Nordic Construction GmbH", "regeneration", "Wisconsin v. Yoder", "MODES", "1943", "Thomson", "1950s", "2010", "two", "Genghis Khan", "Serbian", "Warsaw", "Skylab", "The United Methodist Church", "Jerome Schurf", "a computer network funded by the U.S. National Science Foundation (NSF)", "14th to 17th centuries", "through homologous recombination", "University of Washington", "Falls"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9189338235294118}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4117647058823529, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8544", "mrqa_squad-validation-10333", "mrqa_squad-validation-6099", "mrqa_squad-validation-5065", "mrqa_squad-validation-7666", "mrqa_squad-validation-7049"], "SR": 0.90625, "CSR": 0.90625, "EFR": 1.0, "Overall": 0.953125}, {"timecode": 1, "before_eval_results": {"predictions": ["the poor", "higher", "9", "Canterbury", "Mediterranean", "Brown v. Board of Education of Topeka", "A turlough", "three other cricketers have taken more than one Test hat - trick", "Gladys Knight & the Pips", "In Time", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "The main television coverage of Celebrity Big Brother was screened on CBS during the winter of the 2017 -- 18 network television season", "Dawn ( Lizzy Greene ) is the oldest of the quadruplets", "W. Edwards Deming", "Langdon was more frequently seen on the small screen in guest spot roles such as Kitty Marsh during the NBC portion ( 1959 -- 1961 ) of Bachelor Father", "iron", "first message was sent over the ARPANET in 1969", "The Methodist revival began with a group of men, including John Wesley ( 1703 -- 1791 ) and his younger brother Charles ( 1707 -- 1788 )", "the show has run on BBC One since 15 May 2004, primarily on Saturday evenings with a following Sunday night results show", "Henry Gibson as Wilbur, a pig who was almost killed due to being a runt", "the provisional Indian Olympic Committee formally became the Indian Olympic Association ( IOA )", "iron ore production in Western Australia, and Australia as a whole, was negligible, in the range of less than 10 million tons a year", "multinational retail corporation", "Pakhangba", "the ancestral virus, of avian origin, crossed the species boundaries and infected humans as human H1N1", "A mama don't like you, and she likes everyone", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "It now plays a central role in the management of balance of payments difficulties and international financial crises", "by December 1349 conditions were returning to relative normalcy", "Sakshi Malik", "2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "the Baltimore teenagers Ivan Ashford, Markel Steele, Cameron Brown, Tariq Al - Sabir and Avery Bargasse", "Jacob Packer is the Ripper", "Jerry Leiber and Mike Stoller", "TLC - All That", "muscle contraction", "It is the heaviest fully enclosed armoured fighting vehicle ever built", "1700 Cascadia earthquake", "April and Andy ask Leslie and Ben for advice regarding the prospect of having children", "Wednesday, May 24, 2017", "With an initial worldwide gross of over $1.84 billion, Titanic was the first film to reach the billion - dollar mark", "the algebraic rules for the elementary operations of arithmetic with such numbers", "Gustav Bauer, the head of the new government, sent a telegram stating his intention to sign the treaty if certain articles were withdrawn", "No. 23 retired by Chicago Bulls", "May 3, 2005", "Bieber's R&B vocals over a backdrop containing a dance infused beat", "Tami Lynn", "90 \u00b0 N 0 \u00b0 W", "A substitute good is one good that can be used instead of another", "1985", "the chant was first adopted by the university's science club in 1886", "It intertwines with the rete ridges of the epidermis and is composed of fine and loosely arranged collagen fibers", "J. S. Bach", "Christmas", "Westminster Abbey", "Liverpool", "Gracie Mansion", "2004 Paris Motor Show", "order and its tactics", "a rally", "dual nationality", "Burt's Bees is an \"Earth Friendly, Natural Personal Care Company.", "the Indiana Republican Party", "Beautiful Swimmers: Watermen, Crabs"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4559287094484462}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1, 0.4444444444444445, 1.0, 0.07999999999999999, 0.0, 0.2222222222222222, 0.12121212121212122, 0.18181818181818182, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 0.10526315789473684, 0.3636363636363636, 1.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.10526315789473684, 0.0, 0.1904761904761905, 0.0, 1.0, 0.0, 1.0, 0.6, 0.2857142857142857, 1.0, 0.5, 0.09523809523809523, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.4, 1.0, 0.3636363636363636, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-3971", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6130", "mrqa_hotpotqa-validation-2137", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-4059", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-748"], "SR": 0.34375, "CSR": 0.625, "EFR": 0.9285714285714286, "Overall": 0.7767857142857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Oxygen therapy", "Lou Diamond Phillips", "Fran", "North Holland", "International Hotel", "21 July 2015", "Porto of Portugal", "University of Texas at Austin", "Pharrell Williams", "the 1990 Census", "G\u00e9rard Depardieu", "jet-powered tailless delta wing high-altitude strategic bomber", "Francophone and French", "Polish", "Jim Davis", "Lockhart", "1946", "34.9 kilometres", "South African", "Christopher Tin", "Los Angeles Dance Theater", "Anglo-Frisian", "South Africa", "clapper (\"izikeyi\")", "Macau", "Premier League", "Rule of three", "\"media for the 65.8 million,\"", "2009", "Bhaktivedanta Manor", "Vyd\u016bnas", "47,818", "\"The Catcher in the Rye\"", "Fyvie Castle", "Gerard Marenghi", "Dyn", "Dennis Kux", "The Spiderwick Chronicles", "Lerotholi Polytechnic FC", "La Familia Michoacana", "Nikolai Alexandrovich Morozov", "Rural Electrification Act", "Golden Gate National Recreation Area", "July 22, 1946", "King \u00c6thelred the Unready", "John of Gaunt", "Treaty of Trianon", "Sturt", "Kind Hearts and Coronets", "Conservative Party", "2015 Monaco GP2 Series round", "Ravi Shastri", "four volumes", "Highlands County, Florida, United States", "the tallest building in the world for 41 years", "whitsun", "Downton Abbey", "MS Columbus", "\"The skull was hit and shave off... by a scraper", "Swedish Prime Minister Fredrik Reinfeldt", "The Lord is my shepherd", "The total binding energy released in fission of an atomic nucleus", "whitsun", "\"Merchant's Harbor\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5424727182539683}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-2307", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1462", "mrqa_naturalquestions-validation-9144", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-523", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-11990"], "SR": 0.46875, "CSR": 0.5729166666666667, "EFR": 1.0, "Overall": 0.7864583333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["Frankfort", "Ada Monroe", "Priscilla Beaulieus", "B.B. King", "Australia", "Blair House", "\"Don't trespass on my land!\"", "King Menkaure (Mycerinus)", "the case of Von Bischoff", "Anja Prson", "Thomas Lanier", "Albert Einstein", "polar bear", "if you come to Kenya on an organized tour, you're likely to have your time", "a marooned Scottish soldier", "is a young cotton picker", "The British Earl of Yankerville", "Sweden", "Poseidon", "played", "Geraldine A. Ferraro", "The pharynx or throat", "the Chesapeake", "games named for the skull", "a little tiger", "is a serious threat to Mexico", "hiss had passed copies of stolen State Department documents to him", "New Zealand's Minister of Economic Development Jim", "red Madagascar mineral", "wilinda Doolittle", "Ecuador", "\"The Gopher State\"", "a senior naval officer", "Baha Bariay", "arroyo", "Anyone who hate children and dogs", "pnctu", "Fleetwood Mac", "white blood cells", "Meryl Streep", "the Chesapeake Bay to Maine", "is the bulging (or prolapse) of one or both of the mitral", "Marlon Brando", "Ishmael Beah", "Wilbur Wright", "the Pelican", "wille des poupes", "Stephen Townly Crane", "the duck (bill and webbed feet), beaver (tail), and otter (body and fur)", "Elaine Risley", "wilayeth Paltrow", "Dougie MacLean", "claims adjusters", "Geophysicists", "Columbus", "Omid Djalili", "red", "concentration camp", "79 AD", "1998", "183", "\"German roots\"", "the Kurdish militant group in Turkey", "\"Makua Hanai\""], "metric_results": {"EM": 0.265625, "QA-F1": 0.29407051282051283}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-7799", "mrqa_searchqa-validation-9678", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-13362", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-3001", "mrqa_searchqa-validation-5662", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-16871", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11908", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-12812", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-9842", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-2811", "mrqa_searchqa-validation-3798", "mrqa_searchqa-validation-12303", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-6857", "mrqa_triviaqa-validation-96", "mrqa_hotpotqa-validation-3406", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1508", "mrqa_hotpotqa-validation-5606"], "SR": 0.265625, "CSR": 0.49609375, "EFR": 1.0, "Overall": 0.748046875}, {"timecode": 4, "before_eval_results": {"predictions": ["Woodstock", "the World Health Organization", "1789", "Sugarloaf Mountain", "a brain injury", "\"Big Dipper\"", "a Blue Point", "Ken Burns", "two", "Lady Gaga", "George I", "Milwaukee", "(Orycteropus afer)", "Copenhagen", "George Washington", "George IV", "Aidan Crawley", "bitter liqueurs", "acute", "darth vader", "Tears for Fears", "Polish", "bauxite", "\"Bach\"", "the Compact Pussycat", "1 May", "1984", "gin", "Pico d'Aneto", "Otto von Bismarck", "Mathematics", "Edwina Currie", "Wellington", "Pisces", "Montezuma", "A - Alpha", "French Guiana", "Prime Minister of the Commonwealth of Australia", "(Rubiaceae)", "East Berlin and West Berlin", "Burkina Faso", "Kiki", "Daedalus", "Illinois", "Rosa Parks", "San Francisco", "Aeschylus", "\"round-eyed\"", "Hair", "Harry Potter", "a company that provides Internet services", "China", "qualitative data, quantitative data", "Franklin Roosevelt", "Aksel Sandemose", "1992", "the ExCeL Exhibition Centre", "the Dalai Lama", "Iowa", "(l-r)", "The republic", "Megan Fox", "Leontyne Price", "bass"], "metric_results": {"EM": 0.578125, "QA-F1": 0.631671626984127}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-4899", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-415", "mrqa_triviaqa-validation-4566", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-2055", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-5215", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2133"], "SR": 0.578125, "CSR": 0.5125, "EFR": 0.9259259259259259, "Overall": 0.7192129629629629}, {"timecode": 5, "before_eval_results": {"predictions": ["revenge and karma", "functions", "starch", "Cyanea capillata", "Roger Dean Stadium", "1997", "to help bring creative projects to life", "Billy Idol", "Cairo, Illinois", "Idaho", "October 12, 1979", "Speaker of the House of Representatives", "1600 BC", "what is to be done", "the person compelled to pay for reformist programs", "April 6, 1917", "Andy Serkis", "Narendra Modi", "hem Chandra Bose, Azizul Haque and Sir Edward Henry", "manta rays", "Abid Ali Neemuchwala", "Steve Hale", "Manhattan Island", "2017", "nucleotides", "Ethiopia and Liberia", "is the ultimate exercise for the bored and lazy", "2018", "Vice President", "Killer Within", "branch roots", "Javier Fern\u00e1ndez", "1959", "down to the ground", "2013", "appearances", "a man with a face described as looking like the devil - two protrusions emanating from his forehead ( like horns )", "Cedric Alexander", "1998", "September 15, 2012", "gareth Barry", "the Pir Panjal Range", "Lieutenant Templeton `` Faceman '' Peck", "an unknown recipient", "14", "Joseph Heller", "1980", "in the 1820s", "Yuzuru Hanyu", "the septum", "1799", "Iwo Jima", "gums", "east", "the Harpe brothers", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "the League of the Three Emperors", "4 meters (13 feet) high", "Europe's second-tier club competition", "Pakistan", "tango", "j Jacob Marley", "\"In Cold Blood\"", "tennis"], "metric_results": {"EM": 0.5, "QA-F1": 0.6039421695402298}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0689655172413793, 0.5, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-7047", "mrqa_triviaqa-validation-4912", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1173", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-12461"], "SR": 0.5, "CSR": 0.5104166666666667, "EFR": 0.9375, "Overall": 0.7239583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["Arundel", "rounders", "Spain", "gooseberry", "American racing", "Vietnam", "Australia", "The Free Dictionary", "1982", "a new multi-million pound, state-of-the-art brewery in Southwold", "three", "capicum", "Edward Lear", "the failure of the duke of Monmouth\u2019s rebellion", "Chief Inspector of Prisons", "a pervasive pattern of excessive emotionality and attention seeking,", "Rubenshuis Museum", "Gerber Technology", "tomatoes", "Buckinghamshire", "Sneezy", "aurochs", "a young woman once dubbed 'the richest little girl in the world'", "Wannabe", "Robert F. Kennedy", "American West", "The Harvest of a Quiet Eye", "Armley", "guitar", "wool", "Stockton-on-Trent", "equinoxes", "a reddish-purple berry", "Michael Faraday", "SuperiorPics.com", "a grayhound, gazelle hound, often with the aid of falcons", "American", "Blue Ivy", "John Nash", "Miranda v. Arizona", "Judas Iscariot", "The Big Bopper", "a restaurant in New York\u2019s Greenwich Village", "South Korea", "tamale", "The romantic story of the Highland garb", "1994", "Passepartout", "Strabo", "Newcastle Falcons", "The Fortune cookie", "74", "in the northwest, it connects with the Arafura Sea through the Torres Strait", "Vienna", "American actress and comedian", "December 19, 1967", "Marco Fu", "$250,000", "Matthew Perry and Leslie Mann", "Suba Kampong township on the Philippine island of Basilan", "a desktop microcomputer", "the mineral malachite", "an earthquake", "12\u201318"], "metric_results": {"EM": 0.34375, "QA-F1": 0.42309027777777775}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-7569", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-3853", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-28", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4367", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-4384", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-3406", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-3894", "mrqa_hotpotqa-validation-244"], "SR": 0.34375, "CSR": 0.4866071428571429, "EFR": 0.9523809523809523, "Overall": 0.7194940476190477}, {"timecode": 7, "before_eval_results": {"predictions": ["sour", "Edward leeds", "birmingham", "Churchill Downs", "Fotheringhay Castle in England", "a birmingham village", "Charlie Chaplin", "germania", "bactrian", "Narendra Modi", "FBI", "\"Z\"", "Maine", "ascorbic acid", "Vincent Eugene Craddock", "Dilbert", "Lorenzo da Ponte", "jazz", "Celtic", "Taggart", "Mexico", "Zulfikar Ali Bhutto", "China", "Brian Close", "jumanji", "Rocky Horror", "wainwright", "Holy Roman Empire", "maverick", "mazovia", "Venice", "a tomboy", "Damian Green", "Malawi", "Fermanagh", "a menorah", "dacula, GA", "Fancy Dress Shop", "berlin", "carbon", "Blackburn", "Antonia Fraser", "1937", "Superman: The Movie (1978)", "Moon River", "lead singer and performer", "homeopathic physician", "Rabin", "Princeton-Plainsboro Teaching Hospital", "red dwarf", "orange juice", "Border Collie", "December 1, 2009", "60", "raven jean", "2006", "Ang Lee", "fighting charges of Nazi war crimes", "Michael Jackson", "Nineteen", "a pearl of irregular shape", "George Reeves", "chvez", "tempo"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5104166666666667}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-2006", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-4954", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-4132", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-2052", "mrqa_searchqa-validation-4187", "mrqa_searchqa-validation-11191"], "SR": 0.453125, "CSR": 0.482421875, "EFR": 1.0, "Overall": 0.7412109375}, {"timecode": 8, "before_eval_results": {"predictions": ["US general Douglas MacArthur", "w. c. handy", "Don Garlits", "elton John", "job", "hansen", "seabiscuit", "Joan", "jet engine", "Weimar", "horse", "Q", "godot", "belleepson", "Sikhism", "eyebrows", "handel", "Mendenhall Glacier", "ice cream", "badgers", "Army of the Potomac", "salmon", "40 Year Old Virgin", "phlegethon", "Alexander Graham Bell", "30", "Suicide Squad", "Sir Walter Raleigh", "crimson tide", "denali", "Mars", "smith", "shot glass", "stars", "rail", "Nero Wolfe", "almonds", "Riding in Cars with Boys", "Rachel Carson", "stroke", "the American Kennel Club", "belle", "Yuri Gagarin", "Mary (Wollstonecraft) Shelley", "fiber", "Bastille", "Sam Houston", "a circle", "Dick Tracy", "drone", "Ethiopia", "Rukmini", "Bonhomme Carnaval", "April 29, 2009", "birdman", "benyan", "moz conan eriksson", "crimson the Tomboy", "1935", "German", "Tennessee", "fake his own death", "English", "\"Watchmen\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.5869791666666666}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-5680", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-3365", "mrqa_searchqa-validation-4902", "mrqa_searchqa-validation-4714", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-7439", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-8794", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-12805", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-5611", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-6521", "mrqa_hotpotqa-validation-1903", "mrqa_newsqa-validation-2689"], "SR": 0.53125, "CSR": 0.4878472222222222, "EFR": 1.0, "Overall": 0.7439236111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["\"These guys need to take a look around and see that we're facing 10 percent unemployment and an economy on the brink of collapse,\"", "37", "Argentina lays claim not just to the islands, but to any resources that could be found there.", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "peanuts", "President Obama", "10 Awa", "Alexey Pajitnov", "$15 billion", "2001", "we need to get businesses hiring again.", "Cairo", "Casalesi Camorra clan", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi, according to Karachi Police Chief Waseem Ahmad.", "Kuwait", "more than 30", "Kim Jong Un", "the U.S. Holocaust Memorial Museum", "Brian David Mitchell", "three", "the strength of its brand name and the diversity of its product portfolio", "80", "Dennis Davern", "123 pounds of cocaine and 4.5 pounds of heroin", "club managers", "15,000", "a picture of the girl", "Turkish President Abdullah Gul", "a radical Muslim sheikh called Friday for the creation of an Islamic emirate in Gaza", "propofol", "Hillary Clinton", "a monthly allowance", "40", "Aryan Airlines Flight 1625", "Swansea, England", "Spaniard", "Fiorentina", "Tulsa, Oklahoma", "Sporting Lisbon", "Muslim", "a Lion Among Men", "a cyber attack could destroy electrical equipment.", "Yemen", "one", "July 18, 1994", "Eric Besson", "Susan Atkins", "Mad Men", "July for A Country Christmas", "Arthur E. Morgan III", "Draquila", "Dan Stevens", "grace Zabriskie", "in a counter clockwise direction", "a person born within hearing distance of the sound of Bow bells", "Bob Morris", "Gardiner", "Five Summer Stories", "9 February 1971", "Fatih Ozmen", "Kenny G", "Cuba Gooding Jr.", "jade", "Wanda"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5932990385816707}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.20689655172413793, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6956521739130436, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-872", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4647", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-5354", "mrqa_searchqa-validation-6241"], "SR": 0.515625, "CSR": 0.490625, "EFR": 1.0, "Overall": 0.7453125}, {"timecode": 10, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-103", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1170", "mrqa_hotpotqa-validation-1354", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1462", "mrqa_hotpotqa-validation-1623", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1970", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2292", "mrqa_hotpotqa-validation-2307", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3083", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3406", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-949", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-171", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1954", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2064", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3057", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-862", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11331", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-11908", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13362", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14219", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-16135", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-16871", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2590", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2811", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-3001", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3428", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4714", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-501", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-5435", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5680", "mrqa_searchqa-validation-6059", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6603", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7439", "mrqa_searchqa-validation-7445", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7799", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8853", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-9842", "mrqa_squad-validation-10333", "mrqa_squad-validation-10378", "mrqa_squad-validation-1115", "mrqa_squad-validation-1155", "mrqa_squad-validation-1158", "mrqa_squad-validation-1975", "mrqa_squad-validation-2240", "mrqa_squad-validation-2550", "mrqa_squad-validation-2655", "mrqa_squad-validation-2832", "mrqa_squad-validation-317", "mrqa_squad-validation-3705", "mrqa_squad-validation-384", "mrqa_squad-validation-3913", "mrqa_squad-validation-3986", "mrqa_squad-validation-4140", "mrqa_squad-validation-4183", "mrqa_squad-validation-4239", "mrqa_squad-validation-4436", "mrqa_squad-validation-4484", "mrqa_squad-validation-4711", "mrqa_squad-validation-4953", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5095", "mrqa_squad-validation-5174", "mrqa_squad-validation-5311", "mrqa_squad-validation-5315", "mrqa_squad-validation-5503", "mrqa_squad-validation-6099", "mrqa_squad-validation-6763", "mrqa_squad-validation-679", "mrqa_squad-validation-6792", "mrqa_squad-validation-7021", "mrqa_squad-validation-7049", "mrqa_squad-validation-7060", "mrqa_squad-validation-7080", "mrqa_squad-validation-7257", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-7455", "mrqa_squad-validation-7552", "mrqa_squad-validation-7666", "mrqa_squad-validation-7903", "mrqa_squad-validation-7917", "mrqa_squad-validation-8064", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-8313", "mrqa_squad-validation-8544", "mrqa_squad-validation-8846", "mrqa_squad-validation-8954", "mrqa_squad-validation-9149", "mrqa_squad-validation-9782", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-1323", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-1724", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2055", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2893", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-3523", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-415", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-4487", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-4899", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-5224", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-5303", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6815", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6852", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7084", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-905", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-995"], "OKR": 0.87890625, "KG": 0.459375, "before_eval_results": {"predictions": ["24", "$60 billion on America's infrastructure.", "Ike", "top designers, such as Stella McCartney,", "Newark's Liberty International Airport,", "Dick Cheney", "seven", "At least 40", "pilot", "Workers' Party", "Zuma", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "Hamburg.", "drama that pulls in the crowds", "Russia", "Johnny Depp", "two-day, two-city charm offensive", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Sonia Sotomayor", "intricate Flemish tapestries", "\"We've seen Washington launch policy after policy, yet our dependence on foreign oil has only grown, even as the world's resources are disappearing,\"", "House Clerk Lorraine Miller", "opposition party members.", "Joel \"Taz\" Di Gregorio", "Nico Rosberg", "Mandi Hamlin", "boats", "the insurgency", "the United States", "enjoys a cold shower in his home in New Zealand.", "Josef Fritzl,", "a bag", "17", "heart", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "airline", "40", "served in the military", "Obama", "Twilight", "\"To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect.\"", "the fact that the teens were charged as adults.", "Leo Frank", "the Ku Klux Klan", "China", "Sarah", "\"perezagruzka\"", "the legitimacy of that race.", "Brian Thomas, who killed wife Christine while they were on vacation in 2008, be dropped due to a \"unique set of circumstances.\"", "Siri's", "Apple employees", "Elizabeth Dean Lail", "a total bandwidth of 24x64 - kbit / s", "HTTP / 1.1", "Eddie Murphy", "Isles of the Blessed", "Belgium", "Bourbon", "Pinellas", "Winnie the Pooh", "September 8, 2008", "Torah", "Hartford", "tissues in the vicinity of the nose"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4591543872793873}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.15384615384615388, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14545454545454542, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-887", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-1702", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-2818", "mrqa_hotpotqa-validation-2600", "mrqa_searchqa-validation-10607", "mrqa_searchqa-validation-4162"], "SR": 0.390625, "CSR": 0.48153409090909094, "EFR": 1.0, "Overall": 0.7096661931818182}, {"timecode": 11, "before_eval_results": {"predictions": ["Florida", "Kim Wilde", "Mafia", "Patrick Henry", "gondola song", "mme la  Princesse", "In God We Trust", "France", "can also tell us how a church distinguishes itself from other similar faiths.", "couscous", "S. molloyi", "Handel", "Stephenie Meyer", "Ralph Vaughan Williams", "Willie Nelson", "Ridley Scott", "The National Council for the Unmarried Mother and her Child", "Tom Hanks", "Sir Walter Scott", "a system of recording important things.", "Maggie Smith", "United Republic of Tanzania", "Blue Ivy", "6", "the solar system.", "Jupiter", "Genesis", "The Quatermass Experiment", "the Aureus", "Giorgio Armani,", "Jack Ruby", "Rugby", "Model A", "Il Divo", "monodon monoceros", "Marsa", "a machine that cuts the bread finely", "Real Madrid CF", "Duke", "\"mild-mannered\"", "Fernando Lamas", "Silverstone", "McDonalds", "l", "Kansas City", "can be a psychologist", "Fr\u00e9d\u00e9ric", "Venus", "Salt Lake City, Utah", "Jeroboam", "Henkel", "Giancarlo Stanton", "2011", "Houston Dynamo", "Rolling Stones", "Walter R\u00f6hrl", "Chelsea", "2011", "World Wide Village", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Paris", "Jane Hamilton", "Star Wars: The clone Wars", "business."], "metric_results": {"EM": 0.5, "QA-F1": 0.5692302489177489}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.12121212121212123, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-5347", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-4205", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-1608", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-2558", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-6916", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-2620", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-3376", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-178", "mrqa_searchqa-validation-9383"], "SR": 0.5, "CSR": 0.48307291666666663, "EFR": 1.0, "Overall": 0.7099739583333333}, {"timecode": 12, "before_eval_results": {"predictions": ["Lady Gaga", "stella", "sugarloaf", "billy crystal", "Anita Roddick", "Gulf of Aden", "tambourine", "The Four Tops", "Till Death Us Do Part", "Mark Rothko", "Humphrey Lyttelton", "Byker grove", "Hampton Court Palace", "stasis", "sound and light", "phy", "robert nix", "phoebastria", "billy crystal", "cotton", "kappa", "beer slogans", "billy crystal", "meadows", "Kenya", "power station", "phoenician", "Vince Cable", "Uncle Tom\u2019s Cabin", "Luxor", "Love Never Dies", "billy crystal", "billy", "blood", "partridge", "Wolfgang Amadeus Mozart", "diffusion", "2", "cutters", "Beaujolais Nouveau", "billy crystal", "George Fox", "denarii", "Sheffield", "billy crystal", "Devonport", "1978", "Brockenhurst", "billy crystal", "Novak Djokovic", "omerta", "Orange Juice", "The Annunciation", "Super Bowl XXXIX", "Oklahoma Sooners", "Capture of the Five Boroughs", "global asset management", "air support", "billy crystal", "\"I have never thought about taking children away from their father, never,\"", "billy crystal", "knock on wood", "Martha Jane Cannary", "Barcelona"], "metric_results": {"EM": 0.46875, "QA-F1": 0.49911858974358975}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.07692307692307691, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6733", "mrqa_triviaqa-validation-4128", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-222", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-4049", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-5773", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-2321", "mrqa_naturalquestions-validation-1946", "mrqa_hotpotqa-validation-1025", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-1899", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-7610"], "SR": 0.46875, "CSR": 0.48197115384615385, "EFR": 0.9411764705882353, "Overall": 0.6979888998868778}, {"timecode": 13, "before_eval_results": {"predictions": ["Duck", "Clare Torry", "the Chicago metropolitan area", "Doug Pruzan", "Nodar Kumaritashvili", "1881", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "an unknown recipient", "ase", "german princes", "trade", "Ben Willis", "pirates", "6 January 793", "Hellenism", "Gorakhpur Junction", "Jurriaen Aernoutsz", "Ptolemy", "transmissions", "in Poems : Series 1", "1931", "Michael Clarke Duncan", "East India Company", "InterContinental Hotels Group", "the Jos Plateau", "2018", "gravity", "IIII", "Charles Perrault", "1926", "at birth", "Branford College", "Tim McGraw", "D\u00e1in", "compound", "Marty Robbins", "two", "Mexico", "season seven", "nominally a civil service post", "ninth", "2", "maquiladora", "1936", "$2 million", "state legislators of Assam", "the Red Devils", "Thomas Jefferson", "regulatory", "Fall 1998", "USS Chesapeake", "ozone", "uv", "McDonnell Douglas", "Hermione Baddeley", "John Andr\u00e9", "Guardians of the Galaxy", "Carl", "$89", "Crista", "to stay safe", "k Kyrgyzstan", "air power", "the insurgency"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5250915750915751}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-2506", "mrqa_triviaqa-validation-1651", "mrqa_hotpotqa-validation-4345", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-826", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-13535"], "SR": 0.40625, "CSR": 0.4765625, "EFR": 0.9736842105263158, "Overall": 0.7034087171052632}, {"timecode": 14, "before_eval_results": {"predictions": ["the efferent nerves that directly innervate muscles", "Marcus Atilius Regulus", "Omar Khayyam", "The period of being a junior doctor starts when they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme,", "the Han", "an iron -- nickel alloy and some other elements", "Giancarlo Stanton", "the federal level in the United States", "late January or early February", "when the forward reaction proceeds at the same rate as the reverse reaction", "Andy Serkis", "an unnamed 8ft actor who stands in for the 6ft 1in Coltrane in some scenes", "Masha Skorobogatov", "the organ transplant of a kidney into a patient with end - stage renal disease", "Human fertilization", "Sachin Tendulkar", "the first four caliphs ( successors )", "a transformiation, change of heart ; especially : a spiritual conversion", "New York City", "above the age of 18", "the Octopus", "anywhere from 8 to 256 GB", "an engine based on one from their own line up or a generic engine provided by V8 Supercars", "New Jersey", "13 February", "J. Presper Eckert", "an Easter egg", "if the concentration of a compound exceeds its solubility", "land, fresh water, air, rare earth metals and heavy metals", "Arunachal Pradesh ( 25.9 percent )", "961", "Antarctica", "2010", "Currington", "J.P. Zenger High", "Warren Zevon", "in the books of Exodus and Deuteronomy", "Union forces", "Renhe Sports Management Ltd", "W. Edwards Deming", "the referee", "Marshall Sahlins", "Meri", "Joel", "thick skin", "The federal government", "Edward Hyde", "Muhammad Yunus", "the American Kennel Club", "a habitat", "from shore to shore", "Evening Prayer", "Richard Wagner", "the rent doesn't include additional costs such as insurance or business rates.", "the nuclear fission of uranium-235", "\"Loiter Squad\"", "the Mayor of the City of New York.", "1918-1919", "three", "Steven Gerrard", "the Rock and Roll Hall of Fame and Museum in Cleveland, Ohio.", "Marie Osmond", "Keats", "al-Nour al-Maqdessi,"], "metric_results": {"EM": 0.328125, "QA-F1": 0.42546121012507565}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [0.5, 0.0, 1.0, 0.08163265306122448, 0.0, 0.6, 1.0, 0.0, 0.5000000000000001, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.8571428571428571, 0.0, 0.4, 0.0, 0.2222222222222222, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-1438", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-7440", "mrqa_triviaqa-validation-2506", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-2915", "mrqa_newsqa-validation-3356", "mrqa_searchqa-validation-10811", "mrqa_newsqa-validation-2732"], "SR": 0.328125, "CSR": 0.4666666666666667, "EFR": 1.0, "Overall": 0.7066927083333334}, {"timecode": 15, "before_eval_results": {"predictions": ["the Roosevelt family", "a sidereal day", "Baltimore Stallions", "the ship's five anchors", "Italy", "Gang of Four", "the Capulets & the Montagues", "Viggo Mortensen", "Mason-Dixon Line Segment", "the Green Party", "Moses", "the Song Dynasty", "a colander", "\"Little Red Riding Hood\"", "\"Sensible and responsible women don't want to vote\"", "iguanas", "Gershwin", "olly Ringwald", "alto", "Nikolai Gogol", "In Country", "spontaneous", "Nashville", "Abraham Lincoln", "A Chorus Line", "Canada", "savanna", "the Republic of Florence", "the Vulgar Tongue", "Ray Kroc", "Fester", "\"Defying Gravity\"", "Virginia", "Dairy Queen", "New Jersey", "West Virginia", "the Corps of Discovery", "John Grunsfeld", "Transformers", "beans", "Norway", "Hawaii", "the pound sterling", "Mario Puzo", "Wilkie Collins", "George C. Marshall", "3", "Nile", "Kermit Roosevelt", "Richie Rich", "states", "necessary", "2013", "1995", "nijinsky", "Thailand", "matricide", "\"Can't Be Tamed\"", "a schoolmaster", "Japan", "Nechirvan Barzani,", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards", "At least 15", "Lorazepam"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5699404761904762}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false], "QA-F1": [0.5, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-9127", "mrqa_searchqa-validation-92", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-8642", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-7306", "mrqa_searchqa-validation-1336", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-15570", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-7631", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-5207", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-7874", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7554", "mrqa_triviaqa-validation-1555", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-4443", "mrqa_newsqa-validation-3006", "mrqa_naturalquestions-validation-4463"], "SR": 0.453125, "CSR": 0.4658203125, "EFR": 1.0, "Overall": 0.7065234375}, {"timecode": 16, "before_eval_results": {"predictions": ["a pattern", "the word at the top of... 11a was interesting", "the Monitor", "Mt. Everest", "the sorcerer's Stone", "The Thin Red Line", "Smith & Wesson", "a fur hat", "Kindergarten Cop", "Easter", "masks", "Jean-Paul", "Matt Dillon", "Anne Frank", "Endymion", "the masses", "a sandstorm", "Ben & Jerry", "1,000,000", "Just the Way You Are", "Abigail Breslin", "Avril Lavigne", "Robin Williams", "Hephaestus", "Arroyo", "vaudeville", "Hallmark Cards", "a crow", "San Francisco", "Bloomingdale's", "a gravitational field so strong that nothing, not even light, can escape.", "Lady Macbeth", "Harrison Ford", "Martinique", "a sergeant", "Frank Lloyd Wright", "Josephine", "Jack Johnson", "cytokinesis", "Beau Bridges", "Hugh Hendon Miles", "G. Obeyesekere", "Crayola", "Neil Simon", "a 919mm Parabellum pistol", "San Diego", "Princess Leia", "the lithosphere", "Gibraltar", "England", "ballpoint", "The 111th edition of the World Series", "100", "Burbank, California", "Rugby School", "an acid phosphate", "pityriasis", "1989 until 1994", "the Golden Gate National Recreation Area", "Nayvadius DeMun Wilburn", "\"CNN Heroes: An All-Star Tribute\"", "Washington State's decommissioned Hanford nuclear site, once the center of the country's Cold War plutonium production.", "(Les) Bleus", "anales de chimie"], "metric_results": {"EM": 0.375, "QA-F1": 0.4984088827838828}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.3076923076923077, 0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.8, 0.6, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6110", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-14068", "mrqa_searchqa-validation-14048", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-13258", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-1464", "mrqa_searchqa-validation-3548", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-8645", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-46", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-7731", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-2160", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-956", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1229", "mrqa_hotpotqa-validation-4813"], "SR": 0.375, "CSR": 0.46047794117647056, "EFR": 1.0, "Overall": 0.7054549632352941}, {"timecode": 17, "before_eval_results": {"predictions": ["James Brown", "October 29, 2015", "Lorenzo Lamas", "230 million kilometres ( 143,000,000 mi )", "the sperm and ova", "Spain", "December 2, 2013, and the third season concluded on October 1, 2017", "Lionel Hardcastle", "at least 28", "Rockwell", "Lady Gaga", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law", "February 27, 2007", "C.S.A.", "Atharvaveda and Taittiriya Samhita", "Lee Freedman", "13 June 1990", "John Young", "Brazil", "January 15, 2007", "the River of Montevideo", "2012", "Australia", "at the Bronze", "December 25", "two teenage novels by Louise Rennison", "Games", "22 November 1970", "the uterus", "The track is built around an organ accompanied by slow tempo drums and vocals", "a contemporary drama in a rural setting", "a subduction zone", "The management team", "between the stomach and the large intestine", "the Director of National Intelligence", "Duisburg", "an explosion", "U.S. Electoral College", "October 1941", "111", "Paracelsus", "Mohammad Reza Pahlavi", "Nebuchadnezzar", "Vincent Price", "Afghanistan", "775", "Carol Worthington", "Scheria", "Matt Monro", "as late as the 1890s,", "Iran", "Hibernian", "T.S. Eliot", "Tartar", "its eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music.", "Tel Aviv University.", "Kentucky, Virginia, and Tennessee", "Monday night", "a missile", "Missouri", "hydrogen", "Mount Kilimanjaro", "the Alpide belt", "February 26, 1948"], "metric_results": {"EM": 0.5, "QA-F1": 0.595390561015561}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.9090909090909091, 0.5, 1.0, 0.0, 0.0, 0.26666666666666666, 1.0, 1.0, 0.7837837837837839, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.20000000000000004, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-9723", "mrqa_triviaqa-validation-4099", "mrqa_hotpotqa-validation-2696", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3300", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-16690", "mrqa_searchqa-validation-8548"], "SR": 0.5, "CSR": 0.46267361111111116, "EFR": 0.96875, "Overall": 0.6996440972222222}, {"timecode": 18, "before_eval_results": {"predictions": ["somatic cell nuclear transfer", "the human hands and face", "Tara", "Ku - Klip", "a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "3 lines of reflection", "Kyla Coleman", "1939", "Millennium Tower", "1876", "summer", "three high fantasy adventure films directed by Peter Jackson", "September 27, 2017", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "the Finch family's African - American housekeeper", "Coldplay", "Warren Hastings", "April 26, 2005", "12 November 2010", "The highest paid player in Major League Baseball ( MLB ) from the 2013 season is New York Yankees'third baseman Alex Rodriguez with an annual salary of $29,000,000", "in the season seven episode `` Fierce ''", "The Spanish brought the European tradition to Mexico, although there were similar traditions in Mesoamerica, such as the Aztecs'honoring the birthday of the god Huitzilopochtli in mid December", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates", "A firm, flexible cup - shaped device worn inside the vagina", "1980s", "cartilage", "Spektor", "East River", "state of Alaska is in the northwest corner of North America, bordered by Canada to the east and across the Bering Strait from Russia to the west", "Yugoslav model of state organization, as well as a `` middle way '' between planned and liberal economy", "The president", "Los Angeles Dodgers", "cells", "July 18, 2013", "1971 -- 1979", "961", "Hyderabad", "early 19th century", "February 16, 2010", "30 October 1918", "2018", "napkin, and flatware ( knives and spoons to the right of the central plate, and forks to the left )", "Leona Stevenson ( Barbara Stanwyck ) is the spoiled, bedridden daughter of wealthy businessman James Cotterell ( Ed Begley )", "After World War II", "If there are no repeated data values", "Alaska, Arizona, California, Colorado, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington and Wyoming", "states", "bicameral Congress", "Lewis Carroll", "2018", "the government - owned Panama Canal Authority", "IBM", "Charlie Chaplin", "Michigan", "Wonder Woman", "Christian Kern ( ) ; born 4 January 1966) is the incumbent Chancellor of Austria and chairman of the Social Democratic Party of Austria (SP\u00d6)", "Schutzstaffel", "Joe Pantoliano", "Opryland", "a German citizen", "James Hargreaves", "(Casey) Stengel", "sake", "B\u00e9la Bart\u00f3k"], "metric_results": {"EM": 0.34375, "QA-F1": 0.45665763282611105}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.1904761904761905, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.16, 0.888888888888889, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.08695652173913042, 1.0, 0.3846153846153846, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3884", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-3320", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-16906", "mrqa_triviaqa-validation-5761"], "SR": 0.34375, "CSR": 0.4564144736842105, "EFR": 0.9285714285714286, "Overall": 0.6903565554511278}, {"timecode": 19, "before_eval_results": {"predictions": ["January 15, 2007", "up to 100,000", "Nancy Jean Cartwright", "60 by West All - Stars", "the Chinese Exclusion Act in 1882", "birth", "July 25, 2017", "786 -- 802", "7 correct numbers", "A simple majority", "United Nations Peacekeeping Operations", "treats a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus, causing changes in gene expression", "the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "by each state's DMV, which is required to drive", "Confederate victory", "Elstree Studios in England", "the inverted - drop - shaped icon that marks locations in Google Maps", "Rock Island, Illinois", "Quantitative psychological research", "March 2016", "Dan Bern and Mike Viola", "pneumonoultramicroscopicsilicovolcanoconiosis", "Alison in The Inbetweeners Movie and Viviane Wembly in Transformers : The Last Knight", "the player to acquire an advantage without deviating from basic strategy", "May 18, 2010", "30.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "1957", "Indian Standard Time", "A diastema", "January 2018", "growing faster than the rate of economic growth", "London", "1917", "Anglican", "Malina Weissman", "23 % of GDP", "the trunk", "Kaley Christine Cuoco", "1986", "Prafulla Chandra Ghosh", "Zhu Yuanzhang", "Soviet Union", "Sedimentary rock", "Woodrow Wilson", "in cell - mediated, cytotoxic innate immunity ), B cells ( for humoral, antibody - driven adaptive immunity )", "Johannes Gutenberg", "Paul Henreid", "Muhammad", "Valmiki", "the chryselephantine statue of Athena Parthenos", "2004", "karkaroff", "Sparks", "George Eliot", "Coronation Street", "Tom Jones", "David Kossoff", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "13", "a home in an upscale San Fernando Valley neighborhood,", "Lumiere", "Cars", "Conjunction Junction", "his deputy Thabo Mbeki"], "metric_results": {"EM": 0.5, "QA-F1": 0.6068271138583639}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.1111111111111111, 0.15384615384615385, 0.19999999999999998, 0.6666666666666666, 0.15384615384615383, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-4930", "mrqa_triviaqa-validation-436", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-361", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-4349", "mrqa_triviaqa-validation-5245"], "SR": 0.5, "CSR": 0.45859375, "EFR": 0.96875, "Overall": 0.698828125}, {"timecode": 20, "UKR": 0.69140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2292", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-617", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-949", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-22", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2703", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-4966", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5026", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6292", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-846", "mrqa_naturalquestions-validation-858", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9297", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-171", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4140", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-887", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-10811", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-11752", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-12812", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13101", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2071", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3699", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-4187", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-46", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-493", "mrqa_searchqa-validation-5197", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7198", "mrqa_searchqa-validation-7306", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-7631", "mrqa_searchqa-validation-8015", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-8642", "mrqa_searchqa-validation-8696", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-9842", "mrqa_searchqa-validation-996", "mrqa_squad-validation-2240", "mrqa_squad-validation-2550", "mrqa_squad-validation-2832", "mrqa_squad-validation-3", "mrqa_squad-validation-30", "mrqa_squad-validation-317", "mrqa_squad-validation-3781", "mrqa_squad-validation-3913", "mrqa_squad-validation-4059", "mrqa_squad-validation-4484", "mrqa_squad-validation-4752", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5174", "mrqa_squad-validation-5315", "mrqa_squad-validation-6792", "mrqa_squad-validation-7006", "mrqa_squad-validation-7257", "mrqa_squad-validation-7333", "mrqa_squad-validation-7903", "mrqa_squad-validation-7917", "mrqa_squad-validation-8157", "mrqa_squad-validation-8187", "mrqa_squad-validation-8224", "mrqa_squad-validation-8733", "mrqa_squad-validation-8846", "mrqa_squad-validation-9149", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1570", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1640", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2164", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-3884", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4739", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-5242", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-5303", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6386", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6390", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6902", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-953"], "OKR": 0.845703125, "KG": 0.47109375, "before_eval_results": {"predictions": ["Abishag", "Queen", "Shoeless Joe Jackson", "Goodbye My lover", "Lady Chatterley's", "mako shark", "and", "queen Anne", "a stained-glass lamp", "Dwight D. Eisenhower", "salt", "and", "Harvard", "Stephen Hawking", "The Grand Ole Opry", "The Walt Disney Company", "electors", "Poseidon", "a blockage or hemorrhage", "to be admired by the woman he loved", "The Snickers candy bar", "a fish", "1472", "a twelve-year-old rape victim", "and", "New Years Day", "and", "\"Walker, Texas Ranger\"", "manatee", "and storyboarding of the shower scene.", "Daisy", "Peter the Great: His Life and World,", "A Midsummer Night's Dream Police", "Yalta Conference", "and", "four", "and", "Christopher Wren", "insulin", "androids", "George Mason", "a miracle chemical of some sort", "(xvi)", "a lady-led indie rock band from the East Coast of Canada", "and", "pragmatism", "a fertilized egg", "Hugh Laurie", "and Polydeuces,", "and", "and Caesarion (Little Caesar) was the only known son of Julius Caesar.", "In Time", "Jackie Robinson", "traditionally Bulgarian and Romanian sweet leavened bread, which is a type of Stollen", "soybean", "Morgan Spurlock", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions,", "1986", "\"50 best cities to live in.\"", "Melbourne Storm", "\"Three Little Beers,\"", "Gloria Macapagal Arroyo", "her decades-long portrayal of Alice Horton", "Sammi Smith"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5234880570818071}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.25, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.923076923076923, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12958", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-7413", "mrqa_searchqa-validation-3576", "mrqa_searchqa-validation-9046", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-9587", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-16741", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-12403", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-10522", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-7001", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-10762", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-7707", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2112", "mrqa_naturalquestions-validation-7651"], "SR": 0.421875, "CSR": 0.45684523809523814, "EFR": 1.0, "Overall": 0.6930096726190477}, {"timecode": 21, "before_eval_results": {"predictions": ["(James) Jeffords", "Germany", "Mississippi River", "A Clockwork Orange", "jockey", "Vienna Waltz", "Giuseppe Garibaldi", "South Korea", "Amazon River", "a pearl", "(John) Donne", "sperm whale", "Synchronicity", "Emma Stone", "Two Gentlemen of Verona", "Fairbanks", "Adam", "Nancy Sinatra", "Clinton", "Tupelo", "the French Legion of Honour", "a tumbler", "a bee", "Arethusa", "Planet of the Apes", "pajama", "Kermit", "England", "polio", "Sweden", "Tarzan", "Madagascar", "a tunacanned light tunais", "hock", "the sun", "Anna", "(Henry) Ford", "a magnetic compass", "Slovakia", "peanuts", "Orton", "a sharpener", "San Soucie", "Vega$", "Midas", "at the top", "Lake Coeur d'Alene", "the Council of Better Business Bureaus", "Opossum", "a short circuit", "totalitarianism", "Brobee", "1988", "tomato sauce", "Sam Cooke", "a \"major science finding from the agency's ongoing exploration of Mars.\"", "Ankh-Morpork", "main east-west road connecting the inner northern suburbs of Adelaide", "Marilyn Martin", "The Process", "Picasso's muse and mistress, Marie-Therese Walter", "Kevin Kuranyi", "Hapag-Lloyd Cruises", "Lukas Verzbicas"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5228174603174602}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-5321", "mrqa_searchqa-validation-4754", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-5552", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-14957", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-4549", "mrqa_searchqa-validation-16591", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-3440", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-2945", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-6048", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4021", "mrqa_newsqa-validation-637", "mrqa_hotpotqa-validation-2637"], "SR": 0.453125, "CSR": 0.45667613636363635, "EFR": 1.0, "Overall": 0.6929758522727273}, {"timecode": 22, "before_eval_results": {"predictions": ["Marie Van Brittan Brown", "the King James Bible", "1872", "George Halas", "the focal point", "1997", "Identification of the problems and / or goals", "`` Heroes and Villains ''", "1962", "Kaley Christine Cuoco", "a divergent tectonic plate boundary", "Spanish missionaries", "pit road", "Toto", "The Lightning thief", "1977", "the coffee shop Monk's", "Johnny Logan", "Carlos Alan Autry Jr.", "Missouri", "the length of their main span", "Christianity", "Billy Idol", "a thousand years", "her neighbour", "Veronica", "247.3 million", "the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "the raconteur ( Austin Winkler ) and his former lover ( Emmanuelle Chriqui )", "a video game series", "Lana Del Rey", "meaning", "1992", "Lauren Tom", "the mid-1980s", "1871", "Paradise", "Partial", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen )", "the ACU", "Charles Sherrington", "October 30, 2017", "15.1 percent", "Gunpei Yokoi", "the New York Yankees", "the inner core", "medieval", "the 4th century", "Lake Wales", "before the first year begins", "Thorleif Haug", "( Jessica) Smith", "a liqueur", "ghee", "Polish-Jewish", "Cleopatra", "EQT Plaza in Pittsburgh, Pennsylvania", "269,000", "January 24, 2006", "March 4,", "pew", "the Bean Sidhe", "our Country", "ice cream"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5883201433982683}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.375, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.07142857142857142, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.5, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-3482", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7477", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-2509", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4210", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-9572"], "SR": 0.4375, "CSR": 0.4558423913043478, "EFR": 0.9166666666666666, "Overall": 0.6761424365942029}, {"timecode": 23, "before_eval_results": {"predictions": ["Boyd Gaming", "local South Australian and Australian produced content,", "1919", "Ted Nugent", "1998", "Everbank Field", "Alexandre Dimitri Song Billong", "Fiat Chrysler Automobile N.V.", "Carlos Santana", "Jack Posobiec", "7 October 1978", "a basilica", "black", "1988", "2017", "Enkare Nairobi", "the Durban International Convention Centre (ICC Arena)", "between 11 or 13 and 18", "\"The Longest Yard\"", "Elijah Wood", "astronomer and composer of German and Czech-Jewish origin, and brother of fellow astronomer Caroline Herschel, with whom he worked", "C. J. Cherryh", "Dame Harriet Mary Walter", "\"Traceless\" or \"Without a Trace\"", "Ais", "Ben Ainslie", "Dutch", "Prince Sung-won", "the Bank of China Tower", "the 28th season", "Barcelona", "Albert Park", "sim", "Pablo Escobar", "1941", "Hazel Keech", "DJ Scotch Egg", "June 24, 1935", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Chad", "Derry City", "Charles de Gaulle Airport", "the Anglo-Saxon Chronicle", "Tuesday", "the Guilden Morden boar", "Bury St Edmunds, Suffolk, England", "The Lancia Rally", "Michael Jordan", "Harold Lipshitz", "nine", "the Morgaine Stories", "on the southeastern coast of the Commonwealth of Virginia in the United States", "the defendant's negligence was gross, that is, it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "IIII ) and 9 ( VIIII )", "Downton Abbey", "the abbey", "Eminem", "28", "Saturday", "Rod Blagojevich", "Groucho", "centiliter", "the V8", "the bodyguard-turned-informant delivered three machine guns and two silencers"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5478350987133983}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.7499999999999999, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.9387755102040817, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-1226", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2955", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1195", "mrqa_triviaqa-validation-5274", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-5409", "mrqa_newsqa-validation-1242"], "SR": 0.453125, "CSR": 0.45572916666666663, "EFR": 0.9714285714285714, "Overall": 0.6870721726190476}, {"timecode": 24, "before_eval_results": {"predictions": ["the third season", "Morgan Freeman", "The Bangladesh -- India border", "Richard Carpenter", "billy last name", "Phil Gallagher", "A Turtle's Tale : Sammy's Adventures", "Nathan Hale", "Emma Watson", "USS Chesapeake", "1983", "Yugoslavia", "George Harrison", "New England", "1922 to 1991", "total cost", "the nasal septum", "The player named on 75 % or more of all ballots cast", "During Super Saiyan 4 is brought about while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form, he made the hair more `` wild ''", "Christina DiMaggio", "Mitch Murray", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "The Epistle of Paul to the Philippians", "Erica Rivera", "Labour Party", "a Norwegian town", "Thomas Edison", "1959", "The crossing of Highway 68 ( Holman Highway / Sunset Drive ) and 17 - Mile Drive marks the entrance to Pebble Beach", "rotation axes", "Hendersonville, North Carolina", "aorta", "Atlanta, Georgia", "Sonu Nigam", "a flood defense system", "Donna Reed", "Total Drama World Tour", "Staci Keanan", "18th century", "Debbie Gibson", "Masha Skorobogatov", "U.S. Electoral College", "The reservation nourishes the historically disadvantaged castes and tribes", "Richard Stallman", "Pasek and Paul", "Quantitative psychological research", "1603", "September 27, 2017", "Nepal", "BC Jean and Toby Gad", "six", "Sir Robert Walpole", "Lee Harvey Oswald", "Mauretania", "guitar feedback", "John Robert Cocker", "Isla de Xativa", "January 24, 2006.", "March 8", "terminal brain cancer.", "the Panama Canal", "mezcal", "Solidarity", "their heads"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6458962912087911}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.1904761904761905, 0.0, 0.0, 1.0, 0.7692307692307692, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-10070", "mrqa_hotpotqa-validation-4926", "mrqa_searchqa-validation-11851", "mrqa_triviaqa-validation-4719"], "SR": 0.578125, "CSR": 0.46062499999999995, "EFR": 0.9259259259259259, "Overall": 0.6789508101851852}, {"timecode": 25, "before_eval_results": {"predictions": ["one", "Rupolph Valentino", "Crete", "Churchill Downs", "\"Un Giorno di Regno\"", "\"Unified Theory of Everything',", "1963", "\"Corse of Scotland\"", "sea shells", "The Daily Herald", "Danie Green", "a zoom lens", "a curb-roof", "armada", "Moldova", "Florida", "the Radio City Music Hall", "Italy", "The Hague Conventions of 1899 and 1907", "Saltaire", "The Hague", "the Verizon Center", "orange", "The greater COA", "Bunratty Castle", "American rock 'n' roll pioneer", "a piano", "The Merchant of Venice", "\"Rivers of Blood\"", "the Temple of Artemis", "Lindisfarne", "a muffin", "The Undertones", "George Santayana", "Philistines", "The Parting of the Ways", "mitzvot", "Andrew Jackson", "Italy", "Bihari cuisine", "in feces or vomit of someone who has the virus.", "Sandi Toksvig", "Guinea", "Los Angeles", "luster", "Margaret Thatcher (Meryl Streep)", "the Reform Club", "The Cathedral", "pyrotechnic", "a gorilla", "polyhedron", "741 weeks", "Ed Sheeran", "more than 80", "over 281", "Charles L. Clifford", "23 July 1989", "\"GM and Chrysler,", "\"I was a little rusty in the beginning, missed quite a few forehands, but I thought it was a good match overall,\"", "The U.S. State Department and British Foreign Office", "the Diners' Club Card", "Lost in Space", "Calcium", "punish participants in this week's bloody mutiny,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5359375}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2863", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-7284", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-3174", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-1149", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-636", "mrqa_searchqa-validation-16751"], "SR": 0.484375, "CSR": 0.46153846153846156, "EFR": 0.9696969696969697, "Overall": 0.6878877112470863}, {"timecode": 26, "before_eval_results": {"predictions": ["1,776 steps", "Bemis Heights", "H CO ( equivalently OC (OH )", "in Paradise, Nevada", "The Continental Congress", "2017 season", "Charles Darwin", "in florida", "Dr. Rajendra Prasad", "at the 1964 Republican National Convention in San Francisco, California", "Steve Nash", "Annette", "Total Drama Action", "over 800", "Brenda", "Tony Orlando", "Marie Fredriksson", "1830", "stable, non-radioactive rubidium", "45 %", "During the reign of King Beorhtric of Wessex ( 786 -- 802 )", "milling process", "Andrew Gold", "Henry Purcell", "A dipeptide reaction", "General George Washington", "2017 season", "`` Have I Told You Lately ''", "in Middlesex County, Province of Massachusetts Bay", "April 4, 2017", "either the Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee,", "The Ministry of Corporate Affairs", "any person, even for someone who is not a member of the House at all", "Ohio and briefly attended the University of Pittsburgh before transferring to Mount Union, where he played defensive line", "Abid Ali Neemuchwala", "seven units", "2", "the digitization of social systems", "1975", "James Brown", "Kristy Swanson", "a medical emergency as it results from rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "around 10 : 30am", "1966", "a Midwestern theater owner named Glen W. Dickson", "Jonathan Larson", "Manhattan", "Landon Jones", "John J. Flanagan", "Kimberlin Brown", "1955", "benjamin Disraeli", "y Yahoo!", "scotland", "villanelle", "Bayern Munich", "in Atlanta, Georgia", "Barbara Streisand", "Lindsey Vonn", "netherlands", "Bacon Cornbread", "Tom Sennett", "Philippines", "St. Peter's Basilica"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5911871982184482}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.5384615384615384, 0.6666666666666666, 0.8571428571428572, 0.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.0, 0.6, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-8582", "mrqa_triviaqa-validation-1144", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-2873", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1384", "mrqa_searchqa-validation-14755", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-16051"], "SR": 0.421875, "CSR": 0.4600694444444444, "EFR": 0.918918918918919, "Overall": 0.6774382976726727}, {"timecode": 27, "before_eval_results": {"predictions": ["Lake Superior", "Mcescher", "the Ronald Reagan Presidential Library", "spread", "Treasure Island", "Gloria Steinem", "Luxembourg", "curly", "Garage rock", "Timothy Franz \"Tim\" Geithner", "Panera Bread", "volcanic cones", "Louisiana", "East of Eden", "the kitchen sink", "arizonensis", "\"First-rate second-rate men\"", "dressage", "lipos", "A Series of Unfortunate Events", "car", "beautiful black and white", "Misery", "the Rolling Stones", "malaria", "Hamlet", "a Race-based Dessert", "a mitre", "caribou", "Jabberwocky", "Making the Band", "Virgin", "washington", "Federalist Papers", "the Lone Ranger", "The Boeing Everett Factory", "The Ladies' Singles Trophy", "\"The 82nd Annual Academy Awards\"", "nautilus", "Richelieu", "Gwalior", "Rapa Nui National Park", "Madden NFL 06", "Carrie Underwood", "The Guns of Navarone", "eustachian tube", "Florida State", "hiccups", "Epidural Injections", "Brigham Young", "the Department of Transportation", "President Theodore Roosevelt", "just after the Super Bowl", "16,801", "Cami de Repos", "Norman Mailer", "Basel", "the 2007 Formula One season", "San Francisco, California with offices in New York City and Atlanta", "Mos Def", "The Glasgow, Scotland concert", "gang rape of a 15-year-old girl", "Toffelmakaren", "cations"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5421875}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-8261", "mrqa_searchqa-validation-1867", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-11353", "mrqa_searchqa-validation-13722", "mrqa_searchqa-validation-2490", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3723", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-7995", "mrqa_searchqa-validation-6022", "mrqa_searchqa-validation-6617", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-14352", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-4526", "mrqa_searchqa-validation-11202", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-692", "mrqa_hotpotqa-validation-2856", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-1918"], "SR": 0.46875, "CSR": 0.4603794642857143, "EFR": 0.9705882352941176, "Overall": 0.6878341649159665}, {"timecode": 28, "before_eval_results": {"predictions": ["CAIR", "Venus flytrap", "Dolores Haze", "Fl orence's", "1876", "Ty Hardin", "gizzard", "denier", "March", "The Republic of Biafra", "engine", "Colombia", "Lewis Carroll", "a robe", "Rutger Hauer", "the moon", "the Crystal Palace", "Scotland", "Milton Keynes", "Leadbetter", "a lie detector", "Italy", "\"Ivy\" Carter", "\"Gomer\" Pyle", "Dundee", "the Colossus of Rhodes", "60", "(John) Wilkes Booth", "Hyperbole", "Dinosaur Wars", "Manchester", "Kenneth Brighenti", "Bridge", "the Black Sea", "the Reform Club", "U.K.", "West Point", "the Heavyweight title", "phosphorus", "work", "n\u00famero", "Ross Bagdasarian", "switzerland", "Mercedes-Benz", "Alexandria", "Wordsworth", "a bronze medal", "Joshua Tree National Park", "Trainspotting", "a Mammal", "the Yersinia pestis", "1967", "the Atlantic", "Dan Stevens", "The Royal Albert Hall", "Ridley Scott", "2006", "breast cancer", "the American Civil Liberties Union", "Kerstin", "Fat Man", "Thurgood Marshall", "The Mill on the Floss", "four"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5973958333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-978", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2123", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-901", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-5184", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-2275", "mrqa_naturalquestions-validation-8072", "mrqa_hotpotqa-validation-1200", "mrqa_newsqa-validation-359", "mrqa_searchqa-validation-6604"], "SR": 0.5625, "CSR": 0.4639008620689655, "EFR": 0.9285714285714286, "Overall": 0.6801350831280788}, {"timecode": 29, "before_eval_results": {"predictions": ["Florida's Everglades.", "lula da Silva", "three gunmen", "UNICEF", "in Hong Kong's Victoria Harbor as part of its 18-month journey around the world.", "Uighurs", "used", "70,000", "Asashoryu", "President Bush", "last week", "Rawalpindi", "22", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Ashley \"A.J.\" Jewell", "jabs", "\"Itsy bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "in their Naples home.", "acute stress disorder", "the Somali-based pirates", "Manuel Mejia Munera", "Jewish", "Africa", "Omar Bongo", "Turkish company", "tutsi militia led by Kagame defeated the Hutu rebels and took control of the government.", "job", "Dr. Jennifer Arnold and husband Bill Klein,", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "gasoline", "chadian president Idriss Deby", "At least 14", "on Anjuna beach in Goa", "Larry Ellison", "murder", "\"It hurts my heart to see him in pain, but he's doing about as well as could be expected at the period of probably several weeks to a couple of months still left in the hospital", "\"I'm certainly not nearly as good of a speaker as he is.\"", "garth Brooks", "gang rape", "\"Empire of the Sun,\"", "kgalema Motlanthe", "\"A lightning strike was a possibility, particularly since the plane disappeared in a storm-prone area along the equator known as the Intertropical Convergence zone (ITCZ)", "the Genocide Prevention Task Force", "Stratfor", "Ricardo Valles de la Rosa", "colonel", "\" Raiders of the Lost Ark.\"", "bipartisan rhetoric", "two", "one", "kerstin Fritzl", "A footling breech", "During Hanna's recovery masquerade celebration", "cuba", "vincent van gogh", "fourteen", "\"In God we Trust\"", "Katherine Harris", "Sandusky", "Lambic", "cuba", "give love a bad name", "ivory", "Psy"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5984369624300397}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.4, 1.0, 0.47058823529411764, 0.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.7659574468085107, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.33333333333333337, 0.05, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2946", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5631", "mrqa_triviaqa-validation-524", "mrqa_hotpotqa-validation-3991", "mrqa_searchqa-validation-16692"], "SR": 0.46875, "CSR": 0.46406250000000004, "EFR": 0.9705882352941176, "Overall": 0.6885707720588236}, {"timecode": 30, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1631", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-949", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2138", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-22", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5242", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7074", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7802", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9297", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-925", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10522", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11189", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-11486", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13535", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-13722", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14499", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14755", "mrqa_searchqa-validation-14957", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15051", "mrqa_searchqa-validation-1509", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-15260", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16690", "mrqa_searchqa-validation-16692", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4349", "mrqa_searchqa-validation-493", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5218", "mrqa_searchqa-validation-5248", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6022", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7001", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8483", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8707", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-8794", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-9046", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-996", "mrqa_searchqa-validation-9992", "mrqa_squad-validation-2550", "mrqa_squad-validation-2832", "mrqa_squad-validation-317", "mrqa_squad-validation-3781", "mrqa_squad-validation-4059", "mrqa_squad-validation-4752", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5174", "mrqa_squad-validation-6537", "mrqa_squad-validation-679", "mrqa_squad-validation-6792", "mrqa_squad-validation-7257", "mrqa_squad-validation-7903", "mrqa_squad-validation-8157", "mrqa_squad-validation-8187", "mrqa_squad-validation-8224", "mrqa_squad-validation-8733", "mrqa_squad-validation-8954", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-1277", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1640", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1685", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-2123", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-2863", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-3068", "mrqa_triviaqa-validation-3138", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-436", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4684", "mrqa_triviaqa-validation-4719", "mrqa_triviaqa-validation-4739", "mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-5242", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6390", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6902", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7061", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-953"], "OKR": 0.798828125, "KG": 0.43671875, "before_eval_results": {"predictions": ["No. 4", "the murder of", "The Casalesi clan", "\"Dancing With the Stars.\"", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "a pregnant soldier", "the U.N. aid agency", "an angel, she is God-sent,\"", "two Metro transit trains that crashed the day before, killing nine", "The Disasters Emergency Committee,", "boyhood experience in a World War II internment camp", "The Bodyguard Trevor Rees", "Naples", "\"Taz\" DiGregorio, keyboardist and original member of The Charlie Daniels Band,", "Iran's parliament speaker", "14", "Gary Player", "Danish engineer Karl Kr\u00f8yer", "The man ran away, police chased him and a gunfight ensued.", "the Sodra nongovernmental organization,", "246", "about 3,000 kilometers (1,900 miles)", "18th", "Chile", "a U.S. soldier captured by the Taliban", "\"Z Zimbabwe cannot be British, it cannot be American. Yes, it is African,\"", "1981", "Susan Boyle", "forgery", "the way their business books were being handled.", "jazz", "Janet and La Toya", "100 percent", "Ashley \"A.J.\" Jewell,", "well over 1,000 pounds", "bikinis", "Little Rock Central High School", "help the convicts find calmness in a prison culture ripe with violence and chaos.", "a birdie four at the last hole", "12-1", "Dean Martin, Katharine Hepburn and Spencer Tracy", "South Korean President Lee Myung-bak,", "organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "four", "Arizona's bill orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "Roger Federer", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Six members of Zoe's Ark", "\"not generals but businessmen\"", "around 3.5 percent of global greenhouse emissions", "the procedures", "The Nitty Gritty Dirt Band", "Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida", "special economic zones", "Mark Twain", "riyadh", "Blessed", "River Clyde", "on the north bank of the North Esk,", "Debbie Harry", "the Indians", "Sam Houston", "a rubaiyat", "an open field northwest of Bemis Heights belonging to Loyalist John Freeman"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5731623237873238}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.1, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.8, 0.4, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.2, 0.0, 1.0, 1.0, 0.22222222222222224, 0.08888888888888888, 1.0, 0.15384615384615385, 1.0, 0.0606060606060606, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9600000000000001, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3188", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3706", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-921", "mrqa_newsqa-validation-3666", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-6670", "mrqa_triviaqa-validation-4454", "mrqa_hotpotqa-validation-1540"], "SR": 0.46875, "CSR": 0.4642137096774194, "EFR": 0.9705882352941176, "Overall": 0.6743041389943073}, {"timecode": 31, "before_eval_results": {"predictions": ["Lonnie", "solitary confinement", "\"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "David F. Wherley Jr.", "\"Allahu akbar,\"", "Gavin de Becker", "Brian Smith", "over 1,000 pounds", "31 meters (102 feet) long and 15 meters (49 feet) wide", "3,000 kilometers (1,900 miles),", "April 28", "monarchy", "Gary Player", "Three aid workers", "Kurdistan Freedom Falcons,", "30", "provide safety and physical security, but also could further assist with the reconstruction projects, such as building hospitals, schools, sanitation facilities and investment projects that would have direct impact on the socioeconomic development of the Afghan and Pakistani societies.", "Jesus Mendez, 16, of being in a group that poured alcohol over brewer and set him ablaze in a dispute over $40, a video game and a bicycle.", "end of respect of his public office and reputation, and in hopes of keeping our children from just this type of public exposure.", "UNICEF", "Conway, Arkansas", "Tibetan exile leaders,", "Susan Atkins", "two", "heavy turbulence about 02:15 a.m. local time Monday", "North Korea intends to launch a long-range missile in the near future,", "CEO of an engineering and construction company with a vast personal fortune of more than 30 billion won ($30.2 million) to the poor.", "'overcharged.'\"", "\"Five of us for the United States and two against us because they were stranded in Japan\"", "her landlord defaulted on the mortgage and that the lease was no longer valid.", "Noida,", "at least 25", "misdemeanor assault", "19-year-old", "Michoacan Family,\"", "$5.5 billion", "near the Somali coast", "Russia", "his comments", "Nearly eight in 10", "jobs", "she's in love,", "end of our Mountain View, California, campus.", "Philippines", "industrialized", "refusal or inability to \"turn it off\"", "Dan Brown", "mental health and recovery.", "Caster Semenya", "\"Dancing With the Stars.\"", "1940's", "in 1972", "Acid rain", "Dunedin, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough", "Pandore", "Paul Cezanne", "Gandalf", "three", "Mazda Capella", "Mountain West Conference", "the War of 1812", "arthropoda", "Merahi metua no Tehamana", "St. George"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49912789875565605}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.04, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 0.4615384615384615, 0.0, 0.11764705882352941, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-990", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2738", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-1304", "mrqa_triviaqa-validation-3723", "mrqa_triviaqa-validation-2273", "mrqa_hotpotqa-validation-4710", "mrqa_searchqa-validation-5260", "mrqa_searchqa-validation-5038"], "SR": 0.40625, "CSR": 0.46240234375, "EFR": 1.0, "Overall": 0.67982421875}, {"timecode": 32, "before_eval_results": {"predictions": ["parents", "The move frees up a place for another non-European Union player in Frank Rijkaard's squad.", "in central Cairo,", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "\"I think that you would agree that there is no place for racism in America today.\"", "1973's", "Kim Il Sung", "at \"CNN Heroes: An All-Star Tribute\" as a", "\" Reddit was able to force the House Budget Chair to reverse course -- shock waves will be felt throughout the establishment in Washington today, and other lawmakers will take notice.\"", "Franklin, Tennessee,", "Tom Baer.", "four", "consumer confidence", "Queen Elizabeth's birthday", "United States, NATO member states, Russia and India", "since 1983.", "The show allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from what they eat to how they should entertain themselves.", "the insurgency,", "The United States", "the two-state solution to the Mideast conflict,", "Stanford University", "Alicia Keys", "Dennis Davern,", "breast cancer.", "\"We are doing our best to dissuade the North Koreans from going forward, because it is provocative action,\"", "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\"", "More than 150,000", "finance", "a defense minister,", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "Washington State's decommissioned Hanford nuclear site,", "15", "anaphylactic shock.", "June 6, 1944,", "Iran's nuclear program.", "Juliet,", "NATO's Membership Action Plan, or MAP,", "going out of business for one reason or another,", "for strategy, plans and policy on the Army staff.", "Indian Army", "Obama", "supplies power to almost 9 million Americans,", "1913,", "Baghdad", "the 2009 Swamp Soccer World Championship held in Scotland", "were intended to demonstrate for lawmakers actual production models of vehicles that may cut the nation's reliance on petroleum-based fuels.", "India", "neck", "military commissions", "Steven Chu", "4,", "The stability, security, and predictability of British law and government", "Majandra Delfino", "Costa Rica, Brazil, and the Philippines", "Albert Finney,", "Kosovo", "the Violin", "Cyclic Defrost", "Peter O'Toole,", "Illinois", "the death vestments", "Blackbird", "General Hospital", "the final episode of the series"], "metric_results": {"EM": 0.4375, "QA-F1": 0.559530009920635}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [0.2222222222222222, 0.375, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.09523809523809523, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06666666666666668, 0.2857142857142857, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-2655", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2740", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-7398", "mrqa_hotpotqa-validation-5856", "mrqa_searchqa-validation-4039"], "SR": 0.4375, "CSR": 0.4616477272727273, "EFR": 1.0, "Overall": 0.6796732954545455}, {"timecode": 33, "before_eval_results": {"predictions": ["northern Israel", "played two secret concerts in Vegas earlier this summer.", "off the coast of Dubai", "revise our policies", "a long-range missile", "a \"new chapter\" of improved governance", "he fears a desperate country with a potential power vacuum that could lash out.", "Japan", "The United Nations", "133 people", "the Southern Baptist Convention,", "has kept the details on both the timing and selection of the running mate under wraps.", "lightning strikes", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "August 4, 2000", "Wednesday.", "Marc Jacobs", "Nairobi, Kenya,", "Bangladesh", "her father's home in Satsuma, Florida,", "discusses his roots", "Sabina Guzzanti", "the Southeast,", "137", "young self-styled anarchists", "the insurgency,", "start a dialogue of peace", "Elizabeth Birnbaum", "heavy flannel or wool", "school,", "1913,", "Portuguese water dog", "U.N. General Assembly", "2011.", "Misty Croslin,", "took an obscure story of flowers", "Thailand", "consistent and accessible.", "September,", "an acid attack by a spurned suitor.", "\"People have lost their homes, their jobs, their hope,\"", "public-television", "The oceans", "August 19, 2007.", "Haiti", "March 24,", "in his 60s,", "1616.", "the children of street cleaners and firefighters.", "Ralph Lauren,", "about 5:20 p.m. at Terminal C", "~ 55 - 75", "The prequel film Revenge of the Sith", "the late 16th century in villages near Colchester, Essex, East England", "the government", "Liechtenstein", "centre-right", "Rockhill Furnace, Pennsylvania,", "Giacomo Puccini", "seal hunting", "Denmark", "Canada", "Blue Hawaii", "Kirk Douglas"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6209441600066601}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.20000000000000004, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3674", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3480", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-6881", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5738", "mrqa_hotpotqa-validation-1332"], "SR": 0.578125, "CSR": 0.4650735294117647, "EFR": 0.9629629629629629, "Overall": 0.6729510484749455}, {"timecode": 34, "before_eval_results": {"predictions": ["$22 million", "Philippines", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "phone calls or by text messaging,", "the punishment for the player", "L'Aquila earthquake,", "a man accused of sexually assaulting a toddler", "two and a half hours.", "A Brazilian supreme court judge", "Wednesday at home in Stanford Alto, California,", "40 lash after he was convicted of drinking alcohol in Sudan", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential", "Fullerton, California,", "Sarah,", "a \"prostitute\"", "Hillary Clinton", "\"They were nothing,\"", "the United States", "the Rockies", "the leader of a drug cartel", "Dennis Ray Gerwing", "state senators who will decide whether to remove him from office", "4.6 million", "the Dalai Lama", "Texas", "Austin Wuennenberg,", "learn in safer surroundings.", "a reward car.", "$14.1 million.", "Christmas", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Nineteen", "the Brundell family in Deene Park, England,", "the Beatles", "Iran's", "Paul Biya,", "Joe Harn of the Garland police said.", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "fight back against Israel in Gaza.", "Six", "the first five Potter films", "Sunday's security breach", "\"The Kirchners have been weakened by this latest economic crisis,\"", "Haiti.", "80,", "her home for 12 of the past 18 years.", "\"His address in the Turkish parliament was one of the greatest speeches made by an American leader in such a setting:", "Authorities in Fayetteville, North Carolina,", "\"We Found Love\"", "Marcell Jansen", "British Prime Minister Gordon Brown's", "ending aggressive militarism", "In Test cricket", "between $10,000 and $30,000", "England", "minivans", "Lesley Hornby,", "Oklahoma", "\"Aloha \u02bbOe\"", "Boyd Gaming.", "gift of Psalm 90:4 and II Peter", "Anna Karenina", "the Pacific Ocean", "Mississippi"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5304404677924415}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.4, 0.2, 0.15384615384615385, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.09523809523809525, 0.6666666666666666, 1.0, 0.36363636363636365, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.32, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-2519", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4768", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-2416", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-6092", "mrqa_searchqa-validation-14603"], "SR": 0.40625, "CSR": 0.46339285714285716, "EFR": 0.9736842105263158, "Overall": 0.6747591635338346}, {"timecode": 35, "before_eval_results": {"predictions": ["Partial weight - bearing", "in the season - five premiere episode `` Second Opinion ''", "Database - Protocol driver", "the governor of West Virginia", "Charles Lebrun", "cella", "John Bull", "Charlotte Thornton", "January 1923", "Noel Kahn", "7000301604928199000 \u2660 3.016 049 281 99 ( 23 ) u", "James Earl Jones", "biblical covenants", "Thomas Lennon", "Robber Barons", "in people and animals", "10.5 %", "in 1993", "Mike Leeson and Peter Vale", "collect menstrual flow", "LeoArnaud or L\u00e9o Arnaud", "President pro tempore of the Senate", "September 2017", "in the early 20th century", "liberales", "Charles Path\u00e9", "Mel Tillis", "Florida", "four", "O'Meara", "demonstrations", "November 17, 2017", "Bobby Darin", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Ben Findon, Mike Myers and Bob Puzey", "Donny Osmond", "system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning", "Maximilien Robespierre", "the distribution and determinants of health and disease conditions in defined populations", "the nerves and ganglia outside the brain and spinal cord", "`` Tip and Ty ''", "24 November 1949", "Martin Lawrence", "In 1984", "September 29, 2017", "in 1963", "commercial at", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "Ali Daei", "copper ( Cu )", "Colon Street", "Duke of Wharton", "shoji", "peppers", "Sunday, November 2, 2003,", "Citizens for a Sound Economy", "H. R. Haldeman", "Judge Sonia Sotomayor,", "closure to the families of three missing military men,", "off east  Africa", "Honolulu", "Derek Jeter", "sheep", "U.S. ocean surveillance ship"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6222841522244096}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 0.5714285714285715, 0.8571428571428571, 0.3333333333333333, 0.20000000000000004, 0.4444444444444445, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.2222222222222222, 0.0, 0.625, 1.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7843137254901961, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2818", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-31", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-3795", "mrqa_hotpotqa-validation-3489", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-3307"], "SR": 0.4375, "CSR": 0.46267361111111116, "EFR": 1.0, "Overall": 0.6798784722222222}, {"timecode": 36, "before_eval_results": {"predictions": ["that they'd get to bring a new puppy with them to the White House in January.", "on the bench", "the deployment of 30,000 additional U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.", "a Yemeni cleric", "Adriano", "central Cairo,", "at least 300", "how it will proceed.", "Red River", "Bialek", "Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "Haiti", "700 guests", "Kris Allen,", "Akshay Kumar", "advocate for early detection and helping other women cope with the disease.", "the 11th anniversary of the September 11, 2001, terror attacks.", "Dennis Rodman, Tom Green and Brian McKnight,", "Goa", "Robert Kimmitt.", "Kaka,", "World War I", "Roy Miller", "in Amstetten,", "the Internet", "183 people, including 137 children,", "between 1917 and 1924 when he was in his late 30s and early 40s.", "John Lennon and George Harrison,", "stylish clothes", "Christiane Amanpour", "Muslim festival of Eid al-Adha.", "Alfredo Astiz,", "folding table", "beetles.", "upper respiratory infection", "Bob Bogle,", "\"Beats digging ditches.\"", "rally at the State House", "Williams' body", "Al-Shabaab", "Siemionow,", "Pope XVI", "Opryland.", "ALS6,", "the FDA sent warning letters to nine companies telling them to stop manufacturing 14 unapproved narcotics that are widely used to treat pain.", "that Birnbaum had resigned \"on her own terms and own volition.\"", "\"She was focused so much on learning that she didn't notice,\"", "Susan Atkins' request should not be sent to the sentencing court for consideration,", "heavy brush,", "frees up a place", "India", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "Billy Stritch", "1961", "Leeds", "Backgammon", "Chongqing", "Paige O'Hara", "Champion Jockey", "Arthur Miller", "Sinclair Lewis", "fish", "sand", "mortar"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7098076387550072}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true], "QA-F1": [0.962962962962963, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.19999999999999998, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9, 0.3076923076923077, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.8, 0.0, 1.0, 0.10526315789473685, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1263", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-4031", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-1679", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3625", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-3672", "mrqa_triviaqa-validation-206", "mrqa_hotpotqa-validation-206"], "SR": 0.5625, "CSR": 0.4653716216216216, "EFR": 0.9642857142857143, "Overall": 0.6732752171814671}, {"timecode": 37, "before_eval_results": {"predictions": ["space shuttle Discovery", "Martin Aloysius Culhane", "five female pastors", "Jeannie Longo-Ciprelli", "the river will crest Saturday about 20 feet above flood stage.", "more than 20 times", "Tutsis the privileged ethnicity,", "2,000 euros ($2,963)", "Wednesday", "India", "Bob Dole,", "terrorism.", "Manuel Mejia Munera was a drug lord with ties to paramilitary groups,", "since 1983.", "Zuma", "to hold onto his land", "how great it must be to live in a rich country like America,", "Sunday", "the Somali coast", "Chinese", "southern city of Naples", "38", "a meeting with the president to discuss her son.", "Marc Jacobs and Rag & Bone.", "civilians", "digging ditches.", "the coalition", "grossing $55.7 million during its first frame,", "eteen", "jazz", "racial intolerance.", "he will represent himself.", "Felipe Calderon", "Wednesday", "a female cadaver", "collaborating with the Colombian government,", "Three", "6-2 6-1", "Scarlett Keeling", "\u00a320 million ($41.1 million) fortune", "Israel", "the man facing up, with his arms out to the side.", "George Washington", "be silent.", "Africa's largest producer.", "whether he should be charged with a crime,", "Maj. Nidal Malik Hasan, MD,", "Manmohan Singh's Congress party,", "Peter Maiyoh,", "last week", "Dilshan scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai on Wednesday.", "Julia Ormond", "about 6 : 00 p.m.", "all transmissions", "Chicago", "Stephen King", "Paris", "the Big 12", "Mim.", "Messiah Part II", "1.80653 Cl ppt;", "a rheumatoid type of this is 3 times as common in women as in", "the Boer War", "I Love You"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5910884492685964}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.14814814814814817, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-2898", "mrqa_naturalquestions-validation-3373", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4382", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-2921", "mrqa_searchqa-validation-6687", "mrqa_searchqa-validation-12745"], "SR": 0.484375, "CSR": 0.4658717105263158, "EFR": 1.0, "Overall": 0.6805180921052632}, {"timecode": 38, "before_eval_results": {"predictions": ["acid attack", "More than 22 million people in sub-Saharan Africa are infected with HIV,", "Bryant Purvis,", "Kurdistan Freedom Falcons, known as TAK,", "A portrait", "Halloween", "700", "to coalition forces in Afghanistan", "criminals", "Jenny Sanford,", "near Fort Bragg in North Carolina.", "Tottenham", "former U.S. secretary of state.", "helped nations trapped by hunger and extreme poverty,", "$3 billion,", "Larry Ellison,", "the National September 11 Memorial Museum", "Kim Jong Il's", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "Gary Brooker", "a baseball bat", "closing these racial gaps.", "Bollywood", "up to $50,000 for her", "bankruptcies", "A witness", "Gary Coleman", "fifth", "forgery and flying without a valid license,", "fake his own death by crashing his private plane into a Florida swamp.", "to secure more funds from the region.", "Fukuoka,", "1994", "Rima Fakih", "a pair of hot-looking, two-seater sports cars.", "Roy Foster's", "Ryan Adams.", "41,", "the immorality of these deviant young men", "the lead plaintiff in perhaps the most controversial case involving Judge Sonia Sotomayor,", "it would investigate the video and any group that tries to take justice into its own hands.", "gun", "World leaders", "he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "3,000 kilometers (1,900 miles),", "Oxbow,", "pirates", "\"Americans always believe things are better in their own lives than in the pretty bad category,", "Caster Semenya", "reducing\" greenhouse emissions.", "A Whiter Shade of Pale", "north", "Randy", "Jonathan Breck", "Achille Lauro", "Rio de Janeiro", "Jane Austen", "Loch Shiel", "Vikram, Jyothika and Reemma Sen", "Tennessee", "foxes", "Battle of San Juan Hill", "his involvement in the 2007 \"D.C. Madam\" scandal.", "Joe DiMaggio"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6982458023558568}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.25, 1.0, 0.9090909090909091, 0.0, 1.0, 0.608695652173913, 1.0, 1.0, 1.0, 0.3333333333333333, 0.1111111111111111, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.33333333333333337, 0.4, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-316", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-1595", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2748", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-5421", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-4624", "mrqa_searchqa-validation-2738"], "SR": 0.578125, "CSR": 0.46875, "EFR": 1.0, "Overall": 0.6810937499999999}, {"timecode": 39, "before_eval_results": {"predictions": ["Sunday", "\" Michoacan Family,\"", "Steven Green", "More than 150,000", "revelry", "April 24 through May 2.", "12-hour-plus", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "\"The Block is Hot\" and \"Lollipop,\"", "House-passed bill that eliminates the 3% withholding requirement for government contractors", "Robert Park", "30", "calls for him to step down as majority leader.", "Meredith Kercher.", "10 percent", "being evicted", "Siemionow", "vice-chairman of Hussein's Revolutionary Command Council.", "it will be the longest domestic relay in Olympic", "Muslim festival", "China", "Laura Ling and Euna Lee", "Ankara, Turkey,", "Mugabe", "Many of those who haven't bought converters are poor, older than 55, rural residents or racial minorities,", "Muslim", "Herman Thomas", "about the shootings, handed over the AR-15 and two other rifles and left the cabin.", "consumer confidence", "South Africa", "Sunday", "Iowa's critical presidential caucuses", "a bank", "Long Island", "President Barack Obama,", "in-cabin lighting system", "vitamin injections", "\"fusion teams,\"", "maintain an \"aesthetic environment\" and ensure public safety,", "the Russian air force,", "CNN/Opinion Research Corporation", "U.S. Army scout", "between June 20 and July 20", "opium", "to help Haiti recover from the devastating effects of the earthquake and Argentina's conflict with Great Britain over oil drilling offshore from the Falkland Islands.", "15-year-old", "2-1", "Sharon Bialek", "Afghanistan", "Thaksin Shinawatra,", "a man's lifeless, naked body", "Lula", "April 10, 2018", "in the central plains", "Monaco Historic Friday 13th 14th 15th May 2016 Grand Prix", "Anglesey", "Fancy Dress Shop", "1992", "American", "Ellie Kemper", "Old Maid, Go Fish", "owl", "New Orleans", "the Harpe brothers"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7088867079101454}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.6666666666666666, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06250000000000001, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3875", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-687", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2224", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-2669", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-7661", "mrqa_triviaqa-validation-2959", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-16859"], "SR": 0.640625, "CSR": 0.473046875, "EFR": 1.0, "Overall": 0.6819531249999999}, {"timecode": 40, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1450", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2637", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-845", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1539", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-2224", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2501", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3579", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-572", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-687", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-951", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10811", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11486", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12745", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14404", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-1474", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-16906", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4069", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7033", "mrqa_searchqa-validation-723", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7731", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10344", "mrqa_squad-validation-1158", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4140", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-6763", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4487", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7078", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-978"], "OKR": 0.79296875, "KG": 0.48828125, "before_eval_results": {"predictions": ["jest", "4.58 million", "transept", "the Lincoln National Monument", "14 lines.", "Thomas Alva Edison", "on or directly in front of the pitching rubber,", "Cy Young", "Judith", "Delap", "Frederic", "Frasier", "pajamas", "Las Vegas", "Australian", "the Department of Justice", "Peter Goldmark", "Utah", "British Broadcasting Corporation (BBC)", "Greek Meatballs", "Pope John Paul II", "Nancy Drew", "where's the beef", "Google", "the stethoscope", "Yedioth Ahronoth", "Harry S. Truman Library", "crystals", "a James Clavell novel", "manacles", "a lighthouse", "Edinburgh", "colored sands", "1903", "wren", "Cubism", "shoes", "the Erie Canal", "All the President's Men", "Simon & Garfunkel", "The Silence of the Lambs", "the banjo", "The Exorcist", "An Evening with Claire", "poetry", "Iggy Azalea", "the Tigris", "Leyden", "Bobby Jones", "Halley's", "March", "orbit", "the Naturalization Act of 1790", "25 September 2007", "new york", "Captain Hook", "in various cities around the globe.", "John W. Henry", "B-17 Flying Fortress", "Eugene O'Neill", "Tuesday.", "Nineteen", "4.6 million", "the Emperor's Ministry of Work"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6021949404761904}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 0.5, 0.0, 1.0, 0.0, 0.5714285714285715, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7421", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-220", "mrqa_searchqa-validation-8762", "mrqa_searchqa-validation-3517", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-1404", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-15206", "mrqa_searchqa-validation-15613", "mrqa_searchqa-validation-11090", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-7203", "mrqa_searchqa-validation-156", "mrqa_searchqa-validation-5932", "mrqa_searchqa-validation-2705", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-4531", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-15712", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-13632", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-6972", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-5030", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-2044", "mrqa_naturalquestions-validation-8907"], "SR": 0.484375, "CSR": 0.47332317073170727, "EFR": 1.0, "Overall": 0.6911490091463415}, {"timecode": 41, "before_eval_results": {"predictions": ["the spiritual authority of the church", "kinetic", "Bob Dylan", "Oz", "Lanai", "Dorothy", "the Bicentennial Symphony", "burying the dead.", "Wordsworth", "Mount Rushmore National Memorial", "a pentathlon", "Geena Davis", "Marie Osmond", "a region", "Christmas", "John Foster Dulles", "Jack Canfield", "Goodyear", "carbon fiber", "American", "a Trucker", "the Bill of Rights", "the Philippines", "Dublin", "Tainted Love", "Doom", "the Frog", "HP", "Ned", "sound", "Hungary", "Washington, DC", "Goldenrod", "the head", "a hummingbird", "manager", "Wessex", "Alfred Hitchcock", "candy cane", "Somerset Maugham", "Glacier National Park", "Lincoln Park", "Rene Auberjonois", "the Irving G. Thalberg Memorial Award", "the Colosseum", "The Simple Life", "Guitar Hero", "the language", "pharaoh", "Aesop", "Raymond Chandler", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "May 2010", "in 2012 ( Pub. L. 112 -- 257 )", "Tasmania", "glycerol", "Alex Kramer", "Theo James Walcott", "the British Army", "John John Florence", "Caylee,", "how pleased I am with today's", "Camp Lejeune, North Carolina", "seven"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6380208333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12050", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-375", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-7308", "mrqa_searchqa-validation-5905", "mrqa_searchqa-validation-2002", "mrqa_searchqa-validation-2947", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-9410", "mrqa_searchqa-validation-6892", "mrqa_searchqa-validation-7887", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-6743", "mrqa_searchqa-validation-3344", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-1731", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1857"], "SR": 0.546875, "CSR": 0.47507440476190477, "EFR": 1.0, "Overall": 0.6914992559523809}, {"timecode": 42, "before_eval_results": {"predictions": ["the University of Vienna", "orchestrated the first movement piano sketch.", "the Speedway World Championship", "June 11, 1986", "Mercury Records", "526 people", "Count Schlieffen", "German Shepherd", "the Moselle", "the United States", "Hawaii", "The Weeknd", "Friedrich Nietzsche", "Sex Drive", "Tel Aviv,", "Oregon's", "Attorney General and as Lord Chancellor of England", "Martha Wainwright", "David Irving", "2 November 1902 \u2013 27 August 1944", "Harvard University", "Russian Ark", "\"The Land of Enchantment\"", "hunt", "Seoul, South Korea", "Eve Hewson", "northwestern", "Virgin", "Kevin Peter Hall", "Black Panthers", "Newfoundland and Labrador", "the NATO stay-behind organizations,", "\"The Mask\"", "\"She of Little Faith\"", "\"Final Fantasy XII\"", "orisha", "Alfred Preis", "Bayern Munich", "the International Conference on LGBT Human Rights", "the country's second largest city by population", "Philip Quast", "841", "Gabrielle-Suzanne Barbot de Villeneuve", "Father Dougal McGuire", "Lucy Muringo Gichuhi (n\u00e9e Munyiri)", "2017", "the Lommel differential equation", "Sky News", "gweilo", "Canada Goose", "the Strait of Gibraltar", "Bart Millard", "In 1931", "the Jaffa cake", "the armpit", "mad", "the piano", "\"Nu au Plateau de Sculpteur,\"", "a federal judge in Mississippi on March 22,", "Darrin Tuck,", "Grambling State University", "Russia", "the kabuki", "Phoebe ( MacKenzie Mauzy )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.575647095959596}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-248", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-2005", "mrqa_hotpotqa-validation-1634", "mrqa_naturalquestions-validation-4414", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-5439", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-381", "mrqa_searchqa-validation-4267", "mrqa_searchqa-validation-2446", "mrqa_searchqa-validation-1023", "mrqa_naturalquestions-validation-8695"], "SR": 0.515625, "CSR": 0.47601744186046513, "EFR": 1.0, "Overall": 0.6916878633720931}, {"timecode": 43, "before_eval_results": {"predictions": ["Bobby Brown", "Bromley-By- Bowen", "\" Bone Wars.\"", "redheaded", "Dodo", "Duke Francis", "Poirot", "George Sand", "Janis Joplin", "July 21", "Gondwana", "rivers", "M69.", "won the most Oscars Walt Disney", "Steptoe and Son", "2011", "Louis Le Vau", "Novak Djokovic", "Midland", "The Centaurs", "Emily Dickinson", "vitamin D", "Usain Bolt", "David Croft", "French", "palelp", "Sam Allardyce", "1951", "Washington", "Dictionary.com", "Trainspotting", "Francisco de Goya", "absinthe", "Ganges", "a crossword puzzle", "AQUAE ARNEMETIAE", "Phaethon", "laryngeal prominence", "twenty", "Carolus", "Granada", "1969", "Paul Maskey", "France", "Fiat", "charity-boy", "Charlie Brown", "mercury", "Tasmanian", "five", "Tony Cozier", "a woman who had a sexual relationship with Paul", "Isaiah Amir Mustafa", "Bonanza Creek Ranch, 15Bonanza Creek Lane, Santa Fe, New Mexico", "Conservative", "1898", "\"The Dragon\"", "the Arab world to use the Internet for fun and not interfere with government and serious issues,", "public opinion", "poems telling of the pain and suffering of children just like her", "The Color Purple", "children's hospitals", "Polar Year", "anaphylaxis,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6189553093964858}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8235294117647058, 1.0, 0.8181818181818182, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-5893", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-6929", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-987", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-4375", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-3189", "mrqa_triviaqa-validation-6111", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-4746", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-3073", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-3059"], "SR": 0.546875, "CSR": 0.47762784090909094, "EFR": 0.9655172413793104, "Overall": 0.6851133914576802}, {"timecode": 44, "before_eval_results": {"predictions": ["woodstock", "kaleidoscopes", "socialism", "henry beery, Jr.", "louis xiii", "cush", "Israel", "\" Robin Hood Men in Tights\"", "The Blue Boy", "leavenworth", "Ascot", "Only Fools and Horses", "Japan", "earth", "cooperative", "Spanish", "henry blizzard", "king Ferdinand", "Joanne Harris", "ancient optical illusion toy", "barrow", "vitamin c", "white setter", "purple", "whey", "\"the meadows,\"", "secretary", "henry king", "rubrolineata", "Big Brother", "henry blizzard", "human rights organization", "seattle", "tabinet", "1833", "jimmy smith", "Jennifer Lopez", "muffin man", "yidu city", "Wat Tyler", "the Blackstone River", "woodstock", "paddy doherty", "henry vincent van Gogh", "hartman", "king england", "robert hooke", "kirkcaldy", "yellow", "brazil", "checker boards", "Jason Flemyng", "mind your manners '', `` mind your language ''", "division", "Welterweight division", "Bonkyll Castle", "21", "melt as soon as 2050,", "three", "Arnold Drummond", "claymore", "nadal", "lake champlain", "nanna popham Britton"], "metric_results": {"EM": 0.34375, "QA-F1": 0.41968984962406014}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8571428571428571, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.631578947368421, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-2397", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-5599", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-796", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-1705", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-5317", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-7732", "mrqa_triviaqa-validation-3813", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-10364", "mrqa_hotpotqa-validation-5029", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2935", "mrqa_newsqa-validation-1828", "mrqa_searchqa-validation-605", "mrqa_hotpotqa-validation-2720"], "SR": 0.34375, "CSR": 0.4746527777777778, "EFR": 0.9523809523809523, "Overall": 0.681891121031746}, {"timecode": 45, "before_eval_results": {"predictions": ["Liberia", "windmill", "chorizo", "arthur", "julia sawalha", "scurvy", "Tony Blair", "Iran", "cogs", "tonsure", "horses", "washington", "lance-corporal", "tainan", "david kynaston", "cast", "pulsar", "cade", "T.S. Eliot", "Fleet river", "adolphe adam", "\"A Metro\u2013Goldwyn Mayer Picture\u201d", "washington", "Francis Matthews", "Mickey Mouse", "Peter Townsend", "gillingham", "across the river", "green", "Malaysia", "blyton", "arthur", "jamaican", "Reservoir", "henry v", "stilts", "Gary Oldman", "13", "five", "benjamin disraeli", "gooseberry", "Lisieux", "Vimto vimto", "arthur i", "Tripoli", "Andrew Jackson", "dali", "Michelin man", "Mull", "turnip", "a samovar", "altitude", "2015", "Elizabeth Dean Lail", "December 1974", "off the northwest tip of Canisteo Peninsula in Amundsen Sea.", "\"Polovetskie plyaski\"", "56", "Obama", "in the neighboring country of Djibouti,", "Hillary Clinton's", "sandman", "Fred Rogers", "Republic of Ireland"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5433517156862745}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 0.0, 0.33333333333333337, 0.5, 0.6666666666666666, 0.5, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-5214", "mrqa_triviaqa-validation-569", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-4732", "mrqa_naturalquestions-validation-413", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4284", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-2341", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-8212", "mrqa_searchqa-validation-11910", "mrqa_hotpotqa-validation-988"], "SR": 0.46875, "CSR": 0.47452445652173914, "EFR": 1.0, "Overall": 0.6913892663043478}, {"timecode": 46, "before_eval_results": {"predictions": ["honey", "h Hesiod", "accordion", "Chicago", "alphabets", "Halloween", "entropy", "hickory", "david mccarthy", "kentucky", "Japanese silvergrass", "willy burke", "ThunderCats", "27", "h Herbert Lom Dies", "Gambia", "table salt", "jamaica", "le Havre", "phobias", "Annie Lennox", "henna", "Hungary", "graphite", "david Jason", "burke", "shanghai", "osculation", "James Chadwick", "old sparky", "aracens", "bankside power station", "spain", "Aldi", "1969", "atrium", "Billy Fury", "hugh Laurie", "zanzibar", "palladium", "cyclops", "John Quidor", "denarii", "p Pablo Picasso", "Narendra Modi", "rabin", "australia", "e", "Superstar", "centaurs", "caiaphas", "June 12, 2018", "Thomas Middleditch", "Mike Mushok", "Hayley Catherine Rose Vivien Mills", "\"White Horse,\"", "October 3, 2017", "five", "Vertikal-T,", "Mexicans who are unemployed or underemployed", "Jeopardy", "Christmas", "abertoberfest", "Emily Blunt"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5744791666666667}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 0.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-7677", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-5422", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-6405", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-5224", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-6202", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2078", "mrqa_naturalquestions-validation-1089", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-3446", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9285", "mrqa_searchqa-validation-9466", "mrqa_naturalquestions-validation-1618"], "SR": 0.484375, "CSR": 0.4747340425531915, "EFR": 0.9696969696969697, "Overall": 0.6853705774500323}, {"timecode": 47, "before_eval_results": {"predictions": ["james ryan", "gland", "kuiper Belt", "364", "alba longa", "The Herald of Free Enterprise", "james blunt", "no matter how much it rains", "Jim hacker", "Wikipedia", "vietnam", "australia", "bertrand russell", "human rights lawyer", "australia", "Mir", "no", "Don and Phil Everly", "the chord", "november", "november", "Cannes Film Festival", "adamemnon", "nigger", "Tombstone", "woodstock", "parkinson's disease", "washington", "translations", "Arctic Monkeys", "highest Honor", "temperature", "algeciras", "november", "a police sergeant (Lee Ingleby)", "Iceland", "eggs benedict", "kent", "today", "newcastle aberland", "jean-Paul gaultier", "Cockermouth", "28", "drew Daniel Nicols", "dry rot", "cleckheaton", "county november", "japan", "Brian Clough", "\"The Practice\"", "no-talent gay director", "Houston Hornets", "a `` skin - changer ''", "Space is the Place", "Elizabethan", "Bob Gibson", "the Corps of Discovery", "different women coping with breast cancer", "\"wacko.\"", "Transportation Security Administration", "an AK47", "airbags", "seva", "coercivity"], "metric_results": {"EM": 0.4375, "QA-F1": 0.48958333333333326}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5190", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-3709", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-7632", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-5710", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-3656", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-2717", "mrqa_triviaqa-validation-1266", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-939", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-5639", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-5235", "mrqa_triviaqa-validation-6764", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-1552", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-4751", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-417", "mrqa_naturalquestions-validation-5927"], "SR": 0.4375, "CSR": 0.47395833333333337, "EFR": 0.9722222222222222, "Overall": 0.685720486111111}, {"timecode": 48, "before_eval_results": {"predictions": ["the ISS", "Akon", "wake", "cycling", "Wrigley", "0-6-0", "priam of troy", "wind turbines", "doe", "city", "army", "greece", "jack bygraves", "14", "7", "madonna", "william russell", "Belarus", "samovar", "brest-Litovsk", "Pete Best", "tigon", "Challenger", "sedimentary", "George Lucas", "the tarsal plate", "jacob metius", "nigeria", "Guatemalan", "james II", "st Moritz", "Christine Keeler", "j\u00f8rn Utzon", "jacob epstein", "lepus hare", "robert kennedy", "japan", "narcolepsy", "james mccartney", "Dirty Dancing", "Saga Noren", "algae", "cardigan", "jacob epstein", "scotia", "zimbabwe", "jacob gently", "scotia", "stoned to death", "Syriza", "russell", "Missi Hale", "Salman Khan", "Battle of Antietam", "CBS", "Lewis Carroll's", "Arsenal Football Club", "of the Movement for Democratic Change,", "\"The Rosie Show,\"", "Eleven people", "goal", "Purple Heart", "to live in a palace", "big cats"], "metric_results": {"EM": 0.40625, "QA-F1": 0.475}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-5779", "mrqa_triviaqa-validation-7161", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-1234", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-65", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-7287", "mrqa_naturalquestions-validation-6806", "mrqa_hotpotqa-validation-1677", "mrqa_hotpotqa-validation-2335", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-335", "mrqa_searchqa-validation-8159", "mrqa_searchqa-validation-15427", "mrqa_searchqa-validation-13413"], "SR": 0.40625, "CSR": 0.4725765306122449, "EFR": 1.0, "Overall": 0.690999681122449}, {"timecode": 49, "before_eval_results": {"predictions": ["lake Mead", "Jesus", "Jamie Lee Curtis", "a porter", "bridge on the river Kwai", "egg", "swaziland", "Donkey", "Niccol Machiavelli", "space shuttle", "Sarah Bernhardt", "fermentation", "Simon Legree", "egypt", "\"Tragedy of Coriolanus\"", "Pinocchio", "the Battle of San Juan Hill", "Drag and drop", "Henry Hudson", "bridge on the river kwai", "ron howard", "Gemini", "a crossword", "The Untouchables", "b Boris Godunov", "Frasier", "principality", "a fudge sundae", "Muhammad Ali", "a howitzer", "Kodak", "a scrambled egg", "paleoconservatism", "r Richard branson", "Arlington", "Andorra", "student loan", "endymion", "a late-night party", "centigrade", "james", "a hamburger", "nautilus", "sarto", "hydroelectric", "high school", "analog", "huckleberry Finn", "olives", "snakes", "Rennie", "frontal lobe", "electron donors", "the blood to the liver", "microwave oven", "jules verne", "birds", "August 10, 1933", "india", "\"boundary river\"", "the Oaxacan countryside of southern Mexico", "Wally", "Wigan Athletic", "Francis Hutcheson"], "metric_results": {"EM": 0.5, "QA-F1": 0.5876420454545455}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-968", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-6407", "mrqa_searchqa-validation-3766", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-16203", "mrqa_searchqa-validation-6276", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-15767", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6795", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-2895", "mrqa_searchqa-validation-8703", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16361", "mrqa_searchqa-validation-9008", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-6380", "mrqa_searchqa-validation-8057", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-511", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-1179", "mrqa_triviaqa-validation-6070", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1698"], "SR": 0.5, "CSR": 0.473125, "EFR": 1.0, "Overall": 0.691109375}, {"timecode": 50, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1450", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4261", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-845", "mrqa_hotpotqa-validation-848", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-22", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12081", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12399", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13413", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14404", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-1474", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16906", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2786", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5469", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7033", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-8703", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-9951", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10344", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2397", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-2953", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3738", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4935", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5190", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5344", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5473", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-978"], "OKR": 0.80859375, "KG": 0.515625, "before_eval_results": {"predictions": ["Armageddon", "Lackawanna Six", "Mark Darcy", "red", "Sparks", "Anne Frank", "Wembley London,", "Uganda", "R\u00edo de la Plata River", "Lady Gaga", "U", "four-6-2", "\u201cSwan Lake\u201d", "Paraguay", "Wales", "Nepal", "Bashir", "Steve Davis", "the Crusades", "(Richard) Hannay", "Rick James", "Hebrew", "Barbadian", "steel", "David Bowie", "Midsomer Murders", "Some Like It Hot", "to celebrate the anniversary of a person's birth", "Russian", "Duke Morrison", "Alex Kramer", "golf", "ram", "Patrick Roy", "Kirgizstan", "Alaska", "le Havre", "rhino", "Pogo Hearts", "Big Ben", "Genesis", "Carmen", "Jenik", "Venus", "Whitney Houston", "igneous", "sweden", "John McEnroe", "Venus", "Tamar", "Thermopylae", "Macon Blair", "Saphira hatches", "Darlene Cates", "Archbishop of Canterbury", "James J. Hill's Great Northern Railway transcontinental railway line", "Two Pi\u00f1a Coladas", "about the shootings,", "Anil Kapoor", "a nuclear weapon", "Hypothermia", "NFL", "Noah Bennet", "local authorities"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6638392857142856}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-4685", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-2430", "mrqa_triviaqa-validation-214", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-3308", "mrqa_triviaqa-validation-3158", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-248", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-6558", "mrqa_naturalquestions-validation-2873", "mrqa_hotpotqa-validation-3417", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-6631"], "SR": 0.609375, "CSR": 0.475796568627451, "EFR": 1.0, "Overall": 0.7076593137254902}, {"timecode": 51, "before_eval_results": {"predictions": ["australia", "g\u00e9rard Depardieu", "Guanabara Bay", "khaki uniforms", "Arthur Hailey", "hot", "English", "johnny taylor", "Dee Caffari", "germany", "Julie Andrews", "Anita Roddick", "duke", "john keats", "john tommie conan", "jean taylor", "gisbert", "european independence", "titanium", "jean taylor", "gretzky", "Wanderers", "spaceflight", "guatemala", "beaver", "philadelphia", "david hockney", "strychnine", "nirvana", "crackerjack", "Buckinghamshire", "bertrand russell", "d\u0113mokritos", "memory-robbing disease", "Gianni Versace", "george best", "My Sweet Lord", "Theodore Roosevelt", "mandible", "european", "birmingham", "calcaneus", "passion", "goat Island", "placebo", "queen", "ecclesiastical", "fluorine", "Dutch", "albinism", "queen", "Jackie Robinson", "Augustus Waters", "1986", "Eliot Cutler", "Lerotholi Polytechnic", "Wu-Tang Clan", "Fort Bragg in North Carolina.", "Mitt Romney", "suicides", "woodcarver", "Delaware", "m Mikhail barysh Lavrov", "Fa Ze bullets"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5733901515151515}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-579", "mrqa_triviaqa-validation-7764", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-2850", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-2272", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-6935", "mrqa_hotpotqa-validation-2492", "mrqa_searchqa-validation-14492", "mrqa_searchqa-validation-10843", "mrqa_naturalquestions-validation-3297"], "SR": 0.546875, "CSR": 0.47716346153846156, "EFR": 1.0, "Overall": 0.7079326923076923}, {"timecode": 52, "before_eval_results": {"predictions": ["james boswell", "Robert Marvin \"Bobby\" Hull, OC", "Vancouver", "Hermione Baddeley", "supreme playwright and poet", "more than 40 million", "in Srinagar", "Winecoff", "Warrington Wolves", "Windermere", "(born February 13, 1946", "her eponymous debut album", "james boswell", "Ghana Technology University College", "Yunnan-Fu (\u4e91\u5357\u5e9c, \"Y\u00fann\u00e1nf\u01d4\")", "Bigfoot", "Chick tract", "Monty Python", "Gainsborough Trinity", "Benny Andersson", "Dana Andrews", "Las Vegas", "George Adamski", "12 April 1961", "Backstreet Boys", "Formula E", "House of Fraser", "KlingStubbins", "Christopher Tin", "Jack St. Clair Kilby", "a skerry", "Humberside", "Polonius", "my Beautiful Dark Twisted Fantasy", "June 11, 1986", "Kathleen O'Brien", "13", "Adult Entertainment Expo", "Long Island", "1,696", "Michelle Anne Sinclair", "Alexander Lippisch", "Linda Ronstadt", "Richa Sharma", "Jack Murphy Stadium", "cricket fighting", "U.S. military", "Louis King", "gamecock", "from 1973 to 1996", "shopping malls", "Brian in the long - running NBC daytime drama Days of Our Lives", "RAF Bovingdon", "to indicate agreement, acceptance, or acknowledgement", "a condor", "eight", "alaska", "a lump in Henry's nether regions was a cancerous tumor.", "Sunday", "150", "red", "caper", "makeup", "Florence"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4932892628205128}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.15384615384615385, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.25, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-2040", "mrqa_hotpotqa-validation-4131", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-427", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5479", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-2046", "mrqa_hotpotqa-validation-5653", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-1581", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-1359", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-7513", "mrqa_newsqa-validation-4028", "mrqa_searchqa-validation-4862", "mrqa_searchqa-validation-14559"], "SR": 0.34375, "CSR": 0.47464622641509435, "EFR": 1.0, "Overall": 0.7074292452830189}, {"timecode": 53, "before_eval_results": {"predictions": ["Teriade", "five", "first Circle-Vision show that was arranged and filmed with an actual plot and not just visions of landscapes, and the first to utilize Audio-Animatronics.", "Argentine", "SM Lifestyle Cities", "2001 NBA All-Star Game", "Ringo Starr", "United States", "Sparky", "James David Lofton", "Samantha Spiro", "New Orleans Saints", "VIMN Russia", "Scottish", "Igor Stravinsky, Carl Orff, Paul Hindemith, Richard Strauss, Luigi Nono, Joaqu\u00edn Rodrigo,", "brigadier general", "Battle of Prome", "Fat Man", "Clovis I", "Salman Rushdie", "Mani", "45th Infantry Division", "a minor basilica", "Wilhelmus Simon Petrus Fortuijn", "Bardot", "Sutton Hoo", "American", "Gust Avrakotos", "President Bill Clinton", "Canadian", "holy servant of Christ", "Australian", "Guthred", "Afghanistan", "1848 to 1852", "City of Peace", "State House in Augusta", "dance", "Sam Bettley", "Roman Republic", "Manchester United", "David Abelevich", "eight", "Colonel Patrick John Mercer, OBE (born 26 June 1956)", "more than 230", "Texas's 27th congressional district", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia.", "Pennacook", "Valeri Vladimirovich \"Val\" Bure", "New York Giants", "Old World fossil representatives", "Hundreds or even thousands", "1987", "the 1930s", "the invasion of Russia was not only the greatest threat", "cotton and wool", "Genghis Khan.", "to share personal information.", "David Beckham", "sportswear", "T rex", "General Hospital", "Joe Lieberman", "San Francisco"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5712292609351433}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.11764705882352941, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5079", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3003", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-486", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8028", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-7348", "mrqa_newsqa-validation-1911", "mrqa_searchqa-validation-12517"], "SR": 0.484375, "CSR": 0.47482638888888884, "EFR": 1.0, "Overall": 0.7074652777777778}, {"timecode": 54, "before_eval_results": {"predictions": ["Woodmere, Hewlett, Cedarhurst, Inwood", "1902", "100 million", "AT&T", "1951", "Cincinnati metropolitan area", "\"The Maze Runner\"", "Gregg Berhalter", "the Ashanti Region", "Wayne Conley", "County Executive", "Disco", "Virginia", "1730", "7 October 1978", "University of Missouri", "Stratfor", "129,007", "the Tangerines", "Symphony No. 7", "Voni Morrison", "boxer", "Liverpool Bay", "1991", "the Crips", "Edward James Olmos", "848", "16\u201321", "Old Town", "the Saint Petersburg Conservatory", "Harold Edward Holt", "Newcastle upon Tyne, England", "Lonestar", "Benjamin Andrew \"Ben\" Stokes", "5,000,000", "Los Angeles", "10 June 1921", "John D Rockefeller", "German", "a heliocentric orbit", "N.I.B.", "Sada Carolyn Thompson", "Polish Army", "21 August 1986", "David Irving", "Neneh Mariann Karlsson", "Norse", "Afro-Russian", "the district of North Kesteven, Lincolnshire", "Captain Marvel", "Koon-hei Chen", "a Celtic people living in northern Asia Minor", "to capitalize on her publicity", "a single layer of flat cells in contact with the basal lamina ( one of the two layers of the basement membrane", "Edward Lear", "India", "10", "Osan Air Base", "30", "carving in the middle of our Mountain View, California, campus", "3800", "dynamite", "Puget Sound", "is being treated there after being admitted on Wednesday."], "metric_results": {"EM": 0.46875, "QA-F1": 0.61973079004329}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4444444444444444, 0.33333333333333337, 0.0, 1.0, 0.8, 0.5, 0.8571428571428571, 1.0, 0.4, 0.0, 1.0, 0.6666666666666665, 1.0, 0.8, 0.0, 0.22222222222222224, 0.19047619047619047, 1.0, 0.0, 0.0, 0.0, 0.5, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-574", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-5881", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-4314", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1763", "mrqa_searchqa-validation-14988", "mrqa_newsqa-validation-1829"], "SR": 0.46875, "CSR": 0.4747159090909091, "EFR": 1.0, "Overall": 0.7074431818181818}, {"timecode": 55, "before_eval_results": {"predictions": ["UTC \u2212 09 : 00", "Milira", "Tsetse can be distinguished from other large flies by two easily observed features", "Norman origin", "April 12, 2017", "the nasal septum", "reduce pressure on the public food supply", "Texas - style chili con carne", "development of electronic computers", "Australia's Sir Donald Bradman", "1975", "shortwave radio", "Eurasian Plate", "Jenny", "Edward G. Robinson", "March 2, 2016", "Glenn Close", "August 18, 1998", "the Serbian army", "skeletal muscle", "The UN General Assembly", "July 2017", "Mike Czerwien", "to ensure party discipline in a legislature", "cells", "24", "Cape Campbell Lighthouse in Marlborough", "2010", "November 5, 2017", "February 7, 2018", "Manchuria", "Brooklyn, New York", "Lex Luger", "in the pouring rain", "Games played", "Paul Revere", "American Revolutionary War", "2015", "Ian Hart", "prenatal development", "Ricky Nelson", "British citizens who were resident in Scotland", "July 4, 1898", "American rock band Panic? at the Disco", "red, white, and blue", "When the others arrive", "Dorothy Gale", "William Strauss and Neil Howe", "to accomplish the objectives of the organization", "October 1927", "six", "Rambo", "earth", "Georgiana Darcy", "119", "Christopher Michael \"Chris\" DeStefano", "his confirmation as OMB Director in 2017.", "September 23,", "44th", "their \"Freshman Year\" experience", "consonants", "Galileo", "Pharmacy", "james boswell"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5971503539862915}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-8346", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-4988", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3011", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-3176", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-7391", "mrqa_searchqa-validation-7319"], "SR": 0.515625, "CSR": 0.4754464285714286, "EFR": 1.0, "Overall": 0.7075892857142858}, {"timecode": 56, "before_eval_results": {"predictions": ["the annual White House Correspondents' Association dinner Saturday,", "rural Tennessee", "Chuck Bass", "he was diagnosed with skin cancer.", "her fianc\u00e9,", "John Wayne", "Cyprus", "Stratfor", "he was injected with drugs by ICE agents against his will.", "May 4", "both countries should be able to take part in NATO's Membership Action Plan, or MAP,", "to reach car owners who haven't complied fully with recalls.", "it was a wrong thing to say, something that we both acknowledge,\"", "The worst snowstorm to hit Britain in 18 years", "one of its diplomats in northwest Pakistan", "three people", "six Africans dead.", "he will be able to gamble in a casino, buy a drink in a pub or see the horror film \"hostel: Part II,\"", "called Israel's actions against Hamas militants \"a gift\" from U.S. President-elect Barack Obama.", "a Columbian mammoth", "J. Crew", "Herman Cain", "\"Watchmen\"", "over the Gulf of Aden,", "Ben Roethlisberger", "$40", "Bill Stanton", "a monthly allowance,", "toxic smoke", "he could have turned everything into a crime scene like O.J. Simpson", "The son of Gabon's former president", "Minerals Management Service Director Elizabeth Birnbaum", "Tim Masters,", "we Found Love", "they did not know how many people were onboard.", "a ban on inflatable or portable signs and banners on public property.", "it was a real journey.", "42 years old", "the refusal or inability to \"turn it off\"", "the Transportation Security Administration", "not guilty", "in a public housing project,", "as many as 250,000", "Washington", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.iReport.com:", "up to $5,600", "Nigeria,", "citizenship", "fighting charges of Nazi war crimes", "about 5:20 p.m.", "he was freed", "the 10th century", "/ ta\u026a\u02c8t\u00e6n\u026ak /", "During his epic battle with Frieza", "Imagine Dragons", "the Basques Basques", "Afghanistan", "Singapore", "consulting", "2006", "Castle Rock Entertainment", "a barber pole", "hatred for someone's race", "Sheev Palpatine"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5305617171793643}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.22222222222222224, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.9523809523809523, 1.0, 0.14814814814814814, 0.16666666666666666, 0.0, 0.2222222222222222, 0.0, 0.6666666666666666, 0.5714285714285715, 0.08, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.16216216216216214, 1.0, 0.5, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-56", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-7115", "mrqa_triviaqa-validation-7106", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-16115", "mrqa_searchqa-validation-6610", "mrqa_naturalquestions-validation-5986"], "SR": 0.390625, "CSR": 0.47395833333333337, "EFR": 1.0, "Overall": 0.7072916666666667}, {"timecode": 57, "before_eval_results": {"predictions": ["allegedly involved in forged credit cards and identity theft", "misdemeanor assault charges", "Manchester United.", "a skilled hacker", "We Found Love", "glass shards", "at least 27", "not looked depressed around me.", "citizenship", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "September 21.", "Two UH-60 Blackhawk helicopters", "Saturday,", "11", "Lebanese", "\"Zed,\"", "Larry King", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Symbionese Liberation Army", "social networking sites", "Kurt Cobain's", "$80,000 a year", "Thursday night", "Arizona", "Ryan Adams.", "Bill Clinton", "auction off one of the earliest versions of the Magna Carta later this year,", "Monday,", "Coptic Church", "Port-au-Prince, Haiti", "Harry Potter and the Order of the Phoenix", "the final resting place for many casualties of the wars in Iraq and Afghanistan.", "all of Lifeway's 100-plus stores nationwide", "2004.", "the man facing up, with his arms out to the side.", "Al-Aqsa mosque", "between Denver and Winter Park.", "cortisone.", "July 1999,", "mated", "1980", "Arsene Wenger", "Ava Zinna ate an allergen-free meal at the Worry Free Dinners event on Sunday.", "on Saturday.", "At least 14", "one-of-a-kind navy dress with red lining", "JBS Swift Beef Company,", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "Toffelmakaren.", "Jason Chaffetz and Jared Polis", "The International Atomic Energy Agency", "Spektor", "Atlanta Hawks", "Atlantic ocean", "downtown Oklahoma City", "Las Vegas", "March 10, 1997,", "Summer Olympic Games", "people working in film and the performing arts,", "The Royal Navy", "a comb", "Korean War", "Philadelphia, Pennsylvania", "The United Arab Emirates"], "metric_results": {"EM": 0.5, "QA-F1": 0.634577922077922}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true], "QA-F1": [0.19047619047619047, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.7272727272727273, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-2269", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-726", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-5567", "mrqa_searchqa-validation-8265"], "SR": 0.5, "CSR": 0.47440732758620685, "EFR": 1.0, "Overall": 0.7073814655172413}, {"timecode": 58, "before_eval_results": {"predictions": ["intravenous vitamin \"drips\"", "club managers,", "city of romance, of incredible architecture and history.", "Olivia Newton-John", "There's no chance", "5", "Ricardo Valles de la Rosa,", "Los Angeles", "40 lashes", "\"I want to express my deepest sympathy to Mikey and his family,\"", "reached an agreement late Thursday", "E. coli", "the United States", "eight", "may", "Museum-worthy pieces", "Susan Atkins", "Bright Automotive,", "The police dogs in Duesseldorf, Germany are now patrolling the pavement in protective shoes that their police-officer handler straps onto their paw.", "Brazil", "American Muslim and Christian leaders", "\"release\" civilians,", "Dr. Jennifer Arnold and husband Bill Klein,", "acid attack", "one", "Lula da Silva", "sailing", "help rebuild the nation's highways, bridges and other public-use facilities.", "try to make viewers feel like they're in good hands with him as Emmy host.", "they did not receive a fair trial.", "Saturday", "wings", "Ross Perot.", "put a lid on the marking of Ashura", "collaborating with the Colombian government,", "evicted", "Iggy Pop", "17", "30-minute", "Sabina Guzzanti", "a hospital", "the Verzasca hydro-electric dam in Switzerland", "billions of dollars", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "a residential dike", "British troops", "last month's Mumbai terror attacks", "Christopher Savoie", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "Sunday.", "Garth Brooks", "2017", "New England Patriots", "Charlene Holt", "Albert Einstein", "Parsley the Lion", "a well.", "American League (AL)", "perjury and obstruction of justice", "December 23, 1977", "orange", "Willa Cather", "Persian Gulf", "bats"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7465104337917003}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.14634146341463414, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9411764705882353, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3474", "mrqa_naturalquestions-validation-9246", "mrqa_triviaqa-validation-6649", "mrqa_hotpotqa-validation-5293", "mrqa_hotpotqa-validation-410"], "SR": 0.65625, "CSR": 0.477489406779661, "EFR": 1.0, "Overall": 0.7079978813559322}, {"timecode": 59, "before_eval_results": {"predictions": ["Kel Mitchell.", "barcode", "Michael Seater", "New York University School of Law.", "2017", "Winter Park, Florida", "USC Marshall School of Business.", "terrorist activity", "New York City", "Max Kellerman", "September 30, 2017", "Adelaide's number one Newstalk radio station", "November 11, 1901", "David Mandel, and Jeff Schaffer, and directed by Schaffer.", "Jahseh Dwayne Onfroy", "Boyd Gaming.", "Port Clinton", "\"Cleopatra\"", "May 5, 1939", "1937", "DS Virgin Racing Formula E Team", "Miss Universe 2010", "domestic cat", "Buddha's delight", "seven", "gesellschaft mit beschr\u00e4nkter Haftung", "James Gregory", "Silvia Navarro", "The Rite of Spring", "Richard Wayne Snell", "December 19, 1998", "1960", "about 26,000.", "The United States presidential election of 2016", "Joshua Rowley", "drummer", "The Austro-Hungarian Army", "beer", "Black Friday", "in Emilia-Romagna, northern Italy,", "bioelectromagnetics.", "English", "St. Louis Cardinals", "the Kalahari Desert", "Helensvale, Queensland", "Danielle Fernandes Dominique Schuelein- Steel", "mastered recordings for many well known musicians,", "Julia McKenzie", "Stephen Crawford Young", "Cody Miller", "1,521", "Puerto Rico ( Rich Port )", "the National September 11 Memorial plaza", "Zeus", "binder", "le Carr\u00e9 Omnibus", "budejovice", "Jeddah, Saudi Arabia,", "Harare.", "Krishna Rajaram,", "Tuscany", "American Civil War", "a police car.", "Westwall,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.657986111111111}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666665, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-3803", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4134", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-5449", "mrqa_hotpotqa-validation-1206", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-10088", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-3661", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-1137", "mrqa_searchqa-validation-8886", "mrqa_triviaqa-validation-4861"], "SR": 0.5625, "CSR": 0.47890625, "EFR": 1.0, "Overall": 0.70828125}, {"timecode": 60, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-255", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2711", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3991", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4476", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5479", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-845", "mrqa_hotpotqa-validation-848", "mrqa_hotpotqa-validation-86", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-22", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-531", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-725", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-1121", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12081", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12399", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13054", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2786", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5469", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-996", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1234", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-1285", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-248", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-2799", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-2953", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-357", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5317", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5344", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-5473", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5701", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7211", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7677", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7764", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-912", "mrqa_triviaqa-validation-974", "mrqa_triviaqa-validation-978"], "OKR": 0.814453125, "KG": 0.47265625, "before_eval_results": {"predictions": ["Home Rule League", "Disha Patani", "Mineola", "Jimmy Ellis", "Vilius Storostas-Vyd\u016bnas", "Mario Lemieux", "Dennis Weaver as Kenneth Yarborough \"K.Y. or Kentucky\" Jones,", "Sugar Ray Robinson", "Pyotr Ilyich Tchaikovsky", "twin sister", "Todd Emmanuel Fisher", "Roger Thomas Staubach", "86 ft", "England", "David Patrick Griffin", "15 October 1988", "People v. Turner", "Lucille D\u00e9sir\u00e9e Ball", "Do Kyung-soo", "Cherokee River", "City of Westminster, London", "Lehmber Hussainpuri", "George Orwell", "1974", "Old World fossil representatives", "fourth-ranking", "\"The Tonight Show\"", "Rebirth", "Peter Chelsom", "Aaron Hall", "Elisha Nelson Manning", "1983", "Douglas Jackson.", "40 million", "Valley Falls", "\"Naked Killer\" (1992)", "Bisexuality", "May 5, 1939", "1959", "in tribute to Eric Morecambe", "Secretary of Defense", "Ambroise Thomas", "Belgian", "first Saturday", "Lochaber, Highland, Scotland", "Firth of Forth", "28,776", "SARS", "8,648", "Moselle", "Cristian S\u00e1ez Vald\u00e9s Castro", "USS", "Mockingjay -- Part 2", "Connecticut", "dal Cristo de la Luz", "Eddie Shoestring", "Paraguay", "graduate from this school district.", "Daniel Wozniak,", "A witness", "hindenburg dust storm", "Dead Ringers", "ethanol", "April 2010."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6628472222222221}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3563", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1570", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-4681", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-1427", "mrqa_triviaqa-validation-5888", "mrqa_newsqa-validation-2695", "mrqa_searchqa-validation-3931"], "SR": 0.546875, "CSR": 0.48002049180327866, "EFR": 1.0, "Overall": 0.7010822233606557}, {"timecode": 61, "before_eval_results": {"predictions": ["\"new chapter\" of improved governance in Afghanistan", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "Somali", "a number of celebrities", "204,000", "Government Accountability Office report", "an antihistamine and an epinephrine auto-injector", "partying", "22", "Lebanese", "Campbell Brown", "his father's parenting skills.", "Los Angeles' George C. Page Museum.", "Haiti", "mated", "apologized", "$7.8 million", "\"entirely an Indian film\"", "\"a striking blow to due process and the rule of law.\"", "the Airbus A330-200", "The eye of Hurricane Gustav", "A large concrete block is next to his shoulder, with shattered pieces of it around him.", "Japan", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "acid attack", "30,000", "opium", "Helmand province, Afghanistan.", "More than 15,000", "Barack Obama", "\"An undated photo of Alexandros Grigoropoulos,", "Ralph Lauren (Rizzoli)", "Euna Lee,", "Consumer Reports", "curfew", "Kurdistan Workers' Party,", "the insurgency,", "U.S. President-elect Barack Obama", "because I opened a lot of shows on the runway", "Spaniard", "CNN", "Aniston, Demi Moore and Alicia Keys", "Dan Parris, 25, and Rob Lehr, 26,", "Coast Guard helicopters and boats, as well as vessels from other agencies,", "\"I've known Mitt and his family for decades. His parents instilled in him a strong work ethic, rock-solid conservative values, and a deep sense of service to others,\"", "drug cartels", "Philip Markoff", "Marie-Therese Walter.", "they did not receive a fair trial.", "Anil Kapoor", "October 29 and November 5.", "privatized", "routing information base ( RIB )", "Arctic Ocean in the north to the Southern Ocean ( or, depending on definition, to Antarctica ) in the south", "Walt Whitman", "Ben Kinglsey", "\"the Oaks\u201d", "Leonard Cohen", "Charlie Wilson", "1905", "a graffunder", "Elisabetta", "Ground traction", "140 million"], "metric_results": {"EM": 0.5, "QA-F1": 0.6423076923076922}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.8571428571428571, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.22222222222222224, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-2306", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-91", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-3784", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-6087", "mrqa_hotpotqa-validation-2151", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-13231", "mrqa_hotpotqa-validation-4810"], "SR": 0.5, "CSR": 0.4803427419354839, "EFR": 1.0, "Overall": 0.7011466733870968}, {"timecode": 62, "before_eval_results": {"predictions": ["he has helped finance the insurgency against", "punish participants in this week's bloody mutiny,", "legitimacy of that race.", "five", "was recognized at \"CNN Heroes: An All-Star Tribute\" as a", "$500,000", "Larry Zeiger", "Thabo Mbeki,", "Phay Siphan, secretary of the Cambodian Council of Ministers.", "cortisone.", "to provide security as needed.", "Miss USA Rima Fakih", "France's famous Louvre museum", "a man's lifeless, naked body", "June 6, 1944,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "climate care,", "semiconductors", "The United States", "Sen. Barack Obama", "The Neptune Pool at Hearst Castle is 104 feet long and 95 feet wide at the alcove.", "Haiti", "Dr. Death in Germany", "Fullerton, California,", "southern city of Naples", "Polo because \"it was the sport of kings.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "his past and his future", "President Nestor Kirchner resigned as leader of the ruling political party Monday following a poor showing in Sunday's elections,", "13-year-old boy", "attempted car-jacking as he dropped his children off at a relative's house,", "Wednesday", "breast cancer.", "1969", "Thabo Mbeki,", "America's infrastructure.", "African National Congress Deputy President Kgalema Motlanthe,", "Meredith Kercher.", "Angola,", "Kurt Cobain", "between 1917 and 1924", "Tokyo", "Jaipur", "Juarez drug cartel.", "my recent 12-day trip to Iran to film a public-television show.", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "\"You people don't make good executives.\"", "suicides", "The new government has two important tasks before it: the writing of a new constitution within a year, and integration of 19,600 Maoist combatants into the security forces.", "10 years", "how health care can affect families.", "One day", "Sebastian Lund ( Rob Kerkovich )", "2001", "HMS Amethyst", "oceania", "worked", "Snowball II", "\"Menace II Society\"", "Dancing with the Stars", "milk", "termite", "Sir Winston Churchill", "a tortoise"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5564267113095238}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 0.0, 0.2, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.04761904761904762, 1.0, 0.0, 0.0, 0.16666666666666669, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6875000000000001, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-965", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3933", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-2132", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-9042", "mrqa_searchqa-validation-4054"], "SR": 0.421875, "CSR": 0.47941468253968256, "EFR": 0.972972972972973, "Overall": 0.6955556561025311}, {"timecode": 63, "before_eval_results": {"predictions": ["Sue Miller", "Roddy Doyle", "Zambezi River", "hepatitis A", "anemia", "Washington State", "zazen", "Abraham Lincoln", "Amherst", "Rand McNally", "Jane Bolling", "Calumet Farm", "Jan Hus", "language", "roulette", "Chesapeake Bay", "Golden, Colorado", "fabric", "Royal Military Academy Sandhurst", "Western fiction", "france", "Battle of Verdun", "Carole King's", "Ritchie", "Betty the Ugly", "Zbigniew Brzezinski", "David Berkowitz", "svengali", "Drumline", "Colorado", "comet Tempel 1", "Hilary Swank", "\"These Boots Are Made for Walkin\"", "Vermont", "paddock", "water levels", "Macbeth", "The Dying Swan", "Cotton Bowl Classic", "Daniel Faraday", "Dracula", "Wind Gods", "economics", "Disturbia", "heartwood", "Zappa", "hypothermia", "axel", "E.E. Cummings", "a bolt", "Sydney", "1924", "1830", "rocks and minerals", "Sabena", "beer", "Black Sea", "Margaret Thatcher", "Hirsch index rating", "Massachusetts", "whether he should be charged with a crime,", "Palestinian Islamic Army,", "President Mohamed Anwar al-Sadat", "1972"], "metric_results": {"EM": 0.5, "QA-F1": 0.6441220238095238}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4433", "mrqa_searchqa-validation-11483", "mrqa_searchqa-validation-7909", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-13794", "mrqa_searchqa-validation-9353", "mrqa_searchqa-validation-8388", "mrqa_searchqa-validation-6383", "mrqa_searchqa-validation-7900", "mrqa_searchqa-validation-1905", "mrqa_searchqa-validation-16303", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-5913", "mrqa_searchqa-validation-11075", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-14528", "mrqa_searchqa-validation-10426", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-16550", "mrqa_searchqa-validation-9134", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-5136", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-13013", "mrqa_naturalquestions-validation-1003", "mrqa_triviaqa-validation-6373", "mrqa_hotpotqa-validation-3165", "mrqa_newsqa-validation-2431"], "SR": 0.5, "CSR": 0.479736328125, "EFR": 1.0, "Overall": 0.701025390625}, {"timecode": 64, "before_eval_results": {"predictions": ["The Bomb Factory", "Captain Cook's Landing Place", "Oneida Community", "Nasty Girl", "1967", "Tallahassee City Commission", "John Mark Galecki", "Titus Lucretius Carus", "23 July 1989", "Jericho Union Free School District", "in the western Highlands of Scotland", "six", "12", "Northrop P-61 Black Widow", "Chief Strategy Officer", "the Beatles", "Italy", "a lauded intellectual", "cruiserweight", "Pac-12", "The Carnabeats", "Shane Meadows", "tempo", "Pope John X.", "Mollie Elizabeth King", "\"Home of the Submarine Force\"", "1957", "two", "\"Grimjack\"", "Battle of Dresden", "Apsley George Benet Cherry-Garrard", "Kansas\u2013Nebraska Act", "1970", "a large green dinosaur.", "November 10, 2017", "Joachim Trier", "11,163", "The Sound of Music", "Disha Patani", "Trey Parker and Matt Stone", "elderships", "the Ruul", "Beno\u00eet Jacquot", "Todd Fisher", "Sir Charles Benedict Ainslie", "James Dean", "Brazil", "Robert Arthur Mould", "Brian A. Miller", "Michael Edwards", "Longford", "`` new version '' of Rent", "location", "September 4, 2000", "South Pacific", "iron", "Trinidad", "Robert Langdon", "1-1", "Brian Smith.", "Bank One", "a parallelogram", "the slave trade", "Dr. Lexie Grey"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7189236111111111}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-3500", "mrqa_hotpotqa-validation-3403", "mrqa_hotpotqa-validation-2879", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-3595", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5526", "mrqa_triviaqa-validation-3684", "mrqa_triviaqa-validation-6596", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2472", "mrqa_searchqa-validation-8371"], "SR": 0.609375, "CSR": 0.4817307692307692, "EFR": 1.0, "Overall": 0.7014242788461538}, {"timecode": 65, "before_eval_results": {"predictions": ["United States", "Russell T Davies", "Squam Lake", "0.500", "1982", "striker", "Peel Holdings", "Dialogues of the Carmelites", "Red", "Ding Sheng", "small forward position", "Homer Hickam, Jr.", "Nick Offerman", "Team Penske", "Lt. Gen. Ulysses S. Grant", "Apalachees", "Bill Lewis", "1993", "Wu-Tang Clan", "David Pajo", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "Philip Quast", "Magnate", "Commerce", "Guns N' Roses", "Albert Bridge", "Laurel, Mississippi", "Vladimir Valentinovich Menshov", "sergeant", "Minnesota", "Dupont Plaza Hotel", "the top 1,500 high schools in the United States", "Kentucky, Virginia, and Tennessee", "White Horse", "David Irving", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan", "Waylon Albright", "Bit Instant", "Tulsa, Oklahoma", "Kevin Spacey", "Nikita Khrushchev", "Algernod Lanier Washington (born July 1, 1976), better known by his stage name Plies,", "ITV", "17", "Kegeyli tumani", "the fourth Thursday", "Larry Eustachy", "Richard Arthur, former owner of the land on which it was built,", "the first and only U.S. born world grand prix champion", "University of Nevada, Reno", "Leona Louise Lewis", "10.5 %", "John Goodman", "Rodney Crowell", "france", "a dove", "a marble campanile, or bell tower", "super-yacht designers", "Brian Mabry", "a man's lifeless, naked body", "Dean Cain", "20 feet", "x", "River Usk"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6659616425241426}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.5, 1.0, 0.8, 0.3076923076923077, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4585", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-2616", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4402", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-1004", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-7155", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-10998", "mrqa_triviaqa-validation-4272"], "SR": 0.53125, "CSR": 0.48248106060606055, "EFR": 1.0, "Overall": 0.7015743371212121}, {"timecode": 66, "before_eval_results": {"predictions": ["44,300", "Walldorf", "Franklin, Indiana", "Levittown", "William Cavendish", "Peter 'Drago' Sell", "three", "New South Wales", "2009", "CBS News", "Ken Howard", "Philip K. Dick", "The Case for Hillary Clinton", "Indooroopilly Shopping Centre", "zoonotic", "Dan Conner", "Morse Road and Karl Road", "187th", "twice", "Mauritius", "Yasiin Bey", "Graham Payn", "28 November 1973", "Charles Quinton Murphy", "Caesars Palace", "Yekaterinburg", "early 20s", "Barbara Bush", "riders are turned upside-down and then back upright", "Wayne Rooney", "1st Marquess of Westminster", "Bill Lewis", "American actress and singer", "Marco Hietala", "Mani", "Everton", "4 April 1963", "Washington, D.C.", "Jay Gruden", "Emad Hashim", "Washington", "Ronnie Schell", "Alan Wray Tudyk", "heavier than a feather", "New Orleans Saints", "London", "hiphop", "Lommel", "1938", "Indian", "Baltimore and Ohio Railroad", "Washington", "203", "in dicots such as buttercups and oak trees, and gymnosperms such as pine trees", "Hartford", "iron", "Red Rock West", "Kurdistan Workers' Party", "Lashkar-e-Tayyiba", "autonomy", "trenchcoat", "Tugboat", "Stars", "Norfolk Island"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6453431984681984}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.28571428571428575, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-809", "mrqa_hotpotqa-validation-5144", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4793", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3255", "mrqa_naturalquestions-validation-8220", "mrqa_triviaqa-validation-2725", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-481", "mrqa_searchqa-validation-32", "mrqa_triviaqa-validation-3945"], "SR": 0.546875, "CSR": 0.48344216417910446, "EFR": 1.0, "Overall": 0.7017665578358209}, {"timecode": 67, "before_eval_results": {"predictions": ["7.6 mm", "Dr. Sachchidananda Sinha", "Setsuko Thurlow", "Kurma", "The difference between tomato paste, tomato pur\u00e9e, and tomato sauce is consistency ; tomato puree has a thicker consistency and a deeper flavour than sauce", "the United States", "Puerto Rico", "John Locke", "a premalignant flat ( or sessile ) lesion", "a virtual reality simulator", "October 29 - 30, 2012", "2003", "a judicial officer, of a lower or puisne court, elected or appointed by means of a commission ( letters patent ) to keep the peace", "Kansas and Oklahoma", "Coconut Cove", "The Enchantress", "John Goodman", "1990 American supernatural horror drama miniseries", "Tom Selleck", "\" pick yourself up and dust yourself off and keep going ', female - empowerment song '' and `` kind of an epiphany song", "in capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "gravitation", "Mike Czerwien", "Cheryl Campbell", "from 13 to 22 June 2012", "Paul Rudd", "the inferior thoracic border", "`` Can't Change Me, ''", "Phillip Schofield and Christine Bleakley", "Nashville, Tennessee", "Evermoist   Whiskey Shivers as Saddle Up", "the concentration of a compound exceeds its solubility", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "seven", "the earliest known official or large - scale", "Missi Hale", "1773", "U.S. Bank Stadium in Minneapolis, Minnesota", "1997", "British Army soldiers shot and killed people while under attack by a mob", "scythe", "right to be served in facilities which are open to the public -- hotels, restaurants, theaters, retail stores, and similar establishments '', as well as `` greater protection for the right to vote ''", "Angola", "more than 420", "1948, 1949, and 1960", "more than a million", "in the late industrial revolution", "California, Utah and Arizona", "pit road speed", "deceased - donor ( formerly known as cadaveric ) or living - donor transplantation", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "The Sea of Azov", "Cantonese zaap6", "a\u00b7me\u00b7lio\u00b7rat\u00b7ed", "National Association for the Advancement of Colored People (NA NAACP)", "Clarence Nash", "Roslyn Castle", "trading goods and services without exchanging money", "state senators", "Kyra and Violet", "a bass drum", "porcelain", "Winston Rodney", "in the Muslim north of Sudan"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5231706421863871}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.3333333333333333, 0.0, 0.0, 0.0909090909090909, 0.0, 1.0, 1.0, 0.8333333333333333, 0.35294117647058826, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6086956521739131, 0.0, 0.5, 1.0, 0.9090909090909091, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.5454545454545454, 0.0, 0.4, 1.0, 0.7499999999999999, 0.09090909090909091, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-7270", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-5674", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-10603", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-10271", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-4434", "mrqa_triviaqa-validation-2262", "mrqa_hotpotqa-validation-1720", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-10401", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-4479", "mrqa_newsqa-validation-1755"], "SR": 0.359375, "CSR": 0.4816176470588235, "EFR": 0.975609756097561, "Overall": 0.6965236056312769}, {"timecode": 68, "before_eval_results": {"predictions": ["the family, which remains united and strong despite the \"tremendous hardship,\" will release more information soon.", "U.S. Consulate in Rio de Janeiro,", "different women coping with breast cancer", "can play an important role in Afghanistan as a reliable NATO ally. The question is: How can", "burned over 65 percent of his body", "AbdulMutallab", "1994", "opium", "his bong smoking scandal", "183", "Stoke City.", "Angels", "between 1917 and 1924", "calls for Reid's dismissal.", "103", "Ameneh Bahrami", "President Obama and Britain's Prince Charles", "Columbia, Illinois,", "his Seattle home.", "Unseeded", "Wednesday.", "The station", "brutal", "Bright Automotive, a small carmaker from Anderson, Indiana,", "the Russian air force,", "tie salesman", "identity documents", "Silvio Berlusconi's Mediaset TV network.", "$8.8 million", "Ameneh Bahrami", "five", "Sonia Sotomayor", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "\"a striking blow to due process and the rule of law.\"", "her son has strong values.", "a cancer-causing toxic chemical.", "a baseball bat", "64,", "gasoline", "urgently to be rescued, fearing the crew could be harmed or killed,", "Appathurai", "Fort Bragg in North Carolina.", "ancient Egyptian antiquities in the world", "Ashley \"A.J.\" Jewell,", "prison inmates.", "up three", "Jenny Sanford,", "Saturday.", "cities throughout Canada.", "dozens", "Drew Kesse,", "chlorine and bromine from manmade organohalogens", "complex sentence", "1955", "1825", "a marrum", "M. Bing", "Statue of Liberty", "XXIV Summer Universiade", "Mickey\\'s Twice Upon a Christmas", "Captain Kangaroo", "Adlai Stevenson", "White Tiger", "Allan McNish (born 29 December 1969)"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6611263736263737}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-1891", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-3331", "mrqa_naturalquestions-validation-654", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6986", "mrqa_hotpotqa-validation-1870", "mrqa_hotpotqa-validation-2459", "mrqa_searchqa-validation-9019", "mrqa_hotpotqa-validation-2473"], "SR": 0.546875, "CSR": 0.48256340579710144, "EFR": 0.9655172413793104, "Overall": 0.6946942544352824}, {"timecode": 69, "before_eval_results": {"predictions": ["VoteWoz.com", "Thomas,", "Ben Roethlisberger", "Mutassim,", "Mumbai suburb of Chembur,", "The sailboat, named Cynthia Woods,", "the administration's progress,", "Dr. Jennifer Arnold and husband Bill Klein,", "Alison Sweeney,", "Quebradillas.", "skeletal remains", "Dr. Cade", "Arsene Wenger", "Ronnie White,", "$40 and a loaf of bread.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "#notakidanymore", "Spaniard", "initiative to develop a common approach to combat global warming", "intention to set up headquarters in Dublin.", "buckling under pressure from the ruling party.", "Izzat Ibrahim al-Douri,", "southern port city of Karachi,", "explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Cash for Clunkers", "June 20 and July 20.", "Nicole", "Congress", "Shanghai", "18", "homicide.", "gasoline", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Sheikh Sharif Ahmed", "heavy turbulence", "bartolom\u00e9 de las Casas", "eight.", "11th year in a row.", "0-0 draw", "Angelo Nieves, an Orange County Sheriff's Department commander,", "Abdullah Gul,", "2010", "in her home", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "Max Foster,", "Anne Frank,", "Web", "UNICEF", "Citizens of the lower house of parliament,", "Secretary of State", "Basel", "Rafael Nadal", "Number 4, Privet Drive, Little Whinging in Surrey, England", "International System of Units ( SI )", "bantu", "George Washington's", "protons", "Perth, Western Australia", "Rocky Boy's Indian Reservation", "gwailou", "Fram", "Golden Girls", "J", "hillsborough"], "metric_results": {"EM": 0.578125, "QA-F1": 0.676165110930736}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.375, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2070", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-834", "mrqa_hotpotqa-validation-2241", "mrqa_triviaqa-validation-1046"], "SR": 0.578125, "CSR": 0.4839285714285714, "EFR": 0.9629629629629629, "Overall": 0.6944564318783069}, {"timecode": 70, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1894", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-1939", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-368", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4205", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5634", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-927", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3281", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1213", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3445", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-603", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-998", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10038", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-1020", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13535", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-7455", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3523", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3813", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5411", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-995"], "OKR": 0.80859375, "KG": 0.4546875, "before_eval_results": {"predictions": ["the endgame", "the Boer War", "lousiana", "A Christmas Story", "Golden Hind", "astronomer", "germany", "Marlon Brando", "1997", "Return of the Jedi", "Earnhardt Jr.", "Billy Graham", "Doug Hamilton", "cayenne pepper", "I Am Legend", "October", "Swamp Thing", "Sam Adams", "Matt Leinart", "Nirvana", "a gull", "nitrous oxide", "Martin Luther", "Swaziland", "Nutty Professor II", "Donna Summer", "chinese", "\"Let there be light\"", "Carver", "the Bastille", "\"LOVE\"", "a no commitment to a decision", "a good typist is faster than the 40", "St. Francis of Assisi", "a DSLR", "chinese", "green cards", "Long Island Sound", "Gatsby", "War Bond", "Bryant", "Charles R. Tunley", "Mercedes-Benz", "Harvard", "Captain Claire Chennault", "Jack Sparrow", "Samuel Beckett", "Christmas", "a third-degree burn", "High Tor", "Sinatra", "Thirty years after the Galactic Civil War", "Muslim women to dress modestly and cover their breasts and genitals", "15", "venezuela", "\"for an epic and psychological narrative art which has introduced a new continent into literature\"", "sand", "Humberside Airport (IATA: HUY, ICAO: EGNJ)", "Don Johnson", "England international player", "death and destruction,", "braces.", "issued his first military orders as leader of North Korea", "wednesday"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4918518981018981}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.15384615384615385, 0.0, 0.14285714285714288, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-9755", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-13102", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-16873", "mrqa_searchqa-validation-5502", "mrqa_searchqa-validation-7325", "mrqa_searchqa-validation-9352", "mrqa_searchqa-validation-16807", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-11506", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-13391", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11411", "mrqa_searchqa-validation-10378", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2544", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-14784", "mrqa_searchqa-validation-15018", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-9467", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-6847", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-3862", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-2778", "mrqa_triviaqa-validation-3882"], "SR": 0.40625, "CSR": 0.4828345070422535, "EFR": 1.0, "Overall": 0.6976606514084507}, {"timecode": 71, "before_eval_results": {"predictions": ["The church that was lukewarm and insipid ( to God )", "Evermoist", "the front of the body", "Mitch Murray", "Leon Huff", "Gospel of John", "Hellenism ( \u1f48\u03bb\u03c5\u03bc\u03c0\u03b9\u03b1\u03bd\u03b9\u03c3\u03bc\u03cc\u03c2 )", "Wendy Fraser", "a large, high - performance luxury coupe", "XLII", "the Jurchen Aisin Gioro clan in Manchuria", "Laura Jane Haddock", "Justin Bieber", "December 31, 1971", "Pasek & Paul", "1945", "Viet Minh", "Colman", "federal government", "spoiled, bedridden daughter of wealthy businessman James Cotterell ( Ed Begley )", "Claims adjuster", "the intersection of Del Monte Blvd and Esplanade Street", "Cody Fern", "247.3 million", "vapor pressure", "Australia", "Scarlett Johansson", "Article 1, Section 2, Clause 3 of the United States Constitution", "908 mbar", "spontaneous fission", "a premalignant flat ( or sessile ) lesion of the colon", "Andreas Vesalius", "JackScanlon", "Alastair Cook", "Squamish, British Columbia, Canada", "2003", "Chandan Shetty", "Germany", "October 2008", "1983", "James Brown", "Kyla Coleman", "UTC \u2212 09 : 00", "IB", "Andy", "Ernest Rutherford", "February 2017", "June 12, 2018", "Pittsburgh", "775", "Amitabh Bachchan", "to the south west of the city of Norwich,", "Tina Turner", "a Bangladeshi monetary unit", "the CayenneSU in 2003", "Karl Kraus", "Smith Act", "22-year-old", "suicides", "warning", "Jonah", "a Mustang", "Aladdin", "The St Andrews Agreement"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6696165966386554}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9411764705882353, 1.0, 1.0, 0.4, 1.0, 1.0, 0.7499999999999999, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6448", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-4993", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-6344", "mrqa_hotpotqa-validation-1399", "mrqa_newsqa-validation-1805", "mrqa_searchqa-validation-15395"], "SR": 0.578125, "CSR": 0.48415798611111116, "EFR": 1.0, "Overall": 0.6979253472222222}, {"timecode": 72, "before_eval_results": {"predictions": ["Amanda Barrie", "Australia", "Helen Gurley Brown", "Sting", "Burma", "Maerten Tromp", "Isaac Newton", "Etruscans", "raven", "both", "Franz Joseph Haydn", "Mr. Brainwash", "Charles I", "Athina,", "March", "Local Defence Volunteers (LDV)", "Caesar", "Theresa May", "a linesider", "ourselves", "GMB", "David Bowie", "Liberator", "hedgehog", "Aaron", "Dik Browne", "known tin deposits", "soap", "Kevin Spacey", "yalta", "a shoji", "comedo", "in the British city of Leicester,", "snapdragons", "hongi", "Angela Merkel,", "scottish", "chess", "switzerland", "pea", "Fiji", "Ireland", "victoria cricket", "a burrow", "\"Archer\"", "The Longest Day", "1619", "Jimmy Carter", "Jimmy Carter", "Max Planck", "drag club", "The Church of England", "1960", "lustrous, purple - black metallic solid", "Knoxville, Tennessee", "Patricia Veryan", "Melbourne Storm", "\"The Da Vinci Code\"", "sovereignty over them.", "May 4", "Song of Solomon", "\"100 Greatest Guitarists\"", "Anthony Minghella", "Carl Perkins"], "metric_results": {"EM": 0.625, "QA-F1": 0.6709449404761905}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-6960", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-3422", "mrqa_triviaqa-validation-123", "mrqa_naturalquestions-validation-9054", "mrqa_hotpotqa-validation-626", "mrqa_searchqa-validation-8757", "mrqa_hotpotqa-validation-5073"], "SR": 0.625, "CSR": 0.48608732876712324, "EFR": 1.0, "Overall": 0.6983112157534246}, {"timecode": 73, "before_eval_results": {"predictions": ["TOSLINK", "American Idol", "A Little Princess", "1986", "MGM Resorts International", "2,627.", "Matt Groening", "a scholar during the Joseon Dynasty", "Ronald Joseph Ryan", "Carson City", "\"Israel movement of ex-senior security officials (IDF, Mossad, Shin Bet and Israel Police),", "Meghan Markle", "Carrefour", "ABC", "Metro Memphis", "England", "diving duck", "\"godfather\"", "\"Darconville\u2019s Cat\"", "November of that year.", "Jos\u00e9 Bispo Clementino dos Santos", "switzerland", "torpedoes", "\"Tromeo and Juliet\"", "heaviest album of all", "850 m", "Talbot School of Theology at Biola University in La Mirada, California.", "various names,", "Shaun of the Dead", "more than 40 million", "wilton mall", "Sunday evenings", "Seth MacFarlane, Mike Barker, and Matt Weitzman", "Attack the Block", "Isabella II", "11,791.", "seven members", "Shane Meadows", "David Cook", "win world titles", "Leslie James \"Les\" Clark", "Montreal, Quebec,", "Venice, Florida", "bass", "southwestern corner", "Military Band of Hanover,", "Carnforth railway station", "Chiltern Hills", "Gold Coast in Queensland", "Luigi Segre", "Mark \"Chopper\" Read", "monounsaturated", "food and clothing", "Bhupendranath Dutt", "HMS Conqueror", "avocados", "london", "Thursday", "Spain,", "state senators", "Frederick Douglass", "Galileo Galilei", "Valentino", "pistol"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6151324803668554}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false], "QA-F1": [0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.14285714285714288, 0.8, 1.0, 1.0, 0.0, 0.125, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-623", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5216", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-4957", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-2026", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-1122", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-5721", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-8163", "mrqa_triviaqa-validation-5811", "mrqa_triviaqa-validation-6185", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-563", "mrqa_searchqa-validation-9417", "mrqa_searchqa-validation-4852", "mrqa_searchqa-validation-3369"], "SR": 0.4375, "CSR": 0.4854307432432432, "EFR": 1.0, "Overall": 0.6981798986486486}, {"timecode": 74, "before_eval_results": {"predictions": ["William Shakespeare", "three", "Oklahoma", "Santa Fe", "seven", "La vendedora de rosas", "Flavivirus", "143,372", "Crown Holdings Incorporated", "E22", "Bea Arthur", "Honolua Bay, Maui", "1912", "quarterly", "microbrewery", "Anna Clyne", "Debbie Harry", "American", "Kneeland Street", "May 4, 2004", "NATO", "seven", "four", "69.7 million", "Lego", "American football running back", "MGM Grand Garden Special Events Center", "Acela Express", "the second", "Abidjan, Ivory Coast", "Jane Ryan", "Perfect Strangers", "Hugh Grosvenor, 3rd Marquess of Westminster", "1985", "Melissa Ivy Rauch", "The Cosmopolitan in Las Vegas", "Vince Guaraldi", "Sergeant First Class", "1.5 million", "The Times Higher Education Guide", "1942", "Tim Allen", "Rome", "World Famous Gold & Silver Pawn Shop", "New York Giants", "1932", "Scotiabank Saddledome", "Rogue", "Electronic Attack Squadron 135 (VAQ-135), known as the \"Black Ravens\"", "Leona Lewis", "\"An All-Colored Vaudeville Show\"", "reared in South Africa", "under the supervision and control of Accounting Standards Board ( ASB )", "Thomas Jefferson", "Playboy", "Perth", "Puerto Rico", "Tupolev Tu-160 strategic bombers", "cardio", "GOP state senators", "Re-Animator", "deuteronomy 29 - Renewal of the Covenant", "Saks Fifth Avenue Enterprises", "muskets, fowling pieces or no weapon at all"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6629218871406372}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2, 1.0, 0.8, 0.75, 0.4, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-3545", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-4078", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-2552", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-2265", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-10411", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4055", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-6962", "mrqa_triviaqa-validation-3975"], "SR": 0.515625, "CSR": 0.48583333333333334, "EFR": 1.0, "Overall": 0.6982604166666666}, {"timecode": 75, "before_eval_results": {"predictions": ["December 23, 1977", "Ars Nova Theater", "Venice", "Dave Cook", "\"Slaughterhouse-Five\"", "Two escorts, Harlow Cuadra and Joseph Kerekes", "Marine Corps Air Station Kaneohe Bay", "paternalistic policies", "Philadelphia", "1916", "Carlos Coy", "The Fault in Our Stars", "Love Letter", "Eucritta melanolimnetes", "Catwoman", "Revengers Tragedy", "Al D'Amato 55% to 44%", "United States Navy", "National Hockey League (NHL)", "February", "Oklahoma Sooners", "24 \"tank\u014dbon\"", "3,000", "Golden Globe Award", "United Healthcare", "The virus is zoonotic,", "1987", "Dwarka", "Perth", "Kentucky Wildcats", "I Am Furious (Yellow)\"", "Trey Parker", "Tamara Ecclestone Rutland", "North Dakota and Minnesota to the south", "Augusta", "broadcaster", "John Boyd Dunlop", "Merrimack County", "Mexico", "Peter Townsend", "Elbow", "five times", "2269", "Bonny Hills", "Wandsworth, London", "Alexander Lippisch", "Netflix", "more than 110 films", "1874", "statistics", "Mark Alan Dacascos", "the president", "Mel Gibson", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor ) is a fictional character and one of the primary antagonists of the Star Wars franchise", "Lehman Bros International", "Bedser", "William Lamb", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "drama of the action in-and-around the golf course", "$40 and a loaf of bread.", "Alexander the Great", "Greece", "mass", "eight"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6976495726495726}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.5, 0.5, 0.8571428571428571, 0.5714285714285715, 0.8, 0.8, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4718", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-3529", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1599", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-1848", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-1383", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-1439", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-5375", "mrqa_naturalquestions-validation-5986", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-6384", "mrqa_newsqa-validation-2481", "mrqa_naturalquestions-validation-225"], "SR": 0.53125, "CSR": 0.4864309210526315, "EFR": 1.0, "Overall": 0.6983799342105262}, {"timecode": 76, "before_eval_results": {"predictions": ["third studio album, \"Nina\"", "James Harrison", "Paula D'Alessandris", "Guardians of the Galaxy Vol. 2", "Floyd Casey Stadium", "1948", "Stage Stores, Inc.", "Jane Mayer", "England", "Talib Kweli", "Australian Defence Force", "Lowestoft, Suffolk", "Chrysler", "banjo player", "The MGM Grand Las Vegas", "Michael Phelps", "2006 St. Louis Cardinals season", "Maurice Ravel", "C. H. Greenblatt", "48,982", "erotic romantic comedy", "Africa", "Neon City", "Althea Rae Janairo", "Roc-A-Fella Records and Priority Records", "The Catholic Church in Ireland", "the Netherlands", "The Division of Cook", "1919", "Qu'est-ce qu'on a fait au Bon Dieu", "1970", "Canadian-American Association of Professional Baseball", "City and County of Honolulu", "Hirsch index rating", "Sydney", "1,925", "the Las Vegas Strip in Paradise, Nevada", "Battleship", "Austrian", "Naomi Wallace", "Chief Strategy Officer", "John M. Dowd", "Bruce R. Cook", "Hockey Club Davos", "4,000", "Neighbours", "Super Junior", "Nashville", "Dabestan-e Mazaheb", "Bombay Talkie", "Trey Parker and Matt Stone", "The Forever People", "the alluvial plain", "the Thane of Lochaber", "pomegranate", "Daedalus", "mackinaw", "a fan", "Tom Hanks", "the bedrooms of their two-floor home", "eastern Nevada", "The Jefferson Airplane", "The Dallas-Fort Worth-Arlington MSA", "The Society of Union Employees"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6594426406926407}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3946", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-4159", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-2860", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1300", "mrqa_newsqa-validation-1153", "mrqa_newsqa-validation-624", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-5822", "mrqa_triviaqa-validation-6174"], "SR": 0.578125, "CSR": 0.4876217532467533, "EFR": 1.0, "Overall": 0.6986181006493506}, {"timecode": 77, "before_eval_results": {"predictions": ["\"Odorama\"", "Ali B.", "19th District", "Ginger Rogers", "J.R. R. Tolkien", "Timothy Allen Dick", "St Augustine\\'s Abbey", "My Father", "fennec fox", "Civic Arena", "VH1's \"100 Greatest Artists of Hard Rock\":", "My Backyard", "three years in prison", "1995 to 2012", "middleweight", "Tudor City", "\"Histoires ou contes du temps pass\u00e9\"", "various registries", "north", "Prada", "1963", "The Bomb Factory", "India", "Albert Park", "Angel Parrish", "The Chamber", "1,467 rooms", "The Nassau Herald", "Christian", "276,170 inhabitants", "PPG Paints Arena", "51,438", "James Brayshaw", "Chris Hemsworth", "Bureau of Investigation", "Inverness", "1977", "Donald Sterling", "American", "Hanna", "Sleepy Hollow", "Westfield Old Orchard", "German", "pre-Hispanic and contemporary music", "Aircraft", "Lionsgate", "Roy Spencer", "Japan", "Whoopi Goldberg", "Trey Snyder", "Unbreakable", "Stephen Foster", "in vitro", "Yuzuru Hanyu", "Queen Victoria and Prince Albert", "Palatine Hill", "The ground has had both ends re-developed during the 1990's", "Brian Smith.", "Symbionese Liberation Army", "Ten South African ministers and the deputy president", "Miles Davis", "doges", "Zionism", "Cincinnati"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7405763507326008}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.28571428571428575, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.4, 0.923076923076923, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4716", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-2912", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-2935", "mrqa_naturalquestions-validation-7226", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-855"], "SR": 0.640625, "CSR": 0.48958333333333337, "EFR": 0.9130434782608695, "Overall": 0.6816191123188406}, {"timecode": 78, "before_eval_results": {"predictions": ["Professor Eobard Thawne", "1995 Mitsubishi Eclipse", "moral", "Rigg", "Simon Callow", "early - to - mid fourth century", "2.5 %", "IBM", "Speaker of the House of Representatives", "a solitary figure who is not understood by others, but is actually wise", "Claudia Grace Wells", "TC", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Fix You", "Matt Jones", "Barry Bonds", "Teri Garr", "during the Siderian period, at the beginning of the Proterozoic eon", "in the United Kingdom ( with the exception of Scotland since August 1, 2016 )", "conquistador Francisco Pizarro", "Marty Robbins", "1933", "product-market fit", "primary consumers ( herbivores )", "1924", "Danny Veltri", "RMS Titanic", "Buddhism", "Americans who served in the armed forces and as civilians", "Eddie Van Halen", "foreign investors", "49 cents", "Tamil Nadu in the south to West Bengal in the north", "Vicente Fox", "Richard Crispin Armitage", "Edward IV of England", "December 19, 1971", "Thomas Edison", "four volumes", "William Roy Hodgson", "about 26,000 light - years", "13 May 1787", "2017 / 18 Divisional Round game", "Hathi Jr", "a young husband and wife", "Michael Crawford", "John Adams", "Mount Sinai", "West Norse sailors", "Ali", "142,907 residents", "germany", "Charlie Cairoli", "watcher", "American pharmaceutical company", "seven members", "Robert Digges Wimberly Connor", "a U.S. helicopter crashed in northeastern Baghdad", "dismissed all charges Wednesday night and ordered the release of the four men", "November 26,", "Jack Lewis", "Monoceros", "the Constitution", "inflation"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6576664977256725}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6956521739130436, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818181, 1.0, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4210526315789474, 0.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.923076923076923, 0.42857142857142855, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-7217", "mrqa_triviaqa-validation-5097", "mrqa_triviaqa-validation-6790", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3805", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-11100"], "SR": 0.53125, "CSR": 0.4901107594936709, "EFR": 0.9666666666666667, "Overall": 0.6924492352320675}, {"timecode": 79, "before_eval_results": {"predictions": ["a sixth season", "Kenny Chesney", "Stephen Stills", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in the ark of the covenant", "mid-March", "in a cell", "David Ben - Gurion", "Billy Idol", "Peking", "2016", "pathology", "Reginald Jeeves", "France", "Americans", "Gertrude Niesen", "U.S. service members who have died without their remains being identified", "1996", "between 1923 and 1925", "Jeff Lynne", "Shakespearean actresses and car salespeople", "a person's personal, work, or school life, as well as sleeping, eating habits, and general health", "Nawab Sir Sahibzada Abdul Qayyum", "dress shop", "Jason Lee", "Massachusetts", "Warren Hastings", "Charles County", "September 30", "Bob Dylan", "A simple majority", "within a dorsal root ganglion", "December 12, 2017", "around 3.45 billion years ago ( 2.45 Ga ), during the Siderian period, at the beginning of the Proterozoic eon", "1913", "16", "the early 1960s", "across western North Carolina including Asheville, Cashiers and Saluda", "winter festivals", "Glenn Frey", "Cheryl Campbell", "7", "Road / Track ( no `` and '' )", "capillary action", "Bonnie Aarons", "twelve", "Bartolomeu Dias", "the anterolateral corner of the spinal cord", "The outermost layer of human skin", "Paracelsus", "the season five episode `` Aqua ''", "table salt", "squid", "Angiotensin Converting Enzyme (ACE)", "Erich Maria Remarque", "Danny Elfman", "SpongeBob SquarePants", "Dublin.", "University of California San Diego", "a colonel in the Rwandan army,", "War of the Worlds", "the Time Warp", "The Three Stooges", "Mikado"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6675686612364243}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.7894736842105263, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.9333333333333333, 1.0, 0.0, 0.4, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1111111111111111, 1.0, 0.25000000000000006, 0.25000000000000006, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-2671", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-3132", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-9047", "mrqa_naturalquestions-validation-7239", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-918", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-393"], "SR": 0.546875, "CSR": 0.4908203125, "EFR": 0.9310344827586207, "Overall": 0.6854647090517241}, {"timecode": 80, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-368", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4205", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-774", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-603", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13391", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-995"], "OKR": 0.822265625, "KG": 0.4875, "before_eval_results": {"predictions": ["the player will be able to control other characters such as Luke Skywalker and Kylo Ren", "Gayla Peevey", "November 27, 2013", "the United States", "22 November 1914", "students defensive techniques", "Bali, Indonesia", "Pakhangba", "Bart Cummings", "David Motl", "Daya Jethalal Gada", "Kristy Swanson", "Oklahoma native Major General Clarence L. Tinker", "the homicidal thoughts of a troubled youth", "Castleford", "1969", "the National Assembly", "dispense summary justice", "South Africa", "needle - like", "regulatory site", "Ernest Rutherford", "Roger Federer", "Audrey II", "Walter Pauk", "1999", "Ashley Roberts", "111", "a transformative change of heart ; especially : a spiritual conversion", "Rachel Sarah Bilson", "Mark Jackson", "the Mayor's son", "Procol Harum", "1,149 feet ( 350 m )", "Thawne", "1912", "Bart Millard", "The User State Migration Tool ( USMT )", "free to anyone who, in one hour or less, can eat the entire meal, consisting of the steak itself, a bread roll with butter, a baked potato, shrimp cocktail, and a salad", "Bob Dylan", "Firoz Shah Tughlaq", "4 January 2011", "the digitization of social systems", "Dan Stevens", "depicting multiple alternative realities rather than a novel", "an Islamic shrine located on the Temple Mount", "at Tandi, in Lahaul", "Arnold Schoenberg", "a centre for international trade", "2010", "79 official PGA Tour events", "ping-pong", "Crystal Gayle", "horiz\u014dn kyklos", "Nicholas John \" Nick\" McCarthy", "Texas Tech University", "3", "Michelle Rounds", "The Bronx County District Attorneys Office", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and", "Jean-Paul Marat", "Chocolate Pecan pie", "Paris", "3"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7339130096483037}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true], "QA-F1": [0.07407407407407407, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-5831", "mrqa_triviaqa-validation-7532", "mrqa_searchqa-validation-6250", "mrqa_searchqa-validation-15255"], "SR": 0.703125, "CSR": 0.49344135802469136, "EFR": 1.0, "Overall": 0.7008757716049383}, {"timecode": 81, "before_eval_results": {"predictions": ["Gillis Grafstr\u00f6m", "eight episode series", "December 1, 2017", "Baker, California, USA", "white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "Kareem Abdul - Jabbar", "Roxette", "all transmissions", "Thomas being one of Jesus'disciples", "Bidar", "1924", "18 m ( 59.05 ft )", "Joanna Moskawa", "Tara / Ghost of Christmas Past", "runoff", "Hellenism", "Zoe McLellan", "Portugal", "the American Civil War", "Jenny", "Harry Kane", "Kate Walsh", "France's Legislative Assembly", "Sunday evenings", "the com TLD, which as of December 21, 2014, had 115.6 million domain names, including 11.9 million online business and e-commerce sites", "Brooklyn, New York", "April 12, 2017", "Hercules", "Australia", "the employer", "Tracy McConnell", "the courts", "early 2014", "marks locations", "Janie Crawford, an African - American woman in her early forties", "a pop ballad", "T - Bone Walker", "in the thylakoid lumen", "Angus Young", "Mickey Mantle", "Norman Pritchard", "Conrad Lewis", "Exodus 20 : 7", "Startup neutron source is a neutron source used for stable and reliable initiation of nuclear chain reaction in nuclear reactors,", "3", "Bon Jovi", "Elvis Presley", "September", "f\u0254n", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "the semilunar pulmonary valve", "ghent", "greece", "David Lodge", "PEN America", "Indian film playback singer, director, writer and producer", "Baudot code", "pipelines and hostage-taking", "President Robert Mugabe", "Three aid workers", "The Devil\\'s Dictionary", "Johann Wolfgang von Goethe", "Band of Brothers", "the Rat"], "metric_results": {"EM": 0.578125, "QA-F1": 0.662054061578518}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.5, 1.0, 0.8, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 0.2727272727272727, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2524", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-10486", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-7385", "mrqa_triviaqa-validation-4882", "mrqa_hotpotqa-validation-367", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3947", "mrqa_searchqa-validation-11427"], "SR": 0.578125, "CSR": 0.4944740853658537, "EFR": 0.9259259259259259, "Overall": 0.686267502258356}, {"timecode": 82, "before_eval_results": {"predictions": ["Ithna", "New York Mets", "Thomas Jefferson", "a sheep", "a sweatshirt", "cramming", "Alice", "Basques", "Radames", "programming", "The Three Musketeers", "a man with a fragile false identity", "(Marsha) Hunt", "(Pierre) Renoir", "Zhou Enlai", "a fox", "Vajrayana", "a beagle", "Qwerty", "the vest", "a heart", "(Saudi Arabia)", "a meter", "Close Encounters", "phloem", "Inspector Clouseau", "Carnation", "the Central Intelligence Agency", "a turquoise", "a nuclear submarine", "( Alexander) Bell", "2,524,125", "Vaslav Nijinsky", "William Wordsworth", "mercury", "Sidney Sheldon", "a spectator", "Charley", "The Weekly World News", "the Aleutian Islands", "the House of Representatives", "Roman Catholicism", "Jor-El", "Devo", "an eponym", "Middle-Earth", "embalming", "Hong Kong", "ride", "Schwarzenegger", "Rachel Carson", "1922", "1994", "Anna Faris", "Salyut 1", "Bobby Vinton", "fingers", "conservative", "Jennifer Aniston", "Eduard Leopold", "Russia", "newcastle", "misdemeanor assault charges", "Sim\u00f3n Jos\u00e9 Antonio de la Sant\u00edsima Trinidad de Bol\u00edvar y Palacios"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5959077380952381}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.13333333333333333]}}, "before_error_ids": ["mrqa_searchqa-validation-2709", "mrqa_searchqa-validation-6805", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-14721", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-1645", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-10832", "mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-13324", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-15764", "mrqa_searchqa-validation-13739", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-1941", "mrqa_triviaqa-validation-4918", "mrqa_hotpotqa-validation-1056", "mrqa_newsqa-validation-2081", "mrqa_hotpotqa-validation-2174"], "SR": 0.484375, "CSR": 0.4943524096385542, "EFR": 1.0, "Overall": 0.7010579819277108}, {"timecode": 83, "before_eval_results": {"predictions": ["he felt a close affinity to Dillinger", "Al-Shabaab,", "1981", "judge Shemsu Sirgaga", "Tillakaratne Dilshan scored his sixth Test century", "because its facilities are full.", "a skilled hacker", "near the George Washington Bridge,", "Greeley, Colorado", "The FARC", "Nirvana", "in Galveston, Texas,", "in his favor,", "34", "The Wall Street Journal Europe", "Sunday", "Peshawar", "Saturday", "four", "Islamabad", "$273 million", "Peverly Hills Chihuahua", "President Obama", "California, Texas and Florida,", "war funding", "Piers Morgan,", "Henrik Stenson", "March 24,", "collusion between the colossus of the North [the United States] and the col Colossus of the South [Brazil,\"", "shelling of the compound", "1918-19.", "supply vessel Damon Bankston", "scored a hat-trick", "Matamoros, Mexico,", "environmental", "The Sopranos", "Both men were hospitalized and expected to survive,", "emergency aid", "Courtney Love,", "it -- you know -- black is beautiful,\"", "Tukel", "I'm not afraid to say it,", "the International Polo Club Palm Beach in Florida.", "\"We tortured (Mohammed al-) Qahtani,\"", "Israeli Navy", "maintain an \"aesthetic environment\" and ensure public safety,", "Kerstin", "clean up Washington State's decommissioned Hanford nuclear site,", "in the nursing home she prefers.", "Robert Kimmitt.", "21,", "10 June 1940", "2002", "c. 1000 AD", "Guru Nanak", "Tacitus", "gaseous", "Chelsea", "2011", "Dominican", "a bowl game", "Baikal", "Beethoven", "France"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6240214646464646}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1957", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1234", "mrqa_triviaqa-validation-7444", "mrqa_searchqa-validation-9555", "mrqa_searchqa-validation-15010"], "SR": 0.578125, "CSR": 0.49534970238095233, "EFR": 0.9629629629629629, "Overall": 0.693850033068783}, {"timecode": 84, "before_eval_results": {"predictions": ["off Somalia's coast.", "30-minute", "Her husband and attorney, James Whitehouse,", "the Afghan opium trade", "Chris Robinson,", "Iran of trying to build nuclear bombs,", "Gen. Stanley McChrystal, the top NATO commander in Afghanistan,", "Revolutionary Armed Forces of Colombia, better known as FARC,", "Scarlett Keeling", "14", "\"extremely weak\"", "Larry King?\"", "strife in Somalia,", "U.S. Army as a German citizen,", "Israel", "bullet-riddled body", "used-luxury market", "I was born in Nizhny Novgorod", "1616.", "the Bronx.", "Rolling Stone", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "The pilot,", "he recruited guitarist Ron Asheton, his drummer brother Scott Asheton and bassist Dave Alexander and formed The Psychedelic Stooges.", "natural disasters", "the Carrousel du Louvre,", "Afghanistan,", "Matthew Fisher,", "English and Russian", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "1831", "one-shot", "Eleven", "Bill Haas", "U.S. Defense Department", "Bill,", "Empire of the Sun,\"", "pelvis", "in the Gaslight Theater.", "Thabo Mbeki,", "gun charges,", "Abhisit Vejjajiva", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "63", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "clogs", "Umar Farouk AbdulMutallab", "Dr. Maria Siemionow,", "his father,", "its intention to set up headquarters in Dublin.", "India", "Atlanta, Georgia", "Matthew Gregory Wise", "the Pir Panjal Range in Jammu and Kashmir", "In God We Trust", "Little Dorrit", "Prussian Landsturm", "Australian", "iTunes", "Internet creators Guild", "Empire of the Sun", "the Pocono Mountains", "the mitral valve", "Wonderwall"], "metric_results": {"EM": 0.53125, "QA-F1": 0.605591804029304}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.07692307692307691, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-501", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-339", "mrqa_searchqa-validation-5505", "mrqa_triviaqa-validation-5896"], "SR": 0.53125, "CSR": 0.49577205882352937, "EFR": 1.0, "Overall": 0.7013419117647058}, {"timecode": 85, "before_eval_results": {"predictions": ["2.5 million", "Mitt Romney", "Michael Schumacher", "25", "Almost all British troops", "Mitt Romney", "$150 billion", "about 5 percent", "the release of the four men", "Michael, Alexander, William, Joshua, Daniel, Jayden, Noah and Anthony.", "in the killings this month", "the Strategic Arms Reduction Treaty and nonproliferation.", "Kenneth Cole", "changed the business of music, to offering the world its first completely full-length computer-generated animated film", "eight.", "Aniston, Demi Moore and Alicia Keys", "Karthik Rajaram,", "the end of the season.", "The Human Rights Watch organization", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "Joe Lieberman, I-Connecticut,", "death", "1995", "he acted in self defense in punching businessman Marcus McGhee.", "Father Cutie and all priests are expected to fulfill that promise with the help of God.", "D.J. Knight of Pearlman, Texas, decided to ride", "leaky valve", "11 to 12 year old", "a sixth member of a Missouri family", "Most of the 103 children", "in a 1,700 year old Roman mosaic entitled Chamber of the Ten Maidens.", "President Obama's surge plan", "Nine out of 10 children", "AbdulMutallab", "Hoover Dam", "four", "A severe famine", "Elena Kagan", "the Golden Gate Yacht Club of San Francisco", "\"one of the most magnificent expressions of freedom and free enterprise in history\" and", "in the Philippines", "Monday and Tuesday", "drafting a new constitution after three decades of Mubarak's rule.", "Manuel Mejia Munera was a drug lord with ties to paramilitary groups,", "bank robber John Dillinger,", "Steven Gerrard", "the former Himalayan kingdom", "A flight engineer,", "Sunday", "Tuesday", "Missouri River", "supervillains", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "Dengue fever", "jellyfish", "Donald Sinclair", "Orson Welles", "ExCeL Exhibition Centre", "The Fault in Our Stars", "John Stuart Mill", "The Howdy Doody Show", "grease", "Alcorn State"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6268663194444444}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false], "QA-F1": [0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.22222222222222224, 0.625, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-860", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2040", "mrqa_naturalquestions-validation-8408", "mrqa_triviaqa-validation-2802", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-16089", "mrqa_hotpotqa-validation-5644"], "SR": 0.53125, "CSR": 0.4961845930232558, "EFR": 1.0, "Overall": 0.7014244186046511}, {"timecode": 86, "before_eval_results": {"predictions": ["Timbaland", "the Grammys", "fluorescent lights", "Pat Sajak", "steam", "Hill Street Blues", "Jason", "nests", "cremation", "Bode Miller", "Piet Mondrian", "The New York Times", "Elizabeth II", "a granite gneisses", "Montana", "the Amstel", "The New Yorker", "home", "a color-coded certification", "Medicaid", "a waffles", "the Silk Road", "a resurfacer", "Russia", "(Vijay) Singh", "a coelenterate", "Stitch", "bones", "Mononucleosis", "a fish", "Mississippi", "a credit card", "The Kiss", "Iran", "a cheese", "Falcon Crest", "Espresso", "Graceland", "a pearl", "Tulip", "Manhattan", "flagella", "Door #2", "President-elect,", "Garfield", "The AV Club", "The Lorax", "Anthony Perkins", "Chile", "tusks", "John Roberts", "Hathi Jr.", "about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Harlem River", "Cornwall", "Timothy", "Gianni Versace", "Lord's Resistance Army", "100 million", "\"The Simpsons\"' thirteenth season", "southern port city of Karachi,", "Reid's dismissal.", "Donald Duck", "45 minutes,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6877095211698869}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9268292682926829, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-2550", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-5232", "mrqa_searchqa-validation-4901", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-15679", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-10383", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-11429", "mrqa_searchqa-validation-10057", "mrqa_searchqa-validation-12947", "mrqa_naturalquestions-validation-6117", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-4078"], "SR": 0.625, "CSR": 0.49766522988505746, "EFR": 1.0, "Overall": 0.7017205459770114}, {"timecode": 87, "before_eval_results": {"predictions": ["Marxist guerrillas", "Marie-Therese Walter.", "diagnosed with skin cancer.", "five", "the \" Michoacan Family,\"", "650", "Bill Stanton", "Don Draper", "Kim Il Sung", "an upper respiratory infection,\"", "The public endorsement", "tickets to Italy by calling Expedia.", "the iReport form", "Expedia", "in early 2008,", "Apple Inc.", "breast cancer.", "part of a planned training exercise designed to help the prince learn to fly in combat situations.", "sought Cain's help finding a job", "a member of the group dubbed the \"Jena 6\"", "Democratic", "spend $60 billion on America's infrastructure.", "December 7, 1941", "at least 12 months.", "David McKenzie", "105-year history.", "2.5 million copies,", "27-year-old", "Sen. Barack Obama", "braces.", "Sporting Lisbon", "10 percent", "304,000", "April.", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "shutting down buses, subways and trolleys that carry almost a million people daily.", "Arsene Wenger", "a one-shot victory in the Bob Hope Classic on the final hole", "the oceans", "Police", "Singapore Airlines, and Thai Airways", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Eden Park", "Afghanistan's restive provinces", "mental health and recovery.", "President Obama", "Turkey,", "40-year-old", "he tried to throw a petrol bomb at the officers,", "Tehran,", "Ronald Cummings,", "Matthew Gregory Wise", "a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year", "during the American Civil War", "Edward learners", "Boxer Rocky Graziano", "Alessandro Giuseppe Antonio Anastasio Volta", "May 2011", "baeocystin", "\" Theme Park\"", "the pronghorn", "The Old Man and the Sea", "Paul Revere", "24"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6946831597222223}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 0.5, 1.0, 1.0, 0.9375, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.8, 1.0, 0.6666666666666666, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-824", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-2741", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-113", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-7957", "mrqa_triviaqa-validation-7246", "mrqa_triviaqa-validation-5210", "mrqa_hotpotqa-validation-3836", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-2533", "mrqa_searchqa-validation-12578"], "SR": 0.5625, "CSR": 0.49840198863636365, "EFR": 0.9642857142857143, "Overall": 0.6947250405844155}, {"timecode": 88, "before_eval_results": {"predictions": ["an empty water bottle down the touchline", "Venezuela", "2008,", "12 hours", "\"Abbey Road.\"", "Thailand", "17", "doesn't get along with her co-star Kristin Davis,", "the club's board", "nuclear program.", "Pakistan's border with Afghanistan", "the defending champions were held to a 1-1 draw at Stoke City.", "hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Museum-worthy pieces", "job training for all service members leaving the military.", "whether he should be charged with a crime,", "Shanghai", "the Democratic VP candidate", "the outdoors, particularly if they have a garden to eat from,", "a point for Bayern Munich", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "five", "Jackson sitting in Renaissance-era clothes and holding a book.", "FBI Special Agent Daniel Cain,", "1994,", "Bryant Purvis", "billions of dollars", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "the opposition group,", "A witness", "Burhanuddin Rabbani,", "4,000", "denied", "The Stooges comedic farce entitled \"Three Little Beers,\"", "Ma Khin Khin Leh,", "$40 and a loaf of bread.", "Wednesday.", "Sunday", "Cologne, Germany.", "15-year-old's", "10", "the Chao Phraya River and its many canals.", "the UK", "boyhood experience in a World War II internment camp", "21-year-old", "the two remaining crew members from the helicopter,", "President Robert Mugabe", "the jaws of a crocodile in northern Australia", "was hanged in 1979", "Pakistani officials,", "stepped into the museum with a rifle and began firing.", "Action Jackson", "Icarus", "Masha Skorobogatov", "the Atlantic Ocean", "Toyota", "David Nixon", "Bolshoi Theatre", "Bardot", "the Knowlton School of Architecture", "USS Enterprise", "soccer", "Christine", "The Twilight Zone"], "metric_results": {"EM": 0.625, "QA-F1": 0.7440953497169529}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337, 0.08695652173913043, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 0.8918918918918919, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-2145", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-66", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-2940", "mrqa_hotpotqa-validation-2716", "mrqa_searchqa-validation-1619"], "SR": 0.625, "CSR": 0.4998244382022472, "EFR": 0.9583333333333334, "Overall": 0.6938190543071161}, {"timecode": 89, "before_eval_results": {"predictions": ["Newcastle", "a crocodile", "Festival Foods", "Michael Jackson", "I, the chief executive officer, the one on the very top,", "the Horn of Africa.", "Osama bin Laden's", "11 healthy eggs and, this week, all 11 of them hatched", "the state's attorney", "3,000", "$1.5 million.", "Basilan", "The American Civil Liberties Union", "Boys And Girls Alone", "Sen. Barack Obama", "11", "three", "Another high tide", "Apple", "August 19, 2007.", "a U.S. military helicopter", "Diego Milito", "\"People have lost their homes, their jobs, their hope,\"", "19-year-old", "a long-range missile", "Larry King Live.", "tells Larry King her son has strong values.", "Bill Clinton", "Nafees Syed", "75", "", "Alwin Landry's supply vessel Damon Bankston", "Egypt.", "200", "later apologized,", "Arkansas", "Kim Clijsters", "Brett Cummins", "11", "India", "6-2 6-1", "a bronze medal in the women's figure skating final,", "Nine out of 10 children", "general astonishment", "the end of the season.", "phone calls or by text messaging", "Samson D'Souza", "Stoke City.", "KBR", "5,600", "grossed $55.7 million", "the Beatles", "the Coriolis effect", "September 1972", "Fram Strait", "Muslim", "five", "University of Kansas", "Salma Hayek", "Stalybridge Celtic", "The Borrowers", "crossword", "Thomas Merton", "John Denver"], "metric_results": {"EM": 0.59375, "QA-F1": 0.695610119047619}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-64", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2978", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-3159", "mrqa_naturalquestions-validation-3310", "mrqa_triviaqa-validation-5995", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-905", "mrqa_hotpotqa-validation-2156", "mrqa_searchqa-validation-16474"], "SR": 0.59375, "CSR": 0.5008680555555556, "EFR": 1.0, "Overall": 0.702361111111111}, {"timecode": 90, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7065", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1821", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14609", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15669", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16185", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2909", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3002", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-995"], "OKR": 0.78515625, "KG": 0.4703125, "before_eval_results": {"predictions": ["Southwest Florida International Airport ( RSW ), located southeast of the city", "Germany", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Palm Sunday celebrations", "Saphira", "Part 2", "1924", "Great G minor", "1979 -- 80 season", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Japan", "Tom Tucker", "John Cooper Clarke", "3,000", "throughout the United States, with small orange collection boxes distributed to millions of trick - or - treaters", "1992", "c. 3000 BC", "the large area needed for effective gas exchange", "Kid Creole and the Coconuts", "France", "New York City", "a strong, weight transferral synovial plane joint", "by the early 1980s", "British and French Canadian fur traders", "the chemical formula H CO", "the Pir Panjal Range", "Baker, California", "1983", "the # 4 School of Public Health in the country", "Toronto, Ontario", "`` Heroes and Villains ''", "Eddie Murphy", "2003", "Bulgaria", "Francis Hutcheson", "the Italian / Venetian John Cabot", "to bring, and \u03bd\u03af\u03ba\u03b7, n\u00edk\u00ea, `` victory '', i.e. `` she who brings victory ''", "Dido", "the Indians", "October 27, 1904", "54 Mbit / s", "Queen M\u00e1xima of the Netherlands", "Richie Cunningham", "August 5, 1937", "Mike Czerwien", "his cousin D\u00e1in", "warplanes", "the southeastern coast of the Commonwealth of Virginia", "2018", "Anthony Hopkins", "Peter Andrew Beardsley MBE", "john john grazley", "cheese", "cuticle", "Harlow Cuadra and Joseph Kerekes", "\"Lonely\"", "Sir Seretse Goitsebeng Maphiri Khama", "Gerardfa Yeatts Lunsmann,", "United States, NATO member states, Russia and India", "off Somalia's coast.", "a crested caracara", "Billy Bathgate", "Spider-Man", "the 2000 PGA Championship"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6931961614774115}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [0.19999999999999998, 0.5, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.8, 0.8, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.6, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 0.4444444444444445, 0.6, 1.0, 1.0, 0.25, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-2425", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-1784", "mrqa_triviaqa-validation-6482", "mrqa_hotpotqa-validation-1218", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2078", "mrqa_searchqa-validation-8850", "mrqa_hotpotqa-validation-1893"], "SR": 0.53125, "CSR": 0.5012019230769231, "EFR": 0.9, "Overall": 0.6715685096153846}, {"timecode": 91, "before_eval_results": {"predictions": ["Tom Brady", "received an SMS which reveals that Dan was directing an episode", "Lori McKenna", "The Stanley Hotel", "octopuses", "passing of the year", "2007", "parthenogenic", "1908", "Rabindranath Tagore", "hyperirritable spots in the fascia surrounding skeletal muscle", "pigs", "Montreal", "Pyeongchang County, South Korea", "Set six months after Kratos killed his wife and child", "Hudson River", "the chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Colon Street", "Justin Timberlake", "New York City", "Rick", "Kerry Shale as Tadheus `` Tad '' Stone", "Shenzi", "the euro", "September 1959", "1890s", "the Alamodome and city of San Antonio", "John Smith", "Thon Maker", "Microfilaments", "is a canid native to the southeastern United States of unresolved taxonomic identity", "Interphase", "the Isle of FERNANDO 'S!, a fictional location based in Puerto de la Cruz, Tenerife", "2000", "September 19 - 22, 2017", "Richardson", "the concept of a fully centralized service with individual user accounts focused on one - on - one conversations", "alveolar bone", "lithium", "1983 -- 84", "2016", "deceased - donor ( formerly known as cadaveric )", "red", "A turlough, or turlach", "lighter", "loosely on Eminem's actual upbringing", "the most comfortable climatic conditions", "password recovery tool for Microsoft Windows", "B.R. Ambedkar", "1975", "no longer a fundamental right, though it is still a constitutional right", "birmingham", "denmark", "Bangladesh", "AVN Adult Entertainment Expo", "Henry Lau", "Mark Andrew Brayshaw", "Michael Jackson", "on the set at \"E! News\"", "Susan Boyle", "the &quot", "William Amis", "the ukulele", "Emily Dickinson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6815252302844215}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 0.7777777777777778, 1.0, 1.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.5555555555555556, 0.0, 1.0, 0.0, 0.1, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.0, 0.058823529411764705, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-7107", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-7133", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-10250", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2242", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-4130", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-15474"], "SR": 0.515625, "CSR": 0.5013586956521738, "EFR": 0.9354838709677419, "Overall": 0.6786966383239832}, {"timecode": 92, "before_eval_results": {"predictions": ["Al Gore", "St. Vincent Millay", "Augusta", "My Therapist", "the Tyger", "Elihu Root", "\" random walk\"", "the Tower of London", "Ho Chi Minh", "Hizbollah", "a fief", "Billy Joel", "Maurice Ravel", "Madagascar", "Cleveland", "a capella group", "Nesquik", "Jack Johnson", "insurance", "Augusta", "an encore", "Anne Boleyn", "John Hurt", "Athena", "Mermaid Parties", "diarrhea", "Secretary of Labor", "gravity", "Mystery Science Theater 3000", "River Thames", "Washington", "Harry Houdini", "13-red-dot", "red", "fiberboard", "Heather Mills", "Robert Louis Stevenson", "articles", "yellow", "Opal", "The Benchwarmers", "the Supreme Court", "Sherlock Holmes", "gallows", "John Edwards", "Bears", "shiatsu", "the Ark", "oxygen", "Porgy and Bess", "Cole Albert Porter", "pigs", "the President of India", "Asuka", "thwaites", "chamomile", "Alexei Kosygin", "Shohola Falls", "Francis Egerton, 3rd Duke of Bridgewater", "\"The Guest\"", "Samson D'Souza,", "the ancient Greek site of Olympia", "murder in the beating death of", "Atlanta"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6763392857142858}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2201", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-975", "mrqa_searchqa-validation-15264", "mrqa_searchqa-validation-8474", "mrqa_searchqa-validation-1956", "mrqa_searchqa-validation-10947", "mrqa_searchqa-validation-10412", "mrqa_searchqa-validation-5876", "mrqa_searchqa-validation-3917", "mrqa_searchqa-validation-7899", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-15364", "mrqa_searchqa-validation-6564", "mrqa_searchqa-validation-10029", "mrqa_searchqa-validation-12133", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-11193", "mrqa_searchqa-validation-10135", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-1626", "mrqa_triviaqa-validation-117", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-1159"], "SR": 0.5625, "CSR": 0.502016129032258, "EFR": 0.9642857142857143, "Overall": 0.6845884936635944}, {"timecode": 93, "before_eval_results": {"predictions": ["Ross Marriott", "the heart", "$72", "about 13,000 astronomical units ( 0.21 ly )", "Indirect rule", "Spencer Treat Clark", "Dan Stevens", "in a Norwegian town", "Lager", "in September 1993", "1916", "Randy VanWarmer", "the sixth series", "Dr. Rajendra Prasad", "Gary Grimes", "all transmissions", "late - night", "Sarah Silverman", "fertilization", "3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "The Walking Dead ( comic book )", "Ace", "the 1994 season", "The 1700 Cascadia earthquake", "1994", "Justin Timberlake", "the economy", "her abusive husband", "introverted Thinking ( Te ), introverted Feeling ( Fi ) and Extroverted Intuition ( Ne ) ) is an abbreviation used in the publications of the Myers -- Briggs Type Indicator ( MBTI )", "the Atlanta Falcons, the San Francisco 49ers", "is the world's second most populous country after the People's Republic of China", "by capillary action", "the Charbagh structure", "Cee - Lo", "a premalignant flat ( or sessile ) lesion of the colon", "Mitch Murray", "northern Arizona", "Podujana Peramuna, led by former president Mahinda Rajapaksa", "native to Asia", "Teddy Randazzo", "in rocks and minerals", "personnel directors", "1961", "Egypt", "Olivia O'Brien", "seven", "Music producer Mike Higham", "Lebanon", "Turner Layton", "dromedary", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "Tallinn", "Tony Hart", "Velazquez", "Kim Jong-hyun", "Allies of World War I", "1998", "Jamaleldine", "Akio Toyoda", "a curfew in Jaipur", "Liechtenstein", "Jordan", "an RSS", "Cate Blanchett"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6145143995098039}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.37499999999999994, 0.4705882352941177, 0.8, 0.0, 0.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-684", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-4263", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-8046", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-4316", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-1616", "mrqa_searchqa-validation-13551", "mrqa_triviaqa-validation-5928"], "SR": 0.46875, "CSR": 0.5016622340425532, "EFR": 0.9705882352941176, "Overall": 0.6857782188673341}, {"timecode": 94, "before_eval_results": {"predictions": ["L.K. Advani", "an adopted daughter of Thanos", "October 20, 1977", "the Pir Panjal Range in Jammu and Kashmir", "forrester Creations", "1038", "Jughead Jones", "the student's transition from the study of preclinical to clinical health sciences", "U.S. Bank Stadium", "Mahatma Gandhi", "Bhupendranath Dutt", "Ariel Winter as Sara Lavrof", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "in the First Folio in 1623", "Leo Arnaud", "Sergeant - Major James Hewson", "an unmasked and redeemed Anakin Skywalker ( formerly Darth Vader )", "for the red - bed country of its watershed", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Nick Kroll", "Malina Weissman", "After Margaret Thatcher became Prime Minister in May 1979", "the Gupta Empire", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "at the turn of the traditional lunisolar Chinese calendar", "Gary Speed", "September 2000", "Middle Eastern alchemy", "the internal anatomy of the rib cage, lungs and heart as well as the inferior thoracic border", "a chimera ( a mixture of several animals )", "1960", "at Thunder Road", "James Arthur", "save, rescue, savior", "Frankel", "The concept is about a psychologist ( tWitch ) who teaches a dancer ( Alex ) to let go of his technique and inhibitions and just dance", "1,350 at the 2010 census", "1850", "1877", "as early as January 3, and as late as February 12", "Kida", "`` Nearer, My God, to Thee ''", "August 22, 1980", "Nikita Khrushchev", "at the center of the Northern Hemisphere", "listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "Mac OS X 10.9 Mavericks", "Queen M\u00e1xima of the Netherlands", "Rococo - era France", "Atlanta", "to collect menstrual flow", "a nerve cell cluster", "the wren", "Gary Barlow", "Cold Spring (itself a Registered Historic Place today)", "Sam Raimi", "lambics, a distinctly Belgian type of beer", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "1-1 draw", "Wednesday", "the gold rush", "the oboe", "The Three Bs of Classical Music", "Taoism"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5633681303718068}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true], "QA-F1": [0.14814814814814814, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428572, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 0.4, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.35294117647058826, 0.33333333333333337, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.2857142857142857, 0.9705882352941176, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-234", "mrqa_triviaqa-validation-2705", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-5433", "mrqa_newsqa-validation-3200", "mrqa_searchqa-validation-7128", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-13034"], "SR": 0.4375, "CSR": 0.5009868421052632, "EFR": 0.9444444444444444, "Overall": 0.6804143823099416}, {"timecode": 95, "before_eval_results": {"predictions": ["Caesar", "the liver", "Kristi Yamaguchi", "the phantom of the Opera", "Otis Elevator Company", "the Sierra Luminous Millipede", "seabirds", "chicken pot pie", "Bull", "Scott", "Birmingham", "trousseau", "The Rights of Man", "rice", "a calcaneus", "Nixon", "Canada", "Ellis Island", "a diamond", "Marie Antoinette l'Autrichienne", "China", "Thomas Stearns Eliot", "the referee", "Dustin Hoffman", "(James) Cook", "Avengers: Age of", "Frank Sinatra", "(James) Brydges", "Agatha Christie", "Ponce de Leon", "Mr. Rogers", "an Eagle Proof Coin", "the Met", "Good Will Hunting", "Lebanon", "Puccini", "Arby\\'s", "a girl", "Wayne Gretzky", "the stock market crash", "Pentecost", "Herod the Great", "Athens", "a rattlesnake", "World War II", "World War I", "Antrim", "Jay", "\"Bojangles\" Robinson", "Jerry Garcia", "Arizona", "behaves as an antagonist ( a substance that binds to a receptor but does not activate and can block the activity of other agonists )", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) or Kozunak", "April 26, 2005", "India", "The Watsons", "Poland", "1945", "Bishop's Stortford Football Club", "My Cat from Hell", "Ricardo Valles de la Rosa,", "Sheikh Sharif Sheikh Ahmed", "Thursday.", "Muhammad Ali vs. Joe Frazier"], "metric_results": {"EM": 0.5, "QA-F1": 0.621577380952381}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.9142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-13671", "mrqa_searchqa-validation-15121", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-13237", "mrqa_searchqa-validation-16186", "mrqa_searchqa-validation-10328", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-6237", "mrqa_searchqa-validation-14689", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-6320", "mrqa_searchqa-validation-11992", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-15715", "mrqa_searchqa-validation-4538", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-11504", "mrqa_searchqa-validation-9759", "mrqa_searchqa-validation-15916", "mrqa_searchqa-validation-1433", "mrqa_naturalquestions-validation-9749", "mrqa_triviaqa-validation-2151", "mrqa_hotpotqa-validation-1164", "mrqa_newsqa-validation-3181", "mrqa_triviaqa-validation-7401"], "SR": 0.5, "CSR": 0.5009765625, "EFR": 1.0, "Overall": 0.6915234375}, {"timecode": 96, "before_eval_results": {"predictions": ["weather", "Ted", "Elizabeth I", "manila", "Ruda", "Fred Trueman", "Charlie Chan", "the Beatles", "krak\u00f3w", "pygmalion", "The Green Mile", "Paul Anka", "milk", "Artemis", "Sean", "december", "Space Oddity", "Gorbachev", "color", "Delaware", "dark", "rhododendron", "manila", "rugby", "Blind Beggar", "Flyweight", "milk and honey", "wood-smoked haddock", "mans", "Help!", "antelope", "a false positive diagnosis of a paraphilia", "Little Dorrit", "William Butler Yeats", "man fit to rule England", "a wise black panther", "horseshoe", "horses", "kendo", "cabbage", "botulism", "small faces", "St. Louis", "a paddington bear", "manila", "half a dozen", "left-wing political", "The Iron Duke", "Virginia", "manila", "Bubba", "the Ramones", "brothers Norris and Ross McWhirter", "its population", "Aamir Khan", "1945", "Delacorte Press", "clogs", "Rany Freeman,", "fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives", "Mercury", "the small intestine", "Islamic Republic", "Gene Wilder"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6295833333333333}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4461", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-5753", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-3030", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-5636", "mrqa_triviaqa-validation-7093", "mrqa_triviaqa-validation-2018", "mrqa_triviaqa-validation-460", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-3848", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-98", "mrqa_searchqa-validation-1848"], "SR": 0.578125, "CSR": 0.5017719072164948, "EFR": 1.0, "Overall": 0.6916825064432989}, {"timecode": 97, "before_eval_results": {"predictions": ["Julius Caesar", "Mary Pickford", "the raffia", "roux", "the Netherlands", "pfeffernuesse", "Vienna", "Lake Champlain", "eight", "abolir", "Slavic", "Deimos", "James Jeffords", "space cadet", "Fen-phen", "Candice Bergen", "Andrew Johnson", "Venice", "Hairspray", "Tatler", "the Lone Ranger", "a fuel cell", "OK Go", "ron howard", "Bay of Fundy", "Savannah", "Bran Mak Morn", "Pearl Jam", "brain", "Gaius Cassius Longinus", "Bob Dylan", "Montessori", "microwave", "Peter Shaffer", "turquoise", "Casablanca", "White", "taro", "the Red Hot Chili Peppers", "Woody Guthrie", "the thyroid gland", "Plutarch", "Hephaestus", "Baghdad", "Whatchamacallit", "Tuscaloosa", "Norman Rockwell", "Mountain Dew", "tabula rasa", "pomegranate", "a wheellock", "December 1, 2017", "Glenn Close", "diastema ( plural diastemata )", "nairobi", "march of the Blues and Royals", "Robinson Crusoe", "Galleria Vittorio Emanuele II", "51st", "Araminta Ross", "Songs penned by Harrison included \"Taxman,\" \"While My Guitar Gently Weeps,\" \" something\" and \"Here Comes the Sun.\"", "football", "three", "Sunday,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6934895833333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14271", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-12001", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-4980", "mrqa_searchqa-validation-14503", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-9453", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-12929", "mrqa_searchqa-validation-2033", "mrqa_searchqa-validation-10355", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-11766", "mrqa_searchqa-validation-7508", "mrqa_triviaqa-validation-1434", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2386"], "SR": 0.625, "CSR": 0.5030293367346939, "EFR": 1.0, "Overall": 0.6919339923469388}, {"timecode": 98, "before_eval_results": {"predictions": ["a written compendium of Rabbinic Judaism's Oral Torah", "In 1804", "1987", "Guwahati", "parthenogenesis", "( 2018)", "October 15, 1997", "Cheap Trick", "Massachusetts", "at the base of the right ventricle", "the lumbar cistern", "Kingsford, Michigan", "a domain ( Latin : regio ), also superkingdom or empire", "During Hanna's recovery masquerade celebration", "as - yet - unknown purpose", "the sixth season", "RMS Titanic", "the British", "Matt Monro", "with nearby objects show a larger parallax than farther objects when observed from different positions", "a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "a photodiode", "Kyla Coleman", "Ben Faulks", "seven", "one retina", "Roger Nichols and Paul Williams", "during the 1890s Klondike Gold Rush", "Ali", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "John F. Kennedy", "1960", "John J. Flanagan", "France", "a husky", "Americans who served in the armed forces and as civilians during World War II", "Melissa Disney", "H CO ( equivalently OC ( OH )", "Tatsumi", "Pradyumna", "a medium with a lower index of refraction, typically a cladding of a different glass, or plastic", "Speaker of the House of Representatives", "homicidal thoughts of a troubled youth", "Wales", "Buddhism", "UNICEF's global programing", "The Maidstone Studios in Maidstone, Kent", "between 3.9 and 5.5 mL / L ( 70 to 100 mg / dL )", "adenosine diphosphate", "Graham McTavish", "Orangeville, Ontario", "France", "Edward lear", "photographer", "Attorney General and as Lord Chancellor of England", "Epic Records", "a churro", "at least 300", "2002", "U.S. Secretary of State Hillary Clinton,", "Ametrine", "(James) Mozart", "cuddle", "New York City Mayor Michael Bloomberg"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6827768891647568}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5714285714285715, 0.25, 0.0, 0.0, 0.7272727272727272, 1.0, 0.8, 1.0, 0.0, 1.0, 0.13333333333333333, 0.8823529411764706, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.8571428571428572, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.4, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-2960", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-6365", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-10260", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-3714", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-2380", "mrqa_hotpotqa-validation-1822", "mrqa_hotpotqa-validation-5770", "mrqa_newsqa-validation-2408", "mrqa_searchqa-validation-15012", "mrqa_searchqa-validation-11997", "mrqa_newsqa-validation-2212"], "SR": 0.53125, "CSR": 0.5033143939393939, "EFR": 0.8666666666666667, "Overall": 0.6653243371212121}, {"timecode": 99, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5270", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-540", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7065", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-9123", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1821", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10355", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-10947", "mrqa_searchqa-validation-11027", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14609", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-15291", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15669", "mrqa_searchqa-validation-15703", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15980", "mrqa_searchqa-validation-16185", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2466", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2909", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3002", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5806", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6237", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-7899", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-976", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1414", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-460", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7069", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-995"], "OKR": 0.833984375, "KG": 0.53125, "before_eval_results": {"predictions": ["its absolute temperature", "optical smoke detector", "May 5, 1904", "Western cultures", "March 31, 2013", "October 27, 1904", "the courts", "in `` 200 ''", "Pakhangba", "4 January 2011", "pulmonary heart disease ( cor pulmonale )", "Billie `` The Blue Bear ''", "4 percent cumulative effect", "a solitary figure who is not understood by others, but is actually wise", "13", "Nueva Extremadura", "the Supreme Court of Canada", "in honey, tree and vine fruits, flowers, berries, and most root vegetables", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Bhupendranath Dutt", "October 2012", "McKim Marriott", "the Mishnah", "revolution or orbital revolution", "1979", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "from pre-monsoonal convective clouds mainly in the form of squall lines also known as the north easterlies", "Billy Idol", "early 1974", "Patris et Filii et Spiritus Sancti", "Tanvi Shah", "administrative supervision over all courts and the personnel thereof", "interstellar medium", "Dr. Rajendra Prasad", "In 1889", "Kiss", "775", "season four", "Akshay Kumar", "Mahatma Gandhi", "across western North Carolina including Asheville, Cashiers and Saluda", "Michael Crawford", "12 '' x 12 '' attached giant - sized booklet with state - of - the - art photography of the band's performance and outdoor session pictures", "Michael Phelps", "Matt Monro", "8 January 1999", "Henry Selick", "1", "94 by 50", "subduction zone", "2009", "Rudolf Hess", "alzheimer's", "the Arafura Sea", "Field of Dreams", "40 Days and 40 Nights", "The Handmaid\\'s Tale", "crocodile eggs", "Thursday", "the two-state solution", "a grave", "a circle", "A Room with a View", "Walford East"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7474788232600732}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.09523809523809525, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809522, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-1857", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-1709", "mrqa_searchqa-validation-708"], "SR": 0.671875, "CSR": 0.505, "EFR": 0.8571428571428571, "Overall": 0.6876629464285714}]}