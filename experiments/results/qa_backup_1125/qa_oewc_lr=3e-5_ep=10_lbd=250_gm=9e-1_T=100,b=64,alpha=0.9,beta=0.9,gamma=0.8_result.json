{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', diff_loss_weight=0, ewc_gamma=0.9, ewc_lambda=250.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4150, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "EFR": 0.9473684210526315, "Overall": 0.8604029605263157}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the western end of the second east-west shipping route", "TLC", "on the south side of the garden", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "Wednesday", "a Swiss French dish that consists of a big central pot of... Tapas is a very social food because diners typically get a bunch of orders... individual dishes set in the center of the table or floor for all to pick from", "the Green Hornet", "the lynch pin of a rugby team", "Danskin", "Kingston", "sanguine", "New Hampshire", "the Tennessee Valley Authority", "the American Kennel Club", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7825314153439153}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-9310", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.71875, "CSR": 0.7552083333333334, "EFR": 0.9444444444444444, "Overall": 0.8498263888888888}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "issues with technical problems and flight delays", "the United States", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "a circle", "the Chickamauga Lake", "a brown one with gold mane", "a jet test facility, a resonant ultrasound spectroscopy lab, Faraday labs and a... The porous media group", "Gaius Maecenas", "Michael", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "a baseball club", "The Diary of a Young Girl", "Orwell's novel", "The Gleaners", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6909722222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1662", "mrqa_squad-validation-5824", "mrqa_squad-validation-2088", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.65625, "CSR": 0.73046875, "EFR": 1.0, "Overall": 0.865234375}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "goxide, superoxide, and singlet oxygen", "2.666 million", "Industry and manufacturing", "violence", "Parish Church of St Andrew", "1262", "New Orleans's Mercedes-Benz Superdome", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "the Autons with the Nestene Consciousness and Daleks in series 1, Cybermen in series 2, the Macra and the Master in series 3, the Sontarans and Davros in series 4", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "450 feet", "opera buffa", "Okinawa", "14", "the g grethra", "gated or ground potato, flour and egg", "Basin Street", "Tarsus", "Bloomingdale's", "Woody Allen", "Jane Austen", "President John F. Kennedy", "Treasure Island", "gTSi", "Charles Marion Russell", "a wine liqueur", "white", "Miss You Already", "in the 1960s", "a gilded Bridge", "Alistair Grant", "they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.693029435331825}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-10140", "mrqa_squad-validation-7729", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.71875, "EFR": 0.9047619047619048, "Overall": 0.8117559523809523}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "By the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center in San Jose", "about one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea to reduce its volume and increase its density", "closed", "21 to 11", "The Earth's crustal rock", "The goal of the congress was to formalize a unified front in trade and negotiations with various Indians", "two", "the network and the connected users via leased lines (using the X.121DNIC 2041)", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element", "39", "The Doctor", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "(G) Parker", "the New Netherland Company", "Monrovia", "Umpire", "Taiwan", "Omaha Nation", "Beniamino", "Nez Perce", "Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "(Teri) Myers", "Inchon", "February 29", "(GMAIL.COM", "Alabama", "(Svevo & Tozzi)", "Giorgio Armani", "In Britain followed the rest of the world in decimalising its currency, the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "the District of Columbia National Guard"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5818190197053456}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.5, 1.0, 0.19354838709677422, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-9640", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.4375, "CSR": 0.671875, "EFR": 0.9444444444444444, "Overall": 0.8081597222222222}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views, prompting Luther to write the hymn \"Ein neues Lied wir heben an\" (\"A new song we raise\")", "the Bible", "water pump", "86.66% (757.7 sq mi or 1,962 km2)", "53% in Botswana to -40% in Bahrain", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "a sample of some of these sculptors' work", "Judith Merril", "The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets.", "Von Miller", "weekly screenings of all available classic episodes", "a type III secretion system", "nearly 10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "oxygen concentration is too high", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "the global village", "Sun City", "Freeport, Maine", "the tapir", "auctions", "Liberty Island", "next of kin", "the American Psychiatric Association", "Lenin", "Bill Hickok", "Amtrak", "a log cabin", "The Pianist", "Patty Duke", "the king", "a Macintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "a vodka & 5 oz. of grapefruit juice", "a seasick one of these alliterative creatures", "in the mountains of eastern Nevada", "Trenton", "copper", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "art", "margarita", "prostate cancer", "DNA's structure", "Pyrenees mountains"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6749539667508417}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 0.0, 0.8750000000000001, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.29629629629629634, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2395", "mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-8923", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.578125, "CSR": 0.6584821428571428, "EFR": 0.9629629629629629, "Overall": 0.8107225529100528}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "1994 Works Council Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini program", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland, terminating Tesla's relationship with Morgan", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "alexandrite", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "Ma Joad", "Eight Is Enough", "Madrid", "Humphrey Bogart", "Foucault", "Thomas Paine", "a dna molluscs", "Fantastic Four", "G4", "LE CINEMA", "Marcus Junius Brutus", "malaria", "Ann Margret", "Hairspray", "Johann Wolfgang von Goethe", "mask", "a Greek letter society", "Spitfire floatplane", "Sherman Antitrust Act", "Hafnium", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "a last running steam-driven, paddlewheeled overnight passenger boat", "Joe Harn", "he to step down as majority leader"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5912882274295317}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.10810810810810811, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-1467", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.53125, "CSR": 0.642578125, "EFR": 1.0, "Overall": 0.8212890625}, {"timecode": 8, "before_eval_results": {"predictions": ["During the 1970s and sometimes later", "Madison Square Garden", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves and national parks", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "him to return to his side", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "leptospirosis", "the Little Engine That Could", "NanoFrazor", "tango", "Texas Hill Country", "bamboos", "Nevil Shute", "Octavia", "Vlad III", "corn and cattle", "ginseng", "Coffee", "Depeche Mode", "pepsi Benches Its Drinks", "Deep brain stimulation", "Pat Sajak", "a hippopotamus", "Roman numeral MCDXCII", "the Madding Crowd", "(M Mikhail) Baryshovo", "Mars", "the Boston Massacre Trials", "a bee", "a 9mm Uzi SMG", "Venice", "Mayoor", "Mrs. Calabash", "Carl Sagan", "she witnessed a mission she had pushed for fail and discovered one of the task force's informant dead. In February 2011, while overseas, she discovered that she was pregnant.", "Hitler", "John Ford", "CNN", "from a donor molecule to an acceptor molecule.", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5795014880952382}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [0.5714285714285715, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_triviaqa-validation-1927", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.484375, "CSR": 0.625, "EFR": 1.0, "Overall": 0.8125}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Francis Marion", "all \"trading rules\" that are \"enacted by Member States\"", "the first Block II CSM and LM", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Mario Addison", "an immunological memory", "more than 70", "movements of nature", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "2,700,000 sq mi", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "(Henry Gondorff)", "George Jetson", "Deus", "an arboretum", "pommel horse", "William McKinley", "PSP", "Daphne du Maurier", "Turkish", "a pithy remark", "a paguaro cacti", "the American Revolution", "Morrie Schwartz", "(Jimmy)", "Mercury and Venus", "Tokyo", "\"America's Best\"", "a gorillas", "the Pentagon", "an oats", "a bushel", "India", "Gone With the Wind", "Edward Albee", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "insulin", "in the mid-1990s", "Hudson Bay", "von ichak", "Melpomene", "Boston Bruins", "James Lofton", "\"cliff effect.\"", "he was letting the likes of Mr. Clemmons out."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5451722756410257}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4329", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_triviaqa-validation-2735", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.46875, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.8046875}, {"timecode": 10, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.919921875, "KG": 0.475, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "the IJssel", "flight delays", "the fact (Fermat's little theorem) that np\u2261n (mod p) for any n if p is a prime number.", "Virgin Media", "Tesla would be killed", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world", "Amtrak San Joaquins", "Kennedy was circumspect in his response to the news, refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "physical control or full-fledged colonial rule", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "the light source", "( Gabriele) Seyfert", "the 5, 2013", "Memoirs of a Geisha", "stability control", "a bolt", "the Black Death", "Aluminium", "Rhett Akins", "the Cenozoic", "the Maghreb", "Reddi-wip", "the baby", "tea", "Larry Fortensky", "the fire surge", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "Lionel's Problem", "the Sopranos", "The Crucible", "Liston", "Impressionists", "Willa Cather", "Aida", "David Thoreau", "the Bergerac region", "the vulnerable populations", "the screw", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "sink rim", "Hal Ashby", "John Ford", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker", "Frank Ricci"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6190026697177726}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9565217391304348, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-1326", "mrqa_squad-validation-2455", "mrqa_squad-validation-3790", "mrqa_squad-validation-9734", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8157", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400"], "SR": 0.515625, "CSR": 0.6008522727272727, "EFR": 1.0, "Overall": 0.7495454545454545}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "the vBNS came on line in April 1995 as part of a National Science Foundation (NSF) sponsored project to provide high-speed interconnection between NSF-sponsored supercomputing centers", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock", "oppidum Ubiorum", "studio 5", "1.7 million", "August 4, 2000", "Abu Zubaydah", "don't have to visit laundromats", "Bob Dole", "1959", "Stratfor", "three", "137", "the green grump", "Opryland", "Asashoryu", "Kris Allen", "How I Met Your Mother", "as adults", "opium", "Chinese", "warren-Arabiya", "war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "127 acres", "\"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis", "Rev. Alberto Cutie", "blind", "the military commissions", "opium", "Obama's race", "named his company Polo", "Hawass", "Arabic, French and English", "the Baseball Hall of Fame", "seven", "Honduras", "island stronghold of the Islamic militant group Abu Sayyaf", "four", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "warren meehan", "middle of the 15th century", "1966", "Vivaldi", "Brainy", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6154373304679808}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4210526315789474, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.896551724137931, 1.0, 0.0, 0.058823529411764705, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0909090909090909, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.515625, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.748125}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "prime powers", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax in Valencia", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "North Korea announcing it would scrap peace agreements with the South, warning of a war on the Korean peninsula and threatening to test a missile capable of hitting the western United States.", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese nationals.", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "The towering figure of a Muslim revolutionary named Malcolm X", "Suwardi, the village leader of Karas in East Java.", "Maj. Nidal Malik Hasan, MD, a Muslim American military psychiatrist at Fort Hood", "U.S. senators", "became a dad.", "Muslim", "California, Texas and Florida", "Robert De Niro", "Argentina", "Three searches are planned for Monday, said Coast Guard spokesman Ricardo Castrodad.", "creation of an Islamic emirate in Gaza", "near Garacad, Somalia", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "he elicits support from his fellow Muslims for \"our weapons, funds and Jihad against the Jews and their allies everywhere.\"", "His treatment met the legal definition of torture.", "Apple employees", "green-card warriors", "Haiti", "Building falls down", "test-launched a rocket capable of carrying a satellite,", "Nieb\u00fcll", "Juan Martin Del Potro.", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,", "in Seoul", "John Wayne", "Afghanistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Tim Rooney", "Ytterby", "George III", "Philadelphia", "Alien Resurrection", "Fester", "Moscow", "The equestrian program as we know it began in the 1912 Olympics with jumping, the 3-day eventing."], "metric_results": {"EM": 0.515625, "QA-F1": 0.610149040741146}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.06666666666666667, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7692307692307693, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.10810810810810811, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.3157894736842105, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9135", "mrqa_squad-validation-5657", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.515625, "CSR": 0.5877403846153846, "EFR": 1.0, "Overall": 0.7469230769230769}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I,", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation were in error", "5 nanometers across,", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "City of Edinburgh Council", "his father, Osama", "rural California,", "Los Angeles, California", "Paul McCartney and Ringo Starr", "Laura Ling and Euna Lee,", "city of Quebradillas.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Adam Yahiye Gadahn,", "iPods", "Pakistan's largest city and the capital of Sindh province.", "John McCain", "South Africa", "1960s", "Iran's nuclear program.", "North Korea,", "Sunday's", "random events", "Haeftling", "i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego,", "tie salesman", "At least 40", "$1,500", "at least 25 dead", "137", "suppress the memories and to live as normal a life as possible;", "Coptic Church spokesman Father Abdelmaseeh Baseet", "poor", "Tom Hanks", "ancient Egyptian antiquities in the world,", "27-year-old", "165-room", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Ali, and Lydia", "Kansas", "September", "modern dance", "Melanie Owen", "Lusitania", "spherical", "General Hospital", "Turkey, Saudi Arabia, and Pakistan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6302293192918192}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5714285714285715, 0.0, 0.9333333333333333, 0.6666666666666666, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2009", "mrqa_squad-validation-8869", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.5, "CSR": 0.5814732142857143, "EFR": 0.96875, "Overall": 0.7394196428571429}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946,", "$2.50 per AC horsepower royalty", "1990s", "organic", "Stagg Field", "2010", "Reuben Townroe", "the Black Death", "a water pump", "high growth rates", "roads, bridges and large plazas", "two forces,", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "bankruptcy", "Inter Milan", "98 people,", "as soon as 2050, some scientists say.", "merit-based civil service system.", "The Ski Train", "severe", "The bodies of Guerline Damas, 32; Michzach, 9; Marven, 6; Maven, 5; Megan, 3; and Morgan, 11 months,", "top designers, such as Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "the surge,", "the port remains shut down,", "onstage demos.", "Tim O'Connor,", "impeachment charges", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "bard", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces", "killing rampage.", "genocide", "The oldest documented bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, don't tell\" policy", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland since 1983.", "The Everglades,", "six-year veteran", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "William Tell", "robin", "Russell Humphreys,", "\"The Guest\"", "\"Basket Case\"", "a skull", "The Oakland Raiders relocation", "6 January 793"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5637372737556561}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_squad-validation-10395", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.515625, "CSR": 0.5770833333333334, "EFR": 1.0, "Overall": 0.7447916666666667}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally", "North", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "they are \"still trying to absorb the impact of this week's stunning events,\"", "President Obama", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined,", "the station", "sculptures", "Atlantic Ocean.", "four university students and a safety officer", "more than 200.", "Greece,", "Patrick McGoohan,", "Michael Partain,", "$627,", "27-year-old's", "Virgin America", "the dependable Camry know what's important in life,", "\"G gossip Girl\"", "Ketchum, Idaho", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "tie salesman", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Secretary of State Hillary Clinton,", "will look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "in Haiti.", "\"Dancing With the Stars\"", "estimated 1 million", "the Maersk Alabama is being held by pirates on a lifeboat off the coast of Somalia.", "more than 1.2 million people.", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother", "pigs", "Matt Flinders", "Isar", "Cal and Aron", "Sam Bettley", "33-member", "Sea of Galilee", "liquid", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6500411777549936}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 0.04761904761904762, 0.33333333333333337, 0.9473684210526316, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-1945", "mrqa_searchqa-validation-5504", "mrqa_triviaqa-validation-5573"], "SR": 0.53125, "CSR": 0.57421875, "EFR": 0.9666666666666667, "Overall": 0.7375520833333333}, {"timecode": 16, "before_eval_results": {"predictions": ["that has a number b that we want to test for primality,", "adjustable spring-loaded valve,", "Grumman", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings", "diversity", "one can include arbitrarily many instances of 1 in any factorization,", "136", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "is a Muslim with Lebanese heritage,", "he was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver on February 14, 2002.", "2nd Lt. Holley Wimunc.", "1918-1919.", "the most high-profile amalgamation of Indian and western talent yet,", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "Barnes & Noble CEO William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "The syndicate, founded by software magnate", "senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Hamas spokesman Sami Abu Zahri", "Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African", "Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award.", "Republican", "\" Teen Patti\"", "Eleven", "Hugo Chavez,", "Four bodies", "attached to another chromosome", "starch", "the United Kingdom of Great Britain and Northern Ireland", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "is a reference to the BBC teletext service Ceefax", "cartilage", "Johannes Brahms,", "the 17th century", "Orson Welles"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6383779688878373}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.5714285714285715, 0.6153846153846153, 0.5714285714285715, 0.18181818181818182, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.06666666666666668, 1.0, 0.5, 1.0, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9023", "mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-4478"], "SR": 0.53125, "CSR": 0.5716911764705883, "EFR": 1.0, "Overall": 0.7437132352941177}, {"timecode": 17, "before_eval_results": {"predictions": ["low skilled jobs becoming more tradeable", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house when in his home", "John Fox", "US$1,000,000", "their Annual Conference", "Colonel Monckton,", "thermodynamic theory", "CNN Moscow Correspondent", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "Boundary County, Idaho,", "a delegation of American Muslim and Christian leaders", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "after they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006,", "the FBI.", "as many as 250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "abuse", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "African-American", "flooding and debris", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "Cutpurse", "seven", "a vigorous deciduous tree", "a transistor,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7121775793650794}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_triviaqa-validation-5425"], "SR": 0.609375, "CSR": 0.5737847222222222, "EFR": 1.0, "Overall": 0.7441319444444445}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty drugs", "Doctor of Theology", "God's", "The Prince of P\u0142ock,", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Arthur Sarsfield Ward,", "Aug 24,", "frax", "a sperm whale", "\u00ef\u00bf\u00bd", "Naboth's", "Jeffrey Archer", "C N Trueman", "Anne Boleyn", "Golda Meyerson", "a round, slightly tapered, fraatian \u0161\u0201pka", "Alan Greenspan", "Thai", "Parsley the Lion", "Japan", "Runic", "plutonium", "Patrick Murray", "blancmange", "baloney", "fraxage", "recorder", "fravelin", "Microsoft", "Austria", "Isambard Kingdom Brunel", "Edward fraem Hunter", "Jamaica", "Francis Ford", "Petronas", "Beyonce", "Microsoft", "Otto I", "Praseodymium", "The Battle of the Three Emperors,", "southern Pacific Ocean,", "Trimdon,", "Midnight Cowboy", "Dada", "FIFA World Cup 2010", "Southwest Airlines,", "Afghanistan", "Thomas M disableitch", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "cannibalism", "\"Royal\"", "Ford Motor Company", "Banff", "frax whale"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5588541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.484375, "CSR": 0.569078947368421, "EFR": 0.9696969696969697, "Overall": 0.7371301834130782}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "a Wi-Fi or Power-line connection", "ash tree", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands,", "Tony Blair", "The Flintstones", "9-1-1", "Jonathan Swift", "South Sudan", "Maria Esther Andion Bueno", "beaters", "Frankie Laine", "July 28, 1948", "Thor", "Austria", "Goosnargh", "a bear", "daed structure", "Montr\u00e9al", "Adidas", "sedgel", "Rocky and Bullwinkle", "Ben Drew", "ilawanna six", "Poland", "Indiana Jones", "John Philip Sousa", "edward Boehm", "Sydney", "bulgaria", "jura", "armoured car", "finger", "meteoroid", "Norman Brookes", "l' Escargot", "lola", "bodhidharma", "Klaus dolly", "Albert Reynolds", "a hook", "Baltic Sea", "Singapore", "cathead", "yellow", "cat food", "Vespa", "Squamish", "an annual income of US $11,770", "Theme Park World", "Cape Cod", "pacific pacific beach volleyball", "10 percent", "867-5309", "quizlet", "Madame Flora", "small intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.4375, "QA-F1": 0.520907738095238}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-556", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-3139"], "SR": 0.4375, "CSR": 0.5625, "EFR": 0.9722222222222222, "Overall": 0.7363194444444445}, {"timecode": 20, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.890625, "KG": 0.43515625, "before_eval_results": {"predictions": ["red algal derived", "pathogens", "1525\u201332", "only a", "solution", "2011", "random noise", "Wardenclyffe", "jules Verne", "Ogaden", "the Washington Post", "prefecture", "Steve Biko", "pottery", "a pennsylvanica", "acute", "nasa bayabasan", "dna", "Beyonce", "Norman Mailer", "Oliver!", "kunsky", "Bolton", "humbert humbert", "klausarevitch", "dennis cabETT", "junk Planet", "Hartford", "majesty", "King George III", "Lincoln", "Severn", "Canada", "nell Nimoy", "preston", "dennis humbert", "Jesse Garon Presley", "komando Pasukan Khusus", "lithium", "40", "The Duchess", "Zig and Zag", "white", "China", "Salt Lake City,", "Perseus", "Capricorn", "a 'rugby-specific fit' short", "Sergio Garc\u00eda Fern\u00e1ndez", "meadow brown", "Jason Alexander", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch Interactive", "department assessment that suggested returning combat veterans could be recruited by right-wing extremist groups.", "cantaloupes", "humbert humbert", "a rhinoceros", "Wes Craven", "Australian", "\"$10,000 Kelly,\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5238839285714285}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.09523809523809523, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_squad-validation-3625", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3117", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.46875, "CSR": 0.5580357142857143, "EFR": 1.0, "Overall": 0.7361383928571429}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "Levi's Stadium", "gold", "Chinese", "Surrey", "tESLAR Satellite", "transient leg syndrome", "Buzz Aldrin", "Lois", "Niger", "Backgammon", "Instagram", "Home alone 2: Lost in New York", "Columbus", "t.S. Eliot", "Venus", "The wailers", "Crusades", "nicky Henderson", "curb-roof", "dennis", "dennis franklin", "tchaikovsky", "Plato", "selene", "Stephen King", "horse", "Catskill Mountains", "paul mccartney", "a kilovolt-ampere", "fluid", "Jordan", "tony huggins", "London", "chainsaw", "Poland", "50th Boy", "forehead", "a lew", "eukharisti\u0101", "100 years", "fruit", "tony states", "Piccadilly Circus", "tundra", "Melbourne, Victoria,", "meowbank thistle", "Tangled", "Vincent Motorcycle Company", "Melissa Duck", "inner core", "novella", "The Prodigy", "jenn white", "Michelle Rounds", "21-year-old", "tony franklin", "Daytona", "benjamin franklin", "Mickey's PhilharMagic", "hiphop"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5043402777777777}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.46875, "CSR": 0.5539772727272727, "EFR": 1.0, "Overall": 0.7353267045454545}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army", "about 63,754", "faith alone", "Ticonderoga Point", "a seal illegally", "Season 4", "yara Greyjoy", "1972 -- 81", "Dottie West", "October 1980", "dava Hulsey", "Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2018", "UK", "variation in plants", "Baltimore, Maryland", "the beginning of the American colonies", "Battle of Antietam", "Paspahegh Indians", "left atrium and ventricle", "Mayflower", "1560s", "Davos", "Prince James", "New Orleans", "2008", "U.S. service members", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "heartbreak", "Annette", "May 2017", "yorkshire", "ABC", "eukaryotic cells", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "between 8.7 % and 9.1 %", "hero", "37.7", "1954", "from 1922 to 1991", "\u201cShine", "preston", "agawu", "Mountain West Conference", "Sydney", "yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "Pastor Paula White", "Department of Homeland Security Secretary Janet Napolitano", "The Mill on the Floss", "Greenland", "cherry"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49149298282362797}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6451612903225806, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_squad-validation-3408", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6295", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-15953"], "SR": 0.390625, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.73390625}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "picturebook Shiji no yukikai ( 1798 )", "In March 2017, Jimmy John's has almost 3,000 stores with plans for expansion up to 5,000 and beyond", "Chinese flower shop", "T'Pau", "Bud Light", "the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "Universal Pictures and Focus Features", "LED illuminated", "a line of committed and effective Sultans", "If there are no repeated data values, a perfect Spearman correlation of + 1 or \u2212 1 occurs when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "84", "the United States economy", "in Wales and Yorkshire", "In Test cricket originally had four balls per over", "in Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "the muscle sarcomeres shrink to a third of their original length", "in the majority of the markets the company has entered", "Jodie Foster", "Kenneth Kaunda", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "meaning", "Massillon, Ohio", "predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "giant", "the RAF", "in Britain", "in New York City", "German", "in 1961 during the Cold War", "Coroebus", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "in Europe  2.1.1", "Pakistan", "Sam Raimi", "7 October 1978", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "transit bombings", "natural disasters", "1819", "wiki", "gaffer"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5759787087912087}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.4, 0.0, 0.07142857142857142, 1.0, 0.19999999999999998, 1.0, 0.14285714285714285, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.16666666666666666, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9047619047619047, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4872", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-894", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.46875, "CSR": 0.5436197916666667, "EFR": 0.9117647058823529, "Overall": 0.7156081495098039}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors. There is a strong presence of Border Reiver surnames, such as Armstrong, Charlton, Elliot, Johnstone, Kerr, Hall, Nixon, Little and Robson", "German creedal hymn \"Wir glauben all an einen Gott\" (\"We All Believe in One True God\")", "April 20", "Tanzania", "March 29, 2018", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "northern China", "Missouri River", "Harrys", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1946", "May 3, 2005", "David Hemmings as Nigel", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "The Broncos were barely competitive during their 10 - year run in the AFL and their first seven years in the NFL", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium in Orlando, Florida", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "Simon Peter", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix on which thousands of genes are encoded", "Tagalog or English", "American rock band R.E.M.", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Juliet", "a semi-independent State of Vietnam, within the French Union", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "various submucosal membrane sites of the body", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "Tremont neighborhood of Cleveland, Ohio", "a hooker and addict", "England", "Ahmad Given ( Real ) and Kamal Givens ( Chance )", "a man who could assume the form of a great black bear", "a best known as the lead singer and lyricist", "a best forewings", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Bow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "a zealulla-dress-of-gentlewomen", "a live goat mascot named Bill", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.5, "QA-F1": 0.5905720492854363}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.06666666666666667, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.41379310344827586, 0.8571428571428571, 0.7741935483870968, 1.0, 0.10526315789473682, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-5910", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.5, "CSR": 0.541875, "EFR": 0.96875, "Overall": 0.72665625}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "\"The meal was required to be ready at eight o'clock... He dined alone, except on the rare occasions when he would give a dinner to a group to meet his social obligations.", "6.4 nanometers", "1894", "the means of production", "Atlanta, Georgia", "Thunder Road", "acidifying", "Bette Midler", "gathering money from the public, which circumvents traditional avenues of investment", "the pyloric valve", "chesc", "Julia Ormond", "anvil", "The Satavahanas", "March 16, 2018", "Hathi Jr", "to prevent the flame from being blown out and enhances a thermally induced draft", "twice", "Asuka", "when matching regions on matching chromosomes break and then reconnect to the other chromosome", "Hathi Jr.", "in the Kananaskis", "computers", "Edward Kenway", "Madison, Wisconsin, United States", "to weaken the British by cutting off its imports, and strike a winning below with German soldiers transferred from the Eastern front, where Russia had surrendered", "May 26, 2017", "1981", "USS Chesapeake", "Iden Versio", "a transformation, change of mind, repentance, and atonement", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hedwig", "Lee Mack", "acquire an advantage without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Clarence Anglin", "at noon of April 1st", "12.65 m ( 41.50 ft ) long", "the Northeast Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "the 17th episode in the third season of the television series How I Met Your Mother and 61st overall", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "from a variety of ancient herding dogs, some dating back to the Roman occupation, which may have included Roman Cattle Dogs, Native Celtic Dogs and Viking Herding Spitzes.", "Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17", "canton", "king Edward VI", "New Orleans"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5104035057220644}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8695652173913044, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07142857142857144, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.28571428571428575, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.1290322580645161, 0.47619047619047616, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1263", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.390625, "CSR": 0.5360576923076923, "EFR": 0.9743589743589743, "Overall": 0.7266145833333333}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "those who already hold wealth", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "a virtual reality simulator", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the Western world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "Johnson", "liver and kidneys", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1932 Games", "Geoffrey Zakarian", "Tommy James and the Shondells", "a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Bonnie Aarons", "March 31, 2018", "Jay Baruchel", "the teacher, Elizabeth `` Sister '' Wilkes ( Diane Lane )", "2004", "rear - view mirror", "Portuguese and Spanish - French origins", "1986", "the terrestrial biosphere", "1937", "the 2017 season", "Beijing", "the court from its members for a three - year term", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "1 through 75", "Famous Players-Lasky Corporation", "Tiffany & Company", "an American politician and environmentalist who served as the 45th Vice President of the United States from 1993 to 2001", "villanelle", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months ago", "a deep-submergence vehicle capable of working in depths of 6000 ft It is... High- strength steels or titanium will be developed for the pressure hull", "Christopher Newport", "rotunda"], "metric_results": {"EM": 0.5, "QA-F1": 0.6348992074919326}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.721311475409836, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.9, 0.0, 1.0, 1.0, 0.8666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.2222222222222222, 0.7272727272727273, 1.0, 1.0, 0.0, 0.4, 0.0, 0.21052631578947367, 0.5, 1.0, 1.0, 0.0, 0.08333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.5, "CSR": 0.5347222222222222, "EFR": 0.90625, "Overall": 0.7127256944444444}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "the next architect to work at the museum was Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "the Seminole Tribe", "one out of every 17 children under 3 years old in America", "Tuesday", "The pilot, whose name has not yet been released,", "the estate with its 18th-century sights, sounds, and scents.", "Roqaya al-Sadat,", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "Obama reaffirmed commitment to lesbian, gay, bisexual and transgender Americans.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Mildred", "at a Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "blew up an ice jam Wednesday evening south of  Bismarck, according to CNN affiliate KXMB.", "Michelle Rounds", "a national telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe,", "Ankara", "Bill Stanton", "humans", "Herman Thomas", "Werder Bremen,", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Arizona", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "al Qaeda,", "Tom Hanks", "outside his house in Najaf's Adala neighborhood after returning from Friday prayers", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive, according to David Peterka, who was ejected because the doors were open for filming, ran back in to rescue the others.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch", "Abdullah Gul,", "1979", "Lynne Tracy", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "violin", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio", "a cabinetmaker", "shrimp", "tentacles"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5621586687443465}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5925925925925926, 0.6666666666666666, 0.23529411764705882, 1.0, 0.2222222222222222, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07142857142857142, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522"], "SR": 0.46875, "CSR": 0.5323660714285714, "EFR": 1.0, "Overall": 0.7310044642857142}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "Sheikh Sharif Sheikh Ahmed", "off east  Africa", "her two Manchester, England shows", "Rod Blagojevich", "gasoline", "Denver, Colorado.", "Dolgorsuren Dagvadorj,", "it does not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain", "Peshawar", "The Casalesi clan", "President Clinton.", "he regrets describing her as \"wacko.\"", "Nick Adenhart", "the earthquake's devastation.", "unemployment benefits", "eco", "2010", "problems with the way Britain implements European Union employment directives.", "France's famous Louvre museum", "More than 15,000", "Tens of thousands of new voters", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam, in the Netherlands,", "Del Potro.", "his wife,", "the finding of \"a whole new treasure hoard of fossils\" on Wednesday. He described it as \"the most important discovery\" for the museum \"of the last 90 years.\"", "acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "Sharon Bialek", "Kurdish militant group in Turkey", "military veterans", "41,", "the job bill's controversial millionaire's surtax,", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "Keating Holland", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the Italian pignatta", "1973", "rugby", "rabies", "Parkinson's", "The 254th episode overall,", "Disha Patani", "Anah\u00ed", "Labour", "Excalibur", "witchcraft"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6443512246852764}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.3448275862068966, 0.6, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.53125, "CSR": 0.5323275862068966, "EFR": 0.9666666666666667, "Overall": 0.7243301005747126}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "quarterback Denver Broncos", "teach by rote", "opposed to meat consumption by covering themselves in fake blood and lying in human-sized meat packages.", "\"Dance Your Ass Off.\"", "Robert Barnett,", "business dealings for possible securities", "Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "Jacob Zuma,", "Charles Cowell.", "great jazz music", "\"falling space debris,\"", "Obama's", "30", "Monday night", "prison inmates.", "Franklin,", "The BBC", "the coalition", "commission of about 35 to pick from in the final stage.", "Elizabeth Smart,", "Christmas", "football", "consumer confidence", "Republican", "only normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "vitamin injections that promise to improve health and beauty.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "chairman of the House Budget Committee,", "top designers, such as Stella McCartney,", "about 5:20 p.m. at Terminal C", "that tries to take justice into its own hands.", "a sixth member of a Missouri family under investigation for allegations of child sexual abuse,", "Casalesi Camorra clan", "Obama and McCain camps", "Sen. Barack Obama", "heavy brush,", "more than 30 Latin American and Caribbean nations", "Empire of the Sun", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday,", "Brazil", "Caylee's,", "health", "6-4 loss,", "between Glen Miller Road in Trenton", "the Western Bloc ( the United States, its NATO allies and others )", "annually in late January or early February", "Galileo Galilei", "Zeus", "paper sales company", "Chancellor Christian Kern", "Indianola,", "Wayne County,", "Charles Dickens", "Akihito,", "Dorothy Parker"], "metric_results": {"EM": 0.375, "QA-F1": 0.5345934141430465}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.07692307692307693, 0.4, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444444, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.5, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.3636363636363636, 0.4444444444444444, 1.0, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.375, "CSR": 0.5270833333333333, "EFR": 1.0, "Overall": 0.7299479166666667}, {"timecode": 30, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.8671875, "KG": 0.49296875, "before_eval_results": {"predictions": ["Super Bowl XX,", "undermining the communist ideology", "67.9", "between pen-pals", "Wendell,", "Sophia", "Pula Arena", "hyaena hyaena", "Google", "electronegativity", "HIV", "a cat", "Jeopardy!", "the Starfighter", "Prone", "the House of Romanov", "a mirror", "fermentation", "Godot", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "a bad one", "Han Solo", "Gutzon Borglum", "Catherine of Aragon", "Paris", "St. Mark", "Oklahoma", "Salman Rushdie", "the United Nations", "Tycho Brahe", "an American sitcom", "the Interior", "elephants", "cloister", "Harry Truman", "Pakistan", "Idiot's", "Clue", "Lucky", "lovely Rita", "President Woodrow Wilson", "animal cookies", "tornado", "Omaha, Nebraska", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Popowo,", "Bobby Kennedy", "Mercury", "marker pen", "Niveda Thomas", "1975", "the Harris Fire.", "CEO of an engineering and construction company with a vast personal fortune. As mayor of Seoul from 2002 to 2004,", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6036458333333334}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-4424", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3688"], "SR": 0.546875, "CSR": 0.5277217741935484, "EFR": 0.9655172413793104, "Overall": 0.7152103031145718}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "quotient", "Carson", "hail", "Chicago", "Manatee Springs", "the Hippocratic Oath", "Latifah", "Golden Retriever", "Shropshire", "the Aegean Sea", "fingernails", "a bogey", "Sinclair Lewis", "Crocodilia", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "Trans Alaska Pipeline", "trout", "Chicago", "Dixie Chicks", "Bob Woodward", "a buffalo", "America", "Istanbul", "Crazy Horse", "look", "Rehab", "the Golden Hind", "Administrative Professionals' Day", "President Nasser", "Van Halen", "a black bear", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "Cashmere", "Diana", "spilled milk", "grasshopper", "carat", "Robin Hood", "White Cliffs", "... yang", "September 29, 2017", "almost entirely in Wake County", "December 1800", "Nicolas Sarkozy", "Democrats", "a Double Whole note", "Rabies", "Environmental Protection Agency", "Robert Gibson", "Mogadishu", "45 minutes, five days a week", "three years"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6952132936507937}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-15383", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-12318", "mrqa_searchqa-validation-3909", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-9922", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.578125, "CSR": 0.529296875, "EFR": 1.0, "Overall": 0.722421875}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "a tornado", "the Taj Mahal", "plantain", "broiler", "John", "London", "The Andy Griffith Show", "Nassau", "the Mediterranean", "Fahrenheit", "Janet Reno", "Spanish", "Seinfeld", "steroids", "Atlantic City", "Galt", "President George W. Bush", "Iraq", "taro", "Sanssouci", "\"Frozone\"", "Peter Ilyich Tchaikovsky", "Malle Babbe", "the Stone Age", "\"Snowy Landscape\"", "Billy Pilgrim", "Louis XVIII", "Cain\\'s", "Prince Charles", "the Heart", "whiskers", "a lighter", "Elmer", "the CretaceousPaleogene extinction", "Peggy Fleming", "Panama", "the metric system", "Sweden", "Castle Rock", "fuchsia", "the Mediterranean", "George W. Bush", "\"One Too Many\"", "\" Buzz\" Windrip", "Daphne du Maurier", "\"Huggy Bear & the Turkey\"", "King Willem - Alexander", "New England Patriots", "an inability to comprehend and formulate language because of damage to specific brain regions", "Damon Albarn", "the duchy of Mazovia,", "Ken Burns", "Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "along the equator between South America and Africa.", "four decades"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5072869915011459}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.47058823529411764, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.8750000000000001, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-2403", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-14396", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16407", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.421875, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.7217708333333335}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual", "echinos", "poker", "Salmon", "Nigeria", "the Mycenaean palatial civilization", "Japan", "Thomas Merton", "ex-wife", "the phantom", "Crystal Car Fathers Day Auto Show", "1963", "life expectancy", "donut", "volcanic", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "ducks, hummingbirds", "Columbia University", "Punky Night", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the mob", "New Mexico", "the French Revolution", "a Purple Heart", "Arkansas", "the CPU", "Colette Walley World", "the katana", "sender", "Jean Lafitte", "Komodo", "Italian", "Churchill", "knitting", "Robbie Turner", "receipt", "Damascus", "(Lu) Hsun", "Innsbruck", "the Noah's flood", "Sea World", "the chest, back, shoulders, torso and / or legs", "Article Two", "Andy Cole", "Genghis Khan", "Roy Rogers", "violet", "the Great Northern Railway", "25 October 1921", "(Debra Janine) Witt", "\"The Orchid Thief\"", "reaching out and opening the door for the man who shot him,", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5568780637254902}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.8235294117647058, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9258", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13753", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_searchqa-validation-15608", "mrqa_naturalquestions-validation-6442", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-6427", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.453125, "CSR": 0.5238970588235294, "EFR": 1.0, "Overall": 0.721341911764706}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "King Kong", "William the Conqueror", "the West India Company", "a daniette Hanck", "Luffa", "Hershey", "a snail", "a crossword", "Muhammad Ali", "Dove", "the Supreme Court", "the Ross Sea", "Putin", "a thunderstorm", "Kennebunkport", "a satellite", "a plague pandemic", "Devon", "Amelia Earhart", "hydroelectric", "a Panty Raid", "French", "cricket", "The Pythian Games", "the Dolphins", "The Lone Ranger", "a rodent", "\"Queen of Crime\" Agatha Christie", "Flying the Unfriendly Skies", "a keypunch", "the Amazons", "The Fugitive", "a world population", "a forge", "Harpers Ferry", "Theano Vision", "a lilac", "a crossword", "Tampa", "a ductile", "the King\\'s Men", "Leo", "first anniversary", "a nautilus", "a salaam", "a Bigfoot", "a Juris Doctorate", "buy the shares", "The Thing", "Special Agent Dwayne Cassius Pride ( Scott Bakula )", "Stephen Curry", "Kusha", "Mars", "Captain America", "The Stock Market Crash", "South America,", "1998", "Picric acid", "Nineteen", "disasters", "Siri"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5696676587301588}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-8656", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-9119", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-15530", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740", "mrqa_newsqa-validation-3365"], "SR": 0.484375, "CSR": 0.5227678571428571, "EFR": 1.0, "Overall": 0.7211160714285715}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "How I Met Your Mother", "the two-state solution", "in-cabin lighting", "little blue booties", "forgery and flying without a valid license,", "Kurdistan Freedom Falcons,", "Lee Myung-Bak", "end of a biology department", "Malawi", "\"fusion teams,\"", "Her husband and attorney, James Whitehouse,", "shut down buses, subways and trolleys that carry almost a million people daily.", "Muslim", "a Muslim festival", "Caster Semenya", "Kishan Kumar,", "GospelToday", "death of cardiac arrest", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "rural Tennessee.", "CNN", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Robert Redford", "\"wider relationship\"", "death squad killings", "Morot & Annat", "July for A Country Christmas,", "down a steep embankment in the Angeles National Forest", "piano", "Amy Bishop Anderson,", "Jennifer Arnold and husband Bill Klein,", "her landlord", "job training", "State Department employee as Steve Farley", "two years,", "Operation Cast Lead", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola", "Leo Frank", "Port-au-Prince", "Buddhism", "Russia", "President George Bush", "independently in different parts of the globe", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "Eisenhower Executive Office Building", "Dele Alli,", "February 22, 1968", "Palatine Hill", "petrol", "Norbit"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5897309287934287}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.3333333333333333, 0.36363636363636365, 1.0, 0.0, 0.923076923076923, 0.3333333333333333, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.8717948717948718, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265"], "SR": 0.484375, "CSR": 0.5217013888888888, "EFR": 1.0, "Overall": 0.7209027777777778}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "resources that could sustain future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "\"Top Gun\"", "us to step up.", "Too many glass shards", "one", "Jaipur", "Barack Obama", "April 6, 1994", "the Democratic VP candidate", "together with two other buildings", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Some truly mind-blowing structures", "the Revolutionary Armed Forces of Colombia,", "Dan Brown", "The pilot,", "Paul McCartney", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "Starbucks", "finance", "Friday.", "diagnosed with skin cancer.", "he exercised in a park in a residential area of Mexico City,", "almost at the exact location where I wanted them to dig.", "more than 5,600", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "21 percent", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "at least $20 million to $30 million,", "five masked men dressed in black appear on the video, sitting behind a long table.", "first used in tennis", "about six to seven million", "10 years", "Jeffrey Archer", "a palla", "Jack Nicholson", "Flatbush Zombies (stylized as Flatbush ZOMBiES)", "Crane Wilbur", "Venice", "a Great Highland Bagpipe", "Special Boat Teams", "Earvin \"Magic\" Johnson Jr.", "`` Fix You ''"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5872966433628198}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.8235294117647058, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.888888888888889, 0.0, 0.5, 0.1, 0.0, 0.0, 0.4444444444444445, 0.0, 0.054054054054054064, 0.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 0.5, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1127"], "SR": 0.46875, "CSR": 0.5202702702702703, "EFR": 1.0, "Overall": 0.7206165540540541}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings", "Ricardo Valles de la Rosa,", "three", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "ferry", "1994,", "Belfast, Northern Ireland", "Cain", "U.S. filmmakers", "Clarkson", "CEO of an engineering and construction company", "the British capital's other two airports, Stansted and Gatwick,", "40 lashings for the incident which is said to have taken place in the capital Khartoum on August 21.", "take immunosuppression drugs for life so that the body does not reject the donated tissue,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "CNN.com", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Bergdahl,", "takes a look at some of the best stunt ever pulled off -- and a few that didn't end so well.", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "about 3,000 kilometers (1,900 miles),", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him", "nuclear", "Iran's parliament speaker", "highest ever position", "services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.", "chosen their rides based on what their cars say", "10", "artificial intelligence.", "There's no chance of it being open on time.", "10", "April 13,", "Samuel Herr,", "London's", "Obama", "16", "Ralph Lauren", "$10 billion", "2,800", "three", "David Ben - Gurion", "Kiss", "20 years from the filing date", "Ben Affleck", "Noises Off", "aeoline", "Mauthausen\u2013Gusen", "Delilah Rene", "first-year", "Pope John Paul II", "art deco", "the Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.5, "QA-F1": 0.6550371626530094}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.16666666666666669, 0.3636363636363636, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.08695652173913043, 0.19999999999999998, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.5, "CSR": 0.5197368421052632, "EFR": 0.96875, "Overall": 0.7142598684210527}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "the downing of two Blackhawk helicopters", "U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "Sunday", "heavy turbulence", "Sophia Stellatos.", "Opryland", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40 lashes", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti", "Hanin Zoabi,", "Desmond Tutu", "84-year-old", "John Rizzo,", "President Bill Clinton", "humans", "the island's dining scene", "chairman of the House Budget Committee,", "CNN.com", "President Robert Mugabe's", "the deployment of 30,000 additional U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.", "more than 30", "Brown", "133", "it would", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "Italian Serie A title", "methylene", "Math teacher Mawise Gumba fled Zimbabwe and found his qualifications mean little as a refugee.", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic on the final hole", "Russian bombers have landed at a Venezuelan airfield where they will carry out training flights for several days,", "Musharraf", "two courses", "first grand Slam,", "the MS Columbus,", "a psychoticic killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The local Republican Party", "1 October 2006", "1834", "caveolae internalization", "guitar", "the 3,209ft Lake District peak", "caffeine", "Keele University", "9,984", "Smithfield, Rhode Island,", "liquid nitrogen", "Donna Rice Hughes", "the albatross", "actor"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5708132636278196}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.1904761904761905, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.8, 0.06666666666666667, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.1142857142857143, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.4375, "CSR": 0.5176282051282051, "EFR": 0.9444444444444444, "Overall": 0.7089770299145299}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "Arroyo and her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "3.5 kilometers (2 miles) west of the town of Na'ameh,", "then-Sen. Obama", "Uighurs,", "Leo Frank", "Michael Arrington", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "Dancy-Power Automotive", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining", "Saturday,", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "Robert", "suicides", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades.", "Oprah Winfrey", "\"We wondered how can we protect our dogs' feet against glass,\"", "over 1,000 pounds", "two satellites", "a time-lapse video showing some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "21 percent", "$50", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "phone calls or by text messaging", "Pakistan", "He already committed crimes against the constitution and the laws", "he wants a \"happy ending\" to the case.He told CNN a family friend was paying for his services.", "fluoroquinolone", "to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.", "\"Empire of the Sun\"", "digging", "1000 square meters", "President Obama", "North Korea", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 -- 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "the tallest building in the world", "81st", "the Premier League,", "the Secret Intelligence Service", "75 mi", "chef salad", "a grasshopper", "the Kneset", "Secretary of the Interior"], "metric_results": {"EM": 0.53125, "QA-F1": 0.625045603905898}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 0.0, 0.09090909090909091, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 0.0, 0.47619047619047616, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.53125, "CSR": 0.51796875, "EFR": 1.0, "Overall": 0.72015625}, {"timecode": 40, "UKR": 0.640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.771484375, "KG": 0.4515625, "before_eval_results": {"predictions": ["1985", "doctors", "eight", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher", "CNN's", "NATO", "Lieberman", "the deputy who responded to the tips.", "the Gulf", "Haiti.", "Peshawar", "Basel", "between Pyongyang and Seoul", "\"I'm not afraid to say it, sometimes she was a pain in the ass,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "22-10.", "Egypt.", "Rima Fakih", "deliver a big speech next Wednesday)", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "a site that has yielded between 3 million and 4 million fossilized bones.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "hundreds of contraband cell phones were found behind bars or in transit to Texas inmates in 2008.", "six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "alternative-energy vehicles", "the Obama's surge plan", "\"Perfidia,\" \"Walk Don't Run '64\" and \"Diamond Head.\"", "damage the reefs if they get too close.", "Communist", "urgently to be rescued, fearing the crew could be harmed or killed,", "aitians", "Sri Lanka's", "telling CNN his comments had been taken out of context.", "summer", "Rev. Alberto Cutie", "since 1983.", "witnesses spotted Caylee since her disappearance.", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse also have a long proboscis, which extends directly forward and is attached by a distinct bulb to the bottom of their heads", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Richard Attenborough", "17", "Latin American culture", "Dolly Parton", "a tour de force", "a martian", "Nippon Professional Baseball"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6008041118238487}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.8571428571428571, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.7142857142857143, 1.0, 0.0, 0.25, 1.0, 0.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.5, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.25, 0.5454545454545454, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-5265", "mrqa_hotpotqa-validation-5556"], "SR": 0.4375, "CSR": 0.5160060975609756, "EFR": 1.0, "Overall": 0.6759355945121952}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "in the mouth.", "Stephen Johns reportedly opened the door for the man police say was his killer.", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "Coast Guard helicopters", "videos and commentaries.", "Bastian Schweinsteiger", "he believed he was about to be attacked himself.", "the Brundell family", "outside the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "a missing sailor whose five Texas A&M University crew mates", "FBI's", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "Pakistan", "Robert", "in a park in a residential area of Mexico City,", "16", "Pixar's \"Toy Story\"", "into the picturesque Gamla Vaster neighborhood", "the Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre sanctuary", "Missouri.", "Tibetan exile leaders,", "Ketamine,", "Haleigh", "two and a half hours.", "Bobby Darin,", "Queen Elizabeth's birthday", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "an obscure story of flowers", "Schalke", "Kris Allen,", "World Wide Village,", "2", "Supplemental oxygen", "Iran", "Harley", "Roy Rogers", "George Washington", "a leo spelaea", "German", "Forbes", "dealings with the devil", "cholesterol", "Stockholm", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.375, "QA-F1": 0.5016180592781203}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.8333333333333334, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_naturalquestions-validation-8733"], "SR": 0.375, "CSR": 0.5126488095238095, "EFR": 0.975, "Overall": 0.6702641369047619}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product-market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$5.4 trillion", "3,000 metres ( 9,800 ft ) at Pisac", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Dr. Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fred E. Ahlert", "Institute of Chartered Accountants of India ( ICAI )", "2018", "Bette Midler", "push the food down the esophageal muscle", "Walter Mondale", "Nick Sager", "Sweden's long - standing policy of neutrality was tested on many occasions during the 1930s", "the 18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Michael Madhusudan Dutta", "5 - 7 teams", "Neal Dahlen", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "DJ Lance Rock", "January 15, 2007", "John Garfield", "branch roots", "pH ( / pi\u02d0\u02c8e\u026at\u0283 / ) ( potential of hydrogen )", "geophysicists", "Billy Colman", "360", "November 17, 2017", "Lulu", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey's mother", "\"The Screening Room\"", "designer", "From street corners, buses and subways to phone calls, e-mails, text messages, online posts and tweets,", "a surrogate", "salt", "Ezzard Charles", "consumer confidence"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6716791344727684}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.7499999999999999, 1.0, 0.19999999999999998, 0.5, 0.5454545454545454, 1.0, 0.9523809523809523, 0.7272727272727273, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.5555555555555556, 0.16, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.546875, "CSR": 0.5134447674418605, "EFR": 0.9310344827586207, "Overall": 0.6616302250400963}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational.", "witness", "34", "Miami Beach, Florida,", "\"French team performed the third partial facial transplant on a man who was disfigured by a genetic disorder that created large tumors on his face.", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Cash for Clunkers", "Kim Clijsters", "it has witnessed only normal maritime traffic around Haiti,", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "the outdoors,", "mother.", "Madrid's Barajas International Airport", "1940's", "tax incentives", "two phone calls for delivery: One for pizza, the other for the drug ketamine.", "people have chosen their rides based on what their cars say", "up three of the last four months.", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "Kirchners", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "October 2007,", "CNN's \"Piers Morgan Tonight\"", "Linda Hogan said the statement amounts to a death threat", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "Zhanar Tokhtabayeba,", "his parents", "nearly 28 years", "above zero (3 degrees Fahrenheit),", "Claude Monet pastel drawing of London's Waterloo Bridge", "Princess Diana", "Consumer Reports", "It educates the consumer on how much they are paying for having a low-MPG car", "nine-wicket win over the world's number one ranked Test nation in Melbourne", "Ames, Iowa,", "\"micro-yachtsman\"", "died Wednesday night from injuries he suffered in a single car wreck in Cheatham County, Tennessee.", "Michael Schumacher", "ten amendments that constitute the Bill of Rights", "Title XIX, which became known as Medicaid", "Julian Albans", "line code", "Harry Bailley", "The Muffin Man", "Childeric I", "Roots: The Saga of an American Family", "Almeda Mall", "Greek cheese", "FRAM", "the ross ice shelf", "Bonita Melody Lysette"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5216379147812971}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.08, 1.0, 0.6666666666666666, 0.19999999999999998, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.15384615384615385, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.04761904761904761, 0.06666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.4375, "CSR": 0.51171875, "EFR": 0.9722222222222222, "Overall": 0.6695225694444444}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40 militants and six Pakistan soldiers", "700", "Mandi Hamlin", "breast cancer.", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5 percent", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27 Awa", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "It's unclear what, if any, action might be taken against the mother.", "The attacks have been concentrated in Johannesburg's poorest areas, and many of the victims were Zimbabweans who have fled repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "The child", "a federal judge", "give detainees greater latitude in selecting legal representation", "he fears a desperate country with a potential power vacuum that could lash out.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "Friday to police at the University of California San Diego that she hung a noose Thursday night in the library,", "smiley", "Wanda E Elaine Barzee.", "When the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "late - September through early January", "Europe", "Asia", "piscinae", "Bible", "Supreme Commander Gen. Douglas MacArthur", "PlayStation 4", "ITV", "cricket fighting", "Patty Duke's", "Galileo Galilei", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6896363986402427}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.1111111111111111, 0.19999999999999998, 0.4, 0.9268292682926829, 1.0, 1.0, 0.2181818181818182, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-1187", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-1685", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.578125, "CSR": 0.5131944444444445, "EFR": 1.0, "Overall": 0.6753732638888889}, {"timecode": 45, "before_eval_results": {"predictions": ["Queenscliff Music Festival, Bells Beach SurfClassic and the Bright Autumn Festival", "1-1", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "\" conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a treadmill", "North Korea", "Piers Morgan", "Mary Phagan,", "well over two decades.", "83 eggs.", "1981 drowning death,", "100,000 who fled to neighboring countries last year alone,", "9-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her;", "\"an incompetent and rude president who is senseless and ignorant as he does not know even elementary diplomatic etiquette and lacks diplomatic ability.\"", "15-year-old", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show,\"", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Big Three\"", "Rolling Stone", "\"We wondered how can we protect our dogs' feet against glass,\"", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North could delay the launch if they experience problems with the weather, or within the leadership,", "\"a striking blow to due process and the rule of law.\"", "\" an occupation and said he had not been consulted.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn", "Sunday", "Rwanda", "cancer", "Jose Manuel Zelaya", "October 3,", "college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "an annual road trip,", "Brian Mabry", "invented the \"wall of sound\" in the 1960s and worked with the Beatles, Ike and Tina Turner and other acts.", "Sunday", "December 2, 2013, and the third season concluded on October 1, 2017", "1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Christopher Lloyd", "Nero", "Ethiopia", "Andes Mountains of Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Robert Downey Jr.", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.5, "QA-F1": 0.6121114417989418}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.1, 0.0, 0.0, 0.8148148148148148, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-2966", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3907", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.5, "CSR": 0.5129076086956521, "EFR": 0.96875, "Overall": 0.6690658967391305}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "an insect sting", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "the war years,", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "around Ciudad Juarez, across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Charlotte Gainsbourg", "U.N.", "Ike,", "the American Civil Liberties Union.", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "rebels", "debris", "8,", "new materials", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "in the mouth.", "over 1000 square meters in forward", "Alfredo Astiz,", "WILL MISS YOU!", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "bodies and heads", "Atlanta", "to make life a little easier for these families", "Muqtada al-Sadr,", "a house party", "Ozzy Osbourne", "almost 100", "$81,88010.", "Magyarorsz\u00e1g z\u00e1szlaja", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "tel\u00e9fono", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "latitude", "the University of Washington", "Holly", "Lundy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6675832055927164}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9565217391304348, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-12477"], "SR": 0.546875, "CSR": 0.5136303191489362, "EFR": 1.0, "Overall": 0.6754604388297872}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust,\"", "ultra-high-strength steel and boron", "200", "Alexey Pajitnov,", "1959.", "a lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth", "$40 billion,", "Wales", "Samoa", "more than 100.", "\"It paints a different picture from the one described by former CIA officer John Kiriakou.", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "The nation's new \"first dog\"", "Long Island convenience store", "recanted her claims that she was lured to a dorm and assaulted in a bathroom stall.", "Damon Bankston", "Authorities in Fayetteville, North Carolina,", "hand-painted", "\"bleaching\"", "guard in the jails of Washington, D.C., and on the streets of post- Katierina New Orleans,", "Ventures", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "Deputy Treasury Secretary", "an Italian and six Africans", "supply vessel Damon Bankston was alongside Deepwater Horizon at the time of the blast.", "warning about tendon problems.", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "art fair,", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "The plane", "269,000", "rearview mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "clafaittakas\u00f6zle\u015fmeeryemanayamerymothereevlad\u0131r\u00e7\u00fck\u00fcnucununk\u0131l\u0131birle\u015fmi\u015fmilletlevlad", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "Noam Chomsky", "Frank", "photoelectric", "April 13, 2018"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5461356207449957}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.1904761904761905, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.060606060606060615, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.14814814814814817, 0.0, 0.888888888888889, 0.0, 0.0, 0.19047619047619047, 1.0, 0.42857142857142855, 1.0, 1.0, 0.125, 0.15384615384615383, 0.4, 0.8, 0.0, 0.7555555555555554, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.22222222222222218, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7763", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_naturalquestions-validation-177"], "SR": 0.421875, "CSR": 0.51171875, "EFR": 0.918918918918919, "Overall": 0.6588619087837838}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "themes about love and loss.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "peppermint oil, soluble fiber, and antispasmodic drugs", "fake his own death by crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Adam Lambert and Kris Allen,", "death", "Sunday", "Haitians", "suppress the memories and to live as normal a life as possible; the culture of his time said that he was fortunate to have survived and that he should get on with his", "1981,", "Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "space shuttle mission to upgrade the Hubble Space Telescope.", "at Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41)", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "three", "$250,000", "the WBO welterweight title from Miguel Cotto", "Courtney Love,", "Lee Myung-bak,", "Bahrain", "54", "\"Mr India,\"", "murder in the beating death of a company boss who fired them.", "African National Congress", "walk", "Carl", "maintain an \"aesthetic environment\" and ensure public safety,", "30.3 %", "season seven", "BeBe Winans", "John Major", "Claire Goose", "Bangladesh", "four", "rhyme", "one of WSU's most famous alumni, Edward R. Murrow", "injecton", "Small", "Cheers", "Coleman Hawkins"], "metric_results": {"EM": 0.5625, "QA-F1": 0.676385648410005}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 0.125, 0.8, 0.2222222222222222, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_hotpotqa-validation-2095", "mrqa_searchqa-validation-12017", "mrqa_hotpotqa-validation-864"], "SR": 0.5625, "CSR": 0.5127551020408163, "EFR": 0.9642857142857143, "Overall": 0.6681425382653061}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"feigning a desire to conduct reconciliation talks,", "35,000.", "curfew in Jaipur", "The towering figure of a Muslim revolutionary named Malcolm X", "Four Americans", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan", "Africa", "Haiti", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "a lump in Henry's nether regions was a cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say,", "\"It was a wrong thing to say,", "David McKenzie", "\"If we're going to revise our policies here,", "Daniel Radcliffe", "\"The Da Vinci Code,\"", "exotic sports cars", "the secrets of Freemasonry", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "$60 million by the time the Presidents Day holiday weekend is over.", "4,000 credit cards and the company's \"private client\" list,", "Alison Sweeney,", "33", "Carrousel du Louvre,", "137", "bartering", "21-year-old", "wanted to change the music on the CD player and", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather and", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher", "to the southern city of Naples", "most devices carry few security risks.", "in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "Both women", "Andy Serkis", "late 1989 and 1990", "Davos", "Malm\u00f6", "Richard Attenborough and wife Sheila Sim", "an eclipse", "\"novel with a key\"", "London", "Comanche County, Oklahoma", "Kevin Nealon", "Ireland's", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6625084984459985}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.7692307692307693, 0.6666666666666666, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.8, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.25, 0.16666666666666669, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3650", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-1243", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-6362", "mrqa_hotpotqa-validation-703", "mrqa_searchqa-validation-1891"], "SR": 0.515625, "CSR": 0.5128125, "EFR": 1.0, "Overall": 0.675296875}, {"timecode": 50, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.82421875, "KG": 0.4890625, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria", "11", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shanghai,", "public-television show.", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad", "March 8", "arson", "remote highway in Michoacan state,", "celebrity-studded gala", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the Little Rock Nine,", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "strong work ethic", "12", "Arabic, French and English,", "nearly 40", "Johannesburg", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "was burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "shoot down the satellite", "posting a $1,725 bail,", "Cal Ripken Jr.", "more than 78,000 parents of children ages 3 to 17.", "Apple Inc.", "London's", "\"fusion teams,\"", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "\"Operation Pipeline Express.\"", "Orwell", "Assam Provincial Congress Committee", "the winter solstice", "Frenchman", "sheep", "daisy", "1812", "musicology", "1902,", "Alaska", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7001917180042181}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.9743589743589743, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.9600000000000001, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7777777777777777, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-239", "mrqa_naturalquestions-validation-3688", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-16778"], "SR": 0.578125, "CSR": 0.514093137254902, "EFR": 1.0, "Overall": 0.7100061274509805}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan and Somali governments", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "meter reader", "Diego Maradona", "London", "near Grand Ronde, Oregon.", "rural Tennessee.", "Fakih", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship were at the Rajprasong intersection in the heart of Bangkok.", "14", "Former Mobile County Circuit Judge", "Monday,", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook tablet", "Vicente Carrillo Leyva,", "Dolgorsuren Dagvadorj,", "said they would not be making any further comments, citing the investigation.", "41,", "actor and producer Anil Kapoor", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "Maersk", "the estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the prime minister's handling of the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs", "Charlton Heston", "administrative supervision over all courts and the personnel thereof", "el Cid of Castile", "the old Prussian Landsturm", "Monopoly", "in New York City", "U.S. states of Kentucky, Virginia, and Tennessee", "1999", "beans", "Mountain Dew", "Whopper Of A Scam:", "Japan"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7808148448773449}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-4624", "mrqa_searchqa-validation-13313"], "SR": 0.6875, "CSR": 0.5174278846153846, "EFR": 1.0, "Overall": 0.7106730769230769}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death of a pregnant soldier whose body was found Saturday morning in a hotel, police said.", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "No. 4", "A family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "United", "planned attacks in the southern port city of", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides,", "Kingman Regional Medical Center,", "bronze medal in the women's figure skating final,", "Long Island", "5,600", "\"I get positive feedback because everybody around me likes Obama,\"", "Sharon Bialek", "The chairs are made by prisoners at the South Dakota State Penitentiary and ultimately delivered in Iraq by the U.S. military.", "two", "\"We get a signal prior to violence,\"", "Muslim", "New York appeals court Thursday overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "Evans", "traveling near the Somali coast to use extreme caution because of the recent pirate attacks.", "$24,000-30,000", "in 2008,", "the Tutsi militia led by Kagame defeated the Hutu rebels and took control of the government.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama", "The flooding was so fast that the thing flipped over,\"", "relatives of the five suspects,", "The sole survivor of the crash that killed Princess Diana", "Dubai", "June 6, 1944,", "The supplemental spending bill also contains a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks,", "a laundry facility that allows students to drop off their laundry and pick it up once its clean and smell of dryer sheets.", "true", "the sex organs", "Josie ( Gabrielle Elyse )", "Rebecca Adlington", "Berkshire", "10", "Consigliere", "2007", "The entity", "Jeopary Questions page 788 - REQUESTS - TriviaBistro.com", "a 'Roommate' inspiration", "launch one ship.", "northern latitudes"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5612610653235652}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809523, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.484375, "CSR": 0.5168042452830188, "EFR": 1.0, "Overall": 0.7105483490566038}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Spc. Megan Lynn Touma,", "threatening behavior.", "Argentine", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "simple puzzle video game,", "\"Dancing With the Stars.\"", "African National Congress", "Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred,", "Sunday's", "help nations trapped by hunger and extreme poverty,", "$10 billion", "prosecutors of buckling under pressure from the ruling party.", "April 22.", "Bob Dole,", "twice.", "Long troop deployments in Iraq,", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "60 euros", "Sen. Barack Obama", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "Obama has recognized a key fact that former President George W. Bush did not:", "Sunday", "a share in the royalties for the tune.", "drug cartels", "in a canyon in the path of the blaze Thursday.", "additional information about the actress' drowning,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "George Bernard Shaw,", "shoes", "Herbert Lom", "Battle of Prome", "Union Hill section of Kansas City, Missouri", "Jean- Marc Vall\u00e9e", "Chance", "a pudge", "Rhonda Revelle", "Kwame Nkrumah"], "metric_results": {"EM": 0.625, "QA-F1": 0.7230099422015183}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5714285714285715, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-35", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.625, "CSR": 0.5188078703703703, "EFR": 1.0, "Overall": 0.710949074074074}, {"timecode": 54, "before_eval_results": {"predictions": ["$249", "diabetes and hypertension,", "manufacturers into more efficient technologies and alternative fuels,", "many different", "at least 27", "last week,", "The Peruvian Supreme Court", "Joan Rivers", "\"Watchmen\"", "any resources that could be found there.", "NATO's Membership Action Plan,", "Bangladesh", "250,000", "a complicated and deeply flawed man", "Tillakaratne Dilshan scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai", "reading a novel", "would slow economic growth with higher taxes.", "voluntary manslaughter", "hosting an awards show.", "South Africa", "The noose incident", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "500 feet down an embankment", "Marxist guerrillas", "1918-1919.", "Rwanda", "U.N. High Commissioner for Refugees", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "58 minutes.", "graduate from this school district.", "CNN", "Jobs", "using recreational drugs", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "country music", "President Obama", "Tuesday", "John Wayne", "No", "Kenyan forces who have entered Somalia,", "his health", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "1947", "March 29", "quartz", "Kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United", "March", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6276223776223776}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.42424242424242425, 0.0, 0.07692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-2095", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.5625, "CSR": 0.5196022727272727, "EFR": 1.0, "Overall": 0.7111079545454546}, {"timecode": 55, "before_eval_results": {"predictions": ["\"to pick up the machine guns and silencers,", "without the", "Mexico", "\"I know England does not have the infrastructure to remove snow like we do in Minnesota,\"", "five", "customers are lining up for vitamin injections", "\"not apartment dogs,\"", "actor", "\"We are resetting,", "Cambodian territory", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "June 6, 1944,", "a lightning strike", "11", "Sen. Barack Obama", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "preserved corpses having sex", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "3rd District of Utah.", "Golfer", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "Somalia's piracy problem", "Alaska or Hawaii.", "Robert Park", "Djibouti,", "\"to give or not to give.\"", "Six", "Bahrain", "delivers a big speech", "Facebook and Google,", "Sheikh Sharif Sheikh Ahmed", "2006,", "five", "March 24,", "Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "\"Empire of the Sun,\"", "New Zealand", "a model of sustainability.", "Mutt Lange", "summer", "79", "mahogany marquetry china cabinets", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "\"Strangers In The Night\"", "mass", "an immature, spoiled brat", "mowlam"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5818300189393939}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.8750000000000001, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.53125, "CSR": 0.5198102678571428, "EFR": 1.0, "Overall": 0.7111495535714286}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "evokes childhood memories in this four-line ode to Mom.", "billboards with an image of the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "(FAAN)", "Pat Quinn", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees", "the most-wanted man in the world", "the Louvre.", "militants are suspected of launching attacks inside Pakistan and in neighboring Afghanistan from their haven in the mountainous tribal region along the northwestern border.", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "shows the world that you love the environment and hate using fuel,\"", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "11", "Henrik Stenson", "an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "to increase the flow of water passing through its network of dams.", "Abdullah Gul,", "alcohol and drug abuse", "11th year in a row.", "journalists and the flight crew will be freed,", "the governor's efforts to apparently raise campaign contributions in exchange for signing a horse-racing bill.", "national telephone", "a curtain", "the shootings,", "Ben Roethlisberger", "Larry Ellison,", "Manchester United's", "228", "\"Golden City,\"", "gasoline", "in the county jail in Spanishfork,", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "militants", "July 18, 1994,", "a Celtic people living in northern Asia Minor", "diastema", "to manage the characteristics of the beer's head", "cryonics", "Cambridge", "Mercury", "13 October 1958", "bassline", "Pansexuality", "(Invisibility)", "Zachary Taylor", "Battlestar Galactica", "m Marilyn Monroe"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6274310760903632}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.06060606060606061, 0.9411764705882353, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.08695652173913045, 1.0, 1.0, 1.0, 0.25, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-10329", "mrqa_triviaqa-validation-3538"], "SR": 0.53125, "CSR": 0.5200109649122807, "EFR": 0.9666666666666667, "Overall": 0.7045230263157896}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "(The Frisky)", "5,600", "AMD", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Alina Cho", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\" Frankie Neylon, the town's mayor said.", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "physicist Steven Chu", "U.N. Security Council", "Department of Homeland Security Secretary Janet Napolitano", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "The model set up the foundation after her near-death experience", "South Africa", "Somali,", "returning combat veterans could be recruited by right-wing extremist groups.", "the results outside the French consulate in the oil-rich city of Port-Gentil, on the country's coast.", "Michael Schumacher", "consumer confidence", "the Lakes Golf Club in Sydney, site of this week's Australian Open,", "Longo-Ciprelli", "Fernando Caceres", "the iPods", "a creatine", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Cologne", "$40 and a bread.", "women's", "No. 1 slot at the box office.", "Republican Gov. Jan Brewer.", "remote part of northwestern Montana", "securities", "$150 billion", "the Berlin School of experimental", "Michael Crawford", "the beginning", "Coconut shy", "Fenn Street School", "the outer skin (epidermis)", "Australian", "Argentinian", "fibre optic cable", "hip-hop", "inducere", "Harvard", "129,007,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6515530438764134}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-2824", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-9174"], "SR": 0.59375, "CSR": 0.5212823275862069, "EFR": 0.9615384615384616, "Overall": 0.7037516578249338}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "\"I strongly believe that Ukraine and Georgia should be given MAP,\"", "Six", "money for charities in the Harlem neighborhood.", "\u00a320 million ($41.1 million) fortune", "40", "he'll try to make viewers feel like they're in good hands with him as Emmy host.", "Arthur E. Morgan III,", "Jason Chaffetz", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "fears the problem is much larger than just the TVA.", "\"a hooligan bereft of any personality as a human being,", "President Obama.", "Jacob Zuma,", "on Christmas Eve, 1944,", "including freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "18", "into the Southeast,", "\"Up,\"", "getting into that Lexus, Lincoln, Infiniti or Porsche you always wanted, without laying out $70,000 or $80,000 for something you're not actually going to live in.", "capture that fascinating transformation that takes place when carving a pumpkin.", "school, their books burned,", "a motor scooter", "learn in safer surroundings.", "$50 less,", "J.Crew", "$106.5 million", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\"", "\"black box\" label warning", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "education about rainforests.", "Virgin America", "humiliate herself by standing next to a story,\"", "\"Oh you're, you're really nice,\"", "Kenyan and Somali", "$4 billion,", "1980,", "a man had been stoned to death by an angry mob.", "on board the U.S. ship that was hijacked off Somalia's coast.", "the most-wanted man in the world", "left - sided heart failure", "( 4.09 )", "Bumblebee", "Madness", "Jelly Roll Morton (ca. October 20, 1890 - July 10, 1941)", "vice-admiral", "George Lawrence Mikan, Jr. (June 18, 1924 \u2013 June 1, 2005), nicknamed Mr. Basketball,", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "the United We Stand, Divided We Fall", "professor henry hig"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5979408039580597}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.4444444444444445, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 0.2857142857142857, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 1.0, 0.06451612903225806, 0.0, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 0.6666666666666666, 1.0, 0.058823529411764705, 0.33333333333333337, 0.4, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-3611", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951", "mrqa_triviaqa-validation-7280"], "SR": 0.46875, "CSR": 0.5203919491525424, "EFR": 1.0, "Overall": 0.7112658898305085}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913.", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543", "\"They are, of course, shattered. They", "11 healthy eggs", "2-0", "64,", "\"Zed,\"", "at least two and a half hours.", "shark River Park in Monmouth County", "\"design its own separate strategies for making progress toward achieving this long-term goal.\"", "a gift to the Obama girls from Sen. Ted Kennedy.", "\"I miss your beautiful face and voice,\"", "More than 15,000", "\"Teen Patti\" (\"Card Game\")", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "sumo", "10 below", "\"The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall notices", "Roy", "Grayback forest-firefighters", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "Marxist guerrillas", "Greeley, Colorado,", "five", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "Jacob Zuma,", "Muslim and a Coptic family", "toxic smoke from burn pits", "Fullerton, California,", "\"This is not the spirit of the revolutionaries or the square,\"", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil", "Theodore Roosevelt", "vice-admiral", "Braves", "Jiles Perry (J.P.) Richardson,", "Greek-American", "feats of exploration", "he is often considered the \"godfather\" of U.S-Mexico border cartels.", "Monarch", "the United States", "Dwight D. Eisenhower", "Audi,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6190104166666666}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 0.08333333333333334, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-282", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.5625, "CSR": 0.52109375, "EFR": 1.0, "Overall": 0.71140625}, {"timecode": 60, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.830078125, "KG": 0.47890625, "before_eval_results": {"predictions": ["183", "Johnny Carson", "the fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "teen", "Obama", "Matthew Chance", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police have opened a criminal investigation", "Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "it is not just $3 billion of new money into the economy.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Africa", "in a hotel,", "the only goal of the game", "France", "launching a general strike,", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993,", "in response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Pipeline Express.\"", "Islamabad", "Williams' body", "Conway", "ConAgra Foods plant", "Lalo Schifrin", "April 17, 1982", "Billy Idol", "public", "David Cameron", "every ten years", "five months", "\"The Dragon\"", "1994", "magnolia", "August 1st", "Jupiter", "mural"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6922591017446961}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.4, 0.4210526315789474, 0.0606060606060606, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5502", "mrqa_searchqa-validation-16357"], "SR": 0.609375, "CSR": 0.5225409836065573, "EFR": 1.0, "Overall": 0.7049769467213115}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "the South Dakota State Penitentiary", "$55.7 million", "Friday,", "the 11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "The leftist guerilla group,", "a baseball bat", "six", "a book.", "Venezuela", "The 19-year-old woman", "$1.45 billion", "Iranian consulate,", "VoteWoz.com", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "The Tutsi ethnic minority and the Hutu majority had been at odds even before 1994.", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai", "the BBC's central London offices", "The Lost Trailers have also partnered with Keep America Beautiful, a national organization dedicated to litter reduction and recycling.", "engineering and construction", "\"Drug trafficking is a transnational threat,", "$5 apiece.", "\"procedure on her heart,\"", "were civilians,", "outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "\"The deceased appeared to have been there for some time.\"", "for businesses hiring veterans as well as job training for all service members leaving the military.", "\"He'd like to forget the devastation wrought at the Port-au-Prince harbor where his fleet of trucks used to pick up cargo.", "the UK", "The most important race facing the country is the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "has a thicker consistency and a deeper flavour than sauce", "in skeletal muscle and the brain", "1985", "Dublin", "The character, played by Honor Blackman in the 1964 film version of Goldfinger,", "Lidice", "Columbia", "Wynonna Judd", "most of the youngest publicly documented people to be identified as transgender,", "the Italian occupation", "a sooie", "Canada", "Bolton"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5994608918128655}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4210526315789474, 0.19999999999999998, 1.0, 0.0, 0.2222222222222222, 0.888888888888889, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 0.33333333333333326, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3154", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-4269", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.515625, "CSR": 0.522429435483871, "EFR": 1.0, "Overall": 0.7049546370967742}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "sperm and ova", "Michael Buffer", "14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "the 1820s", "the Tigris and Euphrates rivers", "third", "Andrew Garfield", "The Fixx", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "0.30 in ( 7.6 mm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison, his former bandmate from the Beatles", "Kristy Swanson", "Chairman of the Monetary Policy Committee,", "the system's model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "The procedure requires anastomosis of a singleesthesidymal tubule ( luminal diameter. 15 -. 25 mm ) to the lumen of the vas deferens ( diameter. 3 -. 4 mm )", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "the antibody proteins", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media,", "Tbilisi, Capital of Georgia", "dry lake beds northeast of Los Angeles", "bypasses, to cross major bridges, and to provide direct intercity connections", "obtain a U.S. passport", "a small potato", "Frank Theodore `` Ted '' Levine", "IIII", "in vitro", "the Maginot Line", "the Allies", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card security", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim himself", "Tasmania", "Laura Robson", "Afghanistan", "Todd McFarlane,", "Massachusetts", "one", "\"significant skeletal remains\" consistent with those of a small child on the outer perimeter of the", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the giant mega-yacht 'Wally Island'", "syrup", "the mouth", "locoweed", "December 1974"], "metric_results": {"EM": 0.453125, "QA-F1": 0.573561051065163}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.5, 1.0, 0.5, 1.0, 0.9166666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.92, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.2666666666666667, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.21052631578947367, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-8026", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-11479"], "SR": 0.453125, "CSR": 0.5213293650793651, "EFR": 1.0, "Overall": 0.7047346230158731}, {"timecode": 63, "before_eval_results": {"predictions": ["Lady Agnes", "the Coriolis force", "1776", "2009", "Roger Dean Stadium", "James Brown", "`` Everywhere ''", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "TC", "Article 1", "Rick Rude", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "in 1877", "24 judges, against a maximum possible strength of 31", "c. 1000 AD", "a bow bridge with 16 arches shielded by ice guards", "Dick Rutan and Jeana Yeager", "near major hotels and in the parking areas of major Chinese supermarkets", "July 1790", "King Saud University", "Hugo Weaving", "Book of Exodus", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest,", "Sean O' Neal", "Toby Kebbell", "1078", "James", "Stefanie Scott", "glycine and arginine", "the book and architecture", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "24 -- 3", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "starting in 1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "Berlin", "Marjorie McGinnis", "the Electorate", "U.S. Representative", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "\"Mulholland Drive,\"", "No Child Left Behind", "part of the proceeds"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5878933623639395}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.5, 0.2222222222222222, 1.0, 0.3636363636363636, 0.5714285714285715, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.1379310344827586, 0.4, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311", "mrqa_searchqa-validation-7843"], "SR": 0.4375, "CSR": 0.52001953125, "EFR": 0.9444444444444444, "Overall": 0.6933615451388889}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "maquila", "Turducken", "Patrick Warburton", "the chief priests", "1960", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Ted Mosby", "Kenny Rogers", "between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "from 35 to 40 hours per week", "Effy", "a body", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1996", "In first, the sound films which included synchronized dialogue, known as `` talking pictures '', or `` talkies '', were exclusively shorts", "Far Away", "John C. Reilly", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves on the electrocardiogram", "Brenda", "1792", "Cyanea capillata", "Bonnie Lipton", "2002", "Tom Brady", "Jennifer Saunders", "translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1992", "pesos", "in North Korea", "\"E! News\"", "carbon fiber", "former presidents", "The Greatest Show", "king marilyn"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6535832156574244}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0909090909090909, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-7117", "mrqa_hotpotqa-validation-2069", "mrqa_newsqa-validation-3238", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16408", "mrqa_searchqa-validation-7674", "mrqa_triviaqa-validation-3010"], "SR": 0.546875, "CSR": 0.5204326923076923, "EFR": 1.0, "Overall": 0.7045552884615385}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "season seven finale", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "Paul", "a comic book series", "the belligerents", "ingredients", "George III", "December 1, 2009, introducing the game's protagonist", "four", "the com TLD", "Neil Young", "Ren\u00e9 Verdon", "The Paris Sisters", "the Director of National Intelligence", "Liam Cunningham", "Elliot Scheiner", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "H CO", "StubHub Center in Carson, California", "the Maryland Senate's actions", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea", "a surname of Norman", "1888", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "performance marker", "in Super Bowl LII, following the 2017 season", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge", "Setsuko Thurlow", "John Joseph Patrick Ryan", "1912", "Luke 6 : 67 -- 71", "Ric Flair", "124 and 800 CE", "Pangaea or Pangea", "2008", "Adam Werritty", "the Jets", "\"The Seven Year Itch\u201d", "Kim Jong-hyun", "Edward II", "Harrods", "\"Most of my friends have put in at least a couple hours,\" but boulders and rocks were put in place to prevent further erosion,", "tax credits", "Gary Coleman", "Richard Nixon", "Great Expectations", "cathode", "\"Lucky\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.6480091579616036}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 0.5714285714285715, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.4, 1.0, 1.0, 0.8695652173913044, 0.0, 0.7272727272727273, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.34782608695652173, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 0.1111111111111111, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_searchqa-validation-3591"], "SR": 0.484375, "CSR": 0.5198863636363636, "EFR": 0.9090909090909091, "Overall": 0.6862642045454546}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "May 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "the Near East", "considered harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "decreases as the soil becomes saturated", "Kathy Najimy", "Nicole Gale Anderson", "Disha Vakani", "a transformative change of heart ; especially : a spiritual conversion", "alcohol or smoking, biological agents, stress, or chemicals", "Richard Crispin Armitage", "Himalayas", "professor in Half - Blood Prince", "volcanic activity", "In 1837", "late - September through early January", "In 2010, a version of the song remixed by Tricky Stewart was leaked", "Joseph Sherrard Kearns", "Confederate", "3 September, after a British ultimatum to Germany to cease military operations was ignored", "a loop ( also called a self - loop or a `` buckle '' )", "Carroll O'Connor", "West Egg on prosperous Long Island", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "Supreme Court Rule 11", "after World War II", "Guwahati", "the west - facing core of the crescent on Salamis Bay, which opens into the Saronic Gulf", "Cheap trick", "October 29, 2015", "Udhampur - Srinagar - Baramulla", "16", "3.5 million years old from Idaho, USA", "federal government", "Tigris and Euphrates rivers", "bicameral Congress", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells, including A Modern Utopia ( 1905 ) and Men Like Gods ( 1923 )", "Sarah Brightman", "password recovery tool for Microsoft Windows", "Indo - Pacific distribution", "Tokyo", "moral", "Lana Del Rey", "NBA", "a greyhound, gazelle hound or tazi", "Aristotle", "Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "the Swat Valley.", "anne", "Louisiana", "Scouts of America", "three"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5935437307897641}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.5, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.25, 0.0, 0.8, 0.45454545454545453, 0.0, 0.0, 1.0, 0.2285714285714286, 1.0, 0.0, 0.0, 0.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5263157894736842, 0.0, 0.058823529411764705, 0.05555555555555555, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-4320"], "SR": 0.46875, "CSR": 0.5191231343283582, "EFR": 0.9705882352941176, "Overall": 0.6984110239244952}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1871", "2010", "Clarence Darrow", "B.F. Skinner", "Spanish explorers", "Anna Murphy", "follows a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "December 1, 2017", "Erica Rivera", "McFerrin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear )", "Amartya Sen", "Georgia", "Domhnall Gleeson", "CeCe Drake", "March 11, 2016", "May 2017", "Thomas Mundy Peterson", "Augustus Waters", "boxing", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "the King James Bible", "Bob Peterson", "into the intermembrane space, producing a thermodynamic state that has the potential to do work", "February 25, 2004", "the breast or lower chest of beef or veal", "all purposes, except for proving that a person has the right to drive", "Dr. Hartwell Carver", "two", "in Super Bowl LII", "Dadra and Nagar Haveli", "Charles R Ranch, County Road 24", "a work of social commentary", "his brother", "the Washington metropolitan area", "The euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "the constituency of Richmond in North Yorkshire", "drinking", "the tissues of the outer third of the vagina", "Bergen County,", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "a skunk", "the republic of Belarus", "Matthew Ross", "a pitcher"], "metric_results": {"EM": 0.453125, "QA-F1": 0.599162749096234}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9473684210526316, 1.0, 0.9846153846153847, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.3636363636363636, 0.06666666666666667, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.375, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.7058823529411764, 0.39999999999999997, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.1818181818181818, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-1464", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_searchqa-validation-808", "mrqa_searchqa-validation-3511", "mrqa_triviaqa-validation-2358"], "SR": 0.453125, "CSR": 0.5181525735294117, "EFR": 1.0, "Overall": 0.7040992647058824}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "21 June 2007", "Peter Klaven ( Paul Rudd ), a Los Angeles real estate agent", "G. Hannelius", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Kalinga Ashoka ( son of Bindusara )", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL coaches", "Davos", "Neil Patrick Harris", "1900", "Joel", "the stems and roots of certain vascular plants", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences", "lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Homer Banks, Carl Hampton and Raymond Jackson", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida", "Iran", "the Harry Potter novels by author J.K. Rowling", "the inventor Bi Sheng", "the Prince - Electors", "1799", "Kid Creole & The Coconuts", "a god of the Ammonites", "late - night", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "By 1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "1807", "Miracle", "Dumfries and Galloway, south-west Scotland", "the Crab Orchard Mountains", "President Obama and Britain's Prince Charles", "NATO fighters", "age 19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "E. E. Cummings", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6735484859939802}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.5, 1.0, 0.8, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8837209302325582, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.26666666666666666, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.53125, "CSR": 0.5183423913043479, "EFR": 0.9666666666666667, "Overall": 0.697470561594203}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Manchester United", "The Intolerable Acts", "skeletal muscle and the brain", "libretto", "up to 13 individuals", "1947, 1956, 1975, 2015", "the St. Louis Cardinals", "Andy Serkis", "Panning", "September 21, 2017", "to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "viburnum", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four distinct levels", "to all of the British colonies of North America", "routing information base ( RIB ), is a data table stored in a ISPs or a networked computer that lists the routes to particular network destinations, and in some cases, metrics ( distances ) associated with those routes", "James Rodr\u00edguez", "in AD 95 -- 110", "Johnson", "2,500", "from the top of the leg to the foot on the posterior aspect", "forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "Ashoka", "keratinization", "Hodel", "October 27, 2017", "Howard Caine", "one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Aegisthus", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the Supreme Court of Canada", "September 29, 2017", "around 10 : 30am", "Angola", "Russia", "Manley", "December 27", "Wyatt", "January 2,", "\"In God We Trust\"", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street", "\"settle and seal\" the case rather than endure the expense and embarrassment of defending even a falsely accused chief executive.", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "Namibia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6745742091045084}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.896551724137931, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.9333333333333333, 0.23529411764705882, 1.0, 0.8571428571428571, 1.0, 0.4, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.4615384615384615, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395", "mrqa_triviaqa-validation-5834"], "SR": 0.546875, "CSR": 0.51875, "EFR": 0.9310344827586207, "Overall": 0.6904256465517242}, {"timecode": 70, "UKR": 0.69140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.84765625, "KG": 0.484375, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as", "Lagaan ( English : Taxation", "Super Bowl XXXIX", "almost exclusively land based powers, able to summon large land armies that were very nearly invincibleable", "September 2017", "Kanawha River", "12.65 m", "in the 1820s", "the customer's account", "his cousin D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "the Supreme Court of Canada", "July 1, 1923", "Firoz Shah Tughlaq", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina", "irsten Simone Vangsness", "Frankie Laine's `` I Believe", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker", "De pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Lewis Hamilton", "Tiger Woods", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's", "mid-size four - wheel drive luxurySU", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "Certificate of Release or Discharge from Active Duty", "eye", "Vietnam", "Rutger Hauer", "Canada", "Robert Jenrick", "Srinagar", "Jewish", "Tibet", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wido", "electric currents and magnetic fields"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6193383623978777}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9180327868852458, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.5454545454545454, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.515625, "CSR": 0.518705985915493, "EFR": 0.7419354838709677, "Overall": 0.6568157939572922}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "the following day", "Conservative Party", "Judi Dench", "a scuffle with the Beast Folk", "six degrees of freedom", "Spanish moss", "John Barry", "1990", "investment bank Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "Daytona Pole Award winners", "Charles Carroll of Carrollton", "1959", "indigenous to many forested parts of the world", "Hermia", "Murry farmhouse", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "the middle of the 15th century", "Hasmukh Adhia", "more than 90 percent of the coal consumed in the United States", "Benzodiazepines", "April 1, 2016", "absolute temperature", "redox ( both reduction and oxidation occurring simultaneously ) reactions", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "a cake", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "spacewar", "2016", "1799", "Alba", "Zachary Taylor", "Oscar Wilde", "Galaxy S6", "The New Yorker", "Citgo", "school in South Africa", "\"[The e-mails]", "Rolling Stone", "nuggets", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6939335792131844}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.25, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.19999999999999998, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-10641"], "SR": 0.59375, "CSR": 0.5197482638888888, "EFR": 0.9615384615384616, "Overall": 0.7009448450854701}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Louis XIV", "Ed", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "142", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "The nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season of the television series How I Met Your Mother and 61st overall", "Kansas City Chiefs", "Yuzuru Hanyu", "April", "Ceramic", "February 26, 2018", "ancient Mesopotamia", "The alveolar process", "Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "the first day of every new Congress and in the event of the death, resignation or removal from the Chair of an incumbent Speaker", "Lisa Stelly", "Ali", "Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "Seton Hall Pirates", "13 episodes", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Darren McGavin", "Empire of Japan", "Djokovic", "almost out of competition, scoring only 37.7", "Judy Collins", "2002", "Georgia Nic Nicholson", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "Suntory", "Victoria", "Judah", "horror adventure game"], "metric_results": {"EM": 0.625, "QA-F1": 0.6928260342322843}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47619047619047616, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.972972972972973, 0.09090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.25, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-14264", "mrqa_hotpotqa-validation-1074"], "SR": 0.625, "CSR": 0.5211900684931507, "EFR": 0.9166666666666666, "Overall": 0.6922588470319635}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the lower motor neurons", "Johannes Gutenberg of Mainz", "Jamie Foxx", "a federal republic composed of 50 states, a federal district, five major self - governing territories, and various possessions", "A regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Germany", "March 31 to April 8, 2018", "the North American theater of the worldwide Seven Years'War of 1756 -- 63", "radius R of the turntable", "the United Kingdom ( UK )", "1945", "Mona Vanderwaal", "April 14, 2017", "post translational modification", "1960 Summer Olympics in Rome", "Congress in 1790 passed the first naturalization law for the United States, the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Coldplay with special guest performers Beyonc\u00e9", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W /\ufffd\ufffd / \ufeff 26.617 \u00b0 N 81.617", "German engineer Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "the king's army", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350 at the 2010 census", "Argentina", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority ''", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2002", "Sadie Calvano", "Cress", "Montr\u00e9al", "Queen Victoria", "Gerald Ford", "Bank of China ( Hong Kong) Limited", "Mumbai, Maharashtra", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "Queen of England", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6482313285162551}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 0.14285714285714288, 0.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.16666666666666666, 1.0, 1.0, 0.4444444444444445, 0.8, 0.5, 1.0, 0.823529411764706, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.4545454545454545, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-5271", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.453125, "CSR": 0.5202702702702703, "EFR": 0.9142857142857143, "Overall": 0.6915986969111969}, {"timecode": 74, "before_eval_results": {"predictions": ["the Lgion d'honneur", "Shaft", "a (nonvirtual, paper kind)", "(Prince) melbourne", "pharaoh", "Tony Dungy", "The Heats", "(tal)", "a pungency", "cell", "(1)", "(BPM)", "Enigma", "a tornado", "ponino", "(1832)", "Laryngitis", "Gentle Ben", "terraces", "a voodoo sorcerer", "aquiline", "Hair", "a cozy", "\"Regular Folks\" Ordinary People 1932: \"Magnificent Inn\" Grand Hotel", "Davenport", "Sammy Sosa", "car", "a (One hundred and one if you count the \"and\")", "the (the)llo", "Mount Parnassus", "a hematoma", "a horse", "a snake", "General William Tecumseh Sherman", "Fess Parker", "a duvet", "(from \"Hairspray)", "(l)crevises", "Japan", "country", "(Prince) quizzes", "William Wrigley (chewing gum)", "Nepal", "the United States Department of Agriculture", "cat scratch fever", "freezing", "Jeop Study Set XXIII Flashcards", "a kangaro court", "Whatchamacallit", "The \"Johnny B. Goode\"", "(Prince) Roarke", "humans", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "lord melbourne melbourne", "Sororicide", "Saint Aidan", "Sulla", "Switzerland", "Parlophone Records", "keyboardist and", "150", "a real person to talk to", "the contestant makes a thirty - second call to one of a number of friends ( who provide their phone numbers in advance ) and reads them the question and answer choices"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5285274621212122}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-13285", "mrqa_searchqa-validation-8968", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-6289", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-3495", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1931", "mrqa_newsqa-validation-1890", "mrqa_naturalquestions-validation-5636"], "SR": 0.4375, "CSR": 0.5191666666666667, "EFR": 1.0, "Overall": 0.7085208333333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "paul newman", "Louisiana", "a rabbit", "Tombs of Kobol", "The Sound and the Fury", "a sandwich", "six", "Cosmo Kramer", "Poetic Justice", "(V) Hugo", "the Great Wall", "(Hugh) Jackman", "Silver", "the Republic of Lebanon", "an eagle", "The Communist Party of China", "comedian", "(Prince) Fortinbras", "Mussolini", "Margot Fonteyn", "( Alfred) Nobel", "lifejackets", "exterus", "General Mills", "Emmitt Smith", "a green substance", "a black hole", "Uganda", "Department of the Clerk of the U.S. House", "Heisenberg", "Sin City", "(David) Hyde", "the early period", "Old North Church", "bones", "Red Bull", "a Jolly Roger", "the North West Territories", "Alaska", "the Electric Company", "Heldenplatz", "the City of Bridgeport", "Red River", "a shrub", "Ellen Wilson", "Esau", "a lachrymist", "Agatha Christie", "Ronald Reagan", "Ford Motor Co.", "1947", "Moira Kelly", "Zoe", "Mt Kenya", "Christian Wulff", "Zelle", "Princess Aisha bint Hussein", "French", "(14 October 1633 \u2013 16 September 1701)", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.5, "QA-F1": 0.6118261946386947}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6363636363636364, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-14607", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.5, "CSR": 0.5189144736842105, "EFR": 1.0, "Overall": 0.7084703947368421}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton", "the Treasury", "Montserrat", "Matthew", "Starland Vocal Band", "gallows", "ohm", "Roll of Thunder, Hear My Cry", "earthquakes", "the Potomac", "Iowa", "Mary, Queen of Scots", "Hulk Hogan", "air pressure", "Belarus", "Adam Sandler", "David Letterman", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "sack dress", "Bobby McFerrin", "the United States Navy", "Capitol Hill", "a glider", "a heart", "Guyana", "jelly", "camels", "drought", "ex post facto", "Jonathan Winters", "Pink", "Rhode Island", "Isaac Newton", "Malawi", "Smith", "Elihu Root", "gold", "Joshua", "Jamestown", "coal", "Seymour Cray", "Private Practice", "corticosteroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "yellow", "chalk quarry", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Antonio Maria Costa,", "insomnia"], "metric_results": {"EM": 0.734375, "QA-F1": 0.771875}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7433", "mrqa_searchqa-validation-5351", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512", "mrqa_newsqa-validation-2179", "mrqa_naturalquestions-validation-4442"], "SR": 0.734375, "CSR": 0.5217126623376623, "EFR": 1.0, "Overall": 0.7090300324675325}, {"timecode": 77, "before_eval_results": {"predictions": ["Leif Ericson", "Eskimo", "Rome", "the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan", "Catherine", "Leon Trotsky", "Belgium", "Wendy Beckett", "1066", "ibuprofen", "vrijbuiter", "Carver", "... Bulldog", "Spooky Salem, MA", "the Persian Gulf", "the Baltic Sea", "\"Nolo contendere\"", "gum", "Abel", "Louis XV", "Edmonton Oilers", "Anna Karenina", "Sacramento", "the Cordillera", "jury dutyserve", "... Scribd", "pantaloons", "Confucius", "Paul Newman", "Charles H McKenzie", "...Hear... A shot glass is a small glass that holds approximately 1.5 oz (45 mL), made for drinks intended to...", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "Haircut 100", "Cairo", "Latin", "Venus", "the Hawthorne", "the Congo River", "Charles VII", "Horatio Nelson", "a crocodile", "Ferrari", "iris", "John Adams", "1886", "Ali", "Tahrir Square", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "R&B vocal group", "Kansas Joe McCoy", "protective shoes", "Madonna", "Transportation Security Administration", "silver"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5931625939849623}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8571428571428571, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5181", "mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-7100", "mrqa_searchqa-validation-13301", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-7197", "mrqa_searchqa-validation-337", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-616"], "SR": 0.46875, "CSR": 0.5210336538461539, "EFR": 1.0, "Overall": 0.7088942307692307}, {"timecode": 78, "before_eval_results": {"predictions": ["Amulius", "March", "Christmas Eve", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "cheddar", "a comet", "wings", "the Enigma", "surface-to-air", "a igloo", "Pluto", "a dermatologist", "Kramer vs Kramer", "The Tempest", "yellow", "Annie's", "tire", "Schwarzenegger", "Lafayette", "Iris Murdoch", "Ironman", "Swahili", "NHL", "silk", "a course", "the Pharaoh", "The Thousand and One Nights", "Scott McClellan", "Jeremiah", "Thomas Edison", "The Chorus Line", "Guadalajara", "Sydney", "pastries", "Dutchman", "Gideon", "the Alamo", "oats", "Zlatan Ibrahimovic", "tuition", "Eric Clapton", "being buried alive", "Ballets: Frank", "KU", "Helsinki", "kidney", "One Flew", "the Nobel Prize in Literature", "copper", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "cooperative", "Big John Studd", "Juan Manuel Mata", "Madeleine L'Engle", "British troops", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.59375, "QA-F1": 0.655109126984127}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-5300", "mrqa_naturalquestions-validation-6417", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.59375, "CSR": 0.5219541139240507, "EFR": 0.9615384615384616, "Overall": 0.7013860150925024}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "New Zealand", "Peter", "Conditional Registration", "New Zealand", "fontanels", "California", "Nero", "the Dalmatians", "(Daniel) Day-Lewis", "cotton", "Bridget Fonda", "South Africa", "truncheons", "the Mediterranean", "Catherine", "pancakes", "the adder", "a Saturday morning", "the Thames", "(PIE) FLINGING", "Pitcairn", "Reynolds", "Mayo", "Jerry Maguire (1996)", "arrested Development", "(the) Renaissance", "(chant-stop) alternation", "Rodeo", "repent", "(Denzel) Washington", "Bonn", "nougat", "Erdman", "rani", "Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the Eifel", "(NaCl)", "Samsonite", "Capsicum", "salaam", "(John) Dalton", "pearls", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Steelers", "Ethel Merman", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "615", "over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6298363095238095}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-8019", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_hotpotqa-validation-5541", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.5625, "CSR": 0.5224609375, "EFR": 1.0, "Overall": 0.7091796875}, {"timecode": 80, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.83203125, "KG": 0.48125, "before_eval_results": {"predictions": ["(George Washington) Washington", "the National Hockey League (NHL)", "blue", "Georgia", "(General) William Devereaux", "scalpels", "the English Channel", "William Shakespeare", "France", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Bartholomew Cubbins", "leukemia", "Target", "Frank Sinatra - A-Z Lyrics", "a possum", "Shaun Of The Dead", "Pamplona", "Easter Island", "(Malle) Babbe", "Madonna", "drought", "a staycation", "a best not to take risks even when it seems boring or difficult", "Canaan", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a high occupancy vehicle", "11:00 am", "Benjamin Harrison", "skyscraper", "Billy the Kid", "The Killing Fields", "Oliver Twist", "a landmark", "lamb", "a loaf of bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the Grand Canal", "Sons Of Liberty", "a telescope", "Catholic", "a trumpet", "a backfire", "a circle", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "16", "dragonflies", "cranberries", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "2006", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui."], "metric_results": {"EM": 0.65625, "QA-F1": 0.6995764652014651}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-2608", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.65625, "CSR": 0.5241126543209876, "EFR": 1.0, "Overall": 0.7065412808641975}, {"timecode": 81, "before_eval_results": {"predictions": ["order", "Warsaw", "Katrina And The Waves", "the French & Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Luck Of TheDraw", "Titanic", "pickles", "a Hellenistic statue", "neurons", "Evian", "a geese", "The Life and Death of a Man of Character", "the olfactory nerve", "a window", "Newton", "SpeedMatch", "Alexander Hamilton", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "comedy", "Charlie Watts", "a widow", "a woods", "Virginia", "abundant", "Albert Schweitzer", "the right hemisphere", "a dive bomber", "Henri de Toulouse-Lautrec", "Helen Hayes", "nihilistic", "woods", "Herbert George Wells", "\"Sex In Crazy Places\"", "Terry Caster and his wife, Barbara", "a hippo", "Friedrich Nietzsche", "a dog eat dog world", "Alexander Hamilton", "American", "Niagara Falls", "a boat", "carrots", "the Flintstones", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "the trunk", "Carrefour", "Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "the Catholic League", "Quentin Tarantino"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5643398268398268}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.060606060606060615, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-16557", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-1250", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_triviaqa-validation-5750"], "SR": 0.453125, "CSR": 0.5232469512195121, "EFR": 0.9428571428571428, "Overall": 0.694939568815331}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "Lost in transportation", "a beaver", "Dorothy", "Survivor: Fiji", "the Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Calvin Klein Eternity", "Marvell", "Quiz Show", "NFL", "acetone", "Heart of Darkness", "Psycho", "Napoleon", "a lullaby", "a tape", "Napoleon", "the Gulf of Aden", "the reticulated", "Munich", "a digestif", "a straggler", "Pope Benedict XVI", "Los Alamos Scientific Laboratory", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "the Goldenrod", "Luke", "the rectum", "pterodactyl", "the frequency", "Grease", "a salamander", "Alexander Solzhenitsyn", "eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "Vanity Fair", "the Big Sky Conference", "the beavers", "Boston", "Michelle Pfeiffer", "a ruckus", "Sweden", "Ajay Tyagi", "the 17th episode in the third season", "94 by 50", "Salix", "the 5th", "the coasts of the British Isles", "the University of Kentucky", "Love at First Sting", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "$10 billion", "Princess Diana"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6970734126984127}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.59375, "CSR": 0.5240963855421688, "EFR": 0.9615384615384616, "Overall": 0.698845719416126}, {"timecode": 83, "before_eval_results": {"predictions": ["the Gulf of Tonkin", "(Phil) Cobb", "( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Christian", "cinnamon", "The Pirates of Penzance", "extreme", "St. Patrick\\'s Day", "beer", "Wall Street", "(XVI)", "Trinity College", "Geneva", "Asklepios", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth", "William Faulkner", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the troposphere", "(Paul) McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "the Marshall Islands", "Nepal", "Thomas Jefferson", "names of God", "American Graffiti", "Hair", "cicadas", "Asbury Park", "(6)", "the saguaro", "(Andrew) Zappa", "hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "bread", "Portugal", "Long Island", "10 years", "Glynis Johns", "Porridge", "Thermopylae", "Magdalene Laundries", "\"$10,000 Kelly,\"", "\u00c6thelwald Moll", "(William) Cavendish", "60 euros", "in solitary confinement", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7552083333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16817", "mrqa_searchqa-validation-3409", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-8538", "mrqa_naturalquestions-validation-960", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-993", "mrqa_newsqa-validation-1509"], "SR": 0.671875, "CSR": 0.5258556547619048, "EFR": 1.0, "Overall": 0.7068898809523809}, {"timecode": 84, "before_eval_results": {"predictions": ["faster than the 40", "a crescent", "a trident", "Abercrombie & Fitch", "Robert Fulton", "Standard Oil", "Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "carbon-based (organic) chemicals", "Ford", "Louis Rukeyser", "Jupiter", "Clinton", "James Powers", "a chemical element", "Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "La bohme", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "Montego Bay", "a body, body part,", "cyclosporine", "the Northern Mockingbird", "a RESTRICTIVE TYPE OF THIS, CLAUSE", "Comedy", "a owls", "Perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "a seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a Big Dipper", "1924", "741 weeks", "January 17, 1899", "Gen. Douglas MacArthur", "Project Gutenberg", "Indonesia", "formal or informal", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million."], "metric_results": {"EM": 0.640625, "QA-F1": 0.7182291666666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-3921"], "SR": 0.640625, "CSR": 0.5272058823529412, "EFR": 1.0, "Overall": 0.7071599264705883}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "silver", "the Mummy", "the Washington Redskins", "an asteroid", "Ellen Holly", "the Prince", "Pushing Daisies", "July 4th", "the reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "New York", "Luther", "rice", "Frasier", "Kansas City", "the arteries", "\"Chinatown.\"", "improvisation", "Hamlet", "a bottle", "The Aviator", "Alkalinity", "Joseph Campbell", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Chapter 5", "Fiddler On the Roof", "Pitcairn Island", "hockey", "etcher", "Mars", "bone", "David", "pay", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "a `` no - compete '' clause he was unable to wrest", "September 25", "Jessica Simpson", "William Schuman", "the rose bush", "Robert Plant", "Oklahoma", "138,535 people", "Martin Scorsese", "her son has strong values.", "the Florida Everglades.", "Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.7281650641025641}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-15852", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.609375, "CSR": 0.5281613372093024, "EFR": 1.0, "Overall": 0.7073510174418605}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "President of Nicaragua", "Chastity", "Frank Sinatra", "Dmitri Mendeleev", "Kathleen Winsor", "Blitzkrieg", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "John Paul Jones", "The Rolling Stones", "Jay-Z", "Samuel A. Alito", "kings", "Civis", "Hermann Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "cor", "a jigger", "folate", "a constitution", "the eastern Mediterranean", "reality", "bass", "The Last Remake", "hot air balloons", "Tarzan & Jane", "an RBIs", "David Berkowitz", "oblique", "pie", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "Vi\u1ec7t Minh and France", "James Cameron", "One Night / I Got Stung", "Japan", "Whittlesey", "the \"Cisleithanian\" half of Austria-Hungary", "Japan", "Monday.", "seven", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6725723622782447}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 0.4, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-4669", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-3254", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-13992", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-6493", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669", "mrqa_newsqa-validation-917"], "SR": 0.578125, "CSR": 0.5287356321839081, "EFR": 1.0, "Overall": 0.7074658764367816}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "a spinning wheel", "onerous", "the Clown portrait", "Fargo", "the Dailies", "fibreboard", "the River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "dementia", "the lightest interchangeable lens full-frame camera", "the Lowest point", "the Golden Fleece", "Your Money and Your Life", "a caveat", "Macaulay Culkin", "the Tom Thumb", "Edwards", "Hawaii", "the JFK assassination", "the Daniel Boone National Forest", "a cab", "hemoglobin", "Nancy Sinatra", "an inflammation of the canal joining the", "the foxes", "a tabby", "Amerigo Vespucci", "Wisconsin", "the states of the Persian Gulf", "Canada", "bipolar disorder", "a brownie", "the anvil", "Alexander Calder", "honey", "Matthew Broderick", "Christopher Columbus", "Spider-Man", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "Kansas City, Minnesota and St. Croix rivers", "an axiom", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "the animal (such as pork.)", "the albatrosses", "Aigeus, Aegeas or Aigeas (\u0391\u03b9\u03b3\u03ad\u03b1\u03c2),", "Agent Carter", "the Sasanian Empire", "\"Kill Your Darlings\"", "planning processes are urgently needed", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7022569444444444}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-680", "mrqa_hotpotqa-validation-172"], "SR": 0.65625, "CSR": 0.5301846590909092, "EFR": 1.0, "Overall": 0.7077556818181818}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "highchair", "Biggie", "the Rabbi", "John Paul II", "Eva", "Ariel Sharon", "Rich Girl", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "the odds", "the nope", "Spain", "scrabble", "the Caspian Sea", "Warrensburg", "Los Angeles Angels of Anaheim", "Cardiff", "the Ten", "n:49", "go back in the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "rowing", "Transamerica", "Xinjiang", "the 1970s", "the Delacorte", "Henry Clay", "the nylon", "Petsmart", "On the Origin of Species", "Electric Avenue", "the novella", "Jerusalem", "Vanna White", "Toyota", "a bell", "Istanbul", "Fitzgerald", "Dixie", "Xero", "Tycho Brahe", "Tudor", "elisabeth", "purification", "the following day", "the early 1960s", "Taron Egerton", "a linesider", "William de Valence", "The Undertones", "GM", "Premier Division", "The SoLow Project", "led from a Los Angeles grand jury room after her indictment", "Herman Cain", "a grizzly bear", "Harrison Ford"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5942708333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-218", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714"], "SR": 0.546875, "CSR": 0.530372191011236, "EFR": 1.0, "Overall": 0.7077931882022472}, {"timecode": 89, "before_eval_results": {"predictions": ["the ermine", "Nemo", "Crayon", "Merriam-Webster", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "kami-no-michi", "in the beginning", "Venus", "the pupil", "Chanel Iman", "Armistice", "Toilet Paper", "the Panama Canal", "Cesare Borgia", "a pearl", "cognac", "Hangman", "Charles Dickens", "October", "Gwine to Run All Night", "henrik Ibsen", "Linkin Park", "a doggy", "storm surge", "the lungs", "gravity", "Captain Cook", "Robert Bruce", "Marlon Brando", "the 17th President of the United States", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-N-Harmony", "the sharks", "Helio Castroneves", "Richard III", "Hugh Grant", "Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Doll", "England, Northern Ireland, Scotland and Wales", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "\" Finding Nemo\"", "Steve Rogers / Captain America", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.625, "QA-F1": 0.6789434523809523}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-5512", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.625, "CSR": 0.5314236111111111, "EFR": 1.0, "Overall": 0.7080034722222223}, {"timecode": 90, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.84375, "KG": 0.48125, "before_eval_results": {"predictions": ["Wisconsin", "business", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "New York", "the guillotine", "the Vampire bat", "Tunisia", "a plexus", "a rattlesnake", "(Nicholas) Massie", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond The Sea", "\"AA\"", "Catherine of Aragon", "flag", "Ravi Shankar", "Bangkok", "Spain", "archery", "oblique", "( Joe) Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "paddy", "Matt Leinart", "Alabama", "a witch", "Queen Anne", "the banjo", "a double feature", "Nabokov", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Tony Orlando and Dawn", "AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1913", "Paris", "Point Place,", "11", "in the National Aviation Hall of Fame class of 2001", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com:", "prisoners at the South Dakota State Penitentiary", "Anne Boleyn"], "metric_results": {"EM": 0.75, "QA-F1": 0.7793898809523809}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-10419", "mrqa_triviaqa-validation-5302", "mrqa_hotpotqa-validation-4018"], "SR": 0.75, "CSR": 0.5338255494505495, "EFR": 0.9375, "Overall": 0.7014526098901099}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a Chile Relleno", "Oliver Twist", "Slayer", "the Vistula", "Coriolanus", "Fort Worth", "an aide-de-camp", "the oblique fracture", "Roman Polanski", "Court TV", "the Sahara", "Jake La Motta", "blog", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Sunni Islam", "Madeleine Albright", "Boggy Peak", "the Renaissance", "Calamity Jane", "John Lennon", "Zayn", "MVP", "daytime running lights", "Tarzan Of The Apes", "Once", "Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Luck Of The draw", "Friday", "Lord North", "Doublemint", "the euro", "the narwhal", "the wall", "a boyfriend or girlfriend", "Wyatt Earp", "Punjabi", "Nymphodorus", "Department of Agriculture", "heels", "Frottage", "a canton", "1999", "cheated on Miley", "2017", "Oskar Schindler", "peterloo massacre", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Lars von Trier", "come together and \"set a long-term goal for reducing\" greenhouse emissions.\"", "Audrey Roberts"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6164930555555554}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-5282", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.546875, "CSR": 0.5339673913043479, "EFR": 1.0, "Overall": 0.7139809782608696}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Tzeitel", "(Usama) Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "handles", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "Monty Python", "Al Gore", "asteroids", "first base", "cork", "Ichabod Crane", "T. rex", "\"Chinatown.\"", "butterfly", "Lolita", "the Rheingold", "tango", "(Wesley) Clark", "a sirloin", "adj-noun", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Galatians", "Lewis Carroll", "meters", "a corn", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "(Edouard) Manet", "sons", "The Hairy Ape", "Jason Flemyng", "eight", "British citizens", "Nicholas Garland", "Abraham Lincoln", "c. 40,000-10,000 years ago", "1968", "Vytautas \u0160apranauskas", "a Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides", "September 1947"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6944444444444444}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-14833", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-6104", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_naturalquestions-validation-2586"], "SR": 0.65625, "CSR": 0.5352822580645161, "EFR": 0.9090909090909091, "Overall": 0.696062133431085}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On The Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a kidney", "Paris", "Gangbusters", "the 1920s", "Maine", "Gertrude Stein", "The Sun Also Rises", "bathroom", "For Heaven's Sake, Don't Touch the Mona Lisa", "cricket", "Death", "Mount Everest", "Rouen", "Varney Air Lines", "Notre Dame", "Tiberius", "Jupiter", "loverly", "scrumhalf", "the Falkland Islands", "1968", "Iceland", "Tintin", "a checkerboard", "a thunderstorm", "Jonathan Swift", "\"Miracle on 34th Street\"", "a turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel", "the Mesozoic", "Eisenhower", "For What It\\'s Worth", "the Fourteen Points", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "Post", "the Misty Mountains", "a cantaloupe", "London", "Sandburg", "federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "The Enchantress", "James Earl Jones", "a messenger", "kenry the kenster", "the Treaty of Waitangi", "Jessica Lange", "Heinkel Flugzeugwerke", "Kenan & Kel", "304,000", "one", "8 p.m. local time Thursday", "digging ditches."], "metric_results": {"EM": 0.625, "QA-F1": 0.6734307359307359}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1904761904761905, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9090909090909091, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-1759", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-7804", "mrqa_searchqa-validation-10915", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-5792", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591"], "SR": 0.625, "CSR": 0.5362367021276595, "EFR": 0.9583333333333334, "Overall": 0.7061015070921985}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "a starfighter", "Muqtada al-Sadr", "zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Jimmy Doolittle", "a riot", "Lon Chaney", "New York", "Joel", "Sicily", "Boston Celtics", "sugar", "Enron", "the fulcrum", "the Central African Republic", "Rudolf Hess", "a fight", "the hippopotamus", "an eye", "Bech", "Ronald Reagan", "Washington Irving", "the White Mountains of California", "the Egyptians", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry Mathers", "Nine to 5", "Ginnie Mae", "Extradition", "the head", "the Nutty Professor II", "Michael Collins", "The Sopranos", "The Sound and the Fury", "a mother and a daughter", "Brazil", "obsessive compulsive", "kevin paul pfeiffer", "oatmeal", "arteries", "1773", "one newton", "Justice", "20 November 1989", "about 8 : 20 p.m. on 25 September 2007", "Andrew Moray and William Wallace", "Nafea Faa Ipoipo", "a window", "St. Ambrose", "Newtonian mechanics", "PETE", "SKUM", "12-hour-plus", "Piers Morgan,", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6303308823529412}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-13381", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-1950", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2638"], "SR": 0.515625, "CSR": 0.5360197368421052, "EFR": 1.0, "Overall": 0.7143914473684211}, {"timecode": 95, "before_eval_results": {"predictions": ["Petro Poroshenko", "paul Solotaroff", "the Kuomindang", "The Goonies", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a porter", "fish", "parens", "Casablanca", "The Dutchess", "the Detroit River", "George Sand", "pears", "the Kilimanjaro", "Nebuchadnezzar", "a flip", "a komodo dragon", "Mordecai Richler", "Roseanne", "The West Wing", "a prika", "a ravens", "quesadillas", "Ladd-Franklin", "Pocahontas", "encephalitis", "John Hersey", "Patricia Arquette", "Ernie Banks", "a grotto", "(Prince) Abraham", "Hanifa Rahmawati", "Charon", "Whig", "(Sacha) Llewellyn", "Maria Callas", "wakame", "the Department of Kings", "George Bernard Shaw", "Alfred", "National Geographic", "Johnny Mercer", "Jerusalem", "a circle", "the Edict of Fontainebleau", "Odysseus", "Omega", "before the first letter of an interrogative sentence or clause", "Dr. Lexie Grey ( Chyler Leigh )", "since 3, 1, and 4", "paul esterh\u00e1zy", "exponentiation", "Worcestershire", "1754", "49", "Lowe's", "Ice jams in rivers", "Fernando Verdasco", "Chester Arthur Stiles", "a wasps"], "metric_results": {"EM": 0.4375, "QA-F1": 0.49646577380952384}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-16080", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-1048", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-13468", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-1367"], "SR": 0.4375, "CSR": 0.5349934895833333, "EFR": 1.0, "Overall": 0.7141861979166666}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "balloons", "fallacy", "Nomar Garciaparra", "John Glenn", "a heron", "Apollo 1", "the White Company", "New Balance", "\"S.F.\"", "Joan of Arc", "finale", "mollusca", "Camille Claudel", "the East", "caricaturist", "the Seven Years' War", "\"Pride and Prejudice\"", "The Wizard of Oz", "madding", "Indian tribes", "(Richard) Branson", "Argentina", "Teddy Roosevelt", "the Osmonds", "sul tuo amore", "Whatchamacallits", "Phil Ramone", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "kelp", "Shi'ite Islam", "backstroke", "Makkah", "Sydney", "Dermatology", "Solomon", "Look Who\\'s Talking", "Chirac", "20", "snowmobil", "My ntonia", "Surinam", "noun", "Czechoslovakia", "the Thessalonians", "dilithium", "the Beatles", "1976", "2010", "1215", "Conchita", "President of the United States", "Mumbai", "Bob Gibson", "four", "skeletal dysplasia,", "\"GoldenEye\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5509672619047619}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14706", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-10078", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-7670", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-3859"], "SR": 0.484375, "CSR": 0.5344716494845361, "EFR": 1.0, "Overall": 0.7140818298969073}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "San Jose", "The Two Gentlemen of Verona", "a cobb", "The Hydra", "part iv--a voyage to Laputa", "the Distant Early Warning Line", "Tordis", "ice cream", "the Sikkim", "sonic boom", "Fergie", "Sacramento", "emeralds", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "atoms", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "(Henry) Shrapnel", "Venezuela", "Aglauros", "Oklahoma City", "Brazil", "\"The Opening\"", "the Dugong", "rain", "1869", "the French & Indian War", "chess", "Waterloo", "a waterbed", "mulatta", "a bagel", "a propeller", "a bonnet", "an acre", "( Alexander) Calder", "a cruller", "Helium", "Tokyo", "cheese", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "alaerobic conditions", "\"Big Dipper\"", "Sofia the First", "Australia", "Ben Elton", "an annual road trip", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6666666666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-11709", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-16676", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3859", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.640625, "CSR": 0.5355548469387755, "EFR": 1.0, "Overall": 0.7142984693877551}, {"timecode": 98, "before_eval_results": {"predictions": ["Marley", "Magnum, P.I.", "the Ottoman Empire", "Helen of Troy", "a whale", "New York", "Himalayas", "Wayne\\'s World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "Los Angeles", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "the sea", "\"The army report\"", "Red Hot Chili Peppers", "Sanskrit", "one", "Rock Hudson", "Spain", "Ford", "Sidney Sheldon", "Surround", "wind", "breakfast", "Krispy Kreme", "a foreign dignitary", "Stan Avery", "Death Valley", "the Cumberland Gap", "a yolk", "Secretary of the Navy", "a dwelling place", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Chirac", "Lincoln (1885-89)", "Destiny\\'s Child", "Luxor Las Vegas", "Spain", "\"I Love You Always Forever\"", "coconut", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "Macbeth of Scotland", "Lulach", "Carol Ann Duffy", "Ravenna", "travel diary", "keeping malls safe", "Marines from here were asked to surge into some of the worst fighting since the start of the war in Afghanistan.", "Bahrami", "make life a little easier"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5962740384615384}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12595", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-4890", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3568", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-9553", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-5084", "mrqa_hotpotqa-validation-1364", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.484375, "CSR": 0.5350378787878788, "EFR": 1.0, "Overall": 0.7141950757575758}, {"timecode": 99, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.8515625, "KG": 0.5125, "before_eval_results": {"predictions": ["the Hundred Years' War", "the vertebral column", "Alfred Binet", "Venial sin", "a caution", "the ruby slippers", "Frank's", "the Spanish Republic", "Vanessa Hudgens", "King Kong", "Wizard", "the islands of Southeast Asia", "Rhiannon", "Scotland", "leave It to Beaver", "Kurdish", "Ann Richards", "a half-staff", "Switzerland", "Langston Hughes", "New Coke", "The Color Purple", "the T.H.X. System", "Macbeth", "El Greco", "General Motors", "Daily Mail", "a shark", "Frankie Valli", "a Dagger", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "thriller", "Schwarzenegger", "AT&T", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Finland", "Students for a Democratic Society (SDS)", "All the King\\'s Men", "Charles Gounod", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "James Mason", "a slide", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale, North West England", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775"], "metric_results": {"EM": 0.625, "QA-F1": 0.7008680555555555}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-13935", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-1139", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-1618", "mrqa_newsqa-validation-1996"], "SR": 0.625, "CSR": 0.5359375, "EFR": 1.0, "Overall": 0.7225781250000001}]}