{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5240, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Britain", "He began investigating what he referred to as radiant energy", "Ps. 31:5", "five", "supporting applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "drawn by the convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Dave Logan", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "The Rankine cycle", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "He was an only son, and he was born six", "It's the only NBA team name that uses a state nickname in place of a city", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "Many who had believed in Spiritualism wrote most pathet- ically", "What separates a Cyberpunk setting from a futuristic setting?... set in a lawless subculture of an oppressive society dominated by computer technology", "additional GI Bill that expands education benefits for veterans who have served since the 9/11 attacks"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7440559280814215}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.125, 0.0, 0.0, 0.10526315789473684, 0.46153846153846156]}}, "before_error_ids": ["mrqa_squad-validation-7291", "mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-694", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.6875, "CSR": 0.765625, "EFR": 0.95, "Overall": 0.8578125}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "his animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "the oil shock", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene welding", "war, famine, and weather", "the Wesel-Datteln Canal", "TLC", "on the south side of the garden", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "he may have intercepted Marconi's European experiments in July 1899", "\"The Lodger\"", "1954", "the Sacred Grounds Cafe", "the French word Fondre", "the Green Hornet", "the scrum-half", "\"not just for dancing\"", "Kingston", "the Old French and Latin words meant \"bloody, blood-colored\"", "New Hampshire", "the Tennessee Valley Authority", "the American Kennel Club", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7690393518518519}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1512", "mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-3947", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.703125, "CSR": 0.7447916666666667, "EFR": 1.0, "Overall": 0.8723958333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "rule", "four", "San Joaquin Light & Power Building", "1972", "three", "books, films, radio, TV, music, live theater, comics and video games", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "the US Supreme Court", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th out of 176 total countries", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "the head of government of a country", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT (1110 AM)", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "the Common Core State Standards", "Chickamauga", "a reddish-brown horse", "the National Center for Physical Acoustics", "Gaius Maecenas", "Christopher Tolkien", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "the Chicago White Stockings", "John James Osborne", "Nineteen Eighty-Four", "the Barbizon school", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7447916666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-412"], "SR": 0.71875, "CSR": 0.73828125, "retrieved_ids": ["mrqa_squad-train-47096", "mrqa_squad-train-48855", "mrqa_squad-train-63143", "mrqa_squad-train-34318", "mrqa_squad-train-17595", "mrqa_squad-train-14640", "mrqa_squad-train-81735", "mrqa_squad-train-6587", "mrqa_squad-train-75102", "mrqa_squad-train-5175", "mrqa_squad-train-13034", "mrqa_squad-train-7621", "mrqa_squad-train-55565", "mrqa_squad-train-45451", "mrqa_squad-train-65876", "mrqa_squad-train-48351", "mrqa_squad-validation-9145", "mrqa_squad-validation-236", "mrqa_squad-validation-1891", "mrqa_squad-validation-2226", "mrqa_squad-validation-7687", "mrqa_squad-validation-8558", "mrqa_squad-validation-694", "mrqa_squad-validation-7574", "mrqa_squad-validation-1512", "mrqa_squad-validation-3130", "mrqa_squad-validation-1941", "mrqa_searchqa-validation-11770", "mrqa_squad-validation-6393", "mrqa_squad-validation-2289", "mrqa_hotpotqa-validation-1297", "mrqa_squad-validation-8927"], "EFR": 1.0, "Overall": 0.869140625}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "the Lord's Prayer", "$5 million", "a important role in the hypersensitive response of plants against pathogen attack", "2.666 million", "Industry and manufacturing", "non-violent", "The Parish Church of St Andrew", "1262", "New Orleans", "April 1523", "radiometric isotopes", "the Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Tuesday afternoon", "Pickawillany", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students", "16", "a theory of everything", "Lucas\u2013Lehmer", "Level 3 Communications", "the Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "the ark", "opera", "Okinawa", "a chestave", "the body", "gregorahs", "Casper", "Tarsus", "a luxury department store", "Henry Schleiff", "Louisa May Alcott", "the John F. Kennedy assassination", "Treasure Island", "Death Watch", "Kerry Moosman", "Liqueur Devoille", "white", "The Crush", "in the 1960s", "gregorfeller", "Alistair Grant", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7178865864527628}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1212121212121212]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-455", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.725, "EFR": 1.0, "Overall": 0.8625}, {"timecode": 5, "before_eval_results": {"predictions": ["an ash leaf", "75,000 to 100,000", "1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "specific devolved matters are all subjects which are not explicitly stated in Schedule 5 to the Scotland Act as reserved matters", "SAP Center", "one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea to reduce its volume", "closed", "21 to 11", "The Earth's crustal rock", "formalize a unified front in trade and negotiations with various Indians", "two", "Datanet 1 was the public switched data network operated by the Dutch PTT Telecom (now known as KPN)", "a separate condenser", "to the North Sea, through the former Meuse estuary, near Rotterdam", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "the third most abundant chemical element in the universe, after hydrogen and helium", "39", "The Deadly Assassin", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "C\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the action of propelling the ball toward the wicket defended by.... The circumference of the ball was specified for the first time in 1838 (its weight had been dictated 60 years earlier)", "Donner", "(G) Parker", "the New Netherland Company", "Monrovia", "the umpire is the person charged with officiating the game", "Taiwan", "Omaha", "Beniamino", "the Nez Perce", "Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "Jack Bauer", "Inchon", "February 29", "two weevils", "Alabama", "Bennington", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "the District of Columbia National Guard"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6070328225015725}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4848484848484849, 1.0, 0.5, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.5, 1.0, 0.07692307692307693, 0.0, 0.1, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 0.07407407407407407, 0.0, 0.4, 0.28571428571428575, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-9640", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.484375, "CSR": 0.6848958333333333, "EFR": 0.9696969696969697, "Overall": 0.8272964015151515}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views", "the Bible", "a water pump", "874.3 square miles", "Gender pay gap", "a Scottish Parliament", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger Goodell", "to avoid being targeted by the boycott", "1964\u20131965", "a guru", "British and Europeans", "Judith Merril", "the connection id in a table", "Von Miller", "weekly screenings of all available classic episodes", "a type III secretion system", "nearly 10,000", "12 May 1191", "The Day of the Doctor", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "oxygen concentration is too high", "to destroy the antichrist", "a global village", "Sun City", "the Paris store", "a deer", "the Internet", "the Statue of Liberty", "a closest living relative", "the American Psychiatric Association", "Lenin", "Wild Bill Hickok", "Amtrak", "a log cabin", "The Pianist", "her son", "the king", "a computer", "Richard Cory", "Jay", "South Africa", "a greyhound", "a sea serpent", "the mountains of eastern Nevada", "Trenton", "copper", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "music", "Mexican alcoholic drink Margarita", "prostate cancer", "DNA's structure", "the Pyrenees Mountains"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6807224025974026}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7449", "mrqa_squad-validation-7622", "mrqa_squad-validation-5589", "mrqa_squad-validation-7772", "mrqa_squad-validation-8923", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_hotpotqa-validation-2949", "mrqa_triviaqa-validation-4255"], "SR": 0.609375, "CSR": 0.6741071428571428, "retrieved_ids": ["mrqa_squad-train-60627", "mrqa_squad-train-50464", "mrqa_squad-train-42501", "mrqa_squad-train-16330", "mrqa_squad-train-69240", "mrqa_squad-train-17085", "mrqa_squad-train-16612", "mrqa_squad-train-65401", "mrqa_squad-train-62664", "mrqa_squad-train-67046", "mrqa_squad-train-14912", "mrqa_squad-train-2776", "mrqa_squad-train-17162", "mrqa_squad-train-21039", "mrqa_squad-train-69554", "mrqa_squad-train-10963", "mrqa_squad-validation-2289", "mrqa_squad-validation-3119", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-1701", "mrqa_squad-validation-4528", "mrqa_squad-validation-9489", "mrqa_squad-validation-6393", "mrqa_searchqa-validation-16886", "mrqa_squad-validation-9640", "mrqa_squad-validation-3130", "mrqa_squad-validation-9918", "mrqa_squad-validation-1512", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-2976"], "EFR": 0.96, "Overall": 0.8170535714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "1994 Works Council Directive", "the Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "the Pittsburgh Steelers", "McManus", "Gemini program", "Mick Mixon", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray imaging", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "to civil disobedients", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "the holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "a second fiddle", "Poor family", "Eight Is Enough", "Stockholm", "Humphrey Bogart", "The Name of the Rose", "Thomas Paine", "bivalve", "Fantastic Four", "G4", "LE CINEMA", "Marcus Junius Brutus", "malaria", "Alexander McQueen", "Hairspray", "Johann Wolfgang von Goethe", "masks", "the Oneida Community", "Spitfire floatplane", "Sherman Antitrust Act", "Hafnium", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "economic opportunities", "Officer Joe Harn", "Reid's dismissal"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6526683964183964}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.14285714285714288, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.640625, "CSR": 0.669921875, "EFR": 1.0, "Overall": 0.8349609375}, {"timecode": 8, "before_eval_results": {"predictions": ["During the 1970s and sometimes later,", "an electrical exhibition at Madison Square Garden.", "the Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "slow to complete division", "him to return to his side.", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "the Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine,", "leptospirosis", "\"I can, I think I can,\"", "a subtype of planet", "tango", "a cave", "bamboos", "Nevil Shute", "Livia", "Dracula", "a rail", "ginseng", "Coffee", "Depeche Mode", "pepsi Benches", "a battery pack that generates electrical signals that cause symptoms of certain diseases.", "Pat Sajak", "a hippopotamus", "1492", "the Madding Crowd", "(M) Baryshakov.", "Mars", "the Boston Massacre Trials", "a hoo", "a gun", "Venice", "Mexican Independence Day", "Mrs. Calabash", "Carl Sagan.", "Lauren '', during an attack on the task force, she was wounded and miscarriageried the baby.", "General Paulus", "John Ford", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "electrons are transferred from a donor molecule to an acceptor molecule.", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.5, "QA-F1": 0.6083192711893369}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false], "QA-F1": [0.5714285714285715, 0.5, 0.16666666666666666, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.5, "CSR": 0.6510416666666667, "EFR": 0.96875, "Overall": 0.8098958333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Francis Marion", "parallel importers", "the first Block II CSM and LM in a dual mission known as AS-207/208, or AS-278", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Mario Addison", "improved response is then retained after the pathogen has been eliminated", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "5,500,000", "an adjustable spring-loaded valve", "classical position variables", "( Ursula) K. Le Guin", "(Henry) Gondorff", "George Jetson", "deus ex machina", "an arboretum", "pommel horse", "President William McKinley", "PSP", "Daphne du Maurier", "Turkey", "antonyms", "a wren", "Daughters of the American Revolution", "Morrie Schwartz", "helium", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "a gorillas", "The Pentagon", "oats", "100", "China", "Gone With the Wind", "Edward Albee", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "the pancreas", "the mid-1990s", "Saskatchewan", "Dr Ichak Adizes", "Melpomen\u0113", "Boston", "James Lofton", "an all-ears era", "rape and murdering a woman"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5669270833333334}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-5338", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.515625, "CSR": 0.6375, "retrieved_ids": ["mrqa_squad-train-3665", "mrqa_squad-train-52832", "mrqa_squad-train-4656", "mrqa_squad-train-55031", "mrqa_squad-train-61948", "mrqa_squad-train-58261", "mrqa_squad-train-62419", "mrqa_squad-train-66868", "mrqa_squad-train-1266", "mrqa_squad-train-5524", "mrqa_squad-train-59369", "mrqa_squad-train-18593", "mrqa_squad-train-84835", "mrqa_squad-train-57141", "mrqa_squad-train-2733", "mrqa_squad-train-45275", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-11427", "mrqa_squad-validation-7307", "mrqa_squad-validation-4528", "mrqa_searchqa-validation-16911", "mrqa_squad-validation-455", "mrqa_naturalquestions-validation-866", "mrqa_searchqa-validation-2022", "mrqa_squad-validation-2564", "mrqa_squad-validation-7772", "mrqa_squad-validation-490", "mrqa_searchqa-validation-700"], "EFR": 1.0, "Overall": 0.81875}, {"timecode": 10, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.90234375, "KG": 0.47734375, "before_eval_results": {"predictions": ["Mike Figgis", "1.7 billion years ago", "southern", "technical problems and flight delays", "the fact (Fermat's little theorem)", "Virgin Media", "he would be killed through overwork", "Times Square Studios", "Philip Webb and William Morris", "service to the neighbor", "Amtrak San Joaquins", "circumspect", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "physical control or full-fledged colonial rule", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "a photoelectric cell", "Peggy", "the dna", "The Femme Fatale", "stability", "a pistol", "Quiz", "Aluminium", "country", "the Cenozoic", "the Horn of Africa", "Reddi-wip", "Jeopardy", "tea", "larry fortensky", "the fire surge", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "The Jeffersons", "The Sopranos", "The Crucible", "Ali", "Lunatics and a woman", "Willa Cather", "Aida", "Walden", "the Bergerac region", "the rights to Free Expression", "the handles", "zero", "Australian & New Zealand", "Maine", "Restoration Hardware and Williams - Sonoma", "water can flow from the sink into the faucet without modifying the system", "Beatty", "John Ford", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker", "New Haven, Connecticut, firefighter Frank Ricci"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6402462121212121}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-3790", "mrqa_squad-validation-9734", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-2708"], "SR": 0.53125, "CSR": 0.6278409090909092, "EFR": 1.0, "Overall": 0.7565838068181818}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\"", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79", "concrete", "anti-colonial movements", "Pleurobrachia", "75%", "$60,000 in cash and stock", "oppidum Ubiorum", "studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "Abu Zubaydah", "free", "Bob Dole", "1959", "hackers", "three men with suicide vests who were plotting to carry out the attacks, said Interior Minister Rehman Malik.", "137", "the green grump", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "three", "the insurgency", "Chinese", "war", "war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Hearst Castle.", "it's all claws, all the time.", "Rev. Alberto Cutie", "blind,", "tribunals", "opium", "Obama's race in 2008.", "named his company Polo because \"it was the sport of kings.", "Hawass", "Arabic, French and English", "a minor league baseball team", "seven", "Roberto Micheletti,", "Abu Sayyaf", "63", "chaos and horrified reactions", "warren beaters", "middle of the 15th century", "1966", "J. S. Bach", "Brainy", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "Andorra", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.5, "QA-F1": 0.5602250957854406}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.5, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-4489", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394"], "SR": 0.5, "CSR": 0.6171875, "EFR": 0.96875, "Overall": 0.7482031250000001}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "Prime ideals", "the Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax in Valencia.", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "The towering figure of a Muslim revolutionary named Malcolm X", "Suwardi,", "militant warriors in his take on American involvements in Afghanistan and Iraq, takes Islam -- and Islam alone -- to task for having a diabolic roughness on its fringes.", "U.S. senators", "became a dad.", "Muslim", "California, Texas and Florida,", "Robert De Niro", "Tonga in the group stage, but roared to a 16-0 halftime lead at Eden Park as wing Vincent Clerc and fullback Maxime Medard crossed for tries, while Dimitri Yachvili kicked two penalties.", "Three searches", "creation of an Islamic emirate in Gaza,", "near Garacad, Somalia,", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "his grandfather was a \"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations,\"", "said al-Qahtani was forced to stand naked in front of a female agent,", "Apple employees", "scout who proudly wears a Stetson hat and spurs on his boots,", "Haiti", "Building falls down", "test-launched a rocket capable of carrying a satellite,", "Nieb\u00fcll", "Juan Martin Del Potro.", "20%", "Seoul,", "John Wayne", "Afghanistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Robert Kraft", "Ytterby", "George III,", "Philadelphia, Pennsylvania", "Alien Resurrection", "The Addams Family", "Moscow", "Equestrian"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6354231635481635}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.7692307692307693, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.10810810810810811, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5657", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_hotpotqa-validation-5014", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.546875, "CSR": 0.6117788461538461, "retrieved_ids": ["mrqa_squad-train-65927", "mrqa_squad-train-35667", "mrqa_squad-train-57952", "mrqa_squad-train-80700", "mrqa_squad-train-2647", "mrqa_squad-train-30366", "mrqa_squad-train-25967", "mrqa_squad-train-24870", "mrqa_squad-train-31005", "mrqa_squad-train-5753", "mrqa_squad-train-51492", "mrqa_squad-train-41871", "mrqa_squad-train-11689", "mrqa_squad-train-71941", "mrqa_squad-train-31412", "mrqa_squad-train-55885", "mrqa_squad-validation-8923", "mrqa_squad-validation-10274", "mrqa_squad-validation-9298", "mrqa_squad-validation-7574", "mrqa_searchqa-validation-3259", "mrqa_squad-validation-3130", "mrqa_searchqa-validation-2175", "mrqa_squad-validation-7622", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-5349", "mrqa_squad-validation-3118", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-16625", "mrqa_squad-validation-2372", "mrqa_squad-validation-8864", "mrqa_squad-validation-3559"], "EFR": 1.0, "Overall": 0.7533713942307693}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council", "the son decided to leave al Qaeda.", "rural California", "Hearst Castle", "Paul McCartney and Ringo Starr", "Laura Ling and Euna Lee,", "the north coast of Puerto Rico", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Gadahn,", "iPods", "in the southern port city of Karachi,", "John McCain", "South Africa", "2006", "Iran's nuclear program.", "North Korea", "December 1", "police car sits outside the Westroads Mall in Omaha, Nebraska,", "Haeftling", "i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\"", "San Diego", "tie salesman", "At least 40", "$1,500", "at least 25 dead", "137", "suppress the memories and to live as normal a life as possible", "Coptic Christians and Muslims", "poor", "Ewan McGregor", "The Louvre", "27-year-old", "165-room", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Tyler, Ali, and Lydia", "Kansas", "October", "dance", "Roger (Pemberton)", "Lusitania", "a flat disc", "Casualty", "Turkmenistan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6385845057720058}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.9333333333333333, 0.6666666666666666, 0.5, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2251"], "SR": 0.5, "CSR": 0.6037946428571428, "EFR": 1.0, "Overall": 0.7517745535714286}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946", "$2.50 per AC horsepower royalty", "1990s", "solvents", "Stagg Field", "2010", "Reuben Townroe", "the Black Death", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh's southern Bhola district.", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "bankruptcies", "Inter Milan", "98 people,", "september as soon as 2050,", "race or its understanding of what the law required it to do.", "The Ski Train", "severe", "The six bodies were found Saturday at about 6:30 p.m.", "top designers, such as Stella McCartney,", "Elspeth Cameron-Ritchie,", "homicide", "The signing comes two weeks before the deadline that Defense Secretary Robert Gates had established for the funds.", "\"We'll starve to death, that's all,\"", "onstage demos.", "Tim O'Connor,", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "Twitter", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "genocide", "genocide,", "The oldest documented bikinis", "in Fullerton, California,", "her mom,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"I'd like to see the president bring his message of support for the freedom to marry to a broader audience,", "Consumer Reports", "two women", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland since 1983.", "the Everglades,", "six-year veteran", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "William Tell", "the robin", "Russell Humphreys,", "The Guest", "\"Basket Case,\"", "a platinum cast of a human skull", "The Raiders'move to Las Vegas comes after years of failed efforts to renovate or replace the Oakland", "The earliest recorded planned Viking raid, on 6 January 793"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5171793027411633}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.4615384615384615, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.453125, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.749765625}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally", "North", "Mohammed Mohsen Zayed,", "\"still trying to absorb the impact of this week's stunning events,\"", "President Obama", "Friday,", "CNN affiliate WFTV.", "mysterious scene Sunday before a polo match near West Palm Beach, Florida,", "the station", "sculptures", "along the equator between South America and Africa.", "the 725-mile Veracruz Regatta race,", "200.", "Greece,", "Patrick McGoohan,", "his parents", "$627,", "27-year-old's", "Virgin America", "know what's important in life,", "\"G gossip Girl\"", "Ketchum, Idaho.", "laundromats", "Sporting Lisbon", "tie salesman", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Secretary of State Hillary Clinton,", "look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars\"", "two women killed in a stampede at one of his events in Angola on Saturday,", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "1.2 million people.", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother", "pigs", "Matt Flinders", "Isar", "East of Eden", "Sam Bettley", "33-member", "a boat", "honey", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6852756892230576}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.5454545454545454, 0.9090909090909091, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.10526315789473685, 0.8, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_searchqa-validation-11087", "mrqa_triviaqa-validation-5573"], "SR": 0.59375, "CSR": 0.59375, "retrieved_ids": ["mrqa_squad-train-83198", "mrqa_squad-train-1000", "mrqa_squad-train-47813", "mrqa_squad-train-22273", "mrqa_squad-train-47557", "mrqa_squad-train-43139", "mrqa_squad-train-58615", "mrqa_squad-train-30839", "mrqa_squad-train-33824", "mrqa_squad-train-17329", "mrqa_squad-train-54498", "mrqa_squad-train-13760", "mrqa_squad-train-39700", "mrqa_squad-train-46521", "mrqa_squad-train-66964", "mrqa_squad-train-50075", "mrqa_squad-validation-4908", "mrqa_squad-validation-9178", "mrqa_newsqa-validation-2753", "mrqa_squad-validation-1441", "mrqa_searchqa-validation-15312", "mrqa_hotpotqa-validation-1028", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-2664", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-10604", "mrqa_newsqa-validation-268", "mrqa_squad-validation-5465", "mrqa_squad-validation-694", "mrqa_newsqa-validation-1210", "mrqa_squad-validation-9298"], "EFR": 1.0, "Overall": 0.749765625}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p)", "adjustable spring-loaded", "Grumman", "Synthetic aperture", "A fundamental error", "recant his writings", "diversity", "one can include arbitrarily many instances of 1 in any factorization", "136", "continental European countries", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "Rima Fakih", "fatally shooting a limo driver on February 14, 2002.", "Holley Wimunc.", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "Barnes & Noble", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "software magnate", "U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Hamas ministry spokesman Taher Nunu", "President Obama", "Karen Floyd", "the U.S. Chamber of Commerce", "Kim Il Sung died", "Juan Martin Del Potro.", "Caylee,", "because its facilities are full.", "25 dead", "more than 200.", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African", "in Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award", "Republican", "\" Teen Patti\"", "Eleven", "Hugo Chavez", "Four bodies", "attached to another chromosome", "starch", "(the UK)", "Diptera", "the 100th anniversary of the first \"Tour de France\" bicycle race,", "BBC teletext service Ceefax", "cartilage", "Johannes Brahms", "the 17th century", "Orson Welles"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6715148483482462}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.42857142857142855, 0.47058823529411764, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-7397", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-4478"], "SR": 0.5625, "CSR": 0.5919117647058824, "EFR": 1.0, "Overall": 0.7493979779411765}, {"timecode": 17, "before_eval_results": {"predictions": ["liberalisation", "14th century", "lymphocytes", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "multi-cultural", "the father of the house when in his home", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton", "thermodynamic", "a Russian-language component", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "a remote part of northwestern Montana", "Iran's President Mahmoud Ahmadinejad", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Louisiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006", "the FBI.", "250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "allegations that a dorm parent mistreated students at the school.", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "portugal", "heavy flooding and scattered debris.", "Oxbow,", "Dolgorsuren Dagvadorj", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford", "Ginger Rogers", "five", "Marine Corps", "Garfield", "(or Mal) Cutpurse", "seven", "a leaf", "a transistor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7202504960317461}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425"], "SR": 0.609375, "CSR": 0.5928819444444444, "EFR": 0.96, "Overall": 0.741592013888889}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty drugs", "Doctor of Theology", "Christ", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Sax Rohmer", "Aug 24,", "fractory", "a", "\u00ef\u00bf\u00bd", "Naboth's", "Jeffrey Archer", "C N Trueman", "Anne Boleyn", "Golda Meyerson", "a round, slightly tapered,", "Jonas Bernanke", "Thai", "Parsley the Lion", "Japan", "Runic", "a plutonium", "Andy Murray", "blancmange", "fraxadella", "frattage", "music", "fravelin", "Microsoft", "Austria", "Isambard Kingdom Brunel", "Edward fraem Hunter", "Jamaica", "John Ford", "Petronas", "Beyonce", "Microsoft", "Charles II", "Praseodymium", "The Battle of the Three Emperors,", "Pacific Ocean,", "Trimdon, County Durham,", "Midnight Cowboy", "Dada", "FIFA World Cup 2010", "Southwest Airlines,", "Afghanistan", "Thomas Middleditch", "Rudolf H\u00f6ss", "3 May 1958", "Tom Hanks", "U.S. ship that was hijacked off Somalia's coast.", "canibalism", "Kent's", "Ford Motor Company,", "Banff", "a calves"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5713541666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.53125, "CSR": 0.5896381578947368, "retrieved_ids": ["mrqa_squad-train-83568", "mrqa_squad-train-39319", "mrqa_squad-train-86053", "mrqa_squad-train-10192", "mrqa_squad-train-57008", "mrqa_squad-train-52508", "mrqa_squad-train-10860", "mrqa_squad-train-80185", "mrqa_squad-train-42888", "mrqa_squad-train-31221", "mrqa_squad-train-58547", "mrqa_squad-train-83740", "mrqa_squad-train-82037", "mrqa_squad-train-41107", "mrqa_squad-train-7606", "mrqa_squad-train-65551", "mrqa_squad-validation-3130", "mrqa_searchqa-validation-9733", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3795", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_triviaqa-validation-6277", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-5298", "mrqa_newsqa-validation-3911", "mrqa_squad-validation-4528", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6737", "mrqa_hotpotqa-validation-400"], "EFR": 1.0, "Overall": 0.7489432565789473}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Sky Q Silver", "ash tree", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands, thoracic aorta and the pulmonary artery", "Tony Blair", "The Flintstones", "911", "Jonathan Swift", "South Sudan", "Maria Bueno", "dill", "Frankie Laine", "July 28, 1948", "Thor", "Austria", "Goosnargh", "a bear", "dna structure", "Montr\u00e9al", "ruda", "p\u00e5 forskellige sprog", "The Rocky and Bullwinkle Show", "Ben Drew", "Lackawanna 6", "Poland", "Indiana Jones", "Sousa", "Hyde Park Corner", "Sydney", "Alabama", "Jura", "armoured", "finger", "a meteoroid", "Lew Hoad", "bobbyjo", "lola", "bodhidharma", "Klaus dolls", "Albert Reynolds", "a", "r\u00fcgen island", "Singapore", "cathead", "yellow", "meat", "Vespa", "Squamish", "2015", "Theme Park World", "Cape Cod", "fify bitsy Teeny Weeny Yellow Polka Dot Bikini", "10 percent", "867-5309", "dill", "King Kong", "the small intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5516927083333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.875, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-3139"], "SR": 0.46875, "CSR": 0.58359375, "EFR": 1.0, "Overall": 0.7477343750000001}, {"timecode": 20, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.904296875, "KG": 0.46171875, "before_eval_results": {"predictions": ["heterokontophyte", "non-specific", "1525\u201332", "only a few", "solution", "2011", "random noise", "Wardenclyffe", "jules Verne", "Ogaden", "the Washington Post", "prefecture", "Steve Biko", "leather", "doleoptera", "acute", "nasa bayabasan", "sour", "Beyonce", "Norman Mailer", "Oliver!", "kunsky", "Bolton", "Hawaii", "tsarevitch", "government", "junk", "Hartford", "your Excellency", "George III", "john philip Booth", "severn", "Canada", "Leonard Nimoy", "preston", "john bercow", "Jesse Garon Presley", "Kopassus", "lithium", "40", "The Duchess", "Nick Owen", "white", "China", "Salt Lake City,", "Perseus", "Capricorn", "a 'rugby-specific fit' short", "sergio Garc\u00eda Fern\u00e1ndez", "butterfly", "jason Alexander", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch Interactive", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "heartbreak Hotel", "a rhinoceros", "Wes Craven", "Australian", "King Kelly"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5682291666666666}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8756", "mrqa_squad-validation-2513", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3822"], "SR": 0.515625, "CSR": 0.5803571428571428, "EFR": 1.0, "Overall": 0.7443526785714286}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "the Florida legislature", "gold", "the Chinese", "Surrey", "tESLAR", "wED", "Buzz Aldrin", "lois", "Niger", "Backgammon", "Instagram", "Home alone", "Columbus", "t.S. Eliot", "Venus", "The Wailers", "the Crusades", "jockey benjamin franklin", "curb-roof", "dennis", "pindar Pythian", "tchaikovsky", "Plato", "selene", "Stephen King", "heavy horse", "Catskill Mountains", "dogs", "watts", "fluid", "Jordan", "jerry huggins", "London", "yokohr", "Poland", "treble clef", "forehead", "dill", "yukarist", "between 1330 and 1344", "maple", "Washington, D.C.", "yerkes", "yok", "Melbourne, Victoria, Australia", "meadowbank", "Tangled", "Vincent", "daffy Duck", "inner core", "novella", "The Prodigy", "John Anthony \"Jack\" White", "Michelle Rounds", "21-year-old", "jerry", "Daytona", "Brent", "Mickey's Twice Upon a Christmas", "hiphop"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5130208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-550"], "SR": 0.46875, "CSR": 0.5752840909090908, "retrieved_ids": ["mrqa_squad-train-19599", "mrqa_squad-train-51574", "mrqa_squad-train-853", "mrqa_squad-train-36534", "mrqa_squad-train-61858", "mrqa_squad-train-10424", "mrqa_squad-train-85187", "mrqa_squad-train-28506", "mrqa_squad-train-38143", "mrqa_squad-train-75635", "mrqa_squad-train-33775", "mrqa_squad-train-38088", "mrqa_squad-train-78347", "mrqa_squad-train-79020", "mrqa_squad-train-1545", "mrqa_squad-train-48656", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-10359", "mrqa_newsqa-validation-5", "mrqa_squad-validation-9489", "mrqa_newsqa-validation-2836", "mrqa_searchqa-validation-33", "mrqa_squad-validation-8294", "mrqa_squad-validation-9640", "mrqa_searchqa-validation-5963", "mrqa_newsqa-validation-1425", "mrqa_squad-validation-8923", "mrqa_searchqa-validation-9605", "mrqa_newsqa-validation-2791", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2175"], "EFR": 1.0, "Overall": 0.7433380681818182}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times", "he explored the mountains in hunter's garb", "63,523", "faith in Christ", "Ticonderoga Point", "a seal", "Season 4", "Tyrion", "81", "Dottie West", "October 1980", "Tim Allen", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2018", "California", "variation in plants", "Baltimore, Maryland", "The United States is the only Western country currently applying the death penalty", "Second Battle of Manassas", "104 colonists and Discovery", "left atrium and ventricle", "Mayflower", "1560s", "Davos", "Prince James", "New Orleans", "2008", "U.S. service members who have died without their remains being identified", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "a pop and R&B ballad", "Annette", "September 21, 2017", "cockie", "ABC", "cytoplasm", "carrying an amino acid to the protein synthetic machinery of a cell", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "9.1", "hero", "37.7", "Flag Day in 1954", "1922", "\u201cShine,\u201d", "Popowo", "Ethiopia", "Mountain West Conference", "Sydney", "Talib Kweli", "\"It's really a generation that we've been looking forward to this moment, and the particle detectors and the computers,", "Pastor Paula White", "returning combat veterans", "The Mill on the Floss", "Greenland", "cherry bombs"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5438501082251082}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1232", "mrqa_squad-validation-2919", "mrqa_squad-validation-2373", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-5728"], "SR": 0.46875, "CSR": 0.5706521739130435, "EFR": 1.0, "Overall": 0.7424116847826088}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "Amsterdam Motor Show in April 1948", "the publication of such works as Sant\u014d Ky\u014dden's picturebook Shiji no yukikai ( 1798 )", "In March 2017, Jimmy John's has almost 3,000 stores with plans for expansion up to 5,000 and beyond", "`` Audrey II ''", "T'Pau", "Millerlite", "American comedy web television series", "Universal Pictures and Focus Features", "LED illuminated", "committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "Since the 1983 -- 84 season, Tim Duncan leads the National Basketball Association ( NBA ) in the points - rebounds combination with 840", "the United States economy first went into an economic recession", "in Wales and Yorkshire", "Since 1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Ishaani Ishaan Sinha", "very important in meat technology", "in its home state of Texas, the company has a 52 % market share. Within five months of its entry into Baton Rouge, Louisiana,", "Jodie Foster", "the head of state and the head Of government of Zambia", "May 18, 2018", "In World War II began with its invasion by Nazi Germany on 10 May 1940", "Sally Field", "King Willem - Alexander", "`` It Ain't Over'til It's Over ''", "Massillon, Ohio", "the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "giant planet", "RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "c. 8000 BC", "New York City", "Egypt", "in 1961 during the Cold War", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "asthma", "Pakistan", "Sam Raimi", "7 October 1978", "that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Alabama", "wiki", "gaffer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6279844576719578}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.2857142857142857, 1.0, 0.4, 0.0, 0.0, 1.0, 0.19999999999999998, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8666666666666666, 0.4, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.07407407407407407, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.07407407407407407, 0.3333333333333333, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.515625, "CSR": 0.568359375, "EFR": 0.9354838709677419, "Overall": 0.7290498991935485}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors", "German creedal hymn", "April 20", "Tanzania", "March 29, 2018", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "Samaria", "northern China", "Missouri", "Harry", "May 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1946", "May 3, 2005", "Sean Connery as Allan Quatermain", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1997 ( XXXII ), 1998 ( XXXIII ), 2015 ( 50 )", "Cody Fern", "22 November 1970", "The Star Spangled Banner", "2007", "Orlando, Florida", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "James", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix", "in either Tagalog or English", "R.E.M.", "a blend of ground beef and other ingredients", "Juliet", "a ruthless and bloody land reform campaign ( C\u1ea3i C\u00e1ch Ru\u1ed9ng \u0110\u1ea5t )", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "in rocks and minerals", "various submucosal membrane sites of the body", "in Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "decline", "in the North Cascades range of, Washington", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad ( Real ) selected Doll, while Kamal ( Chance ) selected Hot Wings", "a man who could assume the form of a great black bear", "Robert Plant", "beetles", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Bow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "a man in disguise", "a lamb Cawl", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.5, "QA-F1": 0.601833620800007}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.41379310344827586, 1.0, 0.7741935483870968, 0.0, 0.6, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.5, "CSR": 0.565625, "retrieved_ids": ["mrqa_squad-train-79715", "mrqa_squad-train-83855", "mrqa_squad-train-64943", "mrqa_squad-train-32611", "mrqa_squad-train-16225", "mrqa_squad-train-25165", "mrqa_squad-train-86026", "mrqa_squad-train-77295", "mrqa_squad-train-2785", "mrqa_squad-train-37246", "mrqa_squad-train-52750", "mrqa_squad-train-67309", "mrqa_squad-train-50764", "mrqa_squad-train-19736", "mrqa_squad-train-78552", "mrqa_squad-train-39716", "mrqa_naturalquestions-validation-7003", "mrqa_searchqa-validation-1843", "mrqa_triviaqa-validation-5775", "mrqa_newsqa-validation-697", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-6722", "mrqa_squad-validation-639", "mrqa_newsqa-validation-3965", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-9116", "mrqa_squad-validation-3119", "mrqa_searchqa-validation-7384", "mrqa_triviaqa-validation-7777", "mrqa_searchqa-validation-6374", "mrqa_newsqa-validation-1396", "mrqa_naturalquestions-validation-6340"], "EFR": 1.0, "Overall": 0.74140625}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "9:00 a.m.", "6.4 nanometers", "1894", "the means of production by a class of owners", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "the pyloric valve", "Martin Roberts", "Julia Ormond", "Incudomalleolar", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "in the pachytene stage of prophase I of meiosis", "Hathi Jr.", "the Lower Mainland in Vancouver", "the development of electronic computers in the 1950s", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Madison, Wisconsin, United States", "to weaken the British by cutting off its imports, and strike a winning below with German soldiers transferred from the Eastern front, where Russia had surrendered", "May 26, 2017", "1981", "USS Chesapeake", "arcade mode", "atransformation of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Mad - Eye Moody", "Lee Mack", "without deviating from basic strategy", "England", "1898", "Clarence Anglin", "April 1st", "18 m ( 59.05 ft )", "the Northeast Monsoon", "Michael Crawford", "the 1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "How I Met Your Mother", "The Parlement de Bretagne", "John Spencer", "phosphorus", "Spencer Perceval", "Scotland", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "Venezuela's", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Stark County, Ohio", "Prince Edward VI", "New Orleans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5731683455379759}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.09090909090909093, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.453125, "CSR": 0.5612980769230769, "EFR": 0.9714285714285714, "Overall": 0.7348265796703297}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "those who already hold wealth", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "a virtual reality simulator", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions (FFIs ) to search their records for customers", "the Western world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Ben Rosenbaum", "Lucille Simmons", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "President since creation of the office in 1789", "creatine and phosphocreatine stores are found in skeletal muscle, while the remainder is distributed in the blood, brain, and other tissues", "the lumbar enlargement and the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the 1928 national games", "Geoffrey Zakarian", "Tommy James and the Shondells", "Sparta, Mississippi", "Bonnie Aarons", "2018", "Jay Baruchel", "De Waynene Warren", "2004", "rearview mirror", "the conquest and settlement of the New World, particularly in Puerto Rico", "1986", "the terrestrial biosphere", "1937", "2017", "Beijing", "the court from its members for a three - year term", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "75", "Famous Players-Lasky Corporation", "Tiffany & Company", "Al Gore", "villanelle", "a lifeless, naked body", "a man's lifeless, naked body", "four months ago", "magnesium", "Christopher Newport", "rotunda"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5967067182021654}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.5806451612903226, 0.0, 1.0, 0.5283018867924527, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.09090909090909091, 0.9090909090909091, 0.9, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.4, 0.13333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.2222222222222222, 0.7272727272727273, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-692", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.4375, "CSR": 0.556712962962963, "EFR": 0.9166666666666666, "Overall": 0.722957175925926}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature on the subject", "Dane", "Albert C. Outler", "Royal Engineers", "the Seminole Tribe", "one out of every 17 children under 3 years old in America", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "\"we have more work to do,\" including on the issue of bullying.", "the leftist Workers' Party.", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels", "step up", "helping to plan the September 11, 2001, terror attacks, and believe he may have sought to participate, possibly as the \"20th hijacker.\"", "the plane appeared to have been put on autopilot at around 2,000 feet, over the Birmingham, Alabama, area, before the pilot parachuted to the ground.", "a lizard-like creature from New Zealand", "Conway, Arkansas,", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe,", "Turkey,", "Bill Stanton", "humans", "Herman Thomas", "Schalke", "a lightning strike", "Deputy Treasury Secretary", "Columbia, Illinois,", "Arizona", "two weeks after Black History Month", "the Somali border town of Afmado in an effort to combat Kenyan forces who have entered Somalia,", "Tom Hanks", "outside his house in Najaf's Adala neighborhood after returning from Friday prayers.", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive, according to David Peterka, who was part of the film crew, but was not aboard the plane.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch in a bruising encounter in his native Denmark on Saturday night.", "a Kurdish militant group in Turkey as a terrorist organization, the State Department said.", "1979", "Heshmatollah Attarzadeh near his home in Peshawar as he headed to work at the Iranian consulate, according to Pakistani police and Iranian diplomatic officials.", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "(Heifetz, Perlman, etc.?) based on his technique, musicality, etc", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio and Al Capone", "the American cabinetmaker", "shrimp", "from the skin of their prey, hold them in place and inject them with a paralyzing poison"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5657785419769914}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.23529411764705882, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.5, 0.2857142857142857, 1.0, 0.4615384615384615, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 0.16, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6816", "mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.4375, "CSR": 0.5524553571428572, "retrieved_ids": ["mrqa_squad-train-64875", "mrqa_squad-train-29937", "mrqa_squad-train-16508", "mrqa_squad-train-44024", "mrqa_squad-train-31117", "mrqa_squad-train-73374", "mrqa_squad-train-51636", "mrqa_squad-train-63240", "mrqa_squad-train-72262", "mrqa_squad-train-13644", "mrqa_squad-train-20844", "mrqa_squad-train-78741", "mrqa_squad-train-80476", "mrqa_squad-train-67990", "mrqa_squad-train-78024", "mrqa_squad-train-8875", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3352", "mrqa_searchqa-validation-12750", "mrqa_squad-validation-4489", "mrqa_newsqa-validation-3053", "mrqa_searchqa-validation-12119", "mrqa_triviaqa-validation-2413", "mrqa_newsqa-validation-2733", "mrqa_searchqa-validation-2617", "mrqa_newsqa-validation-334", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-4910", "mrqa_newsqa-validation-3527", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-2972"], "EFR": 1.0, "Overall": 0.7387723214285715}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "Sheikh Sharif Sheikh Ahmed", "Africa", "Thursday and Friday", "Rod Blagojevich", "gasoline", "Winter Park at Union Station in Denver,", "Asashoryu", "not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain", "Peshawar", "Casalesi Camorra", "President George H.W. Bush", "he regrets describing her as \"wacko.\"", "Nick Adenhart", "The nation's foremost concert producer, Charles Jubert,", "unemployment", "environmental", "2009", "problems with the way Britain implements European Union employment directives.", "Louvre", "More than 15,000", "Tens of thousands of new voters", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam, in the Netherlands,", "Kim Clijsters", "Misty Croslin", "Zed", "Nazi Germany", "Sharon Bialek", "Kurdish militant group in Turkey", "military veterans", "41,", "millionaire's surtax", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "Keating Holland", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify", "Haitians", "Bobby Jindal", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "the Italian pignatta", "1973", "rugby", "rage", "Parkinson's", "seven", "Disha Patani", "Anah\u00ed", "Tony Blair", "Excalibur", "witchcraft"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5709001068376068}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.484375, "CSR": 0.5501077586206897, "EFR": 0.9696969696969697, "Overall": 0.7322421956635319}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Denver Broncos", "teach by rote", "opposed to meat consumption by covering themselves in fake blood and lying in human-sized meat packages.", "The premier of \"Dance\" rated highly for Oxygen,", "Robert Barnett,", "business dealings", "British troops", "Jacob Zuma,", "Susan Boyle", "jazz", "\"falling space debris,\"", "Obama's", "30", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC", "Gen. Stanley McChrystal,", "a nationwide manhunt and search for the girl", "Brian David Mitchell,", "Sunday's Christmas parade", "football", "consumer confidence", "Republican", "only normal", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan:", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan", "Adidas", "Sunday's", "treat themselves as an \"extermination\" force that works as the armed front \"of the people and for the people.\"", "Darrel Mohler", "The Casalesi Camorra clan", "Obama and McCain camps", "Sen. Barack Obama", "steep embankment in the Angeles National Forest", "more than 30", "Empire of the Sun", "30-minute", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday,", "Brazil", "Caylee Anthony", "reached an agreement late Thursday", "12:23 a.m. Thursday,", "Toronto", "the United States, its NATO allies and others", "January or early February", "Galileo Galilei", "gods", "paper sales", "Christian Kern", "Indianola", "Wayne County, Michigan", "Willis Towers Watson", "Akihito,", "Dorothy Parker"], "metric_results": {"EM": 0.5, "QA-F1": 0.6536765449978765}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.07692307692307693, 0.25, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.17391304347826086, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.125, 0.4444444444444445, 0.39999999999999997, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.5, "CSR": 0.5484375, "EFR": 1.0, "Overall": 0.7379687500000001}, {"timecode": 30, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.857421875, "KG": 0.50078125, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters between pen-pals", "8 E 3rd St. Wendell,", "Queen Mary II", "the Pula Arena", "Maggie", "Google", "Electron Affinity", "HIV", "a claws", "Jeopardy!", "the Starfighter", "(to)", "the House of Romanov", "a mirror", "yeast", "Thomas Becket", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "a bad one", "Han Solo", "Gutzon Borglum", "Katharine of Aragon", "Zeus", "St. Mark", "the largest head of any known land", "Salman Rushdie", "the United Nations", "Tycho Brahe", "an American sitcom", "the Interior", "elephants", "cloister", "\" Mail to the Chief\"", "Pakistan", "Idiot's DOS", "Clue", "Heath", "Rita", "Woodrow Wilson", "animal cookies", "a tornado", "Omaha,", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Popowo", "Bobby Kennedy", "Mercury", "Nardwuar the Human Serviette", "Niveda Thomas", "1967", "the fires", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6171875}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5951", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-4424"], "SR": 0.59375, "CSR": 0.5498991935483871, "retrieved_ids": ["mrqa_squad-train-86336", "mrqa_squad-train-16342", "mrqa_squad-train-17290", "mrqa_squad-train-6973", "mrqa_squad-train-53658", "mrqa_squad-train-86238", "mrqa_squad-train-45488", "mrqa_squad-train-9669", "mrqa_squad-train-60054", "mrqa_squad-train-51604", "mrqa_squad-train-8184", "mrqa_squad-train-51769", "mrqa_squad-train-71424", "mrqa_squad-train-21264", "mrqa_squad-train-26862", "mrqa_squad-train-18039", "mrqa_newsqa-validation-2945", "mrqa_squad-validation-3543", "mrqa_naturalquestions-validation-3614", "mrqa_squad-validation-4489", "mrqa_naturalquestions-validation-7352", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5955", "mrqa_squad-validation-6809", "mrqa_squad-validation-7449", "mrqa_searchqa-validation-10823", "mrqa_triviaqa-validation-5909", "mrqa_squad-validation-4829", "mrqa_newsqa-validation-2544", "mrqa_triviaqa-validation-1788", "mrqa_searchqa-validation-6843"], "EFR": 1.0, "Overall": 0.7296673387096775}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Carson Palmer", "hail", "the Sierra Nevada", "Florida State Park", "the Hippocratic Oath", "Queen Latifah", "lindo perro", "Shropshire", "the Aegean Sea", "nails", "a bogey", "Sinclair Lewis", "a crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "the Trans Alaska Pipeline", "a trout", "the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "a Crazy Horse", "a look", "Rehab", "the Golden Hind", "Administrative Professionals Week", "Gamal Abdel Nasser", "Eddie Van Halen", "a black bear, moose", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "cashmere", "Diana", "spilled milk", "grasshopper", "carat", "Robin Hood", "the White Cliffs of Dover", "... and feelings", "September 29, 2017", "Wake County", "December 1800", "Nicolas Sarkozy", "the Republican Party", "a Double Whole note", "Rabies", "Environmental Protection Agency", "Robert Gibson", "Mogadishu", "45 minutes, five days a week", "three years"], "metric_results": {"EM": 0.625, "QA-F1": 0.763095238095238}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.625, "CSR": 0.55224609375, "EFR": 1.0, "Overall": 0.7301367187500001}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the peripheral immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "a tornado", "The Taj Mahal", "a banana", "a salmon", "John", "Liverpool", "\"Banjo-playing Deputy\"", "Nassau", "the Mediterranean", "Celsius", "Janet Reno", "Spanish", "Seinfeld", "steroids", "Atlantic City", "\"Who is John Galt?\"", "Clinton", "Iraq", "a potato", "Sans Souci", "Frozone", "Pyotr Ilyich Tchaikovsky", "Malle Babbe", "the Stone", "Paul Gauguin", "Billy Pilgrim", "Louis XIII", "it wasn't meaty enough", "HMS Bronington", "the Sacred Heart", "whiskers", "a cigarette", "\"Spokescow\"", "the dinosaurs", "Peggy Fleming", "Panama", "the electron", "Mali", "Castle Rock", "fuchsia", "the Sahara", "Barbara Bush", "Michelle Pfeiffer", "Sinclair Lewis", "Dame Daphne du Maurier", "\"Airplane\"", "M\u00e1xima of the Netherlands", "New England Patriots", "comprehend and formulate language", "Damon Albarn", "Mazovia", "Ken Burns", "the Wabanaki Confederacy", "Flashback", "Manchester United", "the Yemeni port city of Aden", "between South America and Africa.", "four decades"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5033752705627705}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.7142857142857143, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-13343", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-11721", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-9880", "mrqa_searchqa-validation-16407", "mrqa_naturalquestions-validation-4053", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.390625, "CSR": 0.5473484848484849, "EFR": 1.0, "Overall": 0.7291571969696969}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual teachers", "echinacea", "poker", "(12 oz) cans", "the South African Airways", "the Bronze Age", "Sulphur Island", "Thomas Merton", "ex-wife", "the phantom", "Rodeo Drive", "Mad Mad Mad World", "74.3", "donut", "volcanoes", "deor", "German", "the Mars", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "birds", "Columbia University", "Halloween", "Sexuality", "Greece", "the Inca", "contagious", "Vin Diesel", "the Mob", "New Mexico", "the French Revolution", "the Purple Heart", "Arkansas", "the 7090 mainframe computer", "\"Sorry folks, park's closed.", "the tsuba", "the Sender", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Atonement", "a receipt", "Damascus", "(Ch'iu) daniels", "Innsbruck", "the Genesis flood", "SeaWorld", "the FUE harvesting method", "Article Two", "Newcastle United", "genghis Khan", "(Tom) Mix", "(African violet)", "the Great Northern Railway", "25 October 1921", "East Germany", "\"The Orchid Thief\"", "gunned down by an 88-year-old white supremacists who stepped into the museum with a rifle and began firing.", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6258996212121212}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.09090909090909091, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-10032", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.5625, "CSR": 0.5477941176470589, "retrieved_ids": ["mrqa_squad-train-61352", "mrqa_squad-train-8110", "mrqa_squad-train-6691", "mrqa_squad-train-16775", "mrqa_squad-train-28603", "mrqa_squad-train-58481", "mrqa_squad-train-19504", "mrqa_squad-train-81184", "mrqa_squad-train-42508", "mrqa_squad-train-52972", "mrqa_squad-train-61101", "mrqa_squad-train-32417", "mrqa_squad-train-36070", "mrqa_squad-train-19396", "mrqa_squad-train-43176", "mrqa_squad-train-14356", "mrqa_newsqa-validation-367", "mrqa_squad-validation-9918", "mrqa_searchqa-validation-16816", "mrqa_squad-validation-1512", "mrqa_naturalquestions-validation-114", "mrqa_triviaqa-validation-7334", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-1844", "mrqa_triviaqa-validation-2760", "mrqa_naturalquestions-validation-290", "mrqa_searchqa-validation-4697", "mrqa_naturalquestions-validation-9246", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-2406", "mrqa_naturalquestions-validation-5640", "mrqa_squad-validation-455"], "EFR": 0.9285714285714286, "Overall": 0.7149606092436975}, {"timecode": 34, "before_eval_results": {"predictions": ["independent of each other", "cortisol and catecholamines", "Moon River", "Mighty Joe Young", "(Leif) Ericson", "the West India Company", "Hans Christian Andersen", "a cucumber", "Hershey", "a plate", "a crossword", "Muhammad Ali", "deodorant", "the Supreme Court", "the north magnetic pole", "Putin", "a thunderstorm", "Kennebunkport", "a satellite", "the Black Death", "Devon", "Earhart", "Hoover", "Panty Raid", "French", "cricket", "\"Stagdell\"", "the NFL", "Tonto", "a rodent", "white", "throw them away", "a keypunch", "the Amazons", "The Fugitive", "China", "a forge", "Harpers Ferry", "20 feet", "lilac", "a curve", "Tampa", "ductile", "the King's Men", "Leo", "first anniversary", "nautilus", "salaam", "Bigfoot", "Juris Doctor", "$1.00", "The Thing", "Special Agent Dwayne Cassius Pride ( Scott Bakula )", "Stephen Curry", "Kusha", "Mars", "Captain America", "the Great Depression", "South America", "1998", "Picric acid", "Nineteen", "emergency aid", "Siri"], "metric_results": {"EM": 0.484375, "QA-F1": 0.572420634920635}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10398", "mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-15530", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740", "mrqa_newsqa-validation-3365"], "SR": 0.484375, "CSR": 0.5459821428571429, "EFR": 1.0, "Overall": 0.7288839285714286}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "How I Met Your Mother", "two-state solution", "in-cabin lighting", "little blue booties.", "forgery and flying without a valid license,", "Kurdistan Freedom Falcons,", "mayor of Seoul from 2002 to 2004,", "end of a biology department faculty", "Malawi", "\"fusion teams,\"", "James Whitehouse,", "all buses, subways and trolleys that carry almost a million people daily.", "Muslim", "Muslim festival", "the IAAF", "Fiona MacKeown", "magazine, GospelToday,", "death of cardiac arrest", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "rural Tennessee.", "The BBC", "Plymouth Rock", "a lukewarm CinemaScore", "seven", "Karen Floyd", "Expedia", "Kenneth Cole", "\"wider relationship\"", "death squad killings", "Zilla Torg.", "July", "down a steep embankment in the Angeles National Forest", "piano", "Amy Bishop,", "Arnold and Klein", "Brown and her family", "job training", "State Department", "two years,", "Operation Cast Lead", "Diego Maradona", "21-year-old", "bartering", "Rawalpindi", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,", "Leo Frank,", "Miami", "Buddhism", "Tupolev TU-160,", "President Bill Clinton", "independently in different parts of the globe, and included a diverse range of taxa", "Sophocles", "Humayun", "Vito Corleone", "Caribbean", "Valletta", "The Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "February 22, 1968", "the Palatine Hill", "petrol", "Norbit"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6719991898345556}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.6666666666666666, 0.15384615384615383, 0.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.36363636363636365, 0.0, 0.6666666666666666, 0.36363636363636365, 1.0, 0.0, 0.6, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9268292682926829, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265"], "SR": 0.546875, "CSR": 0.5460069444444444, "EFR": 1.0, "Overall": 0.7288888888888889}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "former boxing champion Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "\"Top Gun\"", "us to step up.", "glass shards", "one", "Jaipur", "Obama", "April 6, 1994", "(the Democratic VP candidate)", "together with two other buildings", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "$1,200.", "Some truly mind-blowing structures", "FARC rebels.", "Dan Brown", "The pilot,", "Paul McCartney", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "Starbucks", "finance", "Monday.", "he was diagnosed with skin cancer.", "Wednesday's arrest was the fourth detention of a top drug cartel leader in recent weeks.", "never at the exact location where I wanted them to dig.", "5,600", "not", "Nearly eight in 10", "(l-r) Paul McCartney,", "at least $20 million to $30 million,", "a vigilante group whose goal is the eradication of the Zetas", "The term was first used in tennis,", "the fovea centralis", "10 years", "Jeffrey Archer", "a palla", "Jack Nicholson", "Flatbush Zombies", "Louis King", "Venice", "bagpipe", "reconnaissance", "Earvin \"Magic\" Johnson Jr.", "Fix You"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6477888431013431}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.1904761904761905, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.05128205128205128, 0.8, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_searchqa-validation-1127"], "SR": 0.53125, "CSR": 0.5456081081081081, "retrieved_ids": ["mrqa_squad-train-11678", "mrqa_squad-train-64110", "mrqa_squad-train-26137", "mrqa_squad-train-36714", "mrqa_squad-train-22244", "mrqa_squad-train-65869", "mrqa_squad-train-30933", "mrqa_squad-train-2852", "mrqa_squad-train-37216", "mrqa_squad-train-73819", "mrqa_squad-train-13707", "mrqa_squad-train-15555", "mrqa_squad-train-12296", "mrqa_squad-train-25177", "mrqa_squad-train-42007", "mrqa_squad-train-64897", "mrqa_triviaqa-validation-4152", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-11087", "mrqa_naturalquestions-validation-10615", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-3398", "mrqa_naturalquestions-validation-4863", "mrqa_newsqa-validation-2936", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1649"], "EFR": 1.0, "Overall": 0.7288091216216216}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings", "Ricardo Valles de la Rosa,", "three", "Sunni Arab and Shiite tribal leaders", "Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "ferry", "1994,", "Belfast, Northern Ireland", "Herman Cain", "Dan Parris, 25, and Rob Lehr, 26,", "Clarkson", "CEO of an engineering and construction company", "London's Heathrow airport", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "take immunosuppression drugs for life so that the body does not reject the donated tissue,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Bergdahl,", "some of the best stunt ever pulled off", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "3,000 kilometers (1,900 miles),", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "highest ever position", "playing Count Dracula and his roles in \"Lord of the Rings\" and \"Star Wars\" films.", "people have chosen their rides based on what their cars say about them.", "10", "artificial intelligence.", "there's no chance", "10", "April 13,", "Samuel Herr,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "more than 2,800", "three", "David Ben - Gurion", "Kiss", "20 years", "jamese", "Noises Off", "aeoline", "Mauthausen", "Delilah Rene", "Tampa Bay Storm", "Pope John Paul II", "art deco", "(Invisibility)", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6186345057940835}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.16666666666666669, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18604651162790697, 0.14285714285714288, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-10329"], "SR": 0.46875, "CSR": 0.5435855263157895, "EFR": 0.9705882352941176, "Overall": 0.7225222523219814}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "environmental and political events", "the U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "Sunday", "heavy turbulence", "Sophia Stellatos.", "Opryland.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40 lashes", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti", "Hanin Zoabi,", "Archbishop Desmond Tutu", "84-year-old", "John Kiriakou", "President George Bush", "humans", "The island's dining scene", "chairman of the House Budget Committee,", "Vice's broadband television network.", "President Robert Mugabe's", "he rejected the option of committing more forces for an undefined mission of nation-building without any deadlines.", "more than 30 Latin American and Caribbean nations", "Lisa Brown", "133", "it would", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "the Italian Serie A title", "the man of Steel jumped off the page and over the airwaves.", "He walked into the Central Methodist Church in downtown Johannesburg and joined a long queue of people waiting for shelter and food.", "mental health and recovery.", "The National Infrastructure Program,", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "Russian flights were carried out in strict accordance with international rules governing airspace above neutral waters,", "President Pervez Musharraf", "two courses", "first grand Slam,", "MS Columbus,", "Derek Mears has a big hockey mask to fill.", "The local Republican Party", "1 October 2006", "1834", "caveolae internalization", "guitar", "a stone", "caffeine", "the University of Keele,", "9,984", "Smithfield, Rhode Island,", "a Vacuum flask", "Donna Rice Hughes", "the albatross", "actress"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5528909738426526}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.1904761904761905, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.06896551724137931, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1142857142857143, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1882", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185", "mrqa_hotpotqa-validation-3314"], "SR": 0.4375, "CSR": 0.5408653846153846, "EFR": 0.9444444444444444, "Overall": 0.7167494658119657}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "Addis Ababa,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "the market makers", "\"has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\" leaving them \"vulnerable to disruption,\"", "Dancy-Power Automotive", "the fact that the teens were charged as adults.", "\"a crusade\" and \"Islamofascism\"", "a one-of-a-kind navy dress with red lining", "Saturday", "alleviation of their pain", "Robert", "suicides", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes", "talk show queen Oprah Winfrey.", "Too many glass shards", "over 1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three", "$50", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "phone calls or by text messaging", "Pakistan", "on Thursday", "he wants a \"happy ending\" to the case.", "fluoroquinolone", "ICE intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.", "Empire of the Sun", "digging", "100 meter", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "1961", "goalkeeper", "the Secret Intelligence Service", "75 mi", "Salmagundi", "grasshopper", "the Knesset", "Secretary of the Interior"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6543921356421356}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08888888888888888, 0.0, 1.0, 0.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_naturalquestions-validation-9953", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-6954"], "SR": 0.546875, "CSR": 0.541015625, "retrieved_ids": ["mrqa_squad-train-27003", "mrqa_squad-train-14974", "mrqa_squad-train-6263", "mrqa_squad-train-75796", "mrqa_squad-train-79275", "mrqa_squad-train-68040", "mrqa_squad-train-80782", "mrqa_squad-train-39385", "mrqa_squad-train-82945", "mrqa_squad-train-24239", "mrqa_squad-train-81354", "mrqa_squad-train-41487", "mrqa_squad-train-46707", "mrqa_squad-train-51519", "mrqa_squad-train-23650", "mrqa_squad-train-69350", "mrqa_triviaqa-validation-3862", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-4552", "mrqa_squad-validation-825", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-3369", "mrqa_newsqa-validation-1420", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-3102", "mrqa_newsqa-validation-1532", "mrqa_squad-validation-5465", "mrqa_searchqa-validation-9116", "mrqa_naturalquestions-validation-8972", "mrqa_newsqa-validation-821", "mrqa_searchqa-validation-1701", "mrqa_newsqa-validation-601"], "EFR": 1.0, "Overall": 0.727890625}, {"timecode": 40, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.865234375, "KG": 0.4921875, "before_eval_results": {"predictions": ["1985", "a nurse who tried to treat Jackson's insomnia with natural remedies", "eight", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher", "CNN's", "NATO", "Lieberman", "the meter reader", "the Gulf", "Petionville, Haiti,", "northwest Pakistan", "Basel", "Pyongyang and Seoul", "\"It feels good for me to talk about her,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "22-10.", "Cairo", "Fakih", "deliver a big speech", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas", "86", "future relations between the Middle East and Washington.", "contraband cell phones", "six", "2004", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "49,", "a pair of hot-looking, two-seater sports cars.", "Iraq", "\"Perfidia,\" \"Walk -- Don't Run\" and \"Diamond Head.\"", "damage the reefs", "Communist", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka", "telling CNN his comments had been taken out of context.", "summer", "Rev. Alberto Cutie", "since 1983.", "witnesses spotted Caylee since her disappearance.", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse can be distinguished from other large flies by two easily observed features", "the third generation", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Richard Attenborough", "500,000 copies", "Latin American culture", "Sylvester Stallone", "a true story", "a martian", "Nippon Professional Baseball"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6016470791769506}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.08, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8750000000000001, 0.9411764705882353, 0.4, 0.33333333333333337, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.25, 0.5454545454545454, 1.0, 1.0, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-1870", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-5265", "mrqa_hotpotqa-validation-5556"], "SR": 0.453125, "CSR": 0.5388719512195121, "EFR": 1.0, "Overall": 0.7269150152439025}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "Amsterdam.", "in the mouth.", "Stephen Johns reportedly opened the door for the man who shot him,", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR's", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "U.S. Navy helicopter crew", "their \"Freshman Year\" experience", "Marcell Jansen", "the jury at Liverpool Crown Court took a little over an hour to clear Gerrard of charges relating to a fracas in a nightclub bar in the north-western of England city on December 29 of last year.", "the Brundell family", "near the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "a sailboat", "FBI's Baltimore field office", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "Lashkar-e-Jhangvi,", "Robert", "in a park in a residential area of Mexico City,", "16 times.", "iTunes,", "into the picturesque Gamla Vaster neighborhood", "the Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "a lock break", "German Chancellor Angela Merkel", "2,700-acre sanctuary", "Missouri.", "Tibetan exile leaders,", "ketamine.", "Haleigh Cummings,", "two and a half hours.", "Bobby Darin,", "New Year's Day", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008,", "took an obscure story of flowers and turned it into the masterful 1998 best-seller \"The Orchid Thief\" (and was then played by Meryl Streep in the movie version, \"Adaptation\").", "Kuranyi's", "Kris Allen,", "on the family's blog", "2", "Supplemental oxygen", "Iran", "Harley", "Roy Rogers", "Harriet Tubman", "a leo spelaea", "German", "Forbes", "black magic or of dealings with the devil", "cholesterol", "Orlando", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5524787287810311}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6363636363636365, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 0.6, 0.2758620689655173, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-13584", "mrqa_naturalquestions-validation-8733"], "SR": 0.40625, "CSR": 0.5357142857142857, "EFR": 0.9736842105263158, "Overall": 0.7210203242481203}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product / market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic plate", "John DiMaggio", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "in the Chicago metropolitan area", "Coldplay", "$5.4 trillion", "3,000 metres ( 9,800 ft )", "Ann Gillespie", "Brooklyn Heights, New York", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "2009", "Fats Waller", "Institute of Chartered Accountants of India ( ICAI )", "2012", "Bette Midler", "push the food down the esophagus", "Walter Mondale", "Nick Sager", "political energy in the international arena had been directed towards the preservation of the League of Nations", "the 18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Michael Madhusudan Dutta", "5 - 7 teams", "Neal Dahlen", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Lance Robertson U.S.", "January 15, 2007", "John Garfield as Al Schmid", "active absorption of water from the soil by the root is mainly affected by?", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "billy Colman", "360", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Evey's mother", "The Screening Room", "designer", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "a surrogate", "salt", "Rocky Marciano", "consumer confidence"], "metric_results": {"EM": 0.46875, "QA-F1": 0.597542057642435}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.7499999999999999, 0.8571428571428571, 0.19999999999999998, 0.5, 0.4, 1.0, 0.5714285714285715, 0.8, 0.0, 0.35294117647058826, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473682, 0.16, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5714285714285715, 0.125, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.1, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.46875, "CSR": 0.534156976744186, "retrieved_ids": ["mrqa_squad-train-64527", "mrqa_squad-train-50957", "mrqa_squad-train-45549", "mrqa_squad-train-67810", "mrqa_squad-train-14686", "mrqa_squad-train-4871", "mrqa_squad-train-36546", "mrqa_squad-train-49426", "mrqa_squad-train-73985", "mrqa_squad-train-56743", "mrqa_squad-train-83580", "mrqa_squad-train-52962", "mrqa_squad-train-35641", "mrqa_squad-train-39397", "mrqa_squad-train-30115", "mrqa_squad-train-9783", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-16917", "mrqa_squad-validation-1583", "mrqa_searchqa-validation-16210", "mrqa_searchqa-validation-11361", "mrqa_triviaqa-validation-3450", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2940", "mrqa_searchqa-validation-15996", "mrqa_squad-validation-8294", "mrqa_newsqa-validation-2936", "mrqa_searchqa-validation-5955", "mrqa_newsqa-validation-3156", "mrqa_naturalquestions-validation-6022", "mrqa_newsqa-validation-3795", "mrqa_naturalquestions-validation-5554"], "EFR": 0.9705882352941176, "Overall": 0.7200896674076607}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "plastic surgery", "William Jelani Cobb is Associate Professor of History at Spelman College,", "Cash for Clunkers", "American third seed Venus Williams", "Haiti,", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Matt Kuchar", "Columbia", "Omar Bongo,", "the outdoors", "mother.", "Madrid's Barajas International Airport", "1940's", "tax", "pizza,", "people have chosen their rides based on what their cars say about them.", "up three of the last four months.", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "not guilty", "Alinghi", "Mexican military", "Sporting Lisbon", "The Kirchners", "came forward Monday \"for the other women who couldn't or wouldn't.\"\"I really hope that what I did will enable other women to come forward in similar situations,\"", "July 1999", "CNN's", "\"weighing all options necessary to protect his client.\"", "London's O2 arena", "Thirteen", "Col. Elspeth Cameron-Ritchie,", "14 people", "parents", "nearly 28 years", "above zero (3 degrees Fahrenheit),", "Claude Monet", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "nine-wicket win", "Ames, Iowa,", "Plymouth Rock", "Joel \"Taz\" DiGregorio,", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Medicaid", "Julia Roberts", "line code", "Harry Bailley", "The Muffin Man", "Clovis I", "Kunta Kinte", "Almeda Mall", "Greek cheese", "Fram", "the Ross Ice Shelf", "\"He Jests at Scars...\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5284098053529589}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6792452830188679, 0.08333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-4198", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.453125, "CSR": 0.5323153409090908, "EFR": 0.9714285714285714, "Overall": 0.7198894074675325}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40", "700", "Mandi Hamlin", "early detection and helping other women cope with the disease.", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27 Awa", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "did not", "repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "the way their business books were being handled.", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge in Mississippi on March 22,", "give detainees greater latitude in selecting legal representation", "he fears a desperate country with a potential power vacuum that could lash out.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "vary, but 70,000 or so are estimated to be there now.", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "\"Mad Men\"", "Barzee", "The military fired warning shots into the air and sprayed water cannons to disperse the crowd.", "Kim Jong Un", "about 3,000 kilometers (1,900 miles),", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "from late - September through early January", "the euro", "Asia", "piscina", "The Bible", "Douglas MacArthur", "PlayStation 4", "Sky News", "cricket fighting", "Patty Duke", "Galileo", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7810854076479077}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.1714285714285714, 1.0, 0.888888888888889, 0.2181818181818182, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3284"], "SR": 0.703125, "CSR": 0.5361111111111111, "EFR": 1.0, "Overall": 0.7263628472222222}, {"timecode": 45, "before_eval_results": {"predictions": ["sports", "1-1", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "is a warning to those who deny human rights.", "Islamist militia", "a cardio to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "Bahrain.", "Piers Morgan", "Mary Phagan,", "well over two decades.", "83 eggs.", "drowned in the Pacific Ocean", "100,000 who fled to neighboring countries last year alone,", "7-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her;", "\"A chicken soaked in the rain,\"", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Big Three\"", "Rolling Stone", "dogs who walk on ice in Alaska.", "Marco Polo", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"It would not make sense for Pyongyang to make such a move after going through official channels with its plans,", "\"a striking blow to due process and the rule of law.\"", "a top-ranking member of La Familia Michoacana drug cartel.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Maria Reisch,", "Sunday", "Rwanda", "cancer", "Jose Manuel Zelaya", "October 3,", "Monterrey,", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "shot death of B-movie queen Lana Clarkson after a night out in the clubs of Hollywood.", "July", "December 2, 2013, and the third season concluded on October 1, 2017", "the North Atlantic Ocean", "Christopher Lloyd", "Nero", "Ethiopia", "the Andes Mountains of Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6070808531746033}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5555555555555556, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.05714285714285714, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3907", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.53125, "CSR": 0.5360054347826086, "retrieved_ids": ["mrqa_squad-train-12300", "mrqa_squad-train-51419", "mrqa_squad-train-13846", "mrqa_squad-train-26299", "mrqa_squad-train-18176", "mrqa_squad-train-26157", "mrqa_squad-train-3527", "mrqa_squad-train-58991", "mrqa_squad-train-35525", "mrqa_squad-train-8241", "mrqa_squad-train-17197", "mrqa_squad-train-11033", "mrqa_squad-train-22219", "mrqa_squad-train-9251", "mrqa_squad-train-4993", "mrqa_squad-train-70955", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-1420", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-3192", "mrqa_triviaqa-validation-7740", "mrqa_naturalquestions-validation-1802", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-5679", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-3300", "mrqa_naturalquestions-validation-1930", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-3591", "mrqa_naturalquestions-validation-9660"], "EFR": 0.9666666666666667, "Overall": 0.719675045289855}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "an insect sting", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "the war years,", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "23", "around Ciudad Juarez, across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Charlotte Gainsbourg", "U.N.", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "Afghanistan", "debris", "8,", "new steel and boron", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "in the mouth.", "over 1000 square meters", "Alfredo Astiz,", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "the highest ranking former member of Saddam Hussein's regime still at large,", "Michelle Obama", "fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "Seoul.", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Muqtada al-Sadr,", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,88010.", "Hungary ( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "n\u00famero", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "Tanzania", "the University of Washington", "Holly", "Lundy Island"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6644210032117641}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.75, 0.9565217391304348, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 0.4, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-12477"], "SR": 0.53125, "CSR": 0.5359042553191489, "EFR": 0.9666666666666667, "Overall": 0.7196548093971631}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "\" Body Works\"", "\"a striking blow to due process and the rule of law.", "make the new truck safer,", "200", "Alexey Pajitnov", "1959.", "lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth's", "$3 billion,", "Ireland", "Samoa", "more than 100.", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "Portuguese water dog", "Long Island convenience store", "arrested, arraigned and jailed,", "Damon Bankston", "Fayetteville, North Carolina,", "hand-painted", "\"The Rough Guide to Climate Change\"", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "Ventures", "10 toasters,", "Deputy Treasury Secretary", "an Italian and six Africans", "Damon Bankston", "warning", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "expansive sandy beaches", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "Airbus A320-214,", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "bulgaria", "czarevitch", "auk", "Tennessee", "from 1993 to 1996", "Minette Walters", "Tom Sennett", "Frank", "photoelectric", "April 13, 2018"], "metric_results": {"EM": 0.5, "QA-F1": 0.6081378384687208}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.11764705882352941, 0.0, 0.8571428571428571, 0.0, 0.0, 0.19047619047619047, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.8, 0.0, 0.7555555555555554, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222218, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7763", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_naturalquestions-validation-177"], "SR": 0.5, "CSR": 0.53515625, "EFR": 0.9375, "Overall": 0.713671875}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "the influence of his Japanese grandmother that first led him on the path to enka.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "The group, Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "peppermint oil, soluble fiber, and antispasmodic drugs", "crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Kris Allen,", "death", "4-1 Serie A win at Bologna on Sunday", "Haiti's", "suppress the memories and to live as normal a life as possible; the culture of his time said that he should get on with his", "1981,", "in terms of the country's most-wanted list, Mejia Munera was one of Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "to put the \"black box\" warning on Cipro and other fluoroquinolones, and also to warn doctors.", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "Hansa", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "at three people and wounded 15 others,", "$250,000", "WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.", "Courtney Love,", "Hu Jintao", "Bahrain,", "54", "in global hit \"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "African National Congress", "walk", "Carl", "maintain an \"aesthetic environment\" and ensure public safety,", "30.3 %", "season seven", "BeBe Winans", "Pickwick", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "Speed Matheson", "Cheers Boned the Fish When", "Coleman Hawkins"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6437345533973824}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 0.9411764705882353, 0.2, 0.7368421052631579, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.25, 0.0, 0.08333333333333334, 1.0, 0.07407407407407407, 0.8, 0.2222222222222222, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 0.25, 1.0, 0.35294117647058826, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_searchqa-validation-11020", "mrqa_searchqa-validation-11614", "mrqa_hotpotqa-validation-864"], "SR": 0.484375, "CSR": 0.5341198979591837, "retrieved_ids": ["mrqa_squad-train-51316", "mrqa_squad-train-70175", "mrqa_squad-train-66601", "mrqa_squad-train-16908", "mrqa_squad-train-44629", "mrqa_squad-train-65951", "mrqa_squad-train-14512", "mrqa_squad-train-8756", "mrqa_squad-train-69756", "mrqa_squad-train-53011", "mrqa_squad-train-52961", "mrqa_squad-train-54753", "mrqa_squad-train-70297", "mrqa_squad-train-55024", "mrqa_squad-train-52971", "mrqa_squad-train-59230", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-522", "mrqa_triviaqa-validation-381", "mrqa_newsqa-validation-695", "mrqa_triviaqa-validation-4442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2438", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9523", "mrqa_squad-validation-1326", "mrqa_searchqa-validation-12876", "mrqa_squad-validation-2226", "mrqa_naturalquestions-validation-2605", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1425"], "EFR": 0.9696969696969697, "Overall": 0.7199039985312308}, {"timecode": 49, "before_eval_results": {"predictions": ["a delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.", "35,000.", "curfew in Jaipur", "Muslim revolutionary named Malcolm X", "Four", "nude beaches.", "The Falklands,", "Pyongyang and Seoul", "Japan", "Africa", "Haiti", "current and historic conflict zones,", "lump in Henry's nether regions was a cancerous tumor.", "his client, Brett Cummins,", "\"It was a wrong thing to say,", "\"It was a wrong thing to say,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "the boy Wizard in \"Harry Potter and the Order of the Phoenix\"", "The Da Vinci Code", "exotic sports cars", "the secrets of Freemasonry", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "$60 million", "4,000 credit cards and the company's \"private client\" list,", "Alison Sweeney,", "33", "Carrousel du Louvre,", "137", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player and the 34-year-old McGee said the football star had acted aggressively in trying to grab the device.", "\"momentous discovery\" by the Amyotrophic Lateral Sclerosis  Association.", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "teenage", "almost 100 vessels", "Matthew Fisher,", "to the southern city of Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "Davos", "Malm\u00f6", "Richard Attenborough and wife Sheila Sim", "a lunar eclipse", "\"novel with a key\"", "London", "Oklahoma", "Kevin Nealon", "Christianity", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6961985930735931}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.9090909090909091, 0.0, 1.0, 0.09523809523809523, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.25, 0.16666666666666669, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-2906", "mrqa_searchqa-validation-1891"], "SR": 0.5625, "CSR": 0.5346875, "EFR": 1.0, "Overall": 0.726078125}, {"timecode": 50, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.828125, "KG": 0.4921875, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "Two", "in July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Baghdad.", "11", "Shenzhen in southern China.", "\"Den of Spies\"", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Pakistan's", "March 8", "female soldier, missing", "in Michoacan state,", "celebrity-studded gala", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the all-white public high school.", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "strong work ethic", "12", "Arabic, French and English,", "40", "South Africa.", "L'Aquila", "\" Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "was burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "\"We are doing our best to dissuade the North Koreans from going forward,", "posting a $1,725 bail,", "Bill,", "more than 78,000 parents of children ages 3 to 17.", "Apple Inc.", "London's", "usion teams", "martial arts,", "Dr. Jennifer Arnold and husband Bill Klein,", "Operation Crank Call,\"", "Orwell", "Guwahati", "winter solstice", "Frenchman", "sheep", "daisy", "1853", "musicologist", "1902", "the Alaska territory during the battle between Secretary of State William H. Seward and the United", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6716689387001887}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.8, 0.18181818181818182, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9743589743589743, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.9600000000000001, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7777777777777777, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.14285714285714288, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-510", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.546875, "CSR": 0.5349264705882353, "EFR": 1.0, "Overall": 0.7218290441176471}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan forces who have entered Somalia,", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "Caylee Anthony", "Diego Maradona", "London", "Oregon State Senior troopers David Petersen after he was able to catch up with six exotic sports cars on a stretch of Highway 18 near Grand Ronde, Oregon,", "in rural Tennessee.", "Fakih", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship were at the Rajprasong intersection in the heart of Bangkok.", "14", "Former Mobile County Circuit Judge Herman Thomas", "Monday,", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook", "Vicente Carrillo Leyva,", "Dolgorsuren Dagvadorj,", "they would not be making any further comments, citing the investigation.", "41,", "Anil Kapoor.", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "Maersk", "the estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the prime minister's handling of the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs of primary energy", "Charlton Heston", "administrative supervision over all courts and the personnel thereof", "richard attenila", "the Nazi party officials, the Gauleiters and Kreisleiters.", "richard attenie", "in New York City.", "Kentucky, Virginia, and Tennessee.", "1999", "feijoada", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.640625, "QA-F1": 0.721584145021645}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true], "QA-F1": [0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.19999999999999998, 0.8, 1.0, 0.0909090909090909, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623", "mrqa_searchqa-validation-12036"], "SR": 0.640625, "CSR": 0.5369591346153846, "retrieved_ids": ["mrqa_squad-train-5859", "mrqa_squad-train-35220", "mrqa_squad-train-26788", "mrqa_squad-train-40412", "mrqa_squad-train-77125", "mrqa_squad-train-44281", "mrqa_squad-train-43248", "mrqa_squad-train-67071", "mrqa_squad-train-36271", "mrqa_squad-train-51165", "mrqa_squad-train-9695", "mrqa_squad-train-18166", "mrqa_squad-train-53198", "mrqa_squad-train-52974", "mrqa_squad-train-13021", "mrqa_squad-train-17522", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-13853", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-908", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2733", "mrqa_triviaqa-validation-4457", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2568", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-6374"], "EFR": 1.0, "Overall": 0.7222355769230769}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "ousted Honduran President Jose Manuel Zelaya", "mother.", "education", "No. 4", "A family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J. Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "Manchester City", "planned attacks", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Kingman Regional Medical Center,", "98", "Long Island", "5,600", "Pew Research Center", "Sharon Bialek", "generous supporters and an Iraqi humanitarian group", "two", "humans", "Muslim", "The New York appeals court Thursday overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "Evans", "near the Somali coast", "$24,000-30,000 price range.", "2008,", "killing rampage.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Joe Pantoliano,", "The flooding was so fast that the thing flipped over,\"", "relatives of the five suspects,", "Trevor Rees,", "Dubai", "June 6, 1944,", "the surge,", "Free skiing", "is a female given name, the Latin transliteration of the Greek name Berenice, \u0392\u03b5\u03c1\u03b5\u03bd\u03af\u03ba\u03b7", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Mace Coronel", "Rebecca Adlington", "Bedfordshire", "10", "Consigliere", "2007", "The entity", "The Suite Life of Zack & Cody", "1992", "launch one ship.", "northern latitudes"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6044462655400156}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 0.9090909090909091, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.53125, "CSR": 0.5368514150943396, "EFR": 1.0, "Overall": 0.722214033018868}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach", "throwing three punches", "Argentine", "Ferraris, a Lamborghini and an Acura NSX", "death of", "1983", "a simple puzzle video game,", "\"Dancing With the Stars.\"", "African National Congress", "across Greece.", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's strike", "help nations trapped by hunger and extreme poverty,", "$10 billion", "Mokotedi Mpshe,", "April 22.", "Mitt Romney", "twice.", "seeking help", "Mary Phagan,", "pesos", "judge", "Sharon Bialek", "60 euros", "Barack Obama", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "Britain's", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "\"your President Bush doesn't like us Muslims.\"", "Sunday", "a share in the royalties", "drug cartels", "in a canyon in the path of the blaze Thursday.", "number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "w. s. Gilbert", "shoes", "Herbert Lom Dies", "Battle of Prome", "East 31st Street in the Union Hill section of Kansas City, Missouri", "Mildred Iatrou Morgan", "an intrigue card", "a verb", "Rhonda Revelle", "Kwame Nkrumah"], "metric_results": {"EM": 0.625, "QA-F1": 0.7051403985507246}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7313", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.625, "CSR": 0.5384837962962963, "EFR": 0.9583333333333334, "Overall": 0.714207175925926}, {"timecode": 54, "before_eval_results": {"predictions": ["$249", "diabetes and hypertension,", "Jet Republic,", "Muslim", "at least 27", "last week,", "Peru's", "Joan Rivers", "\"Watchmen\"", "sovereignty", "NATO", "Bangladesh", "250,000", "complicated and deeply flawed man who does some awful things, like cheating repeatedly on his wife, Betty, and using information he gleans from her therapist to manipulate her.", "scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai on Wednesday.", "Jenny Sanford.", "\"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"", "voluntary manslaughter", "hosting an awards show.", "South Africa", "The noose incident occurred two weeks after Black History Month", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "500 feet down an embankment", "Marxist guerrillas", "in Europe,", "Rwanda", "The UNHCR", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "six-1 triumph at Key Biscayne.", "come here,", "CNN", "Jobs", "using recreational drugs", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40 ships", "country music.", "former Vice President Dick Cheney,", "Tuesday in Los Angeles.", "Stuntman: Wayne Michaels", "No reason was given for the denial.", "Kenyan forces who have entered Somalia,", "his health", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2017", "October 2", "quartz", "kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United", "month", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5892550285080773}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.30303030303030304, 0.0, 0.3902439024390244, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.4, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-2095", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.515625, "CSR": 0.5380681818181818, "retrieved_ids": ["mrqa_squad-train-47953", "mrqa_squad-train-78302", "mrqa_squad-train-67714", "mrqa_squad-train-31463", "mrqa_squad-train-38109", "mrqa_squad-train-10613", "mrqa_squad-train-23665", "mrqa_squad-train-83614", "mrqa_squad-train-55442", "mrqa_squad-train-85544", "mrqa_squad-train-5302", "mrqa_squad-train-16833", "mrqa_squad-train-46668", "mrqa_squad-train-59515", "mrqa_squad-train-52507", "mrqa_squad-train-59469", "mrqa_searchqa-validation-1085", "mrqa_newsqa-validation-3795", "mrqa_searchqa-validation-10604", "mrqa_newsqa-validation-2991", "mrqa_hotpotqa-validation-5444", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-893", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-1165", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-7719", "mrqa_searchqa-validation-14997", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-1988", "mrqa_naturalquestions-validation-7203"], "EFR": 1.0, "Overall": 0.7224573863636363}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing Friday,", "without the", "Mexico", "\"I know England does not have the infrastructure to remove snow like we do in Minnesota,\"", "three", "customers are lining up for vitamin injections that promise", "\"not apartment dogs,\"", "actor who created one of British television's most surreal thrillers,", "\"We want to reset our relationship and so we will do it together.\"", "the area of the 11th century Preah Vihear temple", "general astonishment", "June 6, 1944,", "a lightning strike", "two", "Barack Obama", "money or other discreet aid for the effort", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "preserved corpses having sex", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "eight in 10", "The paper said the trip had caused fury among some in the military who saw", "3rd District of Utah.", "Golfer Tiger Woods", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "the piracy incident", "Alaska or Hawaii.", "Robert Park", "Djibouti,", "the HPV vaccine", "Six", "Bahrain.", "delivers a big speech", "Facebook and Google,", "Somali", "2006", "five", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday.", "NATO fighters", "\"Empire of the Sun,\"", "New Zealand", "a model of sustainability.", "The Jewel of the Nile", "winter", "79", "neoclassic", "squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "Frank Sinatra", "mass", "a snout beetle", "siegfried Chamberlain"], "metric_results": {"EM": 0.5, "QA-F1": 0.5923346185064935}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.18181818181818182, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.5, "CSR": 0.5373883928571428, "EFR": 1.0, "Overall": 0.7223214285714286}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "\"A Child's Garden of Verses,\"", "World Trade Center", "2.5 million", "almost 100", "137", "1,500", "anaphylaxis Network (FAAN)", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Bodyguard Trevor Rees and the back of Princess Diana's head", "the most-wanted man in the world", "the Louvre.", "Taliban militants.", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "Washington State's", "love the environment and hate using fuel,\"", "The apartment building collapsed together with two other buildings on March 3.", "11", "Henrik Stenson", "CEO of an engineering and construction company with a vast personal fortune.", "Milan", "strife in Somalia,", "cancerous tumor.", "increase the flow of water passing through its network of dams.", "Abdullah Gul,", "alcohol and drug abuse", "11th year in a row.", "the journalists and the flight crew will be freed,", "Campbell Brown", "national telephone", "the thoroughness of the officers involved", "the shootings, handed over the AR-15 and two other rifles and left the cabin.", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "it's historical, inspiring, creative, romantic and beautiful.", "gasoline", "Santaquin City, Utah,", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "U.S. and Pakistani officials", "Monday", "a Celtic people living in northern Asia Minor", "diastema ( plural diastemata )", "a hollow plastic sphere, approximately 3 cm in diameter ( similar in appearance to table tennis ball, but smaller ) with at least one small hole and a seam", "clement", "Cambridge", "mercury", "13 October 1958", "bassline", "omnisexuality", "the invisible man", "Zachary Taylor", "Battlestar Galactica", "(CNN)"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6653674667346542}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.07142857142857142, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 0.2727272727272727, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0625, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_triviaqa-validation-3538"], "SR": 0.5625, "CSR": 0.537828947368421, "EFR": 0.9642857142857143, "Overall": 0.715266682330827}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "Mad Men", "5,600", "the European Commission", "three", "using recreational drugs", "0-0 draw", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "Christopher Savoie", "Alina Cho", "did not speak", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali, Mohamed Mohamud Qeyre.", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "The son of Gabon's former president was declared the winner of the country's presidential elections on Thursday,", "Michael Schumacher", "consumer confidence", "the Lakes Golf Club in Sydney, site of this week's Australian Open,", "Longo-Ciprelli", "Fernando Caceres", "iPods", "treadmill", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "HSH Nordbank Arena", "$40 and a bread.", "Tennis Channel has canceled plans to broadcast a tournament in Dubai because an Israeli player was banned.", "No. 1", "Republican Gov. Jan Brewer.", "Boundary County, Idaho, which borders Canada and abuts the area", "securities", "$4 a gallon.", "experimental", "Michael Crawford", "the beginning", "the French 'Chamboule-tout', and the British 'Aunt Sally'", "Fenn Street School", "inner ear", "Australian", "Argentinian", "TOSLINK", "rap", "inducere", "Harvard", "129,007"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6969491841944288}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.125, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-675", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174"], "SR": 0.59375, "CSR": 0.5387931034482758, "retrieved_ids": ["mrqa_squad-train-29319", "mrqa_squad-train-42750", "mrqa_squad-train-3898", "mrqa_squad-train-51483", "mrqa_squad-train-29866", "mrqa_squad-train-86093", "mrqa_squad-train-25746", "mrqa_squad-train-68209", "mrqa_squad-train-14910", "mrqa_squad-train-67698", "mrqa_squad-train-34339", "mrqa_squad-train-52127", "mrqa_squad-train-12852", "mrqa_squad-train-26548", "mrqa_squad-train-82146", "mrqa_squad-train-22867", "mrqa_naturalquestions-validation-3663", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1962", "mrqa_searchqa-validation-11352", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-772", "mrqa_triviaqa-validation-3450", "mrqa_newsqa-validation-3321", "mrqa_searchqa-validation-7112", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-3827", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-873", "mrqa_triviaqa-validation-7763", "mrqa_searchqa-validation-3932"], "EFR": 0.9615384615384616, "Overall": 0.7149100629973475}, {"timecode": 58, "before_eval_results": {"predictions": ["Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "celebrity clientele.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "5 season", "Arthur E. Morgan III,", "Jared Polis", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal", "No 4,", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"an incompetent and rude president who is senseless and ignorant as he does not know even elementary diplomatic etiquette and lacks diplomatic ability.\"", "President Obama.", "Jacob Zuma,", "1937", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "the Southeast,", "\"Up,\"", "disposable income", "fascinating transformation that takes place when carving a pumpkin.", "school,", "a motor scooter", "learn in safer surroundings.", "$50 less,", "J.Crew", "$106,482,500", "Nearly eight in 10", "personal and credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "full-length computer-generated animated film", "\"black box\" label warning", "drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "breast cancer.", "Virgin America", "to protect her children.", "It's so weird. There's two different versions. There\\'s my version of how it went about, and there's the producer's version.", "Kenyan and Somali", "opium trade", "Wednesday evening", "an angry mob.", "Africa", "Osama", "left - sided heart failure", "4.09", "Devastator", "Madness", "Jelly Roll Morton", "vice-admiral", "George Mikan", "Kait Parker", "Centre-du-Qu\u00e9bec area", "Nguyen", "doughboy", "\"In God We Trust\"", "professor henry higgins"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6779988628334217}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.972972972972973, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951", "mrqa_searchqa-validation-3774"], "SR": 0.546875, "CSR": 0.5389300847457628, "EFR": 0.9310344827586207, "Overall": 0.7088366635008767}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913,", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "11 healthy eggs", "four", "64,", "\"a whole new treasure trove of fossils\"", "at least two and a half hours.", "shark River Park in Monmouth County", "improve the environment", "gift to the Obama girls from Sen. Ted Kennedy.", "Kurt", "More than 15,000", "\"Teen Patti\" (\"Card Game\")", "President Obama", "\"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "the START treaty,", "sumo wrestling", "10 below", "has a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall", "Roy", "\"We spent a few days following a crew of Grayback forest-firefighters", "Josef Fritzl,", "Marxist guerrillas", "Greeley, Colorado,", "five", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "an unprecedented wave of buying amid the elections.", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959", "Muslim north of Sudan", "at least 18 federal agents and two soldiers have been killed since July 11", "North Korea", "33", "Kenneth Cole", "the Devastator", "Brazil", "Theodore Roosevelt", "vice-admiral", "Braves", "the Big Bopper", "Greek-American", "feats of exploration", "he is often considered the \"godfather\" of U.S-Mexico border cartels", "Monarch", "the American theatre", "Truman", "BBC Formula One"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6407451923076923}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true], "QA-F1": [0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-3231", "mrqa_triviaqa-validation-105", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156"], "SR": 0.578125, "CSR": 0.5395833333333333, "EFR": 1.0, "Overall": 0.7227604166666667}, {"timecode": 60, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.853515625, "KG": 0.46953125, "before_eval_results": {"predictions": ["183", "Ed McMahon,", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Sixteen", "Clinton", "both Russian residents and worldwide viewers, in English or in Russian,", "34", "five victims by helicopter,", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police have opened a criminal investigation", "Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "it is not just $3 billion of new money into the economy.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "\"We would just pretty much talk about what they've been doing, stuff like that; the children, her two sons, and then when she found out she was pregnant with Lisa, that's all we talked about,\"", "Obama and McCain camps", "U.S. ship", "in a hotel,", "the only goal of the game to ensure Hamburg remain in touch with the top three", "France", "Roberto Micheletti,", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute", "1991-1993,", "response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Crank Call,\"", "Islamabad", "a man's lifeless, naked body", "Adam Lambert and Kris Allen,", "ConAgra Foods plant", "Lalo Schifrin", "April 17, 1982", "Billy Idol", "diagnostic", "Theresa May", "every ten years", "five months", "\"The Dragon\"", "1994", "Magnolia acuminata", "\"If You Live in the Northern\"", "Jupiter", "mural"], "metric_results": {"EM": 0.625, "QA-F1": 0.7228890620621564}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 0.4, 0.4210526315789474, 0.0606060606060606, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-2042", "mrqa_triviaqa-validation-7704", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-16357"], "SR": 0.625, "CSR": 0.540983606557377, "retrieved_ids": ["mrqa_squad-train-7473", "mrqa_squad-train-33119", "mrqa_squad-train-74761", "mrqa_squad-train-80970", "mrqa_squad-train-5282", "mrqa_squad-train-64852", "mrqa_squad-train-23538", "mrqa_squad-train-63176", "mrqa_squad-train-77412", "mrqa_squad-train-81175", "mrqa_squad-train-61530", "mrqa_squad-train-56058", "mrqa_squad-train-41881", "mrqa_squad-train-62908", "mrqa_squad-train-67969", "mrqa_squad-train-30697", "mrqa_triviaqa-validation-2906", "mrqa_triviaqa-validation-1972", "mrqa_newsqa-validation-3863", "mrqa_naturalquestions-validation-6289", "mrqa_searchqa-validation-3259", "mrqa_hotpotqa-validation-4791", "mrqa_triviaqa-validation-6643", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-336", "mrqa_searchqa-validation-14720", "mrqa_triviaqa-validation-3032", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-416"], "EFR": 1.0, "Overall": 0.7169467213114754}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "prisoners at the South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "Victor Mejia Munera was a drug lord with ties to paramilitary groups,", "a baseball bat", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi", "Daniel Radcliffe", "The Hutus were considered inferior,", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai", "the foyer of the BBC building in Glasgow, Scotland", "\"Lean, Clean and Local\" tour,", "an engineering and construction", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "outstanding performance by a female actor in a drama series", "9:20 p.m. ET", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "the island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "he spent the first night in his car.", "businesses hiring veterans as well as job training for all service members leaving the military.", "sent a quarter-mile pier crumbling into the sea along with two of his trucks.", "UK", "bipartisan", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985 -- 1993", "Dublin", "Goldfinger", "Lidice", "Baltimore", "Wynonna Judd", "to be identified as transgender", "the Italian occupation", "the great horned type", "Canada", "Bolton"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7385319616977225}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.9565217391304348, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_naturalquestions-validation-2943", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.671875, "CSR": 0.5430947580645161, "EFR": 0.9523809523809523, "Overall": 0.7078451420890938}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "greater than 14", "16,801 students", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "in the 1820s", "the Tigris and Euphrates", "third", "Andrew Garfield", "The Fixx", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "0.30 in ( 7.6 mm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison", "Kristy Swanson", "the most senior position in the Bank of England", "simulation", "James Martin Lafferty", "Kenny Anderson", "agriculture", "Vasoepididymostomy", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Universal Pictures", "Tbilisi", "southern California", "tolled ( quota ) highways", "the federal government", "outside cultivated areas", "Frank Theodore `` Ted '' Levine", "IIII", "in vitro", "the Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card verification data", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "Tasmania", "Laura Robson", "Afghanistan", "Todd McFarlane", "Massachusetts", "one", "nothing,", "the forward's lawyer", "Wally", "maple syrup", "the tongue", "locoweed", "December 1974"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6788620448179271}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 1.0, 0.33333333333333337, 0.92, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11479"], "SR": 0.5625, "CSR": 0.5434027777777778, "EFR": 0.9642857142857143, "Overall": 0.7102876984126985}, {"timecode": 63, "before_eval_results": {"predictions": ["Detective Inspector", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Jessica Sanders", "Article 1, Section 2, Clause 3", "Lex Luger and Rick Rude", "November 2, 2010", "Foreign minister Hermann M\u00fcller", "annuity", "Mark Lowry", "1877", "31", "1526", "bow bridge", "Dick Rutan", "a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "July 1790", "King Saud University", "Hugo Weaving", "Book of Exodus", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Andy Serkis", "1078", "James", "Stefanie Scott", "amino acids glycine and arginine", "the art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "24 -- 3", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "1560s", "twice", "a Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "Berlin", "Marjorie McGinnis", "the Electorate", "fourth-ranking", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "\"Mulholland Drive,\"", "Social Security", "part of the proceeds"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6207541365084317}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8387096774193549, 0.0, 1.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.16216216216216214, 0.4, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311", "mrqa_searchqa-validation-7843", "mrqa_searchqa-validation-7607"], "SR": 0.515625, "CSR": 0.54296875, "retrieved_ids": ["mrqa_squad-train-7600", "mrqa_squad-train-65034", "mrqa_squad-train-15102", "mrqa_squad-train-14469", "mrqa_squad-train-70719", "mrqa_squad-train-38478", "mrqa_squad-train-73442", "mrqa_squad-train-46781", "mrqa_squad-train-18027", "mrqa_squad-train-24452", "mrqa_squad-train-60440", "mrqa_squad-train-34270", "mrqa_squad-train-26626", "mrqa_squad-train-13703", "mrqa_squad-train-68492", "mrqa_squad-train-49397", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3285", "mrqa_newsqa-validation-1977", "mrqa_naturalquestions-validation-6289", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3791", "mrqa_naturalquestions-validation-2896", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-2480", "mrqa_squad-validation-4489", "mrqa_naturalquestions-validation-132", "mrqa_triviaqa-validation-3284", "mrqa_newsqa-validation-2799", "mrqa_searchqa-validation-14868"], "EFR": 0.9032258064516129, "Overall": 0.6979889112903226}, {"timecode": 64, "before_eval_results": {"predictions": ["winter solstice", "19 July 1990", "senators", "Rex Harrison", "a maquiladora", "Turducken", "Patrick Warburton", "Judas Iscariot", "1960", "the President of the United States", "the Supreme Court has `` administrative supervision over all courts and the personnel thereof ''", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "the small intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Charles Haley", "Rumplestiltskin", "Sylvester Stallone", "35 to 40 hours per week", "Effy", "the body is one and has many members, and all the members of the body, though many, are one body, so it is with Christ", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1996", "in the mid - to late 1920s", "`` Far Away ''", "John C. Reilly", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "1792", "Cyanea capillata", "Bonnie Lipton", "2002", "Charles Haley", "Dawn French", "a translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1964", "pesos", "in North Korea", "at \"E! News\"", "carbon fiber", "current congressmen", "The Greatest Show on Earth", "eliza"], "metric_results": {"EM": 0.640625, "QA-F1": 0.73967026319774}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8421052631578948, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10810810810810811, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_newsqa-validation-3238", "mrqa_newsqa-validation-70", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.640625, "CSR": 0.5444711538461539, "EFR": 1.0, "Overall": 0.7176442307692308}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "`` The Crossing ''", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "Paul", "a comic book series", "Radiotelegraphy", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Melanie Martinez", "the Director of National Intelligence", "Liam Cunningham", "Elliot Scheiner", "a medium with a lower index of refraction, typically a cladding of a different glass, or plastic", "Ace", "Goths", "H CO ( equivalently OC (OH ) )", "StubHub Center", "Maryland Senate's", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea", "a surname of Norman", "the start of the 20th century", "Nashville, Tennessee", "an SS - 4 construction site at San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "following the 2017 season", "The Seattle Center", "Columbia River Gorge", "the International Campaign to Abolish Nuclear Weapons ( ICAN )", "John Joseph Patrick Ryan", "1912", "the four listings of apostles in the New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13 )", "Ric Flair", "124 and 800 CE", "continental units", "2008 NBA Finals", "Adam Werritty", "the Jets", "a white halter dress", "Kim Jong-hyun", "Edward II", "Harrods", "she's grateful that a 40-year water diversion project is nearly complete.", "tax incentives for businesses hiring veterans as well as job training", "Gary Coleman", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6239041063703581}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.4, 0.0, 1.0, 0.8387096774193548, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 0.34782608695652173, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2200", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_hotpotqa-validation-1697"], "SR": 0.515625, "CSR": 0.5440340909090908, "EFR": 0.967741935483871, "Overall": 0.7111052052785924}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "May 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "on the tournette", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "The rate of infiltration can be measured using an infiltrometer", "Kathy Najimy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "smoking", "Richard Crispin Armitage", "the Himalayas", "Potter", "mid-ocean ridges, where new oceanic crust is formed through volcanic activity and then gradually moves away from the ridge", "Sir Rowland Hill", "late - September through early January", "2010", "Joseph Sherrard Kearns", "The Union's forces", "3 September, after a British ultimatum to Germany to cease military operations was ignored", "a loop", "Carroll O'Connor", "fictional town of West Egg on prosperous Long Island", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "a writ of certiorari", "after World War II", "Guwahati", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Cheap trick", "October 29, 2015", "Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old", "federal government", "Tigris and Euphrates rivers", "bicameral Congress", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells", "Sarah Brightman", "password recovery tool for Microsoft Windows", "across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Tokyo for the 2020 Summer Olympics, Beijing for the 2021 Winter Olympics, Paris for the 2028 Summer Olympics", "moral", "Lana Del Rey", "NBA", "a greyhound, gazelle hound", "Aristotle", "Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "WE LOVE YOU MICHAEL!!!\"", "gun", "between government soldiers and Taliban militants in the Swat Valley.", "Odysseus", "Louisiana", "Boy Scouts of America", "three"], "metric_results": {"EM": 0.5, "QA-F1": 0.6205210884628218}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.5, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 0.9361702127659575, 1.0, 0.0, 0.36363636363636365, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.058823529411764705, 0.5652173913043478, 0.125, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7272727272727273, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-4320"], "SR": 0.5, "CSR": 0.5433768656716418, "retrieved_ids": ["mrqa_squad-train-17310", "mrqa_squad-train-47071", "mrqa_squad-train-1453", "mrqa_squad-train-52789", "mrqa_squad-train-11582", "mrqa_squad-train-28013", "mrqa_squad-train-75593", "mrqa_squad-train-61651", "mrqa_squad-train-70909", "mrqa_squad-train-2105", "mrqa_squad-train-7981", "mrqa_squad-train-79436", "mrqa_squad-train-46864", "mrqa_squad-train-73997", "mrqa_squad-train-59469", "mrqa_squad-train-27692", "mrqa_newsqa-validation-2028", "mrqa_triviaqa-validation-5681", "mrqa_newsqa-validation-3953", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-9024", "mrqa_newsqa-validation-269", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5004", "mrqa_newsqa-validation-2872", "mrqa_naturalquestions-validation-154", "mrqa_searchqa-validation-12302", "mrqa_naturalquestions-validation-10719", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-10799", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-10040"], "EFR": 0.96875, "Overall": 0.7111753731343284}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "Clarence Darrow", "John B. Watson", "Spanish", "Anna Murphy", "a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "December 1, 2017", "Erica Rivera", "McFerrin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Sir Ronald Ross", "Georgia", "Domhnall Gleeson", "CeCe Drake", "March 10, 2017", "March 11, 2018", "Thomas Mundy Peterson", "Augustus Waters", "boxing", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "saecula saeculorum in Ephesians 3 : 21", "John Goodman", "the intermembrane space", "February 25, 2004", "the breast or lower chest of beef or veal", "a nearly - identical `` non-drivers identification card", "Dr. Hartwell Carver", "two", "following the 2017 season", "Arunachal Pradesh", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "condemns rural depopulation and the pursuit of excessive wealth", "his brother", "Washington metropolitan area", "the euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking", "tissues of the outer third of the vagina", "Bergen", "Cartoon Network", "\"When she opened her mouth and she spoke, she really felt whatever it was,\"", "change course", "a federal judge in Mississippi", "a skunk", "the Russian Second Army", "Matthew Ross", "a pitcher"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6256048857940305}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.2105263157894737, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.06666666666666667, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.631578947368421, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8421052631578948, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-1464", "mrqa_hotpotqa-validation-4194", "mrqa_newsqa-validation-2528", "mrqa_searchqa-validation-808", "mrqa_searchqa-validation-3511", "mrqa_triviaqa-validation-2358"], "SR": 0.515625, "CSR": 0.54296875, "EFR": 1.0, "Overall": 0.7173437500000001}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "16 May 2007", "Peter Klaven ( Paul Rudd )", "G. Hannelius", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Ashoka", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "VH1 Storytellers : Johnny Cash & Willie Nelson", "before the first year begins", "NFL coaches", "Davos", "Louis Hynes", "1900", "Elizabeth Weber", "stems and roots", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences", "symbol I and atomic number 53", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida", "between 11000 and 9000 BC", "the beginning of the Wizarding World shared media franchise", "the inventor Bi Sheng", "the Prince - Electors", "1799", "Bob Sinclair & Eddie Amador", "a god of the Ammonites, as well as Tyrian Melqart", "December 2, 2013, and the third season concluded on October 1, 2017", "an official document permitting a specific individual to operate one or more types of motorized vehicles", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Australia's Sir Donald Bradman, with 99.94", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Miracle", "Dumfries and Galloway, south-west Scotland", "High Knob", "President Obama and Britain's Prince Charles", "NATO fighters", "age 19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "E. E. Cummings", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6519188277000777}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3846153846153846, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.7499999999999999, 1.0, 0.26666666666666666, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013"], "SR": 0.515625, "CSR": 0.542572463768116, "EFR": 0.9354838709677419, "Overall": 0.7043612669471716}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford", "The Intolerable Acts", "skeletal muscle and the brain", "the libretto of the `` new version '' of Rent", "prophets and beloved religious leaders", "2015", "the St. Louis Cardinals", "Toby Kebbell", "Panning", "September 21, 2017", "to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "Eastern Redbud", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four", "Benjamin Franklin", "a routing table, or routing information base ( RIB )", "James Rodr\u00edguez", "in AD 95 -- 110", "Johnson, a lifelong Democrat and the Republican majority in Congress", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "from the top of the leg to the foot on the posterior aspect", "Walmart", "Ashoka", "the dermis", "Hodel", "October 27, 2017", "Wolfgang Hochstetter", "King being one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the Supreme Court of Canada", "September 29, 2017", "around 10 : 30am", "Angola", "Russia", "Manley", "The was released as a limited - edition double - vinyl and digital download", "Wyatt", "New Year\u2019s Eve", "IN GOD WE TRUST", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "to avoid bad publicity and embarrassment of defending even a falsely accused chief executive.", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "Namibian War of Independence"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6360646029018875}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.896551724137931, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.8571428571428571, 0.2222222222222222, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395", "mrqa_triviaqa-validation-5834"], "SR": 0.515625, "CSR": 0.5421875, "retrieved_ids": ["mrqa_squad-train-19275", "mrqa_squad-train-17220", "mrqa_squad-train-6848", "mrqa_squad-train-37013", "mrqa_squad-train-32162", "mrqa_squad-train-41945", "mrqa_squad-train-43776", "mrqa_squad-train-4011", "mrqa_squad-train-68703", "mrqa_squad-train-28731", "mrqa_squad-train-13619", "mrqa_squad-train-83279", "mrqa_squad-train-3096", "mrqa_squad-train-40600", "mrqa_squad-train-13047", "mrqa_squad-train-76155", "mrqa_newsqa-validation-1551", "mrqa_searchqa-validation-5298", "mrqa_newsqa-validation-3156", "mrqa_searchqa-validation-3222", "mrqa_triviaqa-validation-3763", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2566", "mrqa_naturalquestions-validation-6917", "mrqa_searchqa-validation-7499", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-2405", "mrqa_triviaqa-validation-959", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3860", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-7251"], "EFR": 0.9354838709677419, "Overall": 0.7042842741935484}, {"timecode": 70, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.892578125, "KG": 0.50546875, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as", "2001 Indian epic sports - drama film, directed by Ashutosh Gowariker, produced by Aamir Khan and Mansoor Khan", "Super Bowl XXXIX", "the close quarters and poor hygiene exhibited at that time Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus", "September 2017", "the Kanawha River", "12.65 m", "1820s", "the customer's account", "his cousin D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "a prison from 1100 ( Ranulf Flambard ) until 1952 ( Kray twins )", "the Supreme Court of Canada", "July 1, 1923", "Firoz Shah Tughlaq", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia )", "Kirsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "the President", "de pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's, on 15 July 2004", "mid-size four - wheel drive luxury SUVs", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "The terrestrial biosphere", "Kitty Softpaws", "Austria - Hungary", "Certificate of Release or Discharge from Active Duty", "eye", "Vietnam war", "Rutger Hauer", "Canada", "Robert Jenrick", "Srinagar", "Jewish", "Tibet's", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "Wido", "electric field"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6764704766071978}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9180327868852458, 0.0, 0.0, 0.29411764705882354, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 1.0, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.59375, "CSR": 0.5429137323943662, "EFR": 0.6923076923076923, "Overall": 0.6782161599404117}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "the National Football League ( NFL )", "the following day", "Labour Party", "Judi Dench", "a scuffle with the Beast Folk", "three", "Spanish moss", "Matt Monro", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who qualified for the 2017 Playoffs", "Charles Carroll", "1959", "many forested parts", "Hermia", "Connecticut", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "Parashara", "the middle of the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Lorazepam", "April 1, 2016", "its absolute temperature", "the transfer of protons ( H ions ) across a membrane", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "biscuit", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "August 21", "1799", "alba", "Zachary Taylor", "Oscar Wilde", "the Galaxy S7", "The New Yorker", "Citgo", "school in South Africa", "\"[The e-mails]", "Rolling Stone.", "a nugget", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.625, "QA-F1": 0.7238814958798512}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978"], "SR": 0.625, "CSR": 0.5440538194444444, "EFR": 0.9583333333333334, "Overall": 0.7316493055555555}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Charles Lebrun", "Shenzi", "1994", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "Colonel Robert E. Lee", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "Kansas City Chiefs", "Yuzuru Hanyu", "Owen Hunt", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "The House of Representatives", "Lisa Stelly", "Ali", "Meg Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates", "13", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Jeff Gillen", "Empire of Japan", "Djokovic", "won gold in the half - pipe", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "2002", "Georgia Nicolson :", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "over 1000 square meters in forward deck space,", "President Sheikh Sharif Sheikh Ahmed", "Brooklyn, New York,", "Suntory", "Victoria", "the yoke", "psychological horror adventure game"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7323660714285715}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7999999999999999, 1.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.2]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.671875, "CSR": 0.545804794520548, "retrieved_ids": ["mrqa_squad-train-85095", "mrqa_squad-train-65069", "mrqa_squad-train-63985", "mrqa_squad-train-44069", "mrqa_squad-train-56676", "mrqa_squad-train-71538", "mrqa_squad-train-32164", "mrqa_squad-train-17376", "mrqa_squad-train-14506", "mrqa_squad-train-36888", "mrqa_squad-train-61492", "mrqa_squad-train-28693", "mrqa_squad-train-13523", "mrqa_squad-train-22179", "mrqa_squad-train-29590", "mrqa_squad-train-86474", "mrqa_searchqa-validation-11087", "mrqa_squad-validation-6757", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-6041", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-1616", "mrqa_squad-validation-117", "mrqa_newsqa-validation-1732", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-4019", "mrqa_searchqa-validation-16660", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-2913", "mrqa_triviaqa-validation-412"], "EFR": 0.8571428571428571, "Overall": 0.7117614053326811}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz, Germany", "Shawn Wayans", "the United States is the world's third - or fourth - largest country by total area and the third-most populous", "A regulatory site", "3", "Dutch United Provinces", "Woodrow Wilson", "Jeff East", "Terry Reid", "Germany", "March 31 to April 8, 2018", "American Indian allies", "cannonball", "the United Kingdom ( UK )", "1945", "CeCe Drake", "April 14, 2017", "post translational modification", "1960 Summer Olympics in Rome", "naturalization", "September 6, 2019", "Bulgaria", "Michael Douglas", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "Werner Ruchti", "Brooklyn, New York", "Chris Rea", "Langdon", "pneumonoultramicroscopicsilicovolcanoconiosis", "In 2010", "General George Washington", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Uruguay", "to oversee the local church", "William Shakespeare's As You Like It", "2002", "Anna Faris", "Cress", "Montr\u00e9al", "Queen Victoria", "Gerald Ford", "Bank of China ( Hong Kong)", "Mumbai, Maharashtra", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "(I) POD:", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7218727886696636}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.5625, "CSR": 0.5460304054054055, "EFR": 0.8928571428571429, "Overall": 0.7189493846525097}, {"timecode": 74, "before_eval_results": {"predictions": ["the Legion of Honor", "Shaft", "(the nonvirtual, paper kind)", "(Berenice)", "pharaoh", "Tony Dungy", "the Rolling Stones", "aria", "cayenne", "thrombocytes", "suffrage", "60", "the Enigma", "a tornado", "afternoon", "\"Elaine the fair maid of Astolat\"", "Laryngitis", "(Prince) Albert", "terraces", "a zombi", "aquiline", "\"The Night Digger\"", "a cozy", "\"Regular Folks\" Ordinary People 1932: \"Magnificent Inn\" Grand Hotel", "Davenport", "Sammy Sosa", "Suzuki", "(One hundred and one if you count the \"and)", "the green-eyed monster", "Mount Olympus", "a hematoma", "Death", "a coral snake", "General William Tecumseh Sherman", "(right)", "a duvet", "(Original Motion Picture Cast)", "crayfish", "Japan", "\"Liberty, Equality, Fraternity\"", "(Prince) Albert", "William Wrigley Jr.", "Nepal", "the agricultural sector", "cat scratch fever", "freezing", "(A Season in Purgatory)", "a kangaro court", "Whatchamacallit", "(Prince) Albert", "Fantasy Island", "pigs", "lying between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "benjamin franklin", "Sororicide", "saint aidan", "Sulla", "Switzerland", "Parlophone Records", "keyboardist and", "150", "a real person to talk to,\"", "the contestant"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6010080645161291}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-8294", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-2572", "mrqa_triviaqa-validation-1931", "mrqa_newsqa-validation-1890", "mrqa_naturalquestions-validation-5636"], "SR": 0.546875, "CSR": 0.5460416666666666, "EFR": 1.0, "Overall": 0.7403802083333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "(Luke) Jackson", "Louisiana", "a clapper", "Tombs of Kobol", "The Sound and the Fury", "a bacon sandwich", "six", "Cosmo Kramer", "Poetic Justice", "Hugo", "the Colossus of Rhodes", "(Hugh) Jackman", "silver", "Lebanon", "the eagle", "The CPC", "Larry King", "King Claudius", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "antonym", "General Mills", "Emmitt Smith", "a clay", "a black hole", "Kampala", "Department of Agriculture", "Heisenberg", "Sin City", "David Hyde Pierce", "the early predecessors of program music", "Old North Church", "bones", "Red Bull", "a pirate ship", "Canada", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport", "the Red River", "plants", "Ellen Wilson", "Esau", "a lachrymist", "Agatha Christie", "Ronald Reagan", "Ford", "1947", "American actress Moira Kelly", "Zuzu", "Mt Kenya", "Christian Wulff", "Mata Hari", "Princess Aisha bint Hussein", "French", "King James II", "Kaka", "133 people", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6039663461538461}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-3115", "mrqa_hotpotqa-validation-3364"], "SR": 0.515625, "CSR": 0.545641447368421, "retrieved_ids": ["mrqa_squad-train-50193", "mrqa_squad-train-19414", "mrqa_squad-train-86248", "mrqa_squad-train-74492", "mrqa_squad-train-68307", "mrqa_squad-train-52175", "mrqa_squad-train-9920", "mrqa_squad-train-34664", "mrqa_squad-train-83970", "mrqa_squad-train-75412", "mrqa_squad-train-39758", "mrqa_squad-train-80366", "mrqa_squad-train-25766", "mrqa_squad-train-47590", "mrqa_squad-train-34283", "mrqa_squad-train-40557", "mrqa_newsqa-validation-3862", "mrqa_searchqa-validation-11207", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-2502", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-3194", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-34", "mrqa_triviaqa-validation-6199", "mrqa_searchqa-validation-1747", "mrqa_newsqa-validation-3089", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3491", "mrqa_squad-validation-7527", "mrqa_naturalquestions-validation-10205", "mrqa_squad-validation-8597"], "EFR": 1.0, "Overall": 0.7403001644736842}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton", "the Treasury", "Montserrat", "a cyclone", "Starland Vocal Band", "the gallows", "the ohm", "Roll of Thunder", "earthquake magnitudes", "the Potomac", "Oregon", "Mary Stuart", "Hulk Hogan", "air pressure", "Russia", "Adam Sandler", "David Letterman", "Melissa Etheridge", "Macbeth", "Erin Go Bragh.", "Lake Victoria", "Thanksgiving", "a sack dress", "It\\'s a Small World", "Fore River", "Capitol Hill", "a glider", "a crustless cheesecake", "Guyana", "jelly", "camels", "drought", "Phrases", "Jonathan Winters", "Pink", "Rhode Island", "Isaac Newton", "the African continent", "Clarabell", "Theodore Roosevelt", "gold", "Joshua", "Jamestown", "Lignite", "Seymour Cray", "Private Practice", "corticosteroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "nell melbourne", "kenopus laevis", "yellow", "chalk quarry", "SBS", "Eternal Flame", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "Benzodiazepines"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7276041666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7433", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-257", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-11245", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-3435", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7709", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512"], "SR": 0.6875, "CSR": 0.5474837662337662, "EFR": 0.95, "Overall": 0.7306686282467532}, {"timecode": 77, "before_eval_results": {"predictions": ["Leif Ericson", "Inuit", "Bologna", "(Billy the Kid) the Kid", "Rudyard Kipling", "Cheers", "Tarzan", "Edward VI", "Leon Trotsky", "Belgium", "Wendy Beckett", "1066", "ibuprofen", "a filibustero", "... George Washington Carver", "a \" Bulldog\" Drummond", "... Hawthorne: The House of the Seven Gables", "the Tigris River", "the Baltic Sea", "...Nolo contendere", "gum", "Abel", "Louis XV", "Edmonton Oilers", "Anna Karenina", "Sacramento", "the Cordillera Real mountain range", "jury dutyserve", "...Sigmund Freud", "Pantaloons", "Muhammad", "Paul Newman", "Charles H. McKenzie", "...Chesagne", "Rhode Island", "...The Simple Life", "Laos", "Vietnam War", "the Philippines", "Kellogg's", "...Haircut", "... Luxor", "Latin", "Venus", "the Hawthorne", "the Congo River", "the dauphin", "Horatio Nelson", "a caiman", "Ferrari", "a iris", "John Adams", "1886", "Ali", "tahrir Square", "1914", "scotland", "ESPN College Football Friday Primetime", "American R&B vocal group", "Memphis Minnie's", "protective shoes", "Diego Maradona", "Transportation Security Administration", "silver"], "metric_results": {"EM": 0.5, "QA-F1": 0.6246031746031746}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5181", "mrqa_searchqa-validation-13301", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-9691", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-5319"], "SR": 0.5, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.7405468749999999}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus and Remus", "March", "Christmas Eve", "The Firm", "Schwalbe", "Circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "the Enigma", "surface-to-air", "the igloo", "Phobos", "a dermatologist", "Kramer vs. Kramer", "The Tempest", "yellow", "Annie's", "rubber", "Schwarzenegger", "Lafayette", "Iris Murdoch", "Ironman", "Swahili", "the NHL", "gown", "a course", "temples", "The Thousand and Second Tale of Scheherazade", "Scott McClellan", "Jeremiah", "Thomas Edison", "A Chorus Line", "Guadalajara", "Sydney", "pastries", "Dutchman", "\"The Janeites\"", "the Alamo", "oats", "Zlatan Ibrahimovic", "tuition bills", "Rush", "being buried alive", "Swan", "KU", "Helsinki", "the kidney", "One Flew Over the Cuckoo's Nest", "Nobel Prize in Literature", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Manuel Mata Garc\u00eda", "Madeleine L'Engle", "British troops", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6687026515151515}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-5300", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.609375, "CSR": 0.5476661392405063, "retrieved_ids": ["mrqa_squad-train-8704", "mrqa_squad-train-80714", "mrqa_squad-train-7826", "mrqa_squad-train-71740", "mrqa_squad-train-75467", "mrqa_squad-train-32529", "mrqa_squad-train-17504", "mrqa_squad-train-76589", "mrqa_squad-train-79127", "mrqa_squad-train-15882", "mrqa_squad-train-33533", "mrqa_squad-train-65528", "mrqa_squad-train-16794", "mrqa_squad-train-11360", "mrqa_squad-train-44239", "mrqa_squad-train-4723", "mrqa_searchqa-validation-16408", "mrqa_newsqa-validation-25", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-64", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-8845", "mrqa_newsqa-validation-3879", "mrqa_searchqa-validation-1791", "mrqa_naturalquestions-validation-4315", "mrqa_newsqa-validation-2613", "mrqa_searchqa-validation-5450", "mrqa_newsqa-validation-3899", "mrqa_naturalquestions-validation-8737", "mrqa_searchqa-validation-4320", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-8446"], "EFR": 0.96, "Overall": 0.7327051028481012}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "litter", "New Zealand", "John", "Southern California", "Nero", "the Dalmatians", "(Daniel) Day Lewis", "cotton", "Bridget Fonda", "South Africa", "(the Espantoon)", "the Mediterranean", "Catherine", "potato pancakes", "the adder", "the Crossword", "the Thames", "(PIE) FLINGING", "Pitcairn", "(Adam) Sandler", "Mayo", "\" Shut up, just shut up\"", "(Arrested) Development", "renaissance", "German", "Rodeo", "repent", "Denzel Washington", "Bonn", "nougat", "(the Lady Judge) Lowell K. Davis", "(4)", "Louis Comfort Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the Eifel", "salt", "Samsonite", "Chile", "salam", "Michael Faraday", "pearls", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers", "Ethel Merman", "Chung", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237 square miles", "over two decades.", "health-care", "14", "8th and 16th"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6246279761904762}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428564, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-3732", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13659", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-3765"], "SR": 0.53125, "CSR": 0.5474609375, "EFR": 0.9666666666666667, "Overall": 0.7339973958333333}, {"timecode": 80, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.873046875, "KG": 0.471875, "before_eval_results": {"predictions": ["Washington", "the National Hockey League (NHL)", "blue", "Georgia", "The Siege", "scalpels", "the English Channel", "William Shakespeare", "French", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "Edgar Allan Poe", "Bartholomew", "leukemia", "Target", "Regrets", "a possum", "\"Three\"", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a staycation", "not to take risks even when it seems boring or difficult", "Makkedah", "Yogi Bear", "Idaho", "Margaret Wood", "a car", "1215", "Benjamin Harrison", "the skyscraper", "William Henry \"Billy the Kid\" Bonney", "The Killing Fields", "Oliver Twist", "a landmark", "eggplant", "bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)", "the Grand Canal", "the Sons of Liberty", "a telescope", "Messianic", "the tuba", "a deep pass route", "a dodecahedron", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Prince Charles", "16", "dragonflies", "cranberries", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Chevy Chase and Will Smith.", "2006", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted."], "metric_results": {"EM": 0.609375, "QA-F1": 0.6589285714285714}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-2637", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-16754", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-3488", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-1748", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.609375, "CSR": 0.5482253086419753, "EFR": 1.0, "Overall": 0.7270669367283951}, {"timecode": 81, "before_eval_results": {"predictions": ["taxonomy", "New York", "Katrina & the Waves", "the French and Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Bonnie Raitt", "As Good as It Gets", "pickles", "a bull", "neurons", "Evian", "definition", "The Life and Death of a Man", "the olfactory nerve", "a window", "Newton", "SpeedMatch", "Alexander Hamilton", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "American television comedy series", "Charlie Watts", "a black widow spider", "a button", "Virginia", "abundant", "Albert Schweitzer", "the right hemisphere", "dive bomber", "Toulouse-Lautrec", "Helen Hayes", "Dada", "a clam", "Herbert George Wells", "\"Sex In Crazy Places\"", "Bill & Melinda Gates", "the Hippopotamus", "Friedrich Nietzsche", "a dog eat dog world", "Alexander Hamilton", "American", "Niagara Falls", "a rudder", "carrots", "Time", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "thoracic", "Carrefour", "Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "Catholic League", "jesseppe Tornatore"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6639306006493506}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.060606060606060615, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-11852", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_triviaqa-validation-5750"], "SR": 0.53125, "CSR": 0.5480182926829269, "retrieved_ids": ["mrqa_squad-train-20771", "mrqa_squad-train-50490", "mrqa_squad-train-1071", "mrqa_squad-train-6120", "mrqa_squad-train-75585", "mrqa_squad-train-45498", "mrqa_squad-train-22252", "mrqa_squad-train-4514", "mrqa_squad-train-6730", "mrqa_squad-train-77879", "mrqa_squad-train-77010", "mrqa_squad-train-42598", "mrqa_squad-train-27273", "mrqa_squad-train-1737", "mrqa_squad-train-5271", "mrqa_squad-train-65691", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-7635", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-359", "mrqa_triviaqa-validation-2333", "mrqa_naturalquestions-validation-3993", "mrqa_newsqa-validation-3433", "mrqa_triviaqa-validation-3538", "mrqa_searchqa-validation-918", "mrqa_triviaqa-validation-381", "mrqa_searchqa-validation-10329", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-352", "mrqa_searchqa-validation-15278", "mrqa_triviaqa-validation-5467", "mrqa_newsqa-validation-2202"], "EFR": 0.9666666666666667, "Overall": 0.7203588668699187}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy", "the beaver", "Dorothy", "Survivor: Fiji", "The Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Eternity", "Marvell", "Quiz Show", "NFL", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "a capuchins", "Napoleon", "the West of Africa", "the Burmese pythons", "Munich", "digestif", "a layer of sedimentary rock", "Pope Benedict XVI", "Los Alamos", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "Goldenrod", "Luke", "the rectum", "a neck warmer or scarf", "the frequency", "Grease", "the salamander", "Solzhenitsyn", "Eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "Vanity Fair", "the Big Sky Conference", "the beaver", "Boston", "Michelle Pfeiffer", "a cheese", "Sweden", "UK Sinha", "the 17th episode in the third season", "94 by 50 feet", "salicin", "the 7th chord", "scotland", "Karl- Anthony Towns", "Love at First Sting", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "$10 billion", "her boyfriend, Mohamed al Fayed,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6707341269841269}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-10537", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-3978", "mrqa_triviaqa-validation-4628", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.578125, "CSR": 0.5483810240963856, "EFR": 1.0, "Overall": 0.727098079819277}, {"timecode": 83, "before_eval_results": {"predictions": ["the East Sea", "(Jimmy) Liebesman", "Joe ( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "The Pirates of Penzance", "family", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepius", "troll", "(The Flying Dutchman)", "Dan Quayle", "Naomi", "Answer Who is", "Nothing without Providence", "a phaser", "Dylan Thomas", "(Jimmy) Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "the Marshall Islands", "Nepal", "( Andrea) Palladio", "the names of God", "Indiana", "Hair", "cicadas", "Asbury Park", "(6)", "the saguaro", "Zappa", "hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Prisoner and Escort", "invasion of Greece", "Magdalene Laundries", "$10,000 Kelly", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.59375, "QA-F1": 0.68046875}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-16817", "mrqa_searchqa-validation-2332", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-1991", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-7750", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-5339", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.59375, "CSR": 0.5489211309523809, "EFR": 1.0, "Overall": 0.7272061011904761}, {"timecode": 84, "before_eval_results": {"predictions": ["typing", "the crescent", "a trident", "Abercrombie & Fitch", "Jefferson", "Standard Oil", "Crustacean", "Laura Ingalls Wilder", "a carriage", "Monet", "carbon", "Ford", "Louis Rukeyser", "Jupiter", "Clinton", "Truisms", "Tin", "Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia", "Giacomo Puccini", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "Montego Bay", "a relic", "Cyclosporine", "the Northern Mockingbird", "Free Flashcards about GRAMMAR/LIT TERMS", "Comedy", "an Owls", "Perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "a colony of Northern Gannet (Morus bassanus)", "Albert Camus", "Iturbide", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "the Big Dipper", "1924", "741 weeks", "January 17, 1899", "Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a farmers' co-op", "John Landis", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7291666666666666}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-9004", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1148"], "SR": 0.6875, "CSR": 0.5505514705882353, "retrieved_ids": ["mrqa_squad-train-67100", "mrqa_squad-train-12209", "mrqa_squad-train-26639", "mrqa_squad-train-71457", "mrqa_squad-train-78063", "mrqa_squad-train-69188", "mrqa_squad-train-42100", "mrqa_squad-train-77184", "mrqa_squad-train-84929", "mrqa_squad-train-68031", "mrqa_squad-train-49496", "mrqa_squad-train-1813", "mrqa_squad-train-65885", "mrqa_squad-train-17423", "mrqa_squad-train-59273", "mrqa_squad-train-3977", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4424", "mrqa_searchqa-validation-13459", "mrqa_naturalquestions-validation-226", "mrqa_searchqa-validation-2338", "mrqa_newsqa-validation-3795", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-3259", "mrqa_hotpotqa-validation-5199", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3072", "mrqa_naturalquestions-validation-1426", "mrqa_newsqa-validation-159", "mrqa_searchqa-validation-1722", "mrqa_naturalquestions-validation-10032", "mrqa_searchqa-validation-10318"], "EFR": 1.0, "Overall": 0.7275321691176471}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Carole Anne Marie Gist", "The Prince & the Pauper", "Pushing Daisies", "July", "the reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "The Trump Organization", "Luther", "rice", "Frasier", "Kansas City", "arteries", "\"Chinatown.\"", "improvisation", "Hamlet", "lime", "The Aviator", "alkaline nedir, ne demek, alkaline anlam", "Robert Duvall", "Joan of Arc", "abundance", "Crete", "Alfred Hitchcock", "Favre", "Chapters 78", "Fiddler on the Roof", "Pitcairn Island", "hockey", "a ground", "Mars", "bone", "Goliath", "pay", "a cookie jar", "Babe Ruth", "the Steak sandwich", "Nicky Hilton", "he was unable to wrest", "2016", "Jessica Simpson", "William Schuman", "a tree", "Robert Plant", "Oklahoma", "138,535 people", "Martin Scorsese", "her son has strong values.", "this area for the reptiles.", "Hurricane Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.671875, "QA-F1": 0.7685668498168498}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909"], "SR": 0.671875, "CSR": 0.5519622093023255, "EFR": 0.9523809523809523, "Overall": 0.7182905073366556}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra", "Mendeleev", "Kathleen Winsor", "Blitzkrieg", "the luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "the Providence", "The Rolling Stones", "Bridge to Terabithia", "Samuel Anthony Alito, Jr.", "ships", "Civic", "Hermann Hesse", "(Nicolaus) Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "\"Rich Girl\"", "Yogi Berra", "courage", "a shot glass", "calcium", "the constitution", "the Eastern Mediterranean", "virtual reality", "bass", "The Last Remake", "hot air balloons", "Tarzan & Jane", "an RBI's", "( David) Berkowitz", "oblique", "a pie", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "The Matrix", "the Bolshevik faction", "1982", "the Garden of Gethsemane", "the Vi\u1ec7t Minh and France", "James Cameron", "elvis Presley", "Japan", "the Lost Battalion", "the region of Dalmatia", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.681079306722689}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5748", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-6493", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-2566", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669"], "SR": 0.609375, "CSR": 0.5526221264367817, "EFR": 0.96, "Overall": 0.7199463002873563}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "the spinning mule", "onerous", "autographs", "Fargo", "the Dailies", "a fiberboard", "the River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "dementia", "exposure", "the Lowest point", "the Golden Fleece", "satisfaction", "a warning", "Macaulay Culkin", "the Tom Thumb", "Edwards", "Hawaii", "John F. Kennedy", "the Daniel Boone National Forest", "a taxi", "Hemoglobin", "Nancy Sinatra", "an ear infection", "the foxes", "a tabby cat", "Amerigo Vespucci", "Wisconsin", "the Hashemite monarchy", "Canada", "bipolar", "a brownie", "the village clock", "Alexander Calder", "honey", "(Matthew) Broderick", "Christopher Columbus", "a second child", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "the Midwestern United States", "axiom", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "kosher", "A- Albatross, B - Whale", "Meta", "Agent Carter", "the Parthian Empire", "\"Kill Your Darlings\"", "a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Iran", "Brett Cummins,", "Brown-Waite,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7083333333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_newsqa-validation-4165"], "SR": 0.640625, "CSR": 0.5536221590909092, "retrieved_ids": ["mrqa_squad-train-49553", "mrqa_squad-train-61483", "mrqa_squad-train-15766", "mrqa_squad-train-30106", "mrqa_squad-train-28296", "mrqa_squad-train-66226", "mrqa_squad-train-23319", "mrqa_squad-train-9774", "mrqa_squad-train-31910", "mrqa_squad-train-24787", "mrqa_squad-train-26488", "mrqa_squad-train-40006", "mrqa_squad-train-34895", "mrqa_squad-train-44815", "mrqa_squad-train-60643", "mrqa_squad-train-56958", "mrqa_newsqa-validation-1055", "mrqa_triviaqa-validation-4363", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-16560", "mrqa_naturalquestions-validation-9386", "mrqa_searchqa-validation-2164", "mrqa_newsqa-validation-2632", "mrqa_naturalquestions-validation-2901", "mrqa_searchqa-validation-3449", "mrqa_hotpotqa-validation-2205", "mrqa_squad-validation-6614", "mrqa_searchqa-validation-3381", "mrqa_newsqa-validation-2533", "mrqa_searchqa-validation-7229", "mrqa_newsqa-validation-2850"], "EFR": 1.0, "Overall": 0.7281463068181818}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "a toddler", "Biggie", "Judas", "John Paul II", "Hillary Clinton", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "a gambler", "Sleepover", "Spain", "Scrabble", "the Caspian Sea", "football", "the Los Angeles Angels of Anaheim", "Cardiff", "the Blacklist", "3", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "the coxswain", "Transamerica", "China", "polski", "the Delacorte", "Henry Clay", "the bottom", "Wal-Mart", "On the Origin of Species", "Electric Avenue", "a bibliography", "Jerusalem", "Vanna White", "Toyota", "a cella", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "purification", "the following day", "early 1980s", "Taron Egerton", "a linesider", "Henry", "The Undertones", "GM", "Premier Division", "The SoLow Project", "stabbed Tate,", "Herman Cain,", "a black bear", "Kevin Costner"], "metric_results": {"EM": 0.53125, "QA-F1": 0.60546875}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-3943", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-3680", "mrqa_triviaqa-validation-7327"], "SR": 0.53125, "CSR": 0.553370786516854, "EFR": 0.9333333333333333, "Overall": 0.7147626989700374}, {"timecode": 89, "before_eval_results": {"predictions": ["the ermine", "Nemo", "easel", "the world", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "Shintoism", "\"reshit\"", "Venus", "an iris", "carrie bradshaw", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "pearl", "liqueur", "Hangman", "Charles Dickens", "October", "Stephen Foster", "Henrik Ibsen", "Linkin Park", "dogie", "Hurricane Matthew", "lungs", "gravity", "Benjamin Franklin", "Robert Bruce", "Marlon Brando", "the United States", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-N-Harmony", "zebras", "Helio Castroneves", "King Edward", "Hugh Grant", "waiting for Godot", "voyeurism", "Articles of Confederation", "Pavlov", "a hull", "Doll", "all UK permanent residents that is free at the point of use, being paid for from general taxation", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "\" Finding Nemo\"", "Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7227158806106174}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.671875, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.7283593749999999}, {"timecode": 90, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.85546875, "KG": 0.50234375, "before_eval_results": {"predictions": ["Wisconsin", "business", "a stagecoach", "Henry Winkler", "faction", "\"Hasta la vista\"", "New York", "the pastry", "bats", "Tunisia", "a plexus", "a rattlesnake", "Catherine the Great", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "\"AA\"", "Catherine of Aragon", "flag", "Ravi Shankar", "Bangkok", "Spain", "archery", "oblique", "(Joe) Torre", "meatballs", "Kennedy Space Center", "Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "paddy", "Leinart", "Alabama", "drink", "Queen Anne", "the banjo", "a second feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Tony Orlando and Dawn", "AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com:", "South Dakota State Penitentiary", "Anne Boleyn"], "metric_results": {"EM": 0.765625, "QA-F1": 0.825}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-10419", "mrqa_newsqa-validation-1144"], "SR": 0.765625, "CSR": 0.5570054945054945, "retrieved_ids": ["mrqa_squad-train-22821", "mrqa_squad-train-31605", "mrqa_squad-train-60956", "mrqa_squad-train-62208", "mrqa_squad-train-50531", "mrqa_squad-train-84751", "mrqa_squad-train-2083", "mrqa_squad-train-29082", "mrqa_squad-train-3468", "mrqa_squad-train-85922", "mrqa_squad-train-49888", "mrqa_squad-train-59707", "mrqa_squad-train-43765", "mrqa_squad-train-52623", "mrqa_squad-train-57901", "mrqa_squad-train-37497", "mrqa_triviaqa-validation-2485", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2689", "mrqa_naturalquestions-validation-3789", "mrqa_squad-validation-4911", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-1640", "mrqa_newsqa-validation-3376", "mrqa_naturalquestions-validation-9063", "mrqa_searchqa-validation-6067", "mrqa_newsqa-validation-3448", "mrqa_triviaqa-validation-6827", "mrqa_naturalquestions-validation-10509", "mrqa_newsqa-validation-2232"], "EFR": 1.0, "Overall": 0.7325729739010989}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a Chile", "Oliver Twist", "a Vampire Slayer", "the Vistula", "Coriolanus", "Regency Energy Partners", "an aide-de-camp", "a fracture", "Roman Polanski", "Court TV", "sharia", "Jake La Motta", "blog", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "(Madeleine) Albright", "Boggy Peak", "the Harlem Renaissance", "Calamity Jane", "John Lennon", "(Jimmy) Branson", "MVP", "daytime running lights", "Tarzan", "Once", "Harding", "Berrigan & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Wrigley's", "the euro", "the narwhal", "the wall", "John", "Wyatt Earp", "Punjabi", "Tyche Roman", "Department of Agriculture", "shoes", "Frottage", "a triangle", "1999", "he cheated on Miley", "2017", "henry ford", "Henry Hunt", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Charlotte Gainsbourg", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "Audrey Roberts"], "metric_results": {"EM": 0.625, "QA-F1": 0.7129261363636363}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.06060606060606061, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.625, "CSR": 0.5577445652173914, "EFR": 0.9583333333333334, "Overall": 0.724387454710145}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler", "(Usama) Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "handles", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a conifer", "running mate", "an asteroid", "first base", "a cork", "Ichabod Crane", "turn up", "\"Chinatown\"", "a butterfly", "Lolita", "Nacre", "tango", "(General) Wesley Clark", "a porterhouse", "a p.p.", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Apostles", "Lewis Carroll", "meters", "corn", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "(Edouard) Manet", "sons", "(The Emperor) Jones", "Jason Flemyng", "eight", "British citizens", "dinky-di but innocent 'ocker", "Abraham Lincoln", "Neanderthals", "1968", "Vinnie Jones, Scot Williams, and Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides", "September 1947"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7147569444444444}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-137", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-14833", "mrqa_searchqa-validation-8145", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-6104", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_naturalquestions-validation-2586"], "SR": 0.65625, "CSR": 0.5588037634408602, "EFR": 0.9090909090909091, "Overall": 0.7147508095063538}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet on the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a kidney", "Paris", "singing machines", "the Shang pantheon", "Maine", "Gertrude Stein", "The Sun Also Rises", "indoor", "The Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Varney Airlines", "Notre Dame", "the northern wars", "Jupiter", "loverly", "rugby league", "the Falkland Islands", "the Broadway musical", "Iceland", "Orwell", "a chessboard", "heat Transfer", "Jonathan Swift", "Miracle on 34th Street", "a turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel", "the Mesozoic", "Dwight D. Eisenhower", "\"For What It\\'s Worth\"", "the Fourteen Points", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "Wiley Post", "theMist Mountains", "a cantaloupe", "London", "Carl Sandburg", "the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "The Enchantress", "James Earl Jones", "the medical profession", "france the france", "the treaty of Waitangi", "Jessica Lange", "Heinkel He 178", "Kenan & Kel", "304,000", "August 11, 12 and 13,", "Thursday", "backbreaking"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5958858543417367}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-10151", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-5792", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-4060"], "SR": 0.546875, "CSR": 0.5586768617021276, "retrieved_ids": ["mrqa_squad-train-75898", "mrqa_squad-train-50761", "mrqa_squad-train-45959", "mrqa_squad-train-53161", "mrqa_squad-train-56337", "mrqa_squad-train-22305", "mrqa_squad-train-27682", "mrqa_squad-train-68714", "mrqa_squad-train-16881", "mrqa_squad-train-32302", "mrqa_squad-train-64877", "mrqa_squad-train-53672", "mrqa_squad-train-39446", "mrqa_squad-train-43932", "mrqa_squad-train-75376", "mrqa_squad-train-64069", "mrqa_naturalquestions-validation-10032", "mrqa_naturalquestions-validation-9107", "mrqa_hotpotqa-validation-862", "mrqa_searchqa-validation-10116", "mrqa_naturalquestions-validation-4495", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1295", "mrqa_triviaqa-validation-5309", "mrqa_naturalquestions-validation-5554", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3097", "mrqa_triviaqa-validation-644", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-5292", "mrqa_newsqa-validation-2261", "mrqa_searchqa-validation-6372"], "EFR": 0.9655172413793104, "Overall": 0.7260106956162876}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Logan's Run", "Ricardo Sanchez Robert Gates", "zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Jimmy Doolittle", "a riot", "Lon Chaney", "New York", "the \"Fargo\" clue", "Sicily", "the Boston Celtics", "wine", "Enron", "the fulcrum", "Central African Republic", "Rudolf Hess", "a fight", "the hippopotamus", "an eye", "Bech at Bay", "Reagan & Bush", "Washington Irving", "a tree", "the Egyptian government", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "(Jerry) Mathers", "Nine to 5", "Housing and Urban Development", "Extradition", "the head", "the Nutty Professor", "Michael Collins", "The Sopranos", "The Sound And The Fury", "a pair", "Brazil", "obsessive-compulsive", "Katie Holmes", "o oats", "arteries", "1773", "a joule", "Justice", "20 November 1989", "25 September 2007", "Andrew Moray and William Wallace", "Nafea Faa Ipoipo?", "a window", "Crispin", "commentary on Isaac Newton's book \"Principia\"", "PET", "SKUM", "12-hour", "Joan Rivers", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6588541666666666}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-2638"], "SR": 0.546875, "CSR": 0.5585526315789473, "EFR": 1.0, "Overall": 0.7328824013157894}, {"timecode": 95, "before_eval_results": {"predictions": ["Arseniy Yatsenyuk", "Broils", "the Communist Party", "The Goonies", "Velvet Revolver", "Disneyland", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a shank", "fish", "a parens", "Casablanca", "\"Get Behind Me\"", "the Detroit River", "Vincennes", "Northern Exposure", "Kilimanjaro", "Nebuchadnezzar", "a flip", "the Komodo dragon", "Canadian author Mordecai Richler", "The Simpsons", "The West Wing", "devilled eggs", "ravens", "cheese", "Ladd-Franklin", "Pocahontas", "viruses", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "Prince Harry", "2 stanza", "Hades", "Henry Harrison", "Capone", "Maria Callas", "a tide", "the Tournament of Kings", "Antony", "Tennyson", "National Geographic", "Disney", "Jerusalem", "a circle", "the Edict of Nantes", "Hector", "Omega", "at the end of an interrogative sentence : `` How old are you? ''", "Dr. Lexie Grey ( Chyler Leigh )", "since 3, 1, and 4 are the first three significant digits of \u03c0", "baron esterh\u00e1zy", "exponentiation", "Worcestershire", "1754", "25", "Lowe's Companies, Inc.", "blowing up an ice jam Wednesday evening south of  Bismarck,", "Fernando Gonzalez", "Chester Arthur Stiles, 38,", "insects"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5961681547619047}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8571428571428571, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-7141", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-827", "mrqa_triviaqa-validation-4855"], "SR": 0.515625, "CSR": 0.55810546875, "EFR": 0.967741935483871, "Overall": 0.7263413558467742}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "personification", "Nomar Garciaparra", "John Glenn", "a heron", "Apollo 1", "The White Company", "New Balance", "The Seminole War", "Joan of Arc", "finale", "mollus", "Camille & Grard Depardieu", "the East", "caricaturist", "the Seven Years' War", "\"Pride and Prejudice\"", "The Wizard of Oz", "madding", "tribes", "(Richard) Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "Whatchamacallits", "The Stranger", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "pickled", "Khomeini", "backstroke", "7th century AD", "Sydney", "Dermatology", "Solomon", "\"See Who\\'s Talking\"", "Chirac", "6", "Snowflyers", "My ntonia", "Guiana", "grow", "Czechoslovakia", "the Corinthians", "dilithium", "Help!", "from 1997, which the following year became known as the G8", "In 2010", "1215", "bearded woman", "President of the United States", "Mumbai", "Bob Gibson", "eighty-seventh", "a pregnancy", "The Screening Room", "$150 billion", "Rio Grande"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5650297619047618}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-10078", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-11872", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387"], "SR": 0.453125, "CSR": 0.5570231958762887, "retrieved_ids": ["mrqa_squad-train-46251", "mrqa_squad-train-27862", "mrqa_squad-train-36837", "mrqa_squad-train-80899", "mrqa_squad-train-64921", "mrqa_squad-train-57407", "mrqa_squad-train-64339", "mrqa_squad-train-43215", "mrqa_squad-train-81662", "mrqa_squad-train-53979", "mrqa_squad-train-62894", "mrqa_squad-train-8510", "mrqa_squad-train-21661", "mrqa_squad-train-6722", "mrqa_squad-train-74063", "mrqa_squad-train-22493", "mrqa_searchqa-validation-8399", "mrqa_hotpotqa-validation-3169", "mrqa_searchqa-validation-5301", "mrqa_naturalquestions-validation-5640", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-7614", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-8177", "mrqa_squad-validation-7574", "mrqa_searchqa-validation-1845", "mrqa_triviaqa-validation-3468", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-5573", "mrqa_hotpotqa-validation-1681", "mrqa_naturalquestions-validation-9896"], "EFR": 1.0, "Overall": 0.7325765141752577}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "the Death Valley", "The Two Gentlemen of Verona", "a cobb", "Hydra", "Gulliver's Travels", "the Distant Early Warning Line", "Tordis", "jelly beans", "Xinjiang-Uygur Autonomous Region", "sonic boom", "Fergie", "Sacramento", "emerald", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "stars", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "Henry Shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "the Amazon", "Chicago", "the dugongs", "\"Treading Water\"", "1880", "the French & Indian War", "a checkerboard", "Waterloo", "a waterbed", "a monkey", "a bagel", "propellers", "bonnet", "an acre", "Degas", "a cruller", "Helium", "Tokyo", "cream", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "streptococcus", "big Dipper", "Sofia the First", "Australia", "Ben Elton", "an annual road trip,", "Stuttgart", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.625, "QA-F1": 0.6510416666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-773", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-13438", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3859", "mrqa_hotpotqa-validation-3521", "mrqa_newsqa-validation-3139", "mrqa_hotpotqa-validation-3237"], "SR": 0.625, "CSR": 0.5577168367346939, "EFR": 0.9583333333333334, "Overall": 0.7243819090136054}, {"timecode": 98, "before_eval_results": {"predictions": ["Marley", "Magnum", "Ottoman Empire", "Helen of Troy", "whale", "New York", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "\"A Beautiful Mind\"", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "the Sargasso Sea", "All Quiet on the Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Spain", "Ford", "Sidney Sheldon", "surround", "Faraday", "breakfast", "Krispy Kreme", "foreign dignitary", "Stanton Avery", "the Death Valley", "the Cumberland Gap", "yolk", "the Navy", "a dwelling place", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Charles de Gaulle", "Grover Cleveland", "Destiny's Child", "Luxor", "Spain", "The Beatles", "salmon", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "james I", "m\u00e1el Coluim", "Carol Ann Duffy", "Ravenna", "travel diary", "\"This is not something that anybody can reasonably anticipate,\"", "Sgt. Jason Bendett of the 3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion,", "Bahrami", "make life a little easier"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6764823717948718}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-5084", "mrqa_hotpotqa-validation-1364", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.578125, "CSR": 0.5579229797979798, "EFR": 0.9629629629629629, "Overall": 0.7253490635521886}, {"timecode": 99, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.875, "KG": 0.5265625, "before_eval_results": {"predictions": ["the Hundred Years' War", "a backbone", "( Alfred) Binet", "Venial sin", "a caveat", "the ruby slippers", "milk", "Spanish Republic", "Vanessa Hudgens", "King Kong", "come! I forgive you", "Japan", "Rhiannon", "Scotland", "Jerry Mathers", "Kurdish", "Ann Richards", "half-staff", "American Samoa", "Langston Hughes", "New Coke", "The Color Purple", "THX-1138", "Macbeth", "El Greco", "General Motors", "Michelle Williams", "shark", "Frankie Valli", "a blade", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "thriller", "Schwarzenegger", "Edison", "Animal Crackers", "Oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Russia", "Students for a Democratic Society", "All the King's Men", "(Gounod) Bonucci", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 14, 2017", "James Mason", "slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6614583333333334}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-13935", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-15432", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-4773", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_triviaqa-validation-2452", "mrqa_newsqa-validation-1996"], "SR": 0.640625, "CSR": 0.5587500000000001, "retrieved_ids": ["mrqa_squad-train-23520", "mrqa_squad-train-10590", "mrqa_squad-train-38350", "mrqa_squad-train-77389", "mrqa_squad-train-29045", "mrqa_squad-train-80463", "mrqa_squad-train-40151", "mrqa_squad-train-63603", "mrqa_squad-train-15265", "mrqa_squad-train-49429", "mrqa_squad-train-33825", "mrqa_squad-train-40616", "mrqa_squad-train-73391", "mrqa_squad-train-32584", "mrqa_squad-train-82556", "mrqa_squad-train-67614", "mrqa_naturalquestions-validation-7605", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-6880", "mrqa_squad-validation-4932", "mrqa_searchqa-validation-3554", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10964", "mrqa_newsqa-validation-2566", "mrqa_searchqa-validation-7550", "mrqa_hotpotqa-validation-2826", "mrqa_newsqa-validation-4063", "mrqa_searchqa-validation-3259", "mrqa_newsqa-validation-562", "mrqa_naturalquestions-validation-288"], "EFR": 0.9565217391304348, "Overall": 0.7353199728260871}]}