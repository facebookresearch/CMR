{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5', diff_loss_weight=0, ewc_gamma=0.9, ewc_lambda=250.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4180, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["1978", "587,000 square kilometres", "itinerant farmers", "Gamal Abdul Nasser", "39", "Philo of Byzantium", "cnidarians", "the dukes", "Christopher Eccleston", "Ed Lee", "Keck and Mithouard", "Word and Image Department", "Water (H2O) and carbon dioxide (CO2)", "BBC Radio 5 Live", "Baptism", "achievement-oriented motivations (\"pull\")", "2 million", "inferior", "until 1796", "dummy upper stages filled with water", "Variable lymphocyte receptors", "progressive folk-rock", "$32 billion", "Derek Wolfe", "Basel", "1937", "tourism", "white", "Midsummer\u2019s Night", "installed electrical arc light based illumination systems", "2016", "all large cases of the problem are hard", "photooxidative damage", "five", "iteratively", "the Sun", "Climate fluctuations during the last 34 million years", "Bible translation", "Hayri Abaza", "a better understanding of the Mau Mau command structure", "Turnagain Lane", "monophyletic", "adaptive immune system", "The Dornbirner Ach", "water flow through the body cavity", "CBSE", "a cubic interpolation formula", "Writers Guild of America", "education", "five", "14th to the 19th century", "extended structure", "the Lisbon Treaty", "the Romantic Rhine", "2012", "No Child Left Behind", "The Deadly Assassin and Mawdryn undead", "higher than normal O2 exposure for a fee", "three to five", "TFEU article 294", "The Northern Chinese were ranked higher", "Tracy Wolfson", "local building authority regulations and codes of practice", "The WB"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8807426948051948}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4257", "mrqa_squad-validation-486", "mrqa_squad-validation-5362", "mrqa_squad-validation-291", "mrqa_squad-validation-1862", "mrqa_squad-validation-9520", "mrqa_squad-validation-3104", "mrqa_squad-validation-3609", "mrqa_squad-validation-8256"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["recreational", "Khorasan", "higher", "bigamy", "pathogens", "the geographical area it covers", "ships", "12 January", "7.5%", "Ren\u00e9 Lalique", "The Hoppings", "the Scots", "double or triple non-French linguistic origins", "wage or salary", "Each packet is labeled with a destination address, source address, and port numbers", "Maria Fold and thrust Belt", "Yinchuan", "1542", "Chivas", "26", "Amazonia: Man and Culture in a Counterfeit Paradise", "1550", "two", "1850", "Greg Brady", "one's superiority, domination and influence upon a person or group of people", "22 October 2006", "10,000", "Foreign Protestants Naturalization Act", "art posters", "Americans", "its unpaired electrons", "Los Angeles", "over 100,000", "wealth", "chromoplasts", "1,548", "The upper Rhine and upper Danube are easily crossed", "Peter Pratt and Geoffrey Beevers", "2010", "Germany and Austria", "Immunoproteomics", "solid economic growth", "ditch digger", "1754\u20131763", "Danny Trevathan", "Jochi", "1999", "rubisco", "August 1914", "public", "affordable housing", "seven", "John Mearsheimer and Robert Pape", "Business Connect", "1550 to 1900", "rules that conflict with morality", "Bauhaus", "2015", "French", "EBSCO", "The official vegetable of Washington State is a sweet onion", "diary", "a place where monks or nuns live"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8047041933760684}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3610", "mrqa_squad-validation-4677", "mrqa_squad-validation-4305", "mrqa_squad-validation-360", "mrqa_squad-validation-9802", "mrqa_squad-validation-9372", "mrqa_squad-validation-7701", "mrqa_squad-validation-6891", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5763", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-206"], "SR": 0.78125, "CSR": 0.8203125, "EFR": 1.0, "Overall": 0.91015625}, {"timecode": 2, "before_eval_results": {"predictions": ["2016", "expansion", "Shirley and Johnson", "Miller", "1892 to 1894", "26", "99.4", "St Thomas Becket", "more convenient and private", "two", "Moscone Center", "Germany", "January", "about thirty", "10", "Greg Olsen", "Prince of P\u0142ock", "electron microscopy", "computability theory", "1275", "Fred Pierce", "liquid nitrogen", "Quasiturbine", "UK", "Daniel 8:9\u201312, 23\u201325", "Ex post facto laws", "cameras and microphones", "time and space hierarchy theorems", "$105 billion", "six", "55 mph", "The Bachelor", "child-killers", "hogs and cattle being shipped from Florida to aid the Confederate cause", "Cretaceous\u2013Paleogene extinction", "almost a month", "George B. Storer", "CD40", "94", "their dispersed population and distance from the Scottish Parliament in Edinburgh", "difference in potential energy", "George Westinghouse", "Geneva", "Court of Justice", "Lake \u00dcberlingen", "Budapest", "Genoese traders", "the \"simple people\"", "Stanford University", "formal language", "systematic economic inequalities", "stealing", "lunar new year", "Steve McQueen", "orange juice", "Sedgefield", "Sirhan Sirhan", "a wooden comb", "Asia", "People!  and The Carnabeats", "a Yemeni cleric and his personal assistant", "monochtitlan", "Xherdan Shaqiri", "Potomac River"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7581597222222223}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6337", "mrqa_squad-validation-7165", "mrqa_squad-validation-1703", "mrqa_squad-validation-3540", "mrqa_squad-validation-4096", "mrqa_squad-validation-599", "mrqa_squad-validation-8534", "mrqa_squad-validation-7096", "mrqa_squad-validation-4206", "mrqa_squad-validation-3945", "mrqa_squad-validation-4771", "mrqa_squad-validation-1509", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4028", "mrqa_newsqa-validation-817", "mrqa_searchqa-validation-13857", "mrqa_hotpotqa-validation-1902"], "SR": 0.71875, "CSR": 0.7864583333333334, "EFR": 1.0, "Overall": 0.8932291666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["Religious Coalition for Reproductive Choice", "Waal", "malaria parasite", "three", "over $40 million", "SyFy", "Ollie Treiz", "five", "younger", "nearly three hundred years", "1 July 1851", "the world's economy", "nine", "the property owner", "1916", "twice", "its unpaired electrons", "Golovin", "how or whether this connection is relevant on microscales", "Matt Smith", "Baltimore Raven", "noble", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "CBS and NBC", "mediaeval music", "southern California", "26", "Masovian Primeval Forest", "Apollo 1 backup crew", "2008", "a hemicycle", "Connectional Table", "waldzither", "Deacons", "performance", "destruction of the forest", "10.0%", "dissension and unrest", "Africa", "an occupancy permit", "one judge from each member state, 28 at present", "Dignity Health", "Battle of Jumonville Glen", "Kony Ealy", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "Napoleon", "2016", "tette", "pasta", "Bones", "The History Book Club", "kennedy", "tethered", "yerevan", "kennedy", "The Solar System is located within the disk", "Hugh S. Johnson", "The Five Stages of Sleep", "k. Dumas and Jonathan Lighter argue that an expression should be considered \"true slang\" if it meets at least two of the following criteria", "various", "1896", "the Carrousel du Louvre", "The Goldstone Report", "Odense Boldklub"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7171226958525345}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19047619047619044, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6008", "mrqa_squad-validation-10427", "mrqa_squad-validation-438", "mrqa_squad-validation-6118", "mrqa_squad-validation-2612", "mrqa_squad-validation-8134", "mrqa_squad-validation-4360", "mrqa_squad-validation-6426", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-6097", "mrqa_searchqa-validation-4289", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-7616", "mrqa_hotpotqa-validation-3780", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-1749"], "SR": 0.671875, "CSR": 0.7578125, "EFR": 1.0, "Overall": 0.87890625}, {"timecode": 4, "before_eval_results": {"predictions": ["13", "he miscalculated the political implications", "178", "1st century BC", "The Five Doctors", "Combined Statistical Area", "Bryan Davies", "In the 1060s", "Fresno", "they finally captured Ticonderoga", "Utopia", "Ron Grainer", "BBC Dead Ringers", "July 11, 1962", "Light", "49\u201315", "pancake-shaped circular disks", "The energy crisis", "nine", "very low tuition fees", "increase its bulk and decrease its density", "his rent at the Hotel New Yorker", "it focuses attention on the threat of punishment and not the moral reasons to follow this law", "Ron Grainer", "symbiotic", "Larry Roberts", "CBS", "leftist/communist/nationalist insurgents/opposition", "RSA", "the wisdom and prudence of certain decisions of procurement", "France's claim to the region was superior to that of the British", "immunosuppressive", "very weak", "two public agencies", "Florida", "August 1992", "71%", "Samuel Phillips", "1962", "Newcastle Diamonds", "five", "demographics and economic ties", "UNICEF", "Robert Mugabe", "civilians", "maintain an \"aesthetic environment\" and ensure public safety", "Pakistani territory", "August 11, 12 and 13", "right-wing extremist groups", "Argentina lays claim not just to the islands, but to any resources that could be found there.", "the largest and perhaps most sophisticated ring of its kind in U.S. history", "Don Draper", "it was beautiful", "the body", "1979", "rapid", "John", "1618", "House of Commons", "\"You're out.", "The ballot", "rapid", "white", "Montezuma"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7173186622405372}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.08888888888888888, 1.0, 0.0, 0.125, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2591", "mrqa_squad-validation-7792", "mrqa_squad-validation-2717", "mrqa_squad-validation-10269", "mrqa_squad-validation-7715", "mrqa_squad-validation-236", "mrqa_squad-validation-8811", "mrqa_squad-validation-1462", "mrqa_squad-validation-6874", "mrqa_squad-validation-7713", "mrqa_squad-validation-6801", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-430", "mrqa_searchqa-validation-8027", "mrqa_triviaqa-validation-1088"], "SR": 0.671875, "CSR": 0.740625, "EFR": 1.0, "Overall": 0.8703125}, {"timecode": 5, "before_eval_results": {"predictions": ["in the Holyrood area of Edinburgh", "missile projects", "The Entertainment Channel", "home viewers who made tape recordings of the show", "cattle and citrus", "quantum electrodynamics", "confirmation", "75th birthday", "a decision problem", "Plasmodium falciparum", "civil disobedience", "Masovian gothic", "wealth and income", "a polynomial-time reduction", "2012", "the European Parliament and the Council of the European Union", "antithetical", "a polynomial time algorithm", "Fred Singer", "William of Volpiano and John of Ravenna", "nitrogen", "Arizona Cardinals", "Seventy percent", "co-chair of TAR WGI", "July 24", "nearly three hundred years", "Encoded Archival description (EAD)", "the Ministry of War", "a citizen's relation to the state and its laws", "the Golden Gate Bridge", "2011", "respiration", "environmental degradation", "Fred Silverman", "2016", "390", "1 million", "in the chloroplasts of C4 plants", "When the reaction occurs in a liquid solution, the solid formed is called the'precipitate '", "Jim Capaldi, Paul Carrack, and Peter Vale", "during Hanna's recovery masquerade celebration, she suddenly regains her memory, revealing that Mona is A.", "in order to halt it following brake failure", "the last time the los angeles lakers won a championship -- and first in Los Angeles -- in 1972", "boy", "1998", "geophysicists", "between Glen Miller Road in Trenton and the Don Valley Parkway / Highway 402 Junction in Toronto", "April 25 -- 30 in Park Avenue, just outside the Waldorf - Astoria Hotel", "fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "December 1, 2009", "in sequence with each heartbeat", "the Colonel Bogey March", "Richard Seddon", "Switzerland", "the \"Black Abbots\"", "1919", "Shankar", "Fernando Verdasco", "Barack Obama", "July 4", "\"Personal Jesus\"", "the Thorn Birds", "the molluscan class Cephalopoda", "Australia"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6670790912518854}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.15384615384615385, 0.7058823529411764, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-2705", "mrqa_squad-validation-1600", "mrqa_squad-validation-6670", "mrqa_squad-validation-2160", "mrqa_squad-validation-3556", "mrqa_squad-validation-4260", "mrqa_squad-validation-7629", "mrqa_squad-validation-5921", "mrqa_squad-validation-8671", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2555", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-2328", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-10685"], "SR": 0.59375, "CSR": 0.7161458333333333, "EFR": 0.9615384615384616, "Overall": 0.8388421474358974}, {"timecode": 6, "before_eval_results": {"predictions": ["2007", "1973", "Siegfried", "June", "27 September 2001", "friendly and supportive", "500", "northern China", "18 February 1546", "tree growth stages", "Karl von Miltitz", "a supervisory church body", "via the ballast tanks of ships", "1279", "plant health status", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "(at the opposite end from the mouth)", "models", "river Deabolis", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "October", "three", "a \"chameleon circuit\"", "sea level", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "geographic scholars under colonizing empires", "the Lippe", "colonialism", "10,000", "layered basaltic lava flows", "political parties", "$5,000,000", "a second Gleichschaltung", "a transient or ongoing role", "11 February 2012", "Sets heart in mediastinum", "a nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "Santo Domingo", "O'Meara", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "first Sunday after Easter", "the player shouts in order to attract the listener's attention", "9.0 -- 9.1 ( M )", "English author Rudyard Kipling", "American Horror Story", "David Joseph Madden", "Baylor", "minor key symphonies", "in Christianity", "London", "Belfast", "St Paul's Cathedral", "Sydney", "Australian", "Battle of Britain and the Battle of Malta", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Casalesi Camorra clan", "not the first tech company to be hit by the European Commission", "bonds", "Crown Princess Juliana of the Netherlands", "Kate Pahls", "Paolo di Dono", "Crete", "Selfie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6521949404761904}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.05714285714285715, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.1714285714285714, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.8, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4506", "mrqa_squad-validation-2468", "mrqa_squad-validation-9408", "mrqa_squad-validation-7554", "mrqa_squad-validation-9865", "mrqa_squad-validation-1866", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1058", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-2268", "mrqa_triviaqa-validation-5713"], "SR": 0.578125, "CSR": 0.6964285714285714, "EFR": 0.9259259259259259, "Overall": 0.8111772486772486}, {"timecode": 7, "before_eval_results": {"predictions": ["annual NFL Experience", "over three days", "second and third run movies, along with classic films", "(radiography", "the government and the National Assembly and the Senate", "lipophilic alkaloid toxins", "the Compromise of 1850", "a multi-party system", "phagosomal membrane", "1969", "The preparation and approval process", "\"basis of reciprocity\"", "between hadrons (the best known example being the force that acts between nucleons in atomic nuclei)", "Downtown San Diego", "1954", "Bermuda 419 turf", "The Judiciary", "they are judged \" wrong\" by an individual conscience, or as part of an effort to render certain laws ineffective, to cause their repeal, or to exert pressure to get one's political wishes on some other issue", "The Space Museum", "homologous recombination", "The Hoppings funfair", "Decision problems", "seven", "Pickawillany", "denying having committed the crime, or by fleeing the jurisdiction", "56.2%", "Financial crisis of 2007\u201308", "Peter Davison", "Percy Shelley", "ctenophores", "American Broadcasting-Paramount Theatres, Inc.", "Ray Henderson", "rises 735 feet ( 224 m )", "B.R. Ambedkar, the chairman of the Drafting Committee, is widely considered to be its chief architect", "Kim Basinger", "John F. Kelly, to the post of White House Chief of Staff by President Donald Trump", "2001 -- 2002 season", "outside world", "two", "In 1973, the University of California, Berkeley recognized the need to provide quality library materials to support the Chicano studies programs", "Prem Lata Agarwal", "line the cavities and surfaces of blood vessels and organs throughout the body", "gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "thirteen if Plank, a board of wood who acts as one character's imaginary friend, is included", "Ron Harper", "Dmitri Mendeleev, who built upon earlier discoveries by scientists such as Antoine - Laurent de Lavoisier and John Newlands, but who is nevertheless generally given sole credit for its development", "The sacroiliac joint or SI joint ( SIJ )", "patron", "South African political leaders Mangosuthu Buthelezi and Harry Schwarz", "John Chilcot", "The Eagle", "Kind Hearts and Coronets", "ummi Vandhaal", "The 2018 Unibet Premier League Darts", "January 2004", "London and Buenos Aires", "Evan Bayh of Indiana and Virginia Gov. Tim Kaine, former Georgia Sen. Jack Reed, New Mexico Gov. Bill Richardson and Kansas Gov. Kathleen Sebelius.", "after Wood went missing off Catalina Island, near the California coast", "the Juarez drug cartel", "use hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off in this type of 10-letter chemical reaction", "Ichthys", "Colorado", "Bonnie Raitt", "between the three towns of Doncaster, Scunthorpe and Gainsborough"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5652923611827612}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.34146341463414637, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9523809523809523, 0.7567567567567568, 0.125, 0.0, 0.12903225806451613, 0.5, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 0.08695652173913042, 0.0, 0.0, 1.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-8316", "mrqa_squad-validation-8687", "mrqa_squad-validation-8496", "mrqa_squad-validation-4589", "mrqa_squad-validation-10444", "mrqa_squad-validation-436", "mrqa_squad-validation-6787", "mrqa_squad-validation-8732", "mrqa_squad-validation-6776", "mrqa_squad-validation-4730", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-2940", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-1140", "mrqa_hotpotqa-validation-5273", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-493", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-12091", "mrqa_hotpotqa-validation-1533"], "SR": 0.46875, "CSR": 0.66796875, "EFR": 1.0, "Overall": 0.833984375}, {"timecode": 8, "before_eval_results": {"predictions": ["Episcopal Areas", "\"vanguard of change and Islamic reform\"", "scoil phr\u00edobh\u00e1ideach", "1993", "Chinatown", "state, relative cost of living, and grade taught", "it stimulated his brain cells", "made tape recordings of the show", "the chosen machine model", "captive import policy", "1855 colonial constitution", "Rhine Gorge", "savanna or desert", "1967", "Vince Lombardi Trophy", "2003", "Kurt Vonnegut", "10", "to become more integral within the health care system", "ten", "12", "1995\u201396", "San Diego International Airport", "hydrogen and helium", "two", "Golden Gate Bridge", "Elders", "1331", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "Billy Beane's ( Brad Pitt ) daughter", "Menorca", "Eurasian Plate", "1947", "John Dalton", "five", "it was on this day in 1930 when Declaration of Indian Independence ( Purna Swaraj ) was proclaimed by the Indian National Congress", "was a naval battle fought between an alliance of Greek city - states under Themistocles and the Persian Empire", "Road / Track", "Pradyumna", "the great comet of 1812", "May 18, 2010", "Newfoundland and Labrador", "Mangal Pandey", "Asuka", "electron shells", "constant pressure", "lmfao", "a true wireworm", "a self-governing colony", "50", "a saint", "Jena Malone", "American", "1993", "civilians", "2050", "requires police to question people if there's reason to suspect they're in the United States illegally", "Piedad Cordoba", "the Treaty of Paris", "a fire-engine shade", "Robert Frost", "congruent", "The Dark Tower", "Ministry of European Integration"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6771924008531152}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8163265306122449, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-2236", "mrqa_squad-validation-1445", "mrqa_squad-validation-7643", "mrqa_squad-validation-8984", "mrqa_squad-validation-6404", "mrqa_squad-validation-3667", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-1119", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-672", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3581", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-11539"], "SR": 0.609375, "CSR": 0.6614583333333333, "EFR": 0.96, "Overall": 0.8107291666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["Sports Night", "c1750", "Newton", "GM", "Political Islam", "51.6%", "public policy", "the superior and the norm", "1985", "standard model of particle physics", "Theory of the Earth", "New Jersey, Rhode Island and Delaware", "2,249", "canceled", "Earth", "prime elements", "T. J. Ward", "27-30%", "often a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "primes", "1995", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "applied mathematics to the construction", "military strategies", "Elizabeth", "1861\u20131865", "1968", "Daimler-Benz", "25", "Baldwin", "May 1801", "John Schlesinger", "hamburgers", "Ellie Kemper", "Ezeiza International Airport", "due to a leg injury", "life insurance", "bioelectromagnetics", "Tennessee", "Innsbruck", "Andes", "Vishal Bhardwaj", "Lincoln Memorial University", "five aerial victories", "main role in Matilda the musical in London's West End", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "Justin Bieber", "The Nitty Gritty Dirt Band", "April", "Lucky the Leprechaun", "Leander", "1", "river", "Dean Martin, Katharine Hepburn and Spencer Tracy", "between South America and Africa", "\"Quiet Nights", "Crandon, Wisconsin", "the Lone Ranger", "z", "The Morris worm", "Cheryl of the Clue Crews", "1922", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Donald Trump"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5876585730446025}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.38095238095238093, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.47058823529411764, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-361", "mrqa_squad-validation-3751", "mrqa_squad-validation-9608", "mrqa_squad-validation-7017", "mrqa_squad-validation-10506", "mrqa_squad-validation-3347", "mrqa_squad-validation-2315", "mrqa_squad-validation-6806", "mrqa_squad-validation-8231", "mrqa_squad-validation-6154", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-3750", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1567", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4272", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2324", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-2892", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5876"], "SR": 0.484375, "CSR": 0.64375, "EFR": 0.9696969696969697, "Overall": 0.8067234848484849}, {"timecode": 10, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1966", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-861", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9963", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-787", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-206", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8027", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10004", "mrqa_squad-validation-10010", "mrqa_squad-validation-10024", "mrqa_squad-validation-10038", "mrqa_squad-validation-10059", "mrqa_squad-validation-10068", "mrqa_squad-validation-10072", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10115", "mrqa_squad-validation-10124", "mrqa_squad-validation-10140", "mrqa_squad-validation-10232", "mrqa_squad-validation-10340", "mrqa_squad-validation-10340", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1172", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1311", "mrqa_squad-validation-1409", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1541", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1634", "mrqa_squad-validation-1637", "mrqa_squad-validation-1651", "mrqa_squad-validation-1703", "mrqa_squad-validation-1762", "mrqa_squad-validation-1817", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2095", "mrqa_squad-validation-2108", "mrqa_squad-validation-2160", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2315", "mrqa_squad-validation-2325", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2403", "mrqa_squad-validation-2461", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2612", "mrqa_squad-validation-2678", "mrqa_squad-validation-2711", "mrqa_squad-validation-2717", "mrqa_squad-validation-2752", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2861", "mrqa_squad-validation-2869", "mrqa_squad-validation-2902", "mrqa_squad-validation-291", "mrqa_squad-validation-2916", "mrqa_squad-validation-2934", "mrqa_squad-validation-2952", "mrqa_squad-validation-2985", "mrqa_squad-validation-3049", "mrqa_squad-validation-3104", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-322", "mrqa_squad-validation-3222", "mrqa_squad-validation-3223", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-334", "mrqa_squad-validation-3347", "mrqa_squad-validation-3416", "mrqa_squad-validation-343", "mrqa_squad-validation-3440", "mrqa_squad-validation-3524", "mrqa_squad-validation-3540", "mrqa_squad-validation-3556", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3609", "mrqa_squad-validation-361", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-3620", "mrqa_squad-validation-3660", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3713", "mrqa_squad-validation-3745", "mrqa_squad-validation-3751", "mrqa_squad-validation-3752", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3866", "mrqa_squad-validation-3871", "mrqa_squad-validation-3873", "mrqa_squad-validation-3954", "mrqa_squad-validation-3957", "mrqa_squad-validation-3962", "mrqa_squad-validation-3986", "mrqa_squad-validation-4026", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-418", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4206", "mrqa_squad-validation-4242", "mrqa_squad-validation-4246", "mrqa_squad-validation-4257", "mrqa_squad-validation-4260", "mrqa_squad-validation-4305", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4447", "mrqa_squad-validation-4451", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-45", "mrqa_squad-validation-453", "mrqa_squad-validation-4533", "mrqa_squad-validation-4547", "mrqa_squad-validation-4575", "mrqa_squad-validation-4589", "mrqa_squad-validation-4630", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-47", "mrqa_squad-validation-4707", "mrqa_squad-validation-4730", "mrqa_squad-validation-4771", "mrqa_squad-validation-4775", "mrqa_squad-validation-4832", "mrqa_squad-validation-486", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-5052", "mrqa_squad-validation-5099", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-519", "mrqa_squad-validation-5230", "mrqa_squad-validation-524", "mrqa_squad-validation-5250", "mrqa_squad-validation-5329", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5362", "mrqa_squad-validation-5364", "mrqa_squad-validation-539", "mrqa_squad-validation-5434", "mrqa_squad-validation-5440", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5558", "mrqa_squad-validation-5562", "mrqa_squad-validation-5597", "mrqa_squad-validation-5650", "mrqa_squad-validation-5671", "mrqa_squad-validation-5693", "mrqa_squad-validation-57", "mrqa_squad-validation-5753", "mrqa_squad-validation-5772", "mrqa_squad-validation-5783", "mrqa_squad-validation-5791", "mrqa_squad-validation-5881", "mrqa_squad-validation-5921", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-5999", "mrqa_squad-validation-6013", "mrqa_squad-validation-6042", "mrqa_squad-validation-6118", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6288", "mrqa_squad-validation-6291", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6491", "mrqa_squad-validation-6552", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6776", "mrqa_squad-validation-6787", "mrqa_squad-validation-6801", "mrqa_squad-validation-6805", "mrqa_squad-validation-6806", "mrqa_squad-validation-6852", "mrqa_squad-validation-6861", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6948", "mrqa_squad-validation-6958", "mrqa_squad-validation-6962", "mrqa_squad-validation-6996", "mrqa_squad-validation-7017", "mrqa_squad-validation-7026", "mrqa_squad-validation-7030", "mrqa_squad-validation-7035", "mrqa_squad-validation-71", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7173", "mrqa_squad-validation-7328", "mrqa_squad-validation-7331", "mrqa_squad-validation-734", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7380", "mrqa_squad-validation-7384", "mrqa_squad-validation-7395", "mrqa_squad-validation-742", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-758", "mrqa_squad-validation-7628", "mrqa_squad-validation-7629", "mrqa_squad-validation-764", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7713", "mrqa_squad-validation-7715", "mrqa_squad-validation-7723", "mrqa_squad-validation-7747", "mrqa_squad-validation-7774", "mrqa_squad-validation-7792", "mrqa_squad-validation-7793", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-7976", "mrqa_squad-validation-7993", "mrqa_squad-validation-8002", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-82", "mrqa_squad-validation-8232", "mrqa_squad-validation-8256", "mrqa_squad-validation-828", "mrqa_squad-validation-8319", "mrqa_squad-validation-8320", "mrqa_squad-validation-8338", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-847", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8671", "mrqa_squad-validation-8679", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-8728", "mrqa_squad-validation-8732", "mrqa_squad-validation-8796", "mrqa_squad-validation-8811", "mrqa_squad-validation-8839", "mrqa_squad-validation-8862", "mrqa_squad-validation-8872", "mrqa_squad-validation-8920", "mrqa_squad-validation-893", "mrqa_squad-validation-8930", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-916", "mrqa_squad-validation-9178", "mrqa_squad-validation-9240", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9331", "mrqa_squad-validation-9351", "mrqa_squad-validation-9408", "mrqa_squad-validation-9413", "mrqa_squad-validation-9470", "mrqa_squad-validation-9520", "mrqa_squad-validation-9532", "mrqa_squad-validation-959", "mrqa_squad-validation-96", "mrqa_squad-validation-9608", "mrqa_squad-validation-9647", "mrqa_squad-validation-9777", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-9865", "mrqa_squad-validation-988", "mrqa_squad-validation-9984", "mrqa_squad-validation-999", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1140", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3602", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-5871", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-6513", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6903", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7616"], "OKR": 0.8671875, "KG": 0.43359375, "before_eval_results": {"predictions": ["November 1979", "Pittsburgh", "William Rainey Harper", "classical element fire", "the Mughal state", "photosynthesis", "first half of the 10th century", "53,000", "animosity toward each other", "giving her brother Polynices a proper burial", "lion, leopard, buffalo, rhinoceros, and elephant", "1998 NFL draft", "Widener Library", "a fee per unit of information transmitted, such as characters, packets, or messages", "Nobel Prize", "Bart Starr", "UK", "Australia", "to 7.8%", "Mars", "2007", "27", "Louis King", "Boston", "Argentina, whose president Bartolom\u00e9 Mitre", "Coronation Street", "The Food and Drug Administration", "Art Deco-style skyscraper", "Lindsey Islands", "music arranger", "Brig Gen Augustine Warner Robins", "flags of dependent territories", "Russian film industry", "Thriller", "Restoration Hardware", "Russell Humphreys", "Iron Man 3", "Andrzej Go\u0142ota", "Waylon Smithers", "Ordos City China Science Flying Universe Science and Technology Co., Ltd.", "The Vanishing", "Wendell Berry", "Che Guevara", "Indians", "gastrocnemius muscle", "1,228 km / h ( 763 mph )", "cells", "a compiler can derive machine code", "typhoid fever", "William Boyd", "Octopussy", "Audi A4", "The first Labor Day in the USA was celebrated on Tuesday, September 5, 1882, in New York City", "actress", "The supplemental spending bill", "genocide", "The Palm Jumeirah", "murder", "Roosevelt", "The member of the Industrial Workers of the World, even if he's not unsteady", "The chemical element", "the Queen Anne's Revenge", "Kellogg's", "Tintin"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7296539224664225}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1054", "mrqa_squad-validation-366", "mrqa_squad-validation-4750", "mrqa_squad-validation-7724", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-201", "mrqa_naturalquestions-validation-9885", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-5525", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-13227", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-16856"], "SR": 0.65625, "CSR": 0.6448863636363636, "EFR": 1.0, "Overall": 0.7375710227272727}, {"timecode": 11, "before_eval_results": {"predictions": ["the difference in potential energy", "Many words", "accessory pigments that override the chlorophylls' green colors", "62", "$20,000", "the Autons with the Nestene Consciousness and Daleks", "1080i HD", "income inequality", "104 \u00b0F (40 \u00b0C)", "conspiracy against Islam by the Western governments", "road engines", "Torah or Bible", "Larry Ellison", "consultant", "Derek Wolfe and Malik Jackson", "luxurious parks and royal gardens", "the Establishment Clause of the First Amendment", "Centrum", "20%", "to win an acquittal and avoid imprisonment or a fine", "commemorating fealty and filial piety", "Melissa Disney", "Theodore Roosevelt", "Andy Cole", "13 February", "200 to 500 mg up to 7 mg", "November 17, 1800", "The 5 - badge limit remained in effect", "Monk's Caf\u00e9", "the Director of National Intelligence", "Francisco Pizarro", "a Czech word, robota", "Kyla Pratt", "in a 1945 NCAA game between Columbia and Fordham", "Michael Crawford", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "2017 / 18 Divisional Round", "New Croton Reservoir in Westchester and Putnam counties", "550 quadrillion Imperial gallons", "Kepner", "for purposes of herd maintenance", "Annette Strean", "Anwar Sadat", "Freia", "The Battle of Kasserine Pass", "Richard Wagner", "Muriel Spark", "Islamic philosophy", "Minnesota", "Kim Yeon-soo", "Scottish", "stoneware, or specifically ornamental beer mugs that are usually sold as souvenirs or collectibles", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "nearly $162 billion in war funding", "the home of San Diego County's lucrative avocado crop, along with other croplands, greenhouses and nurseries.", "Samuel Herr, 26, and Juri Kibuishi, 23, of Irvine", "Old Trafford", "The Truman Show", "The hand of cards which he supposedly held at the time of his death", "infield", "glaciers", "ummi", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "a paragraph about the king and crown prince that makes it illegal to defame, insult or threaten the crown"], "metric_results": {"EM": 0.5, "QA-F1": 0.5900793385536032}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.4, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.36363636363636365, 0.5, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.23529411764705882, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.08, 0.5454545454545454]}}, "before_error_ids": ["mrqa_squad-validation-7729", "mrqa_squad-validation-7577", "mrqa_squad-validation-7162", "mrqa_squad-validation-9632", "mrqa_squad-validation-1850", "mrqa_squad-validation-7087", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-100", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-5538", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-1265", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-2716", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-702"], "SR": 0.5, "CSR": 0.6328125, "EFR": 0.96875, "Overall": 0.72890625}, {"timecode": 12, "before_eval_results": {"predictions": ["San Joaquin Light & Power Building", "2003", "British", "Doritos", "live", "23.9%", "Daewoo", "1777", "34\u201319", "John D. Rockefeller", "Pole Mokotowskie", "Newcastle College", "63%", "Boulton", "can produce both eggs and sperm at the same time", "laws of physics", "buoyancy", "international drug suppliers, rather than consumers", "Oregon", "tuberculosis", "The Young Men's Christian Association", "the centre of Bleak House", "Charlie Drake", "The Iron Duke", "tunisia", "28", "The Golden Child", "Scarborough", "Il Trovatore", "tunisia", "The Kentucky Derby", "tunisia", "Hindi", "alson Welles", "Nowhere Boy", "The Stereophonics", "Lancashire", "the recorder", "Michael J. Fox", "Japanese", "Poland", "October 2, 2017", "By 1912", "Ravi Shastri", "Bohrium", "Anthony Hopkins", "Ella Jane Fitzgerald", "Bill Cosby", "Anna Clyne", "lightweight aluminum foil", "2014", "Hong Kong's Victoria Harbor", "school", "Herman Thomas", "Sunday", "anesthetic and sedative", "alkinsia", "tunisia", "tunis", "al, Weenie Dog, Big Cats, Sausage Dogs", "the First Charter ones and twos", "alipsner", "tunisia", "tunisia"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6529761904761905}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.8, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4645", "mrqa_squad-validation-3492", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-1765", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-4591", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-444", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-134", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-13604", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-16664", "mrqa_searchqa-validation-13542"], "SR": 0.578125, "CSR": 0.6286057692307692, "EFR": 1.0, "Overall": 0.7343149038461538}, {"timecode": 13, "before_eval_results": {"predictions": ["Elders", "WatchESPN", "pulmonary fibrosis", "Series 5", "February 1, 2016", "Sophocles' play Antigone", "1972", "fifty", "Norman", "\u00a341,004", "1806-07", "Executive Vice President of Football Operations and General Manager", "British", "1920s", "poet", "the number of social services that people can access wherever they move", "john Dryden", "21", "Fresh Fields", "fish", "Ecuador", "Brussels", "Hitler", "Egypt", "brow, Carlisle, Whitehaven and Workington", "photographer", "johnson", "Uganda", "dogs", "coffee, soda, and other beverages", "leicestershire", "Bill johnson", "Norman Tebbit", "gold", "Istanbul", "wale", "sally kellson", "Los Angeles", "G. Ramon", "sally kean", "White Christmas", "the nucleus", "12 November 2010", "November 1975", "Afonso IV of Portugal", "southern Turkey", "sitcom \"Barney Miller\"", "Flavivirus", "Nye County", "Eyes Wide Shut", "the \"Pour le M\u00e9rite\"", "San Francisco, California", "rebecca dukakis", "1-0", "Terra Firma, which controls EMI, owner of the recording studios.", "June 6, 1944", "19", "backbreaking labor", "Curly Lambeau", "the Oxford English Dictionary", "rebecca", "Jimmy Lee", "Jezebel", "cheese"], "metric_results": {"EM": 0.578125, "QA-F1": 0.658326768207283}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.5, 0.25, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7762", "mrqa_squad-validation-6638", "mrqa_squad-validation-1135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-5352", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1349", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-804", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-9672", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-4067", "mrqa_searchqa-validation-10768", "mrqa_searchqa-validation-7749"], "SR": 0.578125, "CSR": 0.625, "EFR": 0.9629629629629629, "Overall": 0.7261863425925925}, {"timecode": 14, "before_eval_results": {"predictions": ["Xbox One", "tyrosinase", "reduced wages", "2005", "Art Deco style in painting and art.", "The Prospect Studios", "Advanced Steam movement", "three", "the Moscone Center in San Francisco", "since 2001", "identified change orders or project changes that increased costs", "comedies and family-oriented", "case law by the Court of Justice, international law and general principles of European Union law", "oxide compounds such as silicon dioxide, making up almost half of the crust's mass", "278", "bacterial fusion along a single axis forming chains", "earplugs", "Melvil Dewey", "saxophone", "Thaddaeus", "7 wives", "China", "the foot", "Lincoln Logs", "March 19th", "Diptera", "mumbawa island of Sumbawa,", "paste paste", "fruit in the actual brewing process is still relatively novel", "the sound of the human voice", "Erik Thorvaldson", "tungsten", "Gulliver's Travels", "Black September", "the Dartford Warblers", "jodie Foster", "geodesy", "comets", "the sense of smell", "Niveditha, Diwakar, Shruti", "Tagalog or English", "2013", "in 1861, when Boston confectioner William Schrafft urged people to send his jelly beans to soldiers during the American Civil War", "President James Madison", "Kenny Gamble & Leon Huff", "Southern Illinois University Carbondale", "Marco Hietala", "New Orleans Pelicans", "Worcester County, Massachusetts, United States", "North Carolina", "July 23, 1971", "March 22", "August 19, 2007", "comets", "Ike", "95", "J. Crew", "john Harvard", "Montserrat", "lymphoma and blood disease", "South Carolina", "five thieves enter the Riviera Casino in Las Vegas with guitar cases full of guns, and stage a daring robbery.", "Volitan Lionfish", "Vladimirovich \"Val\" Bure"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5601145382395382}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.09090909090909091, 0.8, 0.0, 0.28571428571428575, 1.0, 0.0, 0.5714285714285715, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-7534", "mrqa_squad-validation-512", "mrqa_squad-validation-3670", "mrqa_squad-validation-1546", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-992", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-978", "mrqa_triviaqa-validation-857", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-2189", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-2518", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-4545", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-3783", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-12049", "mrqa_hotpotqa-validation-4862"], "SR": 0.453125, "CSR": 0.6135416666666667, "EFR": 1.0, "Overall": 0.7313020833333332}, {"timecode": 15, "before_eval_results": {"predictions": ["the Treaties establishing the European Union", "Isaac Newton", "draftsman", "mesoglea", "Associate Membership", "his own men", "Baltimore Ravens", "through sponsors", "61.1%", "the Austrian state of Vorarlberg", "Most Western countries, and some others,", "lion, leopard, buffalo, rhinoceros, and elephant", "25-minute", "Shakespeare Quotes", "Terrence Malick", "Ceefax", "Republic of Ireland", "William Wakefield", "ankara kizilay square", "Homo sapiens", "rocaHoly Prophet", "tondere", "otter", "Joseph W(arren)", "mountain Ash", "Phil Hurtt", "turton reservoir", "euston", "Spice Girls", "a fortnight's duration, in which transactions formerly took place and at the end of which settlements were made", "feet", "South Dakota", "17 pink \"double-word\" squares", "Moldova", "AFC Wimbledon", "electric chair", "Northern Ireland", "roger roger", "the Golden Age of Science fiction Films", "the country", "Thomas Jefferson", "Ben Willis", "Ra\u00fal Eduardo Esparza", "The Lykan Hypersport", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the active site", "mall of Asia Complex", "44", "model, actress and television host", "Fred Willard", "Jeff Meldrum", "Canadian", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "Afghanistan,", "the Kurdish area of northern Iraq.", "Saturday", "Jaime Andrade", "his father", "tritonic", "diamond", "Labor Day", "on the south shore of Staten Island", "Pennsylvania", "crude oil-producing", "anil Kapoor"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4837425595238095}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6128", "mrqa_squad-validation-980", "mrqa_squad-validation-9074", "mrqa_squad-validation-2086", "mrqa_squad-validation-8278", "mrqa_triviaqa-validation-3742", "mrqa_triviaqa-validation-1151", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2434", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-533", "mrqa_triviaqa-validation-3956", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-1998", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-1116", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2857", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-3979"], "SR": 0.40625, "CSR": 0.6005859375, "EFR": 0.9473684210526315, "Overall": 0.7181846217105263}, {"timecode": 16, "before_eval_results": {"predictions": ["1963", "sea water", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy.", "around 5 million", "Van de Graaff generator", "bounding the time", "between 1835 and 1842", "Vivienne Westwood", "absolution", "France", "megaprojects", "demand for higher quality housing increased", "1960s to the mid-1970s", "W. Edwards Deming", "Irsay", "1947, 1956, 1975, 2015 and 2017", "travis", "Kevin Kline", "a computer's hard drive", "Saint Alphonsa", "the President of the United States", "Ed Roland", "Kristy Swanson", "a compact layout to combine keys which are usually kept separate", "Missouri River", "in desperation, with only a small chance of success and time running out on the clock", "Lex Luger and Rick Rude", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "Earth", "1923", "in the town of Carcassonne in Aude, France, for the portrayal of Nottingham and its castle", "as early as 1571, with exports to other states occurring around 1858", "Anna Faris", "Sir Hugh Beaver", "23 September 1889", "October 22, 2017", "Tokyo for the 2020 Summer Olympics", "1980", "a sesame-seed bun", "Missouri", "Australia and Ireland", "travis", "Llandudno", "albinism", "round five of the 2017 season", "John Schlesinger", "Clitheroe Football Club", "1998", "Neighbours", "$7.3 billion", "France's famous Louvre museum", "Kenneth Cole", "a plane carrying then-President Juvenal Habyarimana, a Hutu, was shot down near the capital, Kigali.", "10 years", "citizens", "her decades-long portrayal of Alice Horton", "Arabic", "golden-brown algae", "The Treasure of the Sierra Madre", "Mihammad al-Wahhab", "Everybody Loves Somebody", "copper", "The Ansonia Hotel", "Ashridge"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6077717770490603}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 1.0, 0.060606060606060615, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.923076923076923, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4332", "mrqa_squad-validation-1688", "mrqa_squad-validation-2297", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-7635", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-3943", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2757", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-15859", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-14243", "mrqa_hotpotqa-validation-4089"], "SR": 0.53125, "CSR": 0.5965073529411764, "EFR": 0.9, "Overall": 0.7078952205882353}, {"timecode": 17, "before_eval_results": {"predictions": ["Henry Plitt", "tuition", "lysozyme and phospholipase A2", "the issue of laity having a voice and vote in the administration of the church, insisting that clergy should not be the only ones to have any determination in how the church was to be operated", "The Jetsons", "Thomas Edison and Nikola Tesla", "the established Church", "minimal loss over any terrestrial distance", "greenhouse gas", "the \"scariest TV show of all time\"", "The Scottish Parliament", "the American beer revolution", "Arabic", "lunar module", "QP quick picks", "Triple Sec", "the Coney Island Parachute Jump", "the Coral", "Richard III", "cosmetics", "yellow fever", "geometry", "Mexico", "Postscript", "Season 1", "the Bicentennial", "Maria Full of Grace", "The Pro-Jig Clamp Set", "Arby's", "Stevenson", "the pupil", "Albright", "Union Pacific & the Central Pacific", "J.R. Tolkien", "tarantulas", "the seventeenth century", "Mars", "six 50 minute ( one - hour with advertisements ) episodes", "Sean O' Neal", "Muhammad", "Baker, California, USA", "April 1979", "the International Border", "A Horse With No Name", "Missouri", "alligator Pear", "British Overseas Airways Corporation", "Tahrir Square", "Hercules", "14 December 1990", "Gianna", "first freshman to finish as the runner-up", "The relations between Switzerland and the European Union (EU) are framed by a series of bilateral treaties", "gmbH", "1868", "The Da Vinci Code", "Paul McCartney", "buckling under pressure from the ruling party.Mokotedi Mpshe, head of the National prosecuting Authority, disagreed.", "Chesley \"Sully\" Sullenberger", "\"Abbey Road.\"", "tennis", "In Time", "Milira", "Pangaea"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6306177027935693}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1951219512195122, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9878", "mrqa_squad-validation-1496", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-15676", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-2953", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-3951", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-16905", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4902", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-5401", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1811"], "SR": 0.53125, "CSR": 0.5928819444444444, "EFR": 1.0, "Overall": 0.7271701388888889}, {"timecode": 18, "before_eval_results": {"predictions": ["1960", "Saudi", "Smiljan", "Muqali", "2016", "by technique", "2008", "41", "Liverpool and Manchester Railway", "increasing access to education", "Penance", "Les Miserables", "white", "Bangladesh", "San Francisco", "Joy Division", "(Boesman) Fugard", "known as kettledrums", "the Constitution", "a Tibetan antelope", "glaciers", "Grand Central Station", "the Book of Psalms", "sanguine", "Maine", "the Atlantic", "Pilgrim's Progress", "the Grail", "Macbeth", "known as Chocolate Factory", "Mountain Dew", "Engelbert Humperdinck", "Mike McCready", "SEALS", "Smokey Robinson", "Lhasa", "muscle cramps", "statute or the Constitution itself", "known as Max Morden", "2028", "Patrick Walshe", "Massachusetts", "when Wisconsin achieved statehood in 1848", "Marc Brunel", "Czech Republic", "Cork", "Sir Edwin Landseer", "one person", "hot springs", "Academy Award for Best Animated Feature", "An aircraft", "Minnesota", "Lisburn Distillery F.C.", "(Wilton) Mall", "2002\u201303", "9 a.m.", "a civil disturbance call,", "2008", "known for his role as Ralph Cifaretto on", "Lance Cpl. Maria Lauterbach", "near Warsaw, Kentucky,", "Quin Ivy", "Dublin", "(BTMarch 6)"], "metric_results": {"EM": 0.5, "QA-F1": 0.5822916666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666665, 0.0, 1.0, 1.0, 0.2, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1227", "mrqa_squad-validation-3179", "mrqa_searchqa-validation-15758", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-7430", "mrqa_searchqa-validation-1831", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-3814", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-2639", "mrqa_searchqa-validation-710", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9637", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-3955", "mrqa_hotpotqa-validation-218", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-541", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2573", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-6931"], "SR": 0.5, "CSR": 0.5879934210526316, "EFR": 1.0, "Overall": 0.7261924342105264}, {"timecode": 19, "before_eval_results": {"predictions": ["the Ice Ages", "Oligocene", "Gosforth Park", "Fridays", "refuse to sign bail until certain demands are met, such as favorable bail conditions, or the release of all the activists.", "to encourage investment", "*R\u012bnaz", "Beyonc\u00e9 and Bruno Mars", "\"zip\" the mouth shut when the animal is not feeding, by forming intercellular connections with the opposite adhesive strip", "J\u00f3zsef Pulitzer", "\"Spitting Image\"", "left fielder", "Ellesmere Port, United Kingdom", "Punjabi/Pashtun", "KXII", "Madonna Louise Ciccone", "Manor of More", "Europe", "Adelaide", "Woodsy owl", "\"The Snowman\"", "1994\u201395", "Satchmo, Satch or Pops", "Philip Livingston", "hamburgers", "C. J. Cherryh", "Holston River", "1939", "Tabasco", "\"To Be Fat like Me\"", "Iceal E. \"Gene\" Hambleton", "Houston Rockets", "\"Catch Me If You Can\"", "striker", "a vegetarian dish called Buddha's delight", "The Washington Post", "\"Secrets and Lies\"", "Tim Rooney", "endocrine ( hormonal ) systems", "one - point perspective, and their vanishing point corresponds to the oculus, or `` eye point '', from which the image should be viewed for correct perspective geometry", "Dorothy Gale", "master carpenter Anthony Mayfield on behalf of an investor couple in Austin, Texas", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Cape Town", "the French Revolution", "algebraic expression or \"phrase\"", "Rajasthan", "a little bitter", "Diogenes", "peter Docter", "Dr. Jennifer Arnold and husband Bill Klein,", "American", "Gary Player", "\"full civil equality,\"", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "the 46th Vice President of the United States", "Publius Ovidius Naso", "Martin Van Buren", "pnini", "The Pentagon", "a Beanie Baby", "Wigan Athletic", "Iowa", "social networking sites"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5036547173096158}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.6206896551724138, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.19354838709677416, 0.16, 0.0, 0.5882352941176471, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5456", "mrqa_squad-validation-6931", "mrqa_squad-validation-114", "mrqa_squad-validation-4497", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5075", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2898", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-15956", "mrqa_searchqa-validation-15311", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1182"], "SR": 0.390625, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.72421875}, {"timecode": 20, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3302", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9175", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-560", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-13604", "mrqa_searchqa-validation-13784", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-15676", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-16905", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2639", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3131", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-3894", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-9998", "mrqa_squad-validation-10004", "mrqa_squad-validation-10013", "mrqa_squad-validation-10024", "mrqa_squad-validation-1006", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10199", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1462", "mrqa_squad-validation-1496", "mrqa_squad-validation-1512", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1546", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1637", "mrqa_squad-validation-1684", "mrqa_squad-validation-1762", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-199", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2705", "mrqa_squad-validation-2723", "mrqa_squad-validation-276", "mrqa_squad-validation-2834", "mrqa_squad-validation-2869", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-302", "mrqa_squad-validation-3049", "mrqa_squad-validation-3063", "mrqa_squad-validation-3092", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3398", "mrqa_squad-validation-3416", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3540", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3610", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-3871", "mrqa_squad-validation-3925", "mrqa_squad-validation-3950", "mrqa_squad-validation-3986", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4127", "mrqa_squad-validation-4179", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-4246", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4497", "mrqa_squad-validation-4506", "mrqa_squad-validation-4533", "mrqa_squad-validation-4649", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-4707", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5193", "mrqa_squad-validation-5230", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5562", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5866", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-641", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6891", "mrqa_squad-validation-6942", "mrqa_squad-validation-6996", "mrqa_squad-validation-7096", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-715", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7380", "mrqa_squad-validation-7534", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7701", "mrqa_squad-validation-7708", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7792", "mrqa_squad-validation-7850", "mrqa_squad-validation-7956", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-8231", "mrqa_squad-validation-8287", "mrqa_squad-validation-8362", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8732", "mrqa_squad-validation-878", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9331", "mrqa_squad-validation-9578", "mrqa_squad-validation-96", "mrqa_squad-validation-9606", "mrqa_squad-validation-9608", "mrqa_squad-validation-9632", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-980", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-988", "mrqa_squad-validation-9966", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1201", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1719", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2377", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2731", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4469", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4586", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-4801", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5525", "mrqa_triviaqa-validation-5538", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6661", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7132", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-804"], "OKR": 0.904296875, "KG": 0.47578125, "before_eval_results": {"predictions": ["America's Funniest Home Videos", "the Romantic Rhine", "philanthropy", "1986", "\"Republic of Kenya\"", "PNU and ODM camps", "pseudorandom number generators", "\u20ac5,000", "a baffle", "impact forces", "Spain", "Havana", "Harriet Beecher Stowe", "The MIT Blackjack Team", "the Silence of the Lambs", "Fitzgerald", "the Bladder", "Pulsed Laser", "Richard E. Byrd", "Resident Evil", "Lake Mead", "James Earl Ray", "Lyndon B. Johnson", "the Madding Crowd", "the trumpet", "the Boer War", "Anne Frances Reagan", "Agatha Christie", "Cleveland", "David Hare", "Dust", "calamity", "Lake Alakol", "Starland Vocal Band", "kerosene", "a squash blossom", "U.S. service members who have died without their remains being identified", "Carol Ann Susi", "Peru", "Las Vegas", "Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "Dirk Benedict", "Akshay Kumar", "high-elevation lakes", "Japan", "James", "Nigeria", "The Star Spangled Banner", "The Truman Show", "George Gently", "Leslie Knope", "1978", "University of Missouri", "Nicolas Winding Refn", "Osaka International Airport", "$26 billion", "2013\u201314 Premier League", "Pixar's", "Inter Milan", "the fact that the teens were charged as adults", "If  your ex's loved ones ask why you broke up", "two Israeli soldiers,", "2011", "April 21, 1947"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6687127976190477}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7499999999999999, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9161", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-10745", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-4655", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-11905", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5780", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-7625", "mrqa_triviaqa-validation-7392", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-1034"], "SR": 0.578125, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.742421875}, {"timecode": 21, "before_eval_results": {"predictions": ["punts", "suspended sentences", "Confucian propriety and ancestor veneration", "5,000 years", "Barnett Center", "immunomodulators", "the RSA algorithm", "the Black Ahab", "Francis Scott Key", "San Francisco earthquake", "piccolo", "Alice", "South India", "Rolling Stone", "Marlon Brando", "Fred Foy", "the nasolacrimal sac", "The Old Curiosity Shop", "pearl", "Australia", "defence of defence", "Europa", "Pope John XXIII", "the sun", "the Mercury Seven", "chocolate", "defence", "John Edwards", "Fatah", "the defence of the turkey", "Congo", "the Literaturnaya", "Bombay", "Rome", "lymphatic", "Bed and breakfast", "1038", "19 June 2018", "Mahatma Gandhi", "Virginia Dare", "Lionel Hardcastle", "iOS, watchOS, and tvOS", "The Shard", "The Blue Boy", "Allende", "Sue Ryder", "(Adrian) Edmondson", "sense of taste", "Tony Curtis", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\" and film \"King Jack\", respectively.", "Centre of Excellence", "The Prodigy", "Melesha O'Garro", "New Journalism", "Boyd Gaming", "Douglas Jackson", "2000", "103", "April 13", "Juan Martin Del Potro.", "Opryland", "Anil Kapoor", "(Somalis) prepare Monday to bury murdered Osman Ali Ahmed, the head of the U.N. Development Program.", "1964"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6487847222222223}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6703", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-14559", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-14337", "mrqa_searchqa-validation-14613", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-16301", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7058", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-639", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-3501"], "SR": 0.578125, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.742421875}, {"timecode": 22, "before_eval_results": {"predictions": ["Mike Carey", "Rankine cycle", "May 21, 2013", "iger team", "11:28", "1321", "Dean Windass", "the Tony Award for Best Lighting Design for \"Evita\", \"Cats\", and \"Les Mis\u00e9rables\",", "Rafael Palmeiro", "10", "Central Park", "the Maasai phrase \"Enkare Nairobi\",", "YouTube celebrity PewDiePie", "Thrushcross Grange", "Linda McCartney's Life in Photography", "10 June 1921", "A Bug's Life", "Michelle Anne Sinclair", "Angel Parrish", "\"OSMAG\"", "Bundesliga", "\"Apprendi v. New Jersey\" (2000)", "cleaning services, support services, property services, catering services, security services and facility management", "4,972", "Captain", "Mount Everest", "119", "British", "Red Dead Redemption", "Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "a neo-v\u00f6lkisch movement within black metal", "Sugar Ray Robinson", "the rough patches of skin on its head which appear white due to parasitism by whale lice", "Jeffrey Adam \"Duff\" Goldman", "A Rush of Blood to the Head", "a political ideology is a certain ethical set of ideals, principles, doctrines, myths, or symbols of a social movement, institution, class, or large group that explains how society should work", "Mendel", "John Adams, a leader in pushing for independence, had persuaded the committee to select Thomas Jefferson to compose the original draft of the document, which Congress edited to produce the final version", "Toto", "Pandit Jawaharlal Nehru", "2009", "James Arthur", "fourteen", "A Novel without a Hero", "Route sixty-six", "The Welcome Stranger", "Pour Moi", "aardvark", "Jeremy Bates", "Thessaloniki", "Sixteen", "Mexico", "Dr. Cade", "her husband's infidelity", "the triangular bone within the pelvis.", "Brown and her family", "Rocco", "the assassination of Martin Luther", "Armistice Day", "the double-headed eagle", "Roger Federer", "Hannibal", "Venezuela", "Marie Fredriksson"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6037594117350509}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.26666666666666666, 0.8, 0.33333333333333337, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.47058823529411764, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.10526315789473685, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.13793103448275862, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-136", "mrqa_squad-validation-3926", "mrqa_squad-validation-8142", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-1386", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-1332", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7692", "mrqa_naturalquestions-validation-4033", "mrqa_triviaqa-validation-636", "mrqa_triviaqa-validation-3951", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-9888", "mrqa_searchqa-validation-7840", "mrqa_searchqa-validation-6337", "mrqa_searchqa-validation-45", "mrqa_naturalquestions-validation-9195"], "SR": 0.46875, "CSR": 0.5733695652173914, "EFR": 1.0, "Overall": 0.7414707880434783}, {"timecode": 23, "before_eval_results": {"predictions": ["trans-Atlantic wireless telecommunications facility known as Wardenclyffe", "\"a day of rote learning and often wearying spiritual exercises.\"", "Pitt", "the meeting of the Church's General Assembly", "vocational subjects", "CBS", "Revenge and Retribution", "smell", "Tokyo Prefecture", "colorless", "a wish", "alistair Ferguson Ritchie", "vanity", "stars", "high jump", "milk", "Derbyshire", "James Cameron", "Emma Chambers", "Ebenezer Scrooge", "Spain", "secretary", "the Guardian", "North by Northwest", "oxygen", "Alberto Juantorena", "William Randolph Hearst", "Henry III", "Blind Faith", "Korea", "Space Jam 2", "Paul Maskey", "ambergris", "The Time Machine", "The Lion King", "5 September 1666", "pneumonoultramicroscopicsilicovolcanoconiosis", "William Strauss and Neil Howe", "florida", "Thorleif Haug", "Vermont boarding school Welton Academy", "1978", "July 11, 2016", "Ghana Technology University College", "Charles Quinton Murphy", "12", "Afghanistan", "Washington, D.C.", "Michael Jordan", "Arnoldo Rueda Medina", "\"A waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.\"", "the motherless cub defended by Elphaba", "a bank", "Eintracht Frankfurt", "al Gamaa al-Islamiyya,", "The New Promised Land: Silicon Valley", "Bix", "Black Hole", "Canada", "Olivia Newton-John", "a wish", "Rene Lacoste", "a grill", "the Vampire Intelligences"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5151181463412067}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false], "QA-F1": [0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.6666666666666666, 0.0, 1.0, 0.4, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.06896551724137931, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1384", "mrqa_squad-validation-545", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6688", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10442", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-914", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-30", "mrqa_hotpotqa-validation-1516"], "SR": 0.4375, "CSR": 0.5677083333333333, "EFR": 0.9444444444444444, "Overall": 0.7292274305555555}, {"timecode": 24, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "40,000", "Geordie", "plantar fasciitis in his left foot", "Tesla items in unfriendly hands", "(Arma virumque cano)", "Queen Elizabeth I", "Tokyo", "February 14, 1929", "gungnir", "Samuel Johnson", "8 Standard Champagne Bottles", "West Point", "28 letters", "Caernarfon station", "curling", "adorrah", "boxing", "in 1994", "Al Bundy", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands, thoracic aorta and the pulmonary artery", "\"Ain't No Mountain High Enough\"", "ambulance driver", "Charles Dickens", "New Hampshire", "all Blacks", "u2", "lungs", "Christian Wulff", "mice", "acai berry", "The Jacaranda", "Jesse Garon Presley", "mortadella", "epernay", "its population", "gentry Buddhism", "the Mongol Yuan Dynasty", "65,535 bytes", "A turlough", "Health is usually measured in hit points or health points, shortened to HP", "George Warren Barnes", "The Grandmaster", "Nan Britton", "Nick Cannon", "Marvel Comics", "Taylor Swift", "Ghana, Nigeria, Sierra Leone and on S\u00e3o Tom\u00e9", "September 8, 2017", "CNN's Max Foster", "seven", "the Russian air force", "At least 38 people", "about 3,000 kilometers", "Paul McCartney and Ringo Starr", "lightning strikes", "Clifford Odets", "degaussing", "Fargo", "Cain's", "a branch of botany studies fossil plants", "Washington Irving", "a grizzly bear", "Carrie Underwood"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5728174603174603}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.5555555555555556, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-629", "mrqa_squad-validation-1598", "mrqa_triviaqa-validation-6281", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-3363", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-3001", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-183", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-2205", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-4092", "mrqa_newsqa-validation-3355", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-7", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-9185"], "SR": 0.484375, "CSR": 0.5643750000000001, "EFR": 1.0, "Overall": 0.739671875}, {"timecode": 25, "before_eval_results": {"predictions": ["Gaelic", "to destroy the antichrist", "Satyagraha", "Japanese", "it is neither zero nor a unit", "jesse pinto da Cunha", "7", "Azzurro Savoia", "rounders", "niu tireni", "Daily Mirror", "Fort Sumter", "mountain torrents", "otello", "Jordan", "A New Generation", "downton Abbey", "jiu tireni", "a cheese wheel", "a lie detector", "daedalus", "Una Stubbs", "muscle", "a sea otter", "Fringillidae", "vossische Zeitung", "feb", "paul niu tireni", "paul c\u00e9zanne", "puffer", "A-ha", "a high-speed car crash,", "niu tireni", "All Stars", "jesse", "Noel Kahn", "Mitch Murray", "Rajendra Prasad", "a syllogism", "Alamodome and city of San Antonio", "Chris Rea", "retinal ganglion cell axons and glial cells", "Restoration Hardware", "final of 2011 AFC Asian Cup", "James Harrison", "first train robbery", "northwest tip of Canisteo Peninsula in Amundsen Sea", "Bardot", "E22", "Booches Billiard Hall,", "1998.", "Umar Farouk AbdulMutallab", "Lebanese", "two remaining crew members", "a muddy barley field", "Tehran,", "Taiwan", "Wallachia", "Flamin' Hot", "vodka", "point guard", "jalaneno peppers", "chornobyl", "cherubim"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5333085317460318}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.2857142857142857, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2564", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-4411", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-3316", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-3008", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3097", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-4640", "mrqa_searchqa-validation-7780"], "SR": 0.484375, "CSR": 0.5612980769230769, "EFR": 0.9696969696969697, "Overall": 0.7329958843240092}, {"timecode": 26, "before_eval_results": {"predictions": ["defensins", "that the Earth must be much older than had previously been supposed", "5 to 15 years", "the pre-game and halftime coverage", "sternum", "Finland", "carbon", "albert", "a cat", "a Nor'easter", "Rajasthan", "kerry Katona", "Exile", "Argentina", "a docked yacht", "kia", "power station", "the troposphere", "the Battle of Goose Green", "Ellen Morgan", "Charlie Drake", "Spain", "Richard Noble", "cut Above The Rest", "tintoretto", "Venus", "120 beats per minute", "Hans Lippershey", "Gryffindor", "richmond adams", "Wilkie Collins", "yunte Huang", "avonlea", "Baton Rouge", "Laura Jane Haddock", "Helena", "The Inn at Newport Ranch", "Woodrow Wilson", "querry dangal", "Rumplestiltskin", "Max Martin", "Sean Yseult", "Mount Everest", "18.7 miles", "actor and filmmaker", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "Tony Stewart", "KB Toys Inc.", "at least $20 million to $30 million,", "around 3.5 percent of global greenhouse emissions.", "40 militants and six Pakistan soldiers", "Gen. Stanley McChrystal,", "Cole", "$627,", "WFTV.", "quoit", "The Man Without A Country", "deimos", "Benazir Bhutto", "Dracula", "Ipanema", "a sitar", "the band's logo in gold lettering over black sleeve", "J. Presper Eckert and John William Mauchly's ENIAC"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6443859551396316}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.38095238095238093, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.823529411764706, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7148", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-5619", "mrqa_triviaqa-validation-2360", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6043", "mrqa_triviaqa-validation-4663", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-2839", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-1852", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-2896", "mrqa_searchqa-validation-3888", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3750"], "SR": 0.546875, "CSR": 0.5607638888888888, "EFR": 0.9655172413793104, "Overall": 0.7320531010536399}, {"timecode": 27, "before_eval_results": {"predictions": ["three hours", "Cosgrove Hall", "American Civil Rights Movement", "a means of strengthening the role of backbenchers in their scrutiny of the government and partly to compensate for the fact that there is no revising chamber", "Armin Meiwes ( ; born 1 December 1961) is a German computer repair technician who achieved international notoriety for killing and eating a voluntary victim whom he had found via the Internet.", "Katherine Harris", "The Longest Yard", "WAMC", "Ustad Vilayat Khan", "Croatian", "Food and Agriculture Organization", "Kolkata", "2010 to 2012", "Dorothy", "the lack of any perceptible change in an adult female (for instance, a change in appearance or scent) when she is \"in heat\" and near ovulation", "near North Chicago, in Lake County, Illinois", "Preston, Lancashire, UK", "Mark Dayton", "Black Panther Party", "Chiba, Japan", "S\u00f8nderjyskE Ishockey of the Danish Metal Ligaen", "The Sun", "University of Missouri-Kansas City in Kansas City, Missouri", "October 3, 2017", "Walt Disney and Ub Iwerks", "River Clyde", "Chicago", "Moonstruck", "Fulgencio Batista", "Seventeen", "Kim Sung-su", "ten years of probation and subsequently ordered him to therapy", "The New Yorker, The Atlantic, National Geographic\", and \" Outside\"", "Paul Teutul Jr.", "bypasses, to cross major bridges, and to provide direct intercity connections", "Governor Al Smith", "Charles Carson", "Chris Sarandon", "Beijing", "fourth season", "Schadenfreude", "Poland", "John Mortimer", "nor\u00f0rvegr", "Japanese silvergrass", "the Daily Mail", "richmond", "red", "the Tutsi ethnic minority and the Hutu majority had been at odds even before 1994.", "Saturday,", "the oil painting titled \"The Book\" is believed to be the only portrait for which Jackson sat.", "Iowa's critical presidential caucuses on January 3.", "partying", "12.3 million people worldwide", "they are co-chair of the Genocide Prevention Task Force.", "Doom 3", "Benjamin Franklin", "a global village", "Forrest Gump", "earl derr", "WD-40 L lubricant", "Vermont", "hunting", "lemon"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5104942552060009}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.08, 0.13793103448275862, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08, 0.6, 0.5, 1.0, 0.4, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.13333333333333333, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5285", "mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-5227", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8404", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-225", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2536", "mrqa_newsqa-validation-4019", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2723", "mrqa_searchqa-validation-10221", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-6504", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-13505", "mrqa_triviaqa-validation-647"], "SR": 0.390625, "CSR": 0.5546875, "EFR": 0.9743589743589743, "Overall": 0.7326061698717948}, {"timecode": 28, "before_eval_results": {"predictions": ["Lucas Horenbout", "Barnett Center", "UNESCO's World Heritage list", "punk rock", "Texas", "Argentine cuisine", "Valley Falls", "1942", "Estelle Sylvia Pankhurst", "Rhode Island School of Design", "Battleship", "Arthur Miller", "1964", "Abigail", "Fortean", "Dame Eileen June Atkins", "Toshi Ichiyanagi", "Africa", "color vs. color", "Catwoman", "Nassau County", "Andrew Lloyd Webber", "Mr. Church", "Edward Robert Martin Jr.", "23", "Innviertel", "World War II", "Minette Walters", "Chuck", "Princeton University", "Three's Company", "Rungrado 1st", "Linux Format", "1881", "August 17, 1945", "Guy Carawan", "presidential representative democratic republic", "Sir Henry Cole", "Mainland Greece", "Mel Gibson", "Obama", "Switzerland", "edg Delius", "iron", "33", "the solar system", "The Word", "James Richter knows the rabbit-ear antennas on his old-fashioned television will listen for a signal and hear nothing.", "Felipe Calderon", "Prince George's County Chief Executive Jack Johnson", "Manmohan Singh's", "The woman", "Fernando Caceres", "Veracruz, Mexico", "Gettysburg", "Vermont", "sheep", "Botanya", "Abbey Theatre", "Nile", "out-of-body experiences", "the United States, NATO member states, Russia and India", "Russian air company Vertikal-T", "Judge Ricardo Urbina"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6832589285714286}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.3, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.8571428571428571, 1.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-4874", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4642", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-1159", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-2797", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-1484", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1563"], "SR": 0.578125, "CSR": 0.5554956896551724, "EFR": 1.0, "Overall": 0.7378960129310345}, {"timecode": 29, "before_eval_results": {"predictions": ["over the winter of 1973\u201374", "tablets", "three hundred sixty schools", "Franois Truffaut", "Final Cut Pro", "alcohol", "chiaroscuro", "The Titan", "a broody hen", "Dairy Queen", "Roger Bacon", "Gene Autry", "voice pitch", "polypropylene", "Sydney", "offbeat", "Alexander Graham Bell", "the Gulf War", "Colorado River", "Sing Sing", "the South Beach diet", "hair loss", "the Intihuatana pyramid", "Phnom Penh", "fairbanks", "Russell Simmons", "The Enterprise", "a great god", "Kevin Costner", "New York", "a mansion", "Gunsmoke", "Holiday Inn", "Spacewar", "innermost in the eye", "a normally inaccessible mini-game in the 2004 video game Grand Theft Auto : San Andreas, developed by Rockstar North", "Massachusetts", "flawed democracy", "24 judges, against a maximum possible strength of 31", "The Lightning thief", "mathematics", "Make Your Lives Extraordinary", "Diana Ross", "9", "Taiwan", "Sinclair Lewis", "How wonderful life is", "Fort Bragg", "September 23, 1935", "$10.5 million (USD 8 million)", "\"Dinotopia\"", "Robert Jenrick", "Alien Resurrection", "the arrival of the first Spanish conquistadors in the region of North America now known as Texas in 1519", "Apple announcing plans that could move iTunes into the cloud.", "543", "Deutschneudorf", "The cause of the blast was unknown,", "seven", "Jeffrey Jamaleldine", "30,000", "four", "the titular `` fool '', a solitary figure who is not understood by others, but is actually wise", "November 1961"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6223595848595849}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.8571428571428571, 0.5, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-10085", "mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-1424", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-4657", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-9950", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-14336", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-11919", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-3332", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-7195", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-4754", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2766", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3119"], "SR": 0.515625, "CSR": 0.5541666666666667, "EFR": 0.9354838709677419, "Overall": 0.7247269825268817}, {"timecode": 30, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2691", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-281", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-443", "mrqa_hotpotqa-validation-4465", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-468", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4874", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5760", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9997", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2093", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2536", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-926", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10275", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-11919", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-1584", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16301", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-1958", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3376", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-472", "mrqa_searchqa-validation-4755", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6489", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10395", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1496", "mrqa_squad-validation-1541", "mrqa_squad-validation-1598", "mrqa_squad-validation-1637", "mrqa_squad-validation-1833", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2108", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-2376", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3398", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-360", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3800", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-387", "mrqa_squad-validation-3871", "mrqa_squad-validation-3926", "mrqa_squad-validation-3957", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4186", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-424", "mrqa_squad-validation-4332", "mrqa_squad-validation-4360", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4506", "mrqa_squad-validation-4547", "mrqa_squad-validation-4649", "mrqa_squad-validation-4677", "mrqa_squad-validation-4775", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-5054", "mrqa_squad-validation-510", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5334", "mrqa_squad-validation-5348", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6118", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-629", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-6638", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6996", "mrqa_squad-validation-703", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7850", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-824", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8534", "mrqa_squad-validation-8687", "mrqa_squad-validation-8732", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-90", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9413", "mrqa_squad-validation-9451", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-9802", "mrqa_squad-validation-9849", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-2387", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3001", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-362", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4192", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4910", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5017", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-636", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-6688", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7087", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-746", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-804", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-922"], "OKR": 0.865234375, "KG": 0.49140625, "before_eval_results": {"predictions": ["increasing access to education", "Arab", "Chester, South Carolina", "Veracruz Regatta race,", "safety issues in the company's cars", "High Court Judge Justice Davis", "the Southern Baptist Convention,", "The Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso,", "allegedly involved in forged credit cards and identity theft", "The U.S.-Mexico border", "a music video on his land.", "Mexico City,", "head for Italy.", "file papers shortly with an appeals court seeking an emergency stay", "Long troop deployments,", "$89", "the estate with its 18th-century sights, sounds, and scents.", "27-year-old", "Amanda Knox's aunt", "\"falling space debris,\"", "Chinese tourists", "Cameron-Ritchie,", "officers at a Texas  airport", "the Gulf", "Columbia, Missouri.", "a collapsed apartment building in Cologne, Germany,", "skull.", "At least 40", "Democratic VP candidate", "1831", "Daniel Radcliffe", "Conway", "Nick Adenhart", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "the nucleus", "the date on which the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "the cavities and surfaces of blood vessels and organs throughout the body", "sea water", "a nearly - identical `` non- driver identification card '' to identify persons who are unable or don't want to drive", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 ) is an American actress, best known for her work with American daytime soap operas", "Dilbert", "Johnny Mathis", "chui Chow", "George III", "CHICAGO", "algebra", "Rosslyn Chapel", "Acid house", "Oahu", "Thrushcross Grange", "Rigoletto", "Mudvayne", "The Hertz Corporation", "Elliot Fletcher", "The Lottery", "Berkeley", "Peter Kropotkin", "the Wessex", "Wyoming", "Mall of America", "William", "Michael Cimino", "Bangkok", "Jimmy Ellis"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5686549714582803}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.35294117647058826, 0.22727272727272727, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5882352941176471, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1440", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3301", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-2395", "mrqa_triviaqa-validation-285", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-4972", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-11250", "mrqa_hotpotqa-validation-5460"], "SR": 0.484375, "CSR": 0.5519153225806452, "EFR": 0.9696969696969697, "Overall": 0.720963083455523}, {"timecode": 31, "before_eval_results": {"predictions": ["1969", "Highly combustible materials", "the Intertropical Convergence Zone", "2002", "Joanne Wheatley", "Clarence Williams", "invertebrates", "off the southernmost tip of the South American mainland", "Detective Superintendent Dave Kelly", "warplanes", "The Drew Las Vegas", "1957", "Virginia", "Bilaterally symmetrical", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1648 - 51 war", "the breast or lower chest of beef or veal", "1933", "Jason Lee", "southern Anatolia", "halogenated paraffin hydrocarbons", "The Flash", "Dr. Lexie Grey", "Number 4, Privet Drive, Little Whinging in Surrey, England", "1939", "Alex Skuby", "the way they used `` rule '' and `` method '' to go about their religious affairs", "15,000 BC", "Bacon", "November 27, 2013", "In the television series's fourth season", "one of the most common words in scripture", "Action Jackson", "the giraffe", "1883", "DyfedPowys", "Elizabeth I", "Indonesia", "the crow", "Oskar Schindler", "2013 Cannes Film Festival", "Conservative Party", "1991", "Hjernevask", "corn", "4,000", "1966", "Knox's aunt Janet Huff", "smile", "any records showing that Barlow and the girl were married and any evidence of them having a child.", "bipartisan", "early detection and helping other women cope with the disease.\"", "rising disposable income and an increasing interest in leisure pursuits,", "16", "Stephen Dedalus", "Omaha", "The Communist Party", "ice hockey", "Hillary Clinton", "Cock Robin", "Halloween", "the first trans-Pacific flight", "Michael Fassbender", "Phil Collins"], "metric_results": {"EM": 0.375, "QA-F1": 0.5300481088324338}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.923076923076923, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.0, 0.9, 0.8, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.7272727272727272, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.17391304347826086, 1.0, 1.0, 0.10526315789473685, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3491", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-6374", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5473", "mrqa_hotpotqa-validation-5", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-1451", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-6195", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-6554", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-3464"], "SR": 0.375, "CSR": 0.54638671875, "EFR": 0.95, "Overall": 0.71591796875}, {"timecode": 32, "before_eval_results": {"predictions": ["the time complexity", "pores in the epidermis", "Louis XIII's successor, Louis XIV", "Ajay Tyagi", "The North Pole is the northernmost point on the Earth, lying diametrically opposite the South Pole", "28 %", "West Norse sailors", "vasoepididymostomy", "plant", "Andrew Lloyd Webber", "New England Patriots", "Pedro Espada, Jr.", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Ra\u00fal Eduardo Esparza", "1916", "Milira", "today", "Ledger", "New England Patriots XX, XXXI, XXXVI, XXXVIII, XXXIX, XXXII, XLVI, XLIX, XLII", "USS Chesapeake", "Christopher Lloyd", "the Afghan - Pakistan border", "Pakistan", "a combination of genetics and the male hormone dihydrotestosterone", "Nazi Germany and Fascist Italy", "the remaining surface of the enamel", "Rocinante", "the words spoken to Adam and Eve after their sin", "Brian Lara", "Brad Dourif", "Robert Irsay", "a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "Disney", "a small distinguishing mark", "(Fred) West", "high cooking", "Otto von Bismarck", "hair", "A. Cartier", "amon ritchie", "1995 teen drama \"Kids\"", "Alyson Stoner", "Free Range Films", "Gambaga", "a pioneer in watch design, manufacturing and distribution", "in their home country", "Hermione Baddeley", "can I", "his comments", "can lead to blocked airways, cardiovascular collapse, and even death.", "Sunday", "bartering -- trading goods and services without exchanging money", "the area was sealed off,", "Bob Bogle", "(Count von) Zeppelin", "the Battle of Bosworth Field", "Tsingtao", "incognito", "a Vampire bat", "Galileo", "Life Among the Lowly", "Charlotte Gainsbourg and Willem Dafoe", "2,000 euros ($2,963)", "Seasons of My Heart"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5967638698107449}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 0.125, 0.0, 0.0, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.5714285714285715, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1904761904761905, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-7074", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-3593", "mrqa_triviaqa-validation-2415", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-1155", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-4492", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-16579"], "SR": 0.515625, "CSR": 0.5454545454545454, "EFR": 1.0, "Overall": 0.7257315340909091}, {"timecode": 33, "before_eval_results": {"predictions": ["Paul Nash", "ten", "Charles Perrault", "2", "relaxation if one becomes too anxious", "ten", "Bachendri Pal", "7 July", "usually in May", "Arthur Chung", "`` the bush ''", "Andy Serkis", "France", "Sean O'Neal", "Himadri", "257,083", "to ensure party discipline in a legislature", "Empiricism", "beneath the liver", "George II", "1920s", "under normal conditions", "Karan Patel", "May 26, 2017", "in formal education during the Roman Empire", "Mason Alan Dinehart", "federal law intended to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "Buffalo Lookout", "The Lykan Hypersport is a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "Geoffrey Dyson Palmer", "ice giants", "March 26, 1973", "1991", "Pete Seeger", "Sweden", "1915", "bankside Penitentiary", "onion", "trombone", "12", "Theodore Roosevelt Mason", "Route 37 East", "Robert John Day", "Do Kyung-soo", "Mexico City", "Travis County", "1872", "polo", "Arsene Wenger", "Derek Mears", "Bob Bogle", "Miguel Cotto", "recite her poetry.", "Pakistani officials,", "the Nile", "Pushing Daisies", "Uncle Diana", "light", "Isaac Newton", "Nefertiti", "milk", "three years", "This is not a project for commercial gain.", "Zhanar Tokhtabayeba,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6412950792487083}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.7999999999999999, 1.0, 0.12903225806451613, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5520", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-5909", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-6468", "mrqa_naturalquestions-validation-1127", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-7403", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-3294", "mrqa_hotpotqa-validation-4528", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3076", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-15769", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-3415"], "SR": 0.53125, "CSR": 0.5450367647058824, "EFR": 0.9, "Overall": 0.7056479779411765}, {"timecode": 34, "before_eval_results": {"predictions": ["Warfare and the long occupation", "high", "1986", "The First Battle of Bull Run", "Walter Mondale", "Vienna", "humid subtropical climate", "A footling breech", "Gaelic \u00d3 Brad\u00e1in'descendant of Brad\u00e1n '", "John McConnell", "the times sign or the dimension sign", "the liver and kidneys", "If These Dolls Could Talk", "1992", "on the lateral side of the tibia, with which it is connected above and below", "the university's science club", "Bob Dylan", "Malware", "Konakuppakatil Gopinathan Balakrishnan", "May 19, 2008", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "Jack Barry", "Catherine Tramell", "2026", "more than a million members", "April 13, 2018", "subduction zone", "Jacques Cousteau", "1960", "`` Fix You ''", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Imola", "Wigan", "a person trained to pilot, navigate, or otherwise participate as a crew member of a spacecraft", "Dante Alighieri", "(W.E Forster) hockney", "(Li'l Abner)", "Tokyo", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "Irish Chekhov", "Cody Miller", "Selina D'Arcy", "ten", "the Tallahassee City Commission", "Roosevelt Island", "air support.", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Sabina Guzzanti", "two Israeli soldiers,", "large accumulations of ice in places such as the north Georgia mountains, causing hazardous driving conditions.", "The American Civil Liberties Union", "Zimbabwe", "Captain James Hook", "baldness", "Ani", "Archer Daniels Midland", "the potato", "The History of the World", "the Leyden jar", "14,", "Aniston, Demi Moore and Alicia Keys", "a grocery store"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6611694293945649}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 1.0, 0.4444444444444445, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.5, 0.4210526315789474, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-9387", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-35", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-3633", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1018", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-9057", "mrqa_newsqa-validation-4177"], "SR": 0.578125, "CSR": 0.5459821428571429, "EFR": 0.9259259259259259, "Overall": 0.7110222387566137}, {"timecode": 35, "before_eval_results": {"predictions": ["algorithms have been written that solve the problem in reasonable times in most cases", "1964", "over the last day", "the Internet", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "more than 4,000", "military transport", "speeding and passing in no-pass zones west of Grand Ronde, Oregon.", "John Kiriakou.", "the shoreline of the city of Quebradillas", "at the two-hour finale.", "J.G. Ballard, whose boyhood experience in a World War II internment camp became the novel and film \"Empire of the Sun,\"", "content", "gun charges", "UNICEF", "dismissed all charges", "a sheriff's deputy who killed six young people at a house party in Crandon, Wisconsin,", "Kris Allen,", "Janet and La Toya", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "a secret U.S. program to assassinate terrorists in Iraq.", "too many glass shards left by beer drinkers in the city center,", "has to move out of her rental house because it is facing foreclosure", "one", "NATO fighters", "750", "one", "black, red or white,", "Phay Siphan, secretary of the Cambodian Council of Ministers.", "last week", "more than 9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "two and a half hours.", "a British teenage girl in Goa last month to protect the tourist industry,", "Claims adjuster", "his temporary departure from the Beatles in January 1969, during the troubled Get Back sessions that resulted in their Let It Be album and film", "Tokyo", "Madison's", "Judith Cynthia Aline Keppel", "a Danish - Norwegian patronymic surname meaning `` son of Anders ''", "The Convention's first act, on 10 August 1792, was to establish the French First Republic and officially strip the king of all political powers", "Fu\u00dfball", "Elvis Presley", "collapsible support assembly", "Indiana Jones", "Nadia Comaneci", "Florida", "Abigail van Buren", "1912", "World War I", "Rachel Meghan Markle", "Acid house", "Pease Air National Guard Base", "South African", "1754", "a lemon chiffon", "Artemis", "a hand", "a canticle", "The Sun Also Rises", "a hand", "Tequila Sunrise", "Hawley Harvey Crippen", "Vladivostok", "Clive Cussler"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5076575066579663}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.35294117647058826, 0.8, 0.16666666666666669, 1.0, 0.18750000000000003, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-2980", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-10311", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-4688", "mrqa_triviaqa-validation-68", "mrqa_triviaqa-validation-7594", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-1034", "mrqa_searchqa-validation-4930", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-9366", "mrqa_triviaqa-validation-138"], "SR": 0.4375, "CSR": 0.54296875, "EFR": 0.9444444444444444, "Overall": 0.7141232638888889}, {"timecode": 36, "before_eval_results": {"predictions": ["harassment and, at least to the bystander, somewhat inane", "2014", "Sleeping with the Past", "the church known for having laboured hard and not fainted, and separating themselves from the wicked", "Matt Monro", "19 June 2018", "2020 National Football League ( NFL ) season", "a Norwegian town circa 1879", "in the 18th century", "before the first year begins", "Vicente Fox", "a carpenter had been contracted to provide new choir stalls for St Mary's Church, Nantwich", "Deputy Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "Supplemental oxygen", "1961", "11 %", "his cousin D\u00e1in", "three", "Earle Hyman", "alpha efferent neurons", "becomes saturated", "chain elongation", "1", "the referee", "Fred Ott", "In the television series's fourth season", "adenine ( A ), uracil ( U ), guanine ( G ), cytosine ( T )", "an elected member of the Senate, able to speak or vote on any issue", "September 4, 2000", "somatic cell nuclear transfer", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Eddie Murphy", "Payson, Lauren, and Kaylie", "Bonita Melody Lysette", "his wife, Nicole Brown Simpson", "a pieman", "Singapore", "Malta", "Thumping", "potash", "Rob Reiner", "(George) De La Beckwith, Sr.", "northern Italy's Lombardy region", "Yasiin Bey", "FIFA Women's World Cup", "University of Texas Longhorns football", "Carol Ann Duffy", "May 2000,", "a impromptu memorial for the late singer at the \"Stone Circle,\"", "how health care can affect families.\"", "on the family's blog", "Port-au-Prince, Haiti", "at least 63", "1959.", "Luxor Las Vegas", "Tennessee", "Massachusetts", "himself", "the Gulf of Tonkin", "Amore", "Saint Telemachus", "Fareed Zakaria", "three", "engaging with the Taliban in Pakistan and Afghanistan."], "metric_results": {"EM": 0.390625, "QA-F1": 0.5200184547221393}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 0.4444444444444445, 0.125, 1.0, 1.0, 0.3137254901960785, 1.0, 0.4, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.4444444444444444, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.17391304347826086]}}, "before_error_ids": ["mrqa_squad-validation-7014", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3187", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-7394", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-2390", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-1613", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-1056", "mrqa_searchqa-validation-9778", "mrqa_searchqa-validation-10986", "mrqa_newsqa-validation-4175"], "SR": 0.390625, "CSR": 0.5388513513513513, "EFR": 0.8974358974358975, "Overall": 0.7038980747574497}, {"timecode": 37, "before_eval_results": {"predictions": ["warmer", "Lauren Taylor", "Chris Martin", "Gavin DeGraw", "drivers who were Daytona Pole Award winners", "James `` Jamie '' Dornan", "2012", "Games played", "2013", "the thirteenth series ended on 19 December 2015", "President pro tempore of the Senate", "Jonathan Breck", "one more proton and is less metallic than its predecessor", "landowner", "Madison, Wisconsin, United States", "the professional level", "the 1994 season", "Kingsford, Michigan", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "head - up", "Austria - Hungary", "Lana Del Rey", "Paracelsus", "George Warren Barnes", "ESPN", "If the car is slowed initially by manual use", "New England", "1996", "The legislation made two amendments to the Social Security Act of 1935", "an appearance from Fonzworth Bentley and another from actress Ki Toy Johnson", "Celtic became the first British team to win the competition, coming back from 1 -- 0 down after a Sandro Mazzola penalty to beat Internazionale 2 -- 1 in the Est\u00e1dio Nacional in Lisbon", "the altitude", "defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the Lake", "the nine of diamonds playing card", "Poland", "\"Appaloosa\"", "the plague", "Lichfield Cathedral", "Charlie Sheen", "Bocelli became completely blind at the age of 12", "Pandosia", "Spain", "Ronald Joseph Ryan", "the 2010 census", "1969 until 1974", "the fourth President of Pakistan from 1971 to 1973", "the eight-man invitational tournament.", "18th", "Trevor Rees", "three out of four", "1998.", "the Mediterranean coastline", "Opry Mills", "Marion", "Frankfort", "Doc Holliday", "repellents", "The Backstreet Boys", "the Muscular Dystrophy Association", "a 16 anna stamp booklet", "Cilla Black", "the People", "yellow"], "metric_results": {"EM": 0.421875, "QA-F1": 0.553054533784173}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.20000000000000004, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.7368421052631579, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.06060606060606061, 0.42857142857142855, 0.0689655172413793, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3595", "mrqa_naturalquestions-validation-8308", "mrqa_naturalquestions-validation-8272", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-750", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-5444", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-2275", "mrqa_triviaqa-validation-5476", "mrqa_hotpotqa-validation-1513", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2548", "mrqa_hotpotqa-validation-4940", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-16757", "mrqa_triviaqa-validation-1886"], "SR": 0.421875, "CSR": 0.5357730263157895, "EFR": 0.9459459459459459, "Overall": 0.712984419452347}, {"timecode": 38, "before_eval_results": {"predictions": ["gain support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam\"", "lemurs", "Man Ray", "Billy Bob Thornton", "Oprah Winfrey", "1957", "Italy", "calcium", "Alpha waves", "savings", "Meyer Lansky", "Jordan", "King Edward III", "Live Free or Die Hard", "Balmo", "(Alexander) Solzhenitsyn", "The Fugitive", "sisyphus", "INXS", "A Few Good Men", "auf wiedersehen", "Fyodor Dostoevsky", "Elizabeth Browning", "Quiz Show", "sculpere", "the Norse", "albee", "Jezebel", "Barbara Walters", "Qualcomm", "Frasier", "The Brady Bunch", "1789", "King Harold Godwinson", "1984", "on the shore of Lake Erie in downtown", "southwestern Colorado and northwestern New Mexico", "Gupta Empire", "13", "April 12, 2017", "Atlantic Ocean", "Dylan Thomas", "Aberdeen", "Mnemosyne", "Dakar, Senegal", "Monopoly", "Gilda", "Kentucky Wildcats", "William Shakespeare", "\"Secrets and Lies\"", "26,000", "drawings and approximately 1 million old master prints", "A Bug's Life", "SpongeBob SquarePants 4-D", "10 below in Chicago, Illlinois.", "federal officers' bodies", "\"design its own separate strategies for making progress toward achieving this long-term goal.\"", "UNICEF", "26", "state senators who will decide whether to remove him from office", "24 Rezai", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "seven", "2 September 1990"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6986072954822955}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false], "QA-F1": [0.8095238095238095, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8452", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-13127", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-15072", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-7652", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-7574", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-1375", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-156", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-3283", "mrqa_naturalquestions-validation-4462"], "SR": 0.609375, "CSR": 0.5376602564102564, "EFR": 1.0, "Overall": 0.7241726762820513}, {"timecode": 39, "before_eval_results": {"predictions": ["the Bible", "Raimond Gaita", "Fife", "Isabella Hedgeland", "Daniel Andre Sturridge", "The 2016 Oklahoma Sooners football team", "Overijssel, Netherlands", "20th Century Fox", "1972", "epic verse", "Continental AG", "1976", "Mick Jackson", "Bishop's Stortford", "The Fault in Our Stars", "Bhushan Patel", "Kassie DePaiva", "Araminta Ross", "Vernon Kay", "RAF Tangmere, West Sussex", "Saturday Night Live", "The Dayton Memorial Hall", "test pilot for the North American X-15 program", "1976", "\u00c6thelstan", "three", "KBS2", "16th Street and Georgetown Road", "Houston Rockets", "three", "Henry II", "63 mph", "Maria Szraiber", "annuity", "Inequality of opportunity was higher", "Marley & Me", "2004, when they played a 3 - game interleague series", "exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time", "in the 1898 Treaty of Paris", "Washington", "Cambridge", "squash", "Picasso", "OKLAHOMA", "Bart\u00f3k", "Niger", "kangaroos", "Sheik Mohammed Ali al-Moayad", "more than 1.2 million people.", "33-year-old", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "propofol", "Salt Lake City, Utah,", "Judge Herman Thomas", "Dr. Quinn", "Julia Child", "Johnny Weissmuller", "As Long As He needs Me & \"You've Got To Pick A", "Charlie Parker", "the Hudson", "a twin-lens reflex camera", "the Maryland Senate's actions", "1962", "Dwight Clark"], "metric_results": {"EM": 0.375, "QA-F1": 0.508234126984127}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.16666666666666669, 0.7499999999999999, 1.0, 0.0, 0.2222222222222222, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.5, 0.75, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-4423", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-3053", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-7639", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-8465", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-6370", "mrqa_triviaqa-validation-274", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3593", "mrqa_searchqa-validation-10066", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-8329", "mrqa_searchqa-validation-766", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-4169"], "SR": 0.375, "CSR": 0.53359375, "EFR": 1.0, "Overall": 0.723359375}, {"timecode": 40, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-402", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4664", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8868", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-787", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13843", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_squad-validation-10004", "mrqa_squad-validation-10059", "mrqa_squad-validation-1006", "mrqa_squad-validation-10112", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-10503", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1634", "mrqa_squad-validation-1703", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-2985", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-3269", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3556", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-366", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4044", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-509", "mrqa_squad-validation-5147", "mrqa_squad-validation-5275", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6024", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8033", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-82", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8319", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8370", "mrqa_squad-validation-8374", "mrqa_squad-validation-8452", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8872", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-10", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1528", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.8359375, "KG": 0.4671875, "before_eval_results": {"predictions": ["St. George's United Methodist Church", "in San Francisco", "Kirstjen Nielsen", "6 - 7 % average GDP growth annually", "source code", "English law", "2013", "`` Everywhere ''", "Ric Flair", "September 1980", "October 27, 2017", "31 December 1600", "1608", "1260 cubic centimeters ( cm )", "body system", "December 24, 1836", "sorrow regarding the environment", "Franklin and Wake counties", "seven", "byte - level", "the nerves and ganglia outside the brain and spinal cord", "Norwegian", "Hermann Ebbinghaus", "Sam Waterston", "on the southeastern coast of the Commonwealth of Virginia in the United States", "June 11, 2002", "Stephen A. Douglas", "HTTP / 1.1", "The first European colony, Caparra", "The show's original three lifelines", "Transvaginal ultrasonography", "Nick Chopper", "Schadenfreude", "Tahrir Square", "Purple Rain", "Phaethon", "Northumberland", "Eric Blair", "Cubism", "Metropolitan Borough of Oldham", "The series is based on characters and elements appearing in Thomas Harris' novels \"Red Dragon\" and \"Hannibal\"", "Londonderry", "St. Louis, Missouri", "Brentford, Wimbledon, and Bolton Wanderers", "1,462", "Mary Harron", "The Hotpoint Electric Heating Company", "staff sergeant", "Dennis Davern", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures", "Zimbabwe", "Islamic militants", "ferry vessels", "he was diagnosed with skin cancer.", "TheBrave Little Toaster", "John Paul Jones", "automobile headlights", "Edinburgh", "Mrs. Doubtfire", "arsenic", "Detroit Rock City", "2004", "Katherine Harris", "The pont de Grenelle"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6873293067226891}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.9714285714285714, 1.0, 0.6666666666666666, 0.0, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10030", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-937", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-2698", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6822", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-654", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-7016", "mrqa_searchqa-validation-9233", "mrqa_hotpotqa-validation-1870"], "SR": 0.59375, "CSR": 0.5350609756097561, "EFR": 1.0, "Overall": 0.7102153201219512}, {"timecode": 41, "before_eval_results": {"predictions": ["manually suppress the fire", "John Spinks", "Kida", "Majandra Delfino", "John Adams", "Krypton", "in the duodenum", "left - sided heart failure", "2006, 2007 -- 2008, January -- April 2012, and again in July -- October 2012", "Abbot Suger", "Filipino American History Month", "1997", "9 February 2018", "re-education", "season ten", "Lincolnshire", "Louis XV", "Kyla Coleman", "Carthaginian '', with reference to the Carthaginians'Phoenician ancestry", "a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "painting", "Nicholas Sparks", "performance marker", "IBM", "1997", "Effy", "Phosphorus pentoxide", "a narcissistic ex-lover who did the protagonist wrong", "spiritual ideas, virtues and the essence of scriptures", "Geraldine Margaret Agnew", "Sir Hugh Beaver", "November 1999", "St Pauls", "Norman Hartnell", "becoming bald or fear of being around bald", "bitter liqueurs", "2", "desert", "gin", "Chrysler Automobile N.V.", "McComb, Mississippi", "King James II of England", "Anah\u00ed Giovanna Puente Portilla de Velasco", "in the second half of the third season", "Revolver", "Wycliffe Hall, Oxford", "bankruptcies", "President Pervez Musharraf", "a Christian farmer", "climatecare,", "Leo Frank", "2006", "United States, NATO member states, Russia", "Patrick Dempsey", "\"On Monsters: Or, Nine or More (MonMonster) Not Cannies\"", "Kwai Chang Caine", "an lake located about 15 miles (24 km) east of Lake Wales,", "FDR", "Anaheim", "national", "piracy incident", "for security reasons and not because of their faith.", "a house party in Crandon, Wisconsin,"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5121935708851544}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5263157894736842, 0.0, 0.2758620689655173, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.2857142857142857, 0.9090909090909091, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-8267", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-8314", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1529", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-236", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2075", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-9753", "mrqa_searchqa-validation-9112", "mrqa_searchqa-validation-6175", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-2315"], "SR": 0.390625, "CSR": 0.5316220238095238, "EFR": 0.9230769230769231, "Overall": 0.6941429143772895}, {"timecode": 42, "before_eval_results": {"predictions": ["Magnetic stratigraphers", "other individuals, teams, or entire organizations", "1974", "an explosion and a fire", "Centre of Excellence", "Dominican", "Ministry of European Integration", "writer, essayist, philosopher, historian and playwright", "Patsy Kensit", "1933", "Jonghyun", "Japan", "musician", "Hans Rosenfeldt", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "English", "Groundhog Day", "Barnoldswick", "Tsung-Dao Lee", "Mike Pringle", "2007", "16,116", "1449", "viruses in the family \"Flaviviridae\"", "USC Marshall School of Business", "1 million acre", "the Flatbush section of Brooklyn, New York City", "Louis \"Louie\" Zamperini", "quantum mechanics", "October 20, 2017", "eastern Tennessee, United States", "322,421", "Saint Michael, Barbados", "meaning", "in the duodenum by enterocytes of the duodsenal lining", "28 July 1914 to 11 November 1918", "Italian Campaign", "Kevin Kline", "Bob Dylan", "the anterolateral system", "C\u00f4te d'Or", "Nottingham", "the Barbizon school", "Phil Redmond", "canterbury", "James Mason", "The Man with One Red Shoe", "Inter Milan", "2009", "the motherless cub defended by Elphaba in \"Wicked.\"", "a plaque at the home of his great-grandfather and by making Ali the first honorary \"freeman\" of the town.", "Hamas rulers,", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "prison inmates.", "New York Times", "\"Touch of Grey\"", "El Salvador", "\"Livin\" On A Prayer\"", "\"A bower quiet\"", "San Francisco 49ers", "canterbury", "Solidarity", "Mexico", "business"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6080543154761904}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.125, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2601", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-1778", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5220", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3719", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-7511", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-66", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-2255", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-8520", "mrqa_searchqa-validation-6621", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-7641"], "SR": 0.515625, "CSR": 0.53125, "EFR": 0.967741935483871, "Overall": 0.7030015120967742}, {"timecode": 43, "before_eval_results": {"predictions": ["a six membraned chloroplast", "the third season of the television series How I Met Your Mother", "Massillon Jackson High School", "Aslan", "2016 -- 17", "Arnold Schoenberg", "based on superior measurables such as size, speed, and strength, has increased his `` draft stock '' despite having a possibly average or subpar college career", "ummat al - Islamiyah", "Chelsea", "Akshay Kumar", "based by the stomach cells called `` chief cells '' in its inactive form pepsinogen, which is a zymogen", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "the victory of good over evil, the arrival of spring, end of winter, and for many a festive day to meet others, play and laugh, forget and forgive, and repair broken relationships", "Rafael Nadal", "June 1991", "The Epistle of Paul to the Philippians", "2018", "1999 to 2001", "Isabela Moner", "12 November 2010", "Theodosius I", "Richard Stallman", "Dalveer Bhandari", "November 2016", "A vanishing point", "Covington, Kentucky", "The Han", "the foreign exchange market (FX )", "the European economy had collapsed", "northwest of Bemis Heights", "The Ministry of Corporate Affairs", "Peter Andrew Beardsley MBE", "May 17, 2018", "William Boyd", "raven", "Florentius", "Norman Mailer", "wild palms", "candy bars", "daily Herald", "Diamond Rio", "15,000 people for basketball matches and 15,500 for concerts (with standing public ramp)", "Kristian Eivind Espedal", "Erich Schmidt-Leichner", "Province of Canterbury", "Delaware River", "Hertz", "43 percent", "Nineteen", "July", "Michael Partain,", "Brett Cummins,", "held in a trust fund", "an appearance on CNN's \"Larry King Live\" on March 31.\"", "ice cream", "Walla Walla", "\"The Man Who Kept Oprah Awake At Night\"", "Linus", "plebeians", "a butterfly", "a disease", "fingerspelling", "negligence", "city of Istanbul"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6034169223650887}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.26666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, 0.06896551724137931, 1.0, 0.5, 1.0, 0.0, 1.0, 0.983050847457627, 1.0, 1.0, 0.33333333333333337, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-6113", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-10148", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-7778", "mrqa_hotpotqa-validation-5310", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1279", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-11574"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.9666666666666667, "Overall": 0.7027864583333334}, {"timecode": 44, "before_eval_results": {"predictions": ["Times Square Studios", "Kittie", "George Washington Bridge", "Craig William Macneill", "Oracle", "Kevin Spacey", "\"War & Peace\"", "Thomas Mawson", "British", "Jean Erdman", "Rabies", "Electress of Hanover", "New York City", "New Jersey", "1995", "Frank Sinatra", "Black pudding", "Esteban Ocon", "Kurt Vonnegut Jr.", "mixed martial arts", "Hugh Dowding", "9", "16\u201321", "University of Nevada, Las Vegas", "6,396.", "The Division of Cook", "Radcliffe College", "13", "George Harrison", "The Shins", "Australian band The Captain Matchbox Whoopee Band", "Thomas Allen", "19th", "National League ( NL ) champion Cleveland Indians", "Staci Keanan", "based on the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "March 2003", "Harry", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid, which are then absorbed by the host", "September 2000", "turnip", "Williams F1", "flokland", "gleneagles", "Dublin", "Neighbours", "Margate", "a satellite", "Daniel Wozniak,", "pilgrims sail to Plymouth Rock", "Iran's President Mahmoud Ahmadinejad", "ICE chief Julie Myers.", "Naples", "Jaime Andrade", "trenchcoat", "Eragon", "Matthew Perry", "Ivan the Terrible", "Kingston", "Lisa Gherardini", "Caeasr", "LSD", "island", "four"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6845486111111112}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.06666666666666667, 1.0, 0.0, 0.9777777777777777, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-4505", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2581", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-4732", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1406", "mrqa_triviaqa-validation-1418", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-3876", "mrqa_searchqa-validation-5564", "mrqa_searchqa-validation-13041", "mrqa_triviaqa-validation-2578"], "SR": 0.59375, "CSR": 0.5326388888888889, "EFR": 0.9615384615384616, "Overall": 0.7020385950854701}, {"timecode": 45, "before_eval_results": {"predictions": ["the emergence of the new state of Turkey in the Ottoman Anatolian heartland,", "May 1, 2011", "Bob Day", "English", "23 December 1893", "James Edward Franco", "31", "Sun Valley, Idaho", "the Grand Harbour", "\"The Place Beyond the Pines\"", "girls aged 11 to 18", "Duval County, Florida", "burlesque", "Captain Cook's Landing Place", "alt-right", "Kentucky", "Belladonna", "NBA All-Star Game", "Prince Louis of Battenberg", "Mario Lemieux", "North Greenwich Arena", "Channel 4", "British racing driver", "1921", "\"The Goddess of 1967\"", "Interstate 22", "Ronald Lyle \" Ronnie\" Goldman", "the Falkland Islands", "Azusa Pacific University", "Mel Blanc", "five times", "performed under the mononym Charice until his gender transition to male", "\"Section.80\"", "Kida", "four distinct levels", "a traditional holiday originating in China", "signifies the thirteen British colonies that declared independence from the Kingdom of Great Britain", "the Roman Empire", "Theodore Roosevelt", "1987", "Jamaica", "Ernie (Ernie)", "300", "The Quatermass Experiment", "Honeybees", "hetyt", "Martin Luther King", "Sarah", "2,800", "11", "$273 million", "Iran", "diplomatic relations", "\"tan lines\"", "Islamabad", "Alexander Calder", "Nautilus", "New York City Ballet", "chortle", "igneous", "bridge", "California, Texas and Florida,", "bright Automotive,", "Stuntman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6214020593869731}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false], "QA-F1": [0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.7586206896551724, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9846", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-3332", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-3452", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-3918", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6393", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-3375", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-2582", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3858"], "SR": 0.546875, "CSR": 0.5329483695652174, "EFR": 1.0, "Overall": 0.7097927989130435}, {"timecode": 46, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "Nina Stibbe", "December 1993", "Matt Groening", "Karolina Dean", "New York City", "1916", "ten", "40 million", "Peter G. Kelly", "Wes Archer", "26,000 students", "Anne of Green Gables", "Tarryl Lynn Clark", "general secretary", "first wife Anna", "Acela Express", "The 2004 Nokia Sugar Bowl", "Univision", "The Light in the Piazza", "400 MW", "Fort Hood", "green and yellow", "\"Seducing Mr. Perfect\"", "Juilliard School", "The interview", "Hilo", "\"Enemy\"", "Joseph J. Pulitzer", "Lazio", "migration management", "ONTV", "2,627.", "March 31, 2013", "the mid-1980s", "terrestrial", "Dr. Derek Shepherd ( Patrick Dempsey )", "December 19, 2014", "American country music singer Waylon Jennings", "1992", "Fluorine", "Northwest Territories", "Antoine Henri Becquerel", "ledger", "guitar", "The Comedy of Errors", "Malaysia", "Egypt", "\"Wolfman,\"", "Security was tightened", "fractured pelvis and sacrum", "1.2 million", "three", "maintain an \"aesthetic environment\"", "Baltimore", "Marmaduke", "Natalie Jane Imbruglia", "I Love Rock 'n' Roll", "accomplish", "the Defense Intelligence Agency", "Jeopardy", "Scotland", "The Bulletin", "Wet Wet Wet"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6640625}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4584", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-2079", "mrqa_hotpotqa-validation-5053", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-2267", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-4917", "mrqa_triviaqa-validation-5859", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3602", "mrqa_searchqa-validation-5529", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-9607"], "SR": 0.5625, "CSR": 0.5335771276595744, "EFR": 1.0, "Overall": 0.709918550531915}, {"timecode": 47, "before_eval_results": {"predictions": ["meningitis", "talc", "the (Miami) Dolphins", "Hersey", "cloves", "Lysistrata", "the tulip", "the Old Manse", "the salt of the earth", "Ross Perot", "capable", "the ozone hole", "Spider-Man", "Ezra Cornell", "Arthur Miller", "(John Wilkes) Booth", "Florida", "The Johns Hopkins University", "Lurch", "the Great Mississippi", "London", "a springer spaniel", "(Scott)\" Peterson", "Sylvia Plath", "John Henry", "Wall Street", "Isle of Man", "Fort Sumter", "Orion", "Notre-Dame de Paris", "\"Don Juan De Marco.\"", "Troilus", "Puente Hills Mall", "Francisco Pizarro", "the largest delegation is that of California, with fifty - three representatives", "beta decay", "the Astros would not return to the World Series until 2017, their fifth season as an American League team", "Victor Dhar", "parthenogenesis", "Kim Basinger", "the bison", "the Grail", "in order to eliminate them from succession", "San Francisco", "Malachy McCourt", "Daewoo", "Rihanna", "12", "Ribosomes", "political correctness", "The Royal Family", "the Food and Agriculture Organization", "two or three acts", "Friends", "1983", "First Balkan War", "Britain.", "\"your President Bush doesn't like us Muslims.\"", "Tuesday", "\"Rin Tin Tin: The Life and the Legend\"", "the Bronx.", "an \"unnamed international terror group\"", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "Iran's parliament speaker"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7457215481886534}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-13871", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-3872", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-12962", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-736", "mrqa_hotpotqa-validation-5807", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2215"], "SR": 0.65625, "CSR": 0.5361328125, "EFR": 1.0, "Overall": 0.7104296875}, {"timecode": 48, "before_eval_results": {"predictions": ["Stella McCartney", "Little Women", "the Helsinki Philharmonic", "Injun Joe", "the Rubik's Cube", "Newport", "the lionheart", "salivary glands", "Chinese", "the Solidarity", "Zen", "Aristotle", "(Diane) Arbus", "defense", "the American Indians", "Martina Navratilova", "Sweden", "molten", "Margaret Spellings", "City of Hope", "Frasier Crane", "Like Water for Chocolate", "catalog", "(Lewis) Carroll", "a high school football team in the fictional town of Dillon, Texas", "Signs", "Henry Cisneros", "the Kilimanjaro", "Nguyen", "William Shakespeare", "Scotchgard", "Prince", "Thomas Lennon", "Peter Finch", "plant anatomy", "Marshall Sahlins", "the fourth C key from left on a standard 88 - key piano keyboard", "Julia Roberts", "the player, including atolls, sea forts, Mayan ruins, sugar plantations, and underwater shipwrecks, with a 60 / 40 balance between land and naval exploration", "1980 Summer Olympics", "rabbit", "ogaden", "london", "Hera", "hoggle", "Timothy Carroll", "Aquaman", "percy thrower", "Australian women's national soccer team", "about 5320 km", "She made her film debut in the 1995 teen drama \"Kids\".", "Kim Ji-soo", "People v. Turner", "the Isle of Axholme", "Lucius Cornelius Sulla Felix", "Jenny Bae", "the man ran out of bullets and blew himself up.", "overturned about 5:15 p.m.", "Islamic", "throwing three punching but said only one connected.", "overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "in cities throughout Canada.", "several weeks,", "Madonna"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6288352272727272}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.19999999999999998, 0.4, 1.0, 0.36363636363636365, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-13528", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-14796", "mrqa_searchqa-validation-12851", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-6550", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-12278", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-3947", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-452", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-2407", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-3362"], "SR": 0.546875, "CSR": 0.5363520408163265, "EFR": 0.9655172413793104, "Overall": 0.7035769814391274}, {"timecode": 49, "before_eval_results": {"predictions": ["Children of Men", "(Victor) Hugo", "Call Occupants Of Interplanetary Craft", "teflon", "America", "Shia", "Zionism", "Gaelic", "a martians", "the Triborough Bridge", "the Grimms", "(Jose de) San Martin", "Cancer's Return", "Richard Cory", "Franklin D. Roosevelt", "Perry Mason", "Italy", "Thailand", "Brazil", "a surrogate", "American Bandstand", "Sanrio", "Brazil", "South Carolina", "david bowie", "Dick Cheney", "skin cancer", "comics", "diesel", "Abraham Lincoln", "messenger", "Homeland Security", "the people of the United States", "Thomas Jefferson", "Michigan", "at the turn of the traditional lunisolar Chinese calendar", "the Missouri River", "the meridian", "Teri Garr", "a vertebral column ( spine )", "teflip", "teflon", "Great Victoria", "teflon", "teflon", "teflon", "Lake Placid", "einstein", "C. S. Lewis", "Ford Field", "Benedict of Nursia", "The Spiderwick Chronicles", "May 30, 2005", "1945", "You Can Be a Star", "paper-based card", "Dean Martin, Katharine Hepburn and Spencer Tracy", "$5.5 billion", "forcibly drugging", "heavy turbulence", "April 22.", "birmingham", "Friday,", "Afghanistan"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4723090277777777}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1111111111111111, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-16374", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-9677", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-3218", "mrqa_searchqa-validation-9737", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-12956", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-1673", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5298", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-3139", "mrqa_triviaqa-validation-6846", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-3420", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1054"], "SR": 0.390625, "CSR": 0.5334375, "EFR": 0.9743589743589743, "Overall": 0.7047624198717949}, {"timecode": 50, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-2548", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4009", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7639", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14854", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16297", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3410", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6275", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-509", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.806640625, "KG": 0.478125, "before_eval_results": {"predictions": ["741 weeks", "Baaghi ( English : Rebel )", "the fourth C key from left on a standard 88 - key piano keyboard", "in Rome in 336", "Captaincy General of Guatemala", "1997 ( XXXII )", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "Yuzuru Hanyu", "four", "the first instalment in the long - running Harry Potter film series, and was written by Steve Kloves and produced by David Heyman", "Eddie Van Halen", "Ozzie Smith", "October 1 to September 30", "1986", "Archduke Franz Ferdinand of Austria", "Brad Pitt", "William Wyler", "a sociological perspective which developed around the middle of the twentieth century", "for the red - bed country of its watershed", "31", "By 1770 BC", "the President of the United States", "Welch, West Virginia", "heart muscles", "Super Bowl LII", "the President of the United States", "nine", "a bronze statue designed by Thomas Crawford ( 1814 -- 1857 )", "September 6, 2019", "Norway", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "Marley & Me", "peter-paul-and-mary", "sahara", "Edinburgh", "John Nash", "sahara", "reims", "Paul Wellens", "Gorky", "Trey Parker and Matt Stone", "The Life of Charlotte Bront\u00eb", "The Future", "Nick Weidenfeld", "the superhero Birdman", "Corendon Dutch Airlines", "The Show Band Show", "the Henry Jackson Society", "Anil Kapoor", "sotomayor,", "Unseeded Frenchwoman Aravane Rezai", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "March 3,", "1918-1919.", "two remaining crew members", "a 2,700-acre sanctuary in rural Tennessee.", "Guinness", "President Mikhail Saakashvili", "the Barnard College", "the lion", "the Morse code", "Michelle Kwan", "the English Monarchs", "One Life to Live"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6412418537078863}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.8695652173913044, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.25, 0.8, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-2063", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1566", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1084", "mrqa_searchqa-validation-8101", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-1742"], "SR": 0.53125, "CSR": 0.5333946078431373, "EFR": 0.9666666666666667, "Overall": 0.7054028799019607}, {"timecode": 51, "before_eval_results": {"predictions": ["gravitation", "the physician George Huntington, after whom it is named", "1997", "the Rashidun Caliphs", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "Earle Hyman", "1998", "fiat", "Hasmukh Adhia", "the United States Congress declared war", "its absolute temperature", "Joseph M. Scriven", "Strabo", "pit road speed", "5.7 million", "Magnavox Odyssey", "Mike Alstott", "to the left of the dinner plate", "1", "14 -- 20 April", "present - day southeastern Texas", "Shenzi", "1871", "the Ramones", "British coming of age - comedy film co-written and directed by Gurinder Chadha", "Beorn", "8,850 km", "the United States", "Joseph Sherrard Kearns", "one person", "members of the gay ( LGBT ) community", "a single peptide bond or one amino acid with two peptide bonds", "Wat Tyler", "York", "robert bzelius", "Russia", "Tennessee", "bond", "Prague", "Renoir", "four", "Mumbai", "McLaren-Honda", "1974", "18 December 1975", "Carrefour", "Florida and Oklahoma", "Nelson County", "Pakistan", "onto the college campus.", "orders immigrants to carry their alien registration documents at all times", "Sunday", "breast cancer.", "Wednesday", "Will Smith.", "sexual harassment", "delta", "the William Faulkner.", "Consumers Union Reports", "Amsterdam", "Auguste Escoffier", "Brad Chase", "the Bengali alphabet", "Anne Rice"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6180179195804196}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-607", "mrqa_hotpotqa-validation-5727", "mrqa_hotpotqa-validation-4172", "mrqa_hotpotqa-validation-76", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-4114", "mrqa_searchqa-validation-9847", "mrqa_searchqa-validation-8345", "mrqa_searchqa-validation-13488", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-11102"], "SR": 0.515625, "CSR": 0.5330528846153846, "EFR": 1.0, "Overall": 0.7120012019230769}, {"timecode": 52, "before_eval_results": {"predictions": ["Martha Coolidge", "bioelectromagnetics", "Serial (Bad) Weddings", "Dizzy", "George Raft", "\"The Basketball Diaries\"", "Javed Miandad", "the Shriners", "DJ Scotch Egg", "67,575", "Target Corporation", "Gatwick Airport", "12\u201318", "alpine species", "25 June 1971", "810", "Wal-Mart Canada Corp.", "heaviest", "Borwick railway station", "1834", "5.3 million", "1943", "more than 250 million copies worldwide", "7 June 1985", "1983", "200,000 passengers", "orchestrated the first movement piano sketch", "Danny Glover", "Samuel Burl \"Sam\" Kinison", "Indianapolis Motor Speedway", "The Panthers compete in the Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA) and the East Division of Conference USA (CUSA)", "The cinema of Russia", "erosion", "more than 1,000", "`` Nelson's Sparrow ''", "nearby objects show a larger parallax than farther objects when observed from different positions", "coercivity", "James P. Flynn", "Horace Lawson Hunley", "Sally Field", "Homo sapiens", "tartar", "Ithaca", "Andrew Lloyd Webber", "James Carville", "caryatid", "Ceylon", "piccadilly", "London and Buenos Aires", "Caster Semenya", "Windsor, Ontario,", "Emmy-winning Patrick McGoohan,", "\"to send a loud and clear message\" to House Speaker Nancy Pelosi and other Democratic leaders.", "re-impose order", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "the northeastern Iranian city of Mashhad", "synapses", "Hungarian Rhapsody", "Antwerp", "the Apes", "rampant", "The Sound of Music", "berkowitz", "Angola"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5687015241702742}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.4, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.08333333333333333, 0.888888888888889, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-2209", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-3864", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-5011", "mrqa_hotpotqa-validation-3475", "mrqa_hotpotqa-validation-74", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-1837", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-5927", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-3756", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-1383", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-10896", "mrqa_searchqa-validation-5254"], "SR": 0.46875, "CSR": 0.5318396226415094, "EFR": 0.9705882352941176, "Overall": 0.7058761965871254}, {"timecode": 53, "before_eval_results": {"predictions": ["born 25 October 1921", "Park Seo-joon", "Marty Ingels", "Ecko Unlimited", "Margaret", "Norbertine", "Richard II", "Coahuila, Mexico", "Province of Canterbury", "born 2 May 2015", "Eenasul Fateh", "Ladies' Code", "all U.S. territories except American Samoa", "Yunho", "Debbie Isitt", "Clive Staples Lewis", "2017", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven and Andy Cohen", "evangelical Christian periodical", "the Battelle Energy Alliance", "2012", "British television network ITV", "Bardot", "Steve Carell", "Jesus", "2,615", "Mermaids", "publicly", "General Manager", "Bellagio and The Mirage", "Aksel Sandemose", "January 30, 1930", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "90 \u00b0 N 0 \u00b0 W", "1830", "Authority", "1984", "Elizabeth Dean Lail", "the northern tip of West Virginia", "1898", "Nowhere Boy", "biyya-Oromo", "John", "watt hours", "sambal", "at", "British Airways", "-30", "al Fayed's security team.", "test-fire a long-range missile under the guise of a satellite launch.", "dogs who walk on ice in Alaska.", "February 2008", "Sadr City", "she was a young skater and desperately wanted to make her mother proud.", "NATO's Membership Action Plan,", "Three", "Agatha Christie", "the union", "Converse", "Trinidad and Tobago", "a cogito", "a captain", "\"Solitude\"", "\"The Vision\""], "metric_results": {"EM": 0.359375, "QA-F1": 0.49086025035014}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.8571428571428571, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.37499999999999994, 0.6666666666666666, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21428571428571427, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-2784", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2831", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4844", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-64", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-811", "mrqa_triviaqa-validation-1289", "mrqa_triviaqa-validation-2698", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-16360", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-12630"], "SR": 0.359375, "CSR": 0.5286458333333333, "EFR": 0.9512195121951219, "Overall": 0.701363694105691}, {"timecode": 54, "before_eval_results": {"predictions": ["North Africa and southwest Asia", "Jean Baptiste Le Roy", "south africa", "A Dangerous Man: Lawrence After Arabia", "John J. Pershing", "Barbra Streisand", "Australia and England", "You May Also Like...", "Ken Purdy", "Vevey Switzerland", "a sense of loyalty and dedication to a specific person or persons", "king raleigh", "a mathematician, astronomer, physician, classical scholar, translator, Catholic cleric, jurist, governor, military leader, diplomat and economist", "cyclops", "La Toya Jackson", "Adolf Hitler", "Brits", "a rat", "five-year", "old oakum", "1912", "Amelia Earhart", "a number who's square is equal to 169", "B\u00e9la Bart\u00f3k", "Tom Watson", "Libya", "Iphigenia", "Norwegian", "radishes", "Audi", "Amsterdam", "an amanuensis", "Lew Brown", "Paracelsus", "The weekly Torah portion", "2014", "Times Square in New York City", "Homer Banks, Carl Hampton and Raymond Jackson", "the Constitution of India", "Middle Eastern alchemy", "seven members", "The Russian Ark", "Harry F. Sinclair", "diving duck", "ABC", "perjury and obstruction of justice", "Benj Pasek and Justin Paul", "1974", "Alexandre Caizergues, of France,", "Buenos Aires.", "he exercised in a park in a residential area of Mexico City,", "Prague", "your environmental efforts make even more impact than Harrison Ford's chest.", "Judge Sonia Sotomayor's", "269,000", "FARC rebels.", "Poland", "bottle", "Sweden", "Hiroshima", "Paraguay", "The \"NFL HQ\" crew", "Roger Williams", "James Joyce"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5435917075163398}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 0.19999999999999998, 0.13333333333333333, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3410", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2253", "mrqa_triviaqa-validation-3334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-360", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1394", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-6490", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1037", "mrqa_searchqa-validation-7143", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-1239"], "SR": 0.421875, "CSR": 0.5267045454545455, "EFR": 0.972972972972973, "Overall": 0.7053261286855037}, {"timecode": 55, "before_eval_results": {"predictions": ["severe flooding", "Peshawar", "Australia and New Zealand", "suicide bombing", "Kurdistan Workers' Party,", "District of Columbia National Guard,", "the simple puzzle video game,", "flooding was so fast that the thing flipped over,\"", "regulators in the agency's Colorado office received improper gifts from energy industry representatives and engaged in illegal drug use and inappropriate sexual relations with them.", "over the Gulf of Aden,", "why you broke up", "42 years old", "\"procedure on her heart,\"", "to fritter his cash away on fast cars, drink and celebrity parties.\"", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.\"", "Six", "Brown-Waite", "tennis", "Alinghi", "Michael Schumacher", "The former child actor was hospitalized briefly three months ago after suffering a seizure while being interviewed on a TV show in Los Angeles, California.", "Hong Kong", "Roy Foster's", "I'm certainly not nearly as good of a speaker as he is.\"", "Liverpool", "romantic e-mails", "Mandi Hamlin", "to put a lid on the marking of Ashura this year.", "10 municipal police officers", "Sharon Bialek", "Two", "assist in relief efforts,", "Fall 1998", "October 27, 1904", "February 16, 2018", "June 25, 1938", "12.9 - kilometre ( 8 mi )", "must be at least 18 or 21 years old", "interstate communications by radio, television, wire, satellite, and cable", "starch", "Eva Cassidy", "1925", "patient ben", "Bahrain", "triathletes", "Kenya", "Bobby Darin", "noises Off", "black nationalism", "Great Lakes and Midwestern", "Jarome Iginla", "1980", "MG Cars", "EA-18G Growler", "he established the Salzburg Festival with the performance of Hofmannsthal's \"Jedermann\".", "Fiat S.p.A.", "Washington Irving", "Jamaica Inn", "Suspicious Minds", "a thing done afterward", "conga drums", "Schmidt", "birds", "Big Momma"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6276436237373737}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.8, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2758", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-6607", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5233", "mrqa_searchqa-validation-15538"], "SR": 0.53125, "CSR": 0.5267857142857143, "EFR": 1.0, "Overall": 0.7107477678571429}, {"timecode": 56, "before_eval_results": {"predictions": ["suez Canal", "Basil Feldman", "antelope", "Caroline Garcia", "roger casha", "Czech Republic", "olympia de 1968", "offal", "driving Miss Daisy", "Oklahoma", "QM2", "Djibouti and Yemen", "animals", "Bruce Wayne", "basketball", "geyser", "Craig Kilborn", "kevin", "Wimbledon", "tennis", "beetles", "numbtsman", "Josh Brolin", "Pebble", "sistan-Baluchistan", "flybe", "egypt", "Prudence 'Tuppence' Beresford", "Brat", "kevin", "weasel", "prince eddy", "Gary Player", "May 2017", "Southport, North Carolina", "Columbia River Gorge", "Doug Pruzan", "2019", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "9 February 2018", "Liga MX", "Attorney General and as Lord Chancellor of England", "The Cosmopolitan", "Mel Blanc", "Democratic", "Tak and the Power of Juju", "jet-powered tailless delta wing", "Phil Spector", "30-minute", "12 million", "it should stay that way.", "11,", "1,073", "Serie A", "clues, solving puzzles, and risking his life while dropping cocktail parties' worth of scholarly minutiae.", "breast self-examination.\"", "Port Washington", "caution", "La Florida", "fontanels", "Abbeville", "Bangkok", "Typhoid", "home"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5142931547619047}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-6540", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-7217", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-5067", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5736", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-510", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-2913", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-10064", "mrqa_searchqa-validation-3538", "mrqa_searchqa-validation-4292", "mrqa_searchqa-validation-7493"], "SR": 0.421875, "CSR": 0.5249451754385965, "EFR": 1.0, "Overall": 0.7103796600877194}, {"timecode": 57, "before_eval_results": {"predictions": ["kinshasa, Zaire", "lips", "home runs", "piscinae", "Richard Marx", "peabody", "chiba", "taegeukki", "Vim Tonic", "Saddam Hussein", "vickers-Armstrong", "anthropocene", "shabbat", "Cosmos", "Matlock", "Persuasion", "Planet of Mars", "old Trafford", "eye", "8", "the Soviet Union", "lung", "jocelyn peep", "ugar Baby Love", "15", "Argentina", "king george v", "significant achievement", "jocelyn Jacobi", "points based scoring", "jK", "basil", "4 January 2011", "hairpin turn", "1986", "a vertebrate's immune system", "to represent'a voyage of adventure'on which the programme would set out", "William Fox", "1839", "22 \u00b0 00 \u2032 N 80 \u00b0 00", "comedy", "hiphop", "Juventus of Italy", "Kennebec County", "Magnus Carlsen", "jocelyn jon feldman", "The Walking Dead", "Pylos and Thebes", "david bowie,", "digging at the site", "paid tribute to pop legend Michael Jackson,", "North Korea's reclusive leader Kim Jong-IlThe missiles can travel about 3,000 kilometers (1,900 miles),", "home in Lake Hughes.", "201-262-2800.", "the legitimacy of that race.", "Brian Smith.", "New Balance", "a tumor", "Mystery Science Theater 3000", "Baby Got Back In The U.S.S-R.", "Wales", "Deep Purple", "James II", "Mexico"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5100198412698413}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.09523809523809525, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.5, 0.8, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-6831", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-1095", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-4483", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-5451", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5472", "mrqa_hotpotqa-validation-5299", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-4095", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-15364"], "SR": 0.421875, "CSR": 0.5231681034482758, "EFR": 0.972972972972973, "Overall": 0.7046188402842497}, {"timecode": 58, "before_eval_results": {"predictions": ["Payaya Indians", "March 11, 2018", "vacation in Wisconsin", "1535", "Erica Rivera", "International Baccalaureate", "Tommy James", "alcohol or smoking", "Paul Lynde", "they each supported major regional wars known as proxy wars", "James Rodr\u00edguez", "Hugh S. Johnson", "The Miracles", "spontaneously", "through the buttock and down the lower limb", "Beijing", "Cadillac", "the United States", "Salman Khan", "`` Far Away ''", "development of electronic computers", "the defendant owed a duty to the deceased to take care", "Sri Lanka Podujana Peramuna", "Hold On", "alternative rock", "November 1999", "Lee Bowman", "the government - owned Panama Canal Authority", "Scott Bakula as Dwayne `` King '' Cassius Pride", "the `` round '', the rear leg of the cow", "William Wyler", "Woody Paige", "William Blake", "Miranda v. Arizona", "massively multiplayer online games", "november", "november", "geiger", "Much Ado About Nothing", "Lancashire", "1926 Paris", "flew solo to Scotland", "Estonia", "Walcha", "King James I of England", "Golden Calf", "Edward Charles Spitzka", "nearly 80 years", "Kurt Cobain", "\"active athletes,\"", "London's Heathrow airport", "Goa", "56,", "the results by a chaplain about 1:45 p.m.", "\"The Lost Symbol,\"", "Iranian city of Mashhad", "damn", "out of body", "Athol Fugard", "Jonah", "Treasure Island", "Death Watch", "resolution", "Boiling Lake"], "metric_results": {"EM": 0.5, "QA-F1": 0.5919347145909646}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 0.4444444444444445, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5714285714285715, 0.0, 0.4, 1.0, 0.5, 0.4444444444444445, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-6821", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2822", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-5678", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-3854", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3727", "mrqa_searchqa-validation-8279", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9151"], "SR": 0.5, "CSR": 0.5227754237288136, "EFR": 1.0, "Overall": 0.7099457097457627}, {"timecode": 59, "before_eval_results": {"predictions": ["calls for him to step down as majority leader.", "Long Island convenience store", "Kenneth Cole", "Dennis Davern,", "Harry Nicolaides,", "Russia and China", "Pakistan", "free fixes for the consumer.", "heavy turbulence", "the southern city of Naples", "in next month's run-off election,", "Miss USA Rima Fakih", "\" Teen Patti\"", "is looking at designating the sedative as a \"scheduled\" drug,", "President Obama", "$55.7 million", "ensuring that all prescription drugs on the market are FDA approved,", "$75", "autonomy.", "Dick Cheney,", "Jenny Sanford,", "international search team", "Animal Planet", "would not identify those customers to CNN.", "Monday", "in the $24,000-30,000 price range.", "the leader of a drug cartel", "books", "Dr. Jennifer Arnold and husband Bill Klein,", "Sabina Guzzanti", "a bag", "deputy prime minister", "British Columbia, Canada", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "warplanes, especially by long - range patrol planes that were sent out by those navies to scout for enemy warships, cargo ships, and troop ships", "Arunachal Pradesh", "chain elongation", "1926", "since 3, 1", "Sreejita De", "Il Divo", "honey wine", "perfumes", "november", "albert einstein", "Nancy Astor", "mmorpg", "gofalu", "Carrefour", "Ben Ainslie", "Dalton Gang", "HackThis Site", "Shut Up", "the 2016 U.S. Senate election in Nevada", "Mark Anthony \"Baz\" Luhrmann", "Two Pi\u00f1a Coladas", "Martin Luther", "Penn State", "Joe Hill", "a turkey vulture", "the National Archives", "Hera", "naggers", "the East River"], "metric_results": {"EM": 0.546875, "QA-F1": 0.661145813098938}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true], "QA-F1": [0.8750000000000001, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.07407407407407408, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1826", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-961", "mrqa_naturalquestions-validation-388", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1366", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1501", "mrqa_hotpotqa-validation-37", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2989", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-8933", "mrqa_searchqa-validation-3312"], "SR": 0.546875, "CSR": 0.5231770833333333, "EFR": 1.0, "Overall": 0.7100260416666667}, {"timecode": 60, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-205", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2684", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3167", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5774", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4646", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2416", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-515", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-11869", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2381", "mrqa_searchqa-validation-2582", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-289", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3153", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8737", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2376", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-183", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3784", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.798828125, "KG": 0.50078125, "before_eval_results": {"predictions": ["The Vamps", "Bonnie Lipton", "Benzodiazepines", "19 June 2018", "in the central plains", "giant", "ase", "During Hanna's recovery masquerade celebration", "Gibraltar", "1928", "UTC \u2212 08 : 00", "Bob Peterson", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Beijing", "2017", "1979", "Johnny Cash", "$2 million", "Mel Gibson", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Fox Ranch in Malibu Creek State Park", "rum", "a Scandinavian patronymic surname, meaning son of Hans", "the port of Nueva Espa\u00f1a", "Kelly Reno", "uracil ( U )", "Jon", "to encourage rebellion against the British authorities", "Cairo, Illinois", "Saturn", "Christopher Columbus", "bageecha", "Orrest Head", "trees", "Gremlins", "Audi", "tartare", "Babylonian Empire", "black", "newbury", "al-Qaeda", "2009", "2009", "Humberside Airport", "\"King of Cool\"", "Elizabeth Taylor", "Patricia Jude Francis Kensit", "neuro-orthopaedic Irish veterinary surgeon", "Arthur E. Morgan III,", "\"procedure on her heart,\"", "back at work.", "\"The largest and perhaps most sophisticated ring of its kind in U.S. history.", "Hisung-Bak", "Russian concerns that the defensive shield could be used for offensive aims.", "suicide vests", "to see my kids graduate from this school district.", "Yellowstone National Park", "Dollfu", "\"Black Boss,\"", "John Knox", "the Catholic Church", "the pelvis", "\"Viva La Revolucin!\"", "Anderson Cooper"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6155941611477048}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 0.7272727272727272, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.782608695652174, 0.7368421052631579, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 0.8, 1.0, 0.11320754716981131, 0.0, 1.0, 0.0, 0.4, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-4394", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-1219", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2695", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-8946", "mrqa_searchqa-validation-12686", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-2333"], "SR": 0.484375, "CSR": 0.5225409836065573, "EFR": 0.9393939393939394, "Overall": 0.6964494846000994}, {"timecode": 61, "before_eval_results": {"predictions": ["StubHub Center in Carson, California", "Haiti", "1038", "about 3.5 mya", "Arkansas", "on the lateral side of the tibia", "David Motl", "Andrew Lloyd Webber", "Gene MacLellan", "much of the European industrial infrastructure had been destroyed", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "60", "Waylon Jennings, Tommy Allsup, and Carl Bunch", "Nicklaus", "Amanda Fuller", "The won", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile", "Thomas Jefferson", "the symbol \u00d7", "Bobby Darin", "( as primary antagonist Buffalo Bill )", "Puerto Rico", "Between 1934 and 2013", "marks locations in Google Maps", "Nalini Negi", "Samaria", "Andreas Vesalius", "an unmasked and redeemed Anakin Skywalker ( formerly Darth Vader )", "Southampton ( 1902, then in the Southern League )", "Wisconsin", "a central place in Christian eschatology", "~ 3.5 million years old", "(Borneo)", "9", "The Welcome Stranger", "\"Black Swan\"", "brixham", "Alison Moyet", "Tacoma", "jaws", "Manchester", "black nationalism", "Martin Stadium in Pullman, Washington", "January 28, 2016", "the reigning monarch of the United Kingdom", "2013", "dance moves", "Isabella (Belle) Baumfree", "\"Teen Patti\" (\"Card Game\")", "more than 78,000 parents of children ages 3 to 17.", "His wife, Araceli Valencia,", "Facebook", "an estimated 750", "Scarlett Keeling", "He really didn't come to us and say, 'I want to file for divorce.'\"", "on China, Taiwan, Hong Kong and Mongolia,", "\" Doc\" Holliday", "(George) Cornell", "\"The Biggest Loser\"", "a pacemaker", "the Communists", "mathematics", "(Johnny) B. Goode", "a statue of Apollo"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5616769515207014}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.46153846153846156, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.13333333333333333, 0.5714285714285715, 1.0, 0.2222222222222222, 1.0, 0.4, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7777777777777777, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.26666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-833", "mrqa_triviaqa-validation-797", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-7755", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-3335", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-879", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-15666", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-3147"], "SR": 0.46875, "CSR": 0.5216733870967742, "EFR": 1.0, "Overall": 0.7083971774193548}, {"timecode": 62, "before_eval_results": {"predictions": ["late Tuesday night,", "the legitimacy of that race.", "twice", "Majid Movahedi,", "women and breast cancer.", "Kris Allen,", "a tanker that sailed under a Saudi flag,", "the only goal of the game to ensure Hamburg remain in touch with the top three as they claimed a 1-0 win over Hertha Berlin at the HSH Nordbank Arena", "an insect sting in August.", "Little Rock Central High School in Arkansas.", "5,600", "prisoners at the South Dakota State Penitentiary and ultimately delivered in Iraq", "Jacob Zuma,", "Rihanna", "a dorm parent allegedly grabbed a pupil by the throat and threw her against a wall,", "Paul Ryan", "be silent.", "standing by to provide security as needed.", "peanuts,", "for early detection and helping other women cope with the disease.", "56,", "English,", "suicides", "the shelling of the compound", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "four decades", "in legislative elections in Buenos Aires.", "Turkey,", "a bank", "sexually assaulting a toddler and capturing it on videotape years ago,", "40", "Sunday", "Massachusetts", "Evan Spiliotopoulos", "an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network, and is widely used on the Internet", "in the Mahoning Valley region, where Youngstown is located", "in pilgrimages to Jerusalem", "Starscream", "Rick Rude", "James Martin Lafferty", "Purple Rain", "maqui berry", "lundy", "insulin", "Jerusalem", "Sarek", "eliza", "a karst cave", "Nathan Bedford Forrest", "Mark Neveldine and Brian Taylor", "the 10-metre platform event", "KXII", "Rousillon Rupes", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services.", "National Basketball Development League", "New York Yankees", "the London Bridge", "Passover", "Hormel Foods", "the penny", "a dowry", "1849", "beryl", "Bastille Day"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6620907247851907}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.3333333333333333, 1.0, 0.0, 0.0, 0.6, 1.0, 0.2857142857142857, 0.26666666666666666, 0.0, 0.2222222222222222, 1.0, 0.7058823529411764, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.11764705882352941, 0.2857142857142857, 0.0, 1.0, 0.0, 0.56, 0.0, 0.46153846153846156, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2758", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3875", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-1199", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-3010", "mrqa_triviaqa-validation-7232", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-68", "mrqa_searchqa-validation-11056"], "SR": 0.53125, "CSR": 0.5218253968253967, "EFR": 1.0, "Overall": 0.7084275793650793}, {"timecode": 63, "before_eval_results": {"predictions": ["Mandi Hamlin", "Dan Parris, 25, and Rob Lehr, 26,", "publicly criticized his father's parenting skills.", "in Africa", "martial arts,", "telling CNN his comments had been taken out of context.", "$24.1 million,", "Jacob Zuma,", "President Mohamed Anwar al-Sadat", "Daniel Wozniak,", "His replacement, African National Congress Deputy President Kgalema Motlanthe,", "Senate Democrats", "protest child trafficking and shout anti-French slogans", "June 2004", "March 22,", "Amanda Knox's", "Dr. Maria Siemionow,", "Hong Kong's Victoria Harbor", "hot and humid and it rains almost every day of the year.", "Garth Brooks", "the two-state solution to the Mideast conflict,", "more than 1.2 million", "Michelle Obama", "around 3.5 percent of global greenhouse emissions.", "off the coast of Dubai", "died in the Holmby Hills, California, mansion he rented.", "Olympic medal", "the \" Michoacan Family,\"", "The forward's lawyer", "New York", "South Carolina Republican Party Chairwoman Karen Floyd", "stand down.", "a bow bridge", "Andy Serkis", "Nicolas Anelka", "the chest", "1987", "helps scientists better understand the spread of pollution around the globe", "to turn our will and our lives over to the care of God", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "Gary Puckett", "birds", "Saturday Night Live", "Racing Cars", "Mujib", "Peru", "dogs", "pomegranate", "The Deep Blue Sea", "Indiana University", "University of Texas Longhorns", "Galo (], \"Rooster\")", "Vernier, Switzerland", "Patrick Dempsey and Amanda Peterson", "Indian", "A play-by-post role-playing game", "\"E Eleanor Rigby\"", "auctions", "La Bohme", "Maude", "Prince Edward Island", "Chief Justice of the Supreme Court", "beef", "Bach"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6764588041656057}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.8235294117647058, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8750000000000001, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.24000000000000002, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 0.888888888888889, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2526", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1909", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-1229", "mrqa_searchqa-validation-13433", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-7088", "mrqa_searchqa-validation-13505"], "SR": 0.53125, "CSR": 0.52197265625, "EFR": 1.0, "Overall": 0.70845703125}, {"timecode": 64, "before_eval_results": {"predictions": ["July 18, 1994,", "had the surgery December 13 after lumpectomies failed to eradicate her breast cancer.", "\"E! News\"", "1,073", "a rifle and began firing.", "acid", "concerns that the legislation will foster racial profiling,", "have", "Jewish civil rights activists", "July 4.", "different women coping with breast cancer", "the tune.", "concentration camps,", "Atlantic Ocean.", "heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe.", "The ship", "Phoenix,", "not be allowed to use the words \"explosive device\" or \"bomb\"", "Jacob", "Former detainees of Immigration and Customs Enforcement", "Republican", "Darrel Mohler", "United States, NATO member states, Russia and India", "her boyfriend,", "12-hour-plus shifts of backbreaking labor,", "former CEO of an engineering and construction company", "Madonna", "in the head", "Sen. Barack Obama", "Tuesday,", "WILL MISS YOU! NOT WORmik.", "\"Stagecoach\" (John Ford, 1939)", "larger parts", "Speaker of the House of Representatives", "the world's second most populous country after the People's Republic of China", "to express importance, honor, and majesty", "in teaching elocution", "Randy Watson", "non-ferrous", "spinal cord", "phil Lynott", "Here Comes the bride", "Mars", "a jumper", "Department of Justice", "Uganda", "the large valley", "South Dakota", "the Big 12 Conference in the National Collegiate Athletic Association (NCAA).", "Christopher McCulloch", "Argentine Americans", "\"Waiting for Guffman\"", "villanelle", "2011", "I Am Furious (Yellow)\"", "Liam Cunningham", "red", "Donkey Kong", "oxygen", "the Thirty Years' War", "Massachusetts", "'Arc 'Azur", "the New Testament", "the uvula"], "metric_results": {"EM": 0.390625, "QA-F1": 0.483415913829033}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.11764705882352941, 0.0, 0.06060606060606061, 1.0, 0.2105263157894737, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3859", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-5234", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-5386", "mrqa_triviaqa-validation-2049", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-1848", "mrqa_searchqa-validation-6072", "mrqa_searchqa-validation-8827", "mrqa_searchqa-validation-16606", "mrqa_searchqa-validation-2744"], "SR": 0.390625, "CSR": 0.5199519230769231, "EFR": 0.9743589743589743, "Overall": 0.7029246794871795}, {"timecode": 65, "before_eval_results": {"predictions": ["three", "four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist", "Spc. Megan Lynn Touma,", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "death squad killings", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "She is the Magneto to my Wolverine, the Saruman to my Frodo, the Dr. Octopus to my Spiderman.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Israel's vice prime minister Silvan Shalom", "Mexico", "$250,000 for Rivers' charity: God's Love We Deliver.", "Negotiators aboard a U.S. Navy warship are trying to secure the release of an American freighter captain who is being held by pirates on a lifeboat off the coast of Somalia,", "Muslim", "\"persistent pain.\"", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville", "800,000", "Two other boys ages 13 and 15 have been charged as juveniles.", "Nineteen political", "Paul Schlesselman of West Helena, Arkansas,", "Tim Masters,", "Cash for Clunkers", "immediate release", "Deputy Treasury Secretary", "The federal officers' bodies", "I hope people look at the content of the speech, not just the delivery.", "My family comes from a Muslim background, and we're not defined by religion,\"", "volatile zone along the equator between South America and Africa.", "Anil Kapoor", "former general secretary of the Communist Party,", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "some in the audience had fought in Afghanistan, and some would be deployed in the future.", "Robert Barnett,", "near major hotels and in the parking areas of major Chinese supermarkets", "Lucius Verus", "2018", "23 September 1889", "The National Legislature was moved to Washington prematurely, at the urging of President John Adams, in hopes of securing enough Southern votes in the Electoral College to be re-elected for a second term", "nearby", "March 26, 1973", "100,000", "commander Flashheart", "the Zulu warriors", "ad nauseam", "Strictly Come Dancing", "adnams", "the first discovery of gold at Ophir", "Charlie Chan", "woodentops", "2005", "British Bristol Olympus turbojet", "Indian club", "Republican", "paracyclist", "pubs, bars and restaurants", "Venice", "Albert Park", "\"legal tort\"", "Willy Ronis", "Maine", "Toronto", "17th", "&", "Anne", "the eardrum"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6111921145679369}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.5, 1.0, 1.0, 0.07407407407407407, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8421052631578948, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-2424", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2330", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-1376", "mrqa_hotpotqa-validation-4977", "mrqa_searchqa-validation-3309", "mrqa_searchqa-validation-11525", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-11877", "mrqa_searchqa-validation-3776"], "SR": 0.515625, "CSR": 0.5198863636363636, "EFR": 0.967741935483871, "Overall": 0.7015881598240469}, {"timecode": 66, "before_eval_results": {"predictions": ["William Harold \"Bill\" Ponsford", "The British traders' name for the route was derived from combining its name among the northeastern Algonquian tribes, \"Mishimayagat\" or \"Great Trail\", with that of the Shawnee and Delaware,", "The southernmost large city in Europe, it lies on the Costa del Sol (\"Coast of the Sun\") of the Mediterranean, about 100 km east of the Strait of Gibraltar and about 130 km north of Africa.", "a poll of policymakers by the Bangor Daily News ranked Millett as the ninth most influential person in Maine politics.", "Denmark\u2013Norway", "\u00c6thelwald Moll", "Glam metal", "Mitsubishi", "\"Lend a hand \u2014 care for the land!\"", "Marc Bolan", "Mineola", "lo Stivale", "Arizona State University.", "Jack Thomas Chick", "Milk Barn Animation", "Green Chair", "The iPod Classic", "emperor caligula", "6 February 1699", "Fulham", "duck", "Jay Chou", "July 14, 2009", "1837", "2009", "October Sky", "Tianhe Stadium", "Louis Silvie \"Louie\" Zamperini", "$26 billion", "five", "Alain Robbe-Grillet", "Winter Haven", "2007", "Felicity Huffman", "a flash music video featuring an animated dancing banana", "the churches of Galatia '' ( Galatians 1 : 2 )", "Unwinding of DNA at the origin and synthesis of new strands, accommodated by an enzyme known as ligase, results in replication forks growing bi-directionally from the origin", "Orwell", "Angola", "Austria - Hungary", "Prince Edward Island", "Usain Bolt", "York", "(Thomas) Shadwell", "marbles", "Philippines", "king george vi", "pabbingdon", "1981", "no evidence", "Dolgorsuren Dagvadorj,", "July", "Ronaldinho", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "a review of state government practices completed in 100 days.", "the abduction of minors.", "(Jose) San Martin", "a pantaloon", "(Giacomo) Puccini", "a bore", "Cervantes", "the palace of the Tuileries", "the Orioles", "a risk"], "metric_results": {"EM": 0.5, "QA-F1": 0.6313670592159277}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0625, 0.19047619047619047, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.20689655172413793, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-3893", "mrqa_hotpotqa-validation-1605", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-9670", "mrqa_triviaqa-validation-5110", "mrqa_triviaqa-validation-7416", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-1941", "mrqa_searchqa-validation-15416", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-8137"], "SR": 0.5, "CSR": 0.519589552238806, "EFR": 1.0, "Overall": 0.7079804104477612}, {"timecode": 67, "before_eval_results": {"predictions": ["Sicily", "Diego Maradona", "tennyson", "a Rear-Admiral of the Navy", "Bangladesh", "Mayflower", "Union of Post Office Workers", "minder", "pongo", "Wensum", "Ottawa", "Taiwan", "a pitcher", "rhinos", "evelyn glennie", "new zealand", "jack-the-lad barker", "algiers", "land between two rivers", "narcolepsy", "Carl Smith", "Bombay", "jump jump", "Cubism", "Pink Floyd", "Brian Deane", "Kinshasa", "Dublin", "agincourt", "crested weir", "Ever Decreasing Circles", "heptathlon", "mid-ocean ridges, where new oceanic crust is formed through volcanic activity and then gradually moves away from the ridge", "a maritime signal, indicating that the vessel flying it is about to leave", "in 2003", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "average speed including halts 12049 / 50 : Agra Cantonment - H. Nizamuddin Gatimaan Express", "Pete Maravich", "a cheque guaranteed by a bank, drawn on the bank's own funds and signed by a cashier", "Kody and his first wife Meri", "Delphine Software of France", "Taeko Ikeda", "MTV Russia", "1874", "2006", "11", "the sarod", "National Hockey League", "Jaime Andrade", "to kill members of the Zetas", "Patrick McGoohan,", "more than 30", "her father's home in Satsuma, Florida,", "Friday", "images of the small girl being sexually assaulted.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "Johns Hopkins", "Cardiff", "Norah Jones", "a possums", "Helsingor", "The X-Files", "Rita Mae Brown", "the Civil War"], "metric_results": {"EM": 0.5, "QA-F1": 0.5996130554494525}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.0, 0.35294117647058826, 1.0, 0.14285714285714288, 0.2857142857142857, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.5, 0.25, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-4641", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-2674", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-4538", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-5587", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-2117", "mrqa_searchqa-validation-1152", "mrqa_searchqa-validation-9901"], "SR": 0.5, "CSR": 0.5193014705882353, "EFR": 1.0, "Overall": 0.7079227941176471}, {"timecode": 68, "before_eval_results": {"predictions": ["Martin Aloysius Culhane", "our sincerity", "Indonesian", "a mammoth", "she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.", "the United States is not and will never be at war with Islam.", "hackers", "Formula One world champion Michael Schumacher", "Matthew Fisher", "Dogpatch Labs Europe", "San Diego,", "Technological Institute of Higher Learning of Monterrey,", "10", "\"Dancing With the Stars.\"", "a one-shot victory in the Bob Hope Classic", "3-2", "78,000 parents of children ages 3 to 17.iReport.com:", "machine guns and two silencers", "\"The Screening Room\"", "the death of a pregnant soldier", "16", "Anil Kapoor", "2-1", "CNN", "Joan Rivers", "on the bench", "actress", "only one", "the college campus.", "Kerstin", "1979", "Phillip A. Myers.", "pulmonary heart disease", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "205 lb ( 93 kg ) and under", "Natya Shastra", "American Indian allies", "Bulgaria", "Tom Hanks", "the Seton Hall Pirates men's basketball team", "l Lancashire", "leeds", "south africa", "leeds", "south africa", "vote", "salmon", "Hungarian", "StubHub Center", "Jaguar Land Rover Limited", "Sutton Hoo", "Eminem", "Price's", "Buffalo", "FAI Junior Cup", "2013", "walrus", "inamona", "rapier", "Newton's", "salam", "the hip", "Japan", "Alive"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6481048669467786}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.1, 0.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5882352941176471, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-1743", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2906", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-805", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-57", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-436", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-2043", "mrqa_searchqa-validation-16356", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-8683", "mrqa_searchqa-validation-12454"], "SR": 0.53125, "CSR": 0.5194746376811594, "EFR": 1.0, "Overall": 0.7079574275362319}, {"timecode": 69, "before_eval_results": {"predictions": ["10 Afghan police officers", "Molotov cocktails, rocks and glass.", "rwanda", "more than 100", "Bailey, Colorado,", "Sri Lanka", "Hanin Zoabi,", "Six", "Immigration Minister Eric Besson", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "Iran", "education", "cut off his ear and a finger.", "to review their emergency plans and consider additional security measures", "a female soldier,", "Burhanuddin Rabbani,", "African-Americans", "a place for another non-European Union player in Frank Rijkaard's squad.", "1979", "Congress", "Kim Clijsters", "four months ago,", "Pfc. Bowe Bergdahl", "5-0,", "hundreds", "they're very or somewhat scared about the way things are going in the United States.", "a Royal Air Force helicopter", "$1.5", "about 75 miles east of Yakima", "Brian David Mitchell,", "The Stooges", "allegedly faking a doctor's note", "November 2, 2016", "Nepal", "a English expression meaning `` mind your manners ''", "the name America", "the fourth quarter of the preceding year", "a substance that fully activates the receptor that it binds to )", "for the 1994 season", "Tbilisi, Georgia", "the principal vein of a leaf", "new zealand", "Richard Seddon", "the fallopian tube", "Barry Perry", "france", "\"Holiday Inn\"", "Cedella Marley", "University of Texas at Austin", "Robbie Gould", "Patrick Swayze", "Frank Wentz", "Netrobalane canopus", "Indian classical", "32", "Jocelyn Moorhouse", "Hawaii", "Beethoven", "aardwolf", "ceviche", "Peter Bogdanovich", "the cunning policies set forth", "Michael Phelps", "Milton Glaser"], "metric_results": {"EM": 0.546875, "QA-F1": 0.645354973973395}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9473684210526316, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-1997", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2540", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-8928", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-4557", "mrqa_triviaqa-validation-4913", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-3761", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-2060", "mrqa_searchqa-validation-8460"], "SR": 0.546875, "CSR": 0.5198660714285714, "EFR": 0.9655172413793104, "Overall": 0.7011391625615764}, {"timecode": 70, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-816", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4033", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-12126", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10068", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-2591", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-3723", "mrqa_squad-validation-3745", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-486", "mrqa_squad-validation-512", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5348", "mrqa_squad-validation-5362", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6595", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-689", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-816", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9408", "mrqa_squad-validation-96", "mrqa_squad-validation-9845", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7217", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.810546875, "KG": 0.4734375, "before_eval_results": {"predictions": ["September 13, 2012", "Hank Williams", "September 2, 1945", "Lord Banquo", "King Louie", "Waylon Jennings", "Robert Irsay", "18 episodes", "Kate '' Mulgrew", "Castleford", "October 2012", "Sammi Smith", "2018", "Lake Powell", "in the namesake town of Manchester - by - the - Sea, Massachusetts", "South Dakota", "12 times", "Billy Gibbons", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2", "the southeastern United States", "the sperm and one from the egg", "committed suicide", "2002", "the problems and / or goals", "The Pardoner's Tale", "1956", "the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "the largest part of the brain", "semi-automatic, but not fully automatic", "V\u1e5bksayurveda", "1990", "John Hurt", "Cyprus", "George W. Bush", "Durham", "snakes", "weetabix", "The Hague", "Nigeria", "Harry F. Sinclair", "Russian", "1903", "motor vehicles", "2018", "pastels", "the Saint Petersburg Conservatory", "middleweight", "\"have no problems about the school, they are happy about everything.\"", "CNN", "FBI's Baltimore field office", "ordered the release of the four men", "\"Dancing With the Stars.\"", "228", "22-10.", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "(Peter) Rubens", "Neptune", "( Bugsy) Siegel", "(Albert) Brooks", "Wang Chung", "(Robert) Calder", "France", "the pound"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6272840007215007}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4545454545454545, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5, 1.0, 0.1111111111111111, 0.16666666666666666, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-7651", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-6050", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-2700", "mrqa_searchqa-validation-14226", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12794"], "SR": 0.515625, "CSR": 0.519806338028169, "EFR": 0.9354838709677419, "Overall": 0.6853549167991823}, {"timecode": 71, "before_eval_results": {"predictions": ["Norway", "Morocco", "Casablanca", "Geneva Conventions", "Classics", "New Jersey", "Lake Baikal", "salt", "badger", "Kuiper Belt", "a cat", "Mimi Bobeck", "winter", "nag", "Ned", "Boston Red Sox", "\"The Smoke Filter for Locomotives\"", "Ohio", "pantomime", "halfpipe", "Samuel", "Apple", "Moscow", "automobiles", "a Chin", "Columbo", "William McKinley", "Hannibal", "Ankara", "The Deep", "Orangutans", "\"Death, be not\"", "Anatomy", "16", "8 bytes", "China", "more than 1,000", "Lee Mack", "October 29 - 30, 2012", "Andreas Vesalius", "Hawaii", "oropendola", "neurons", "work", "minder", "potatoes", "hickory", "antelope", "over 281", "44", "Rockbridge County", "Freeform", "1965", "200,167", "Harvard", "Martin Ingerman", "two remaining missing men.", "Sonia Sotomayor", "\"the most beautiful woman in the world.\"", "staff sergeant in the U.S. Air Force", "1913.", "Kim Il Sung", "Ricardo Valles de la Rosa,", "Tuesday"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-7372", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-7819", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-4240", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-6671", "mrqa_triviaqa-validation-950", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-7120", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-4321", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-3848"], "SR": 0.609375, "CSR": 0.5210503472222222, "EFR": 1.0, "Overall": 0.6985069444444445}, {"timecode": 72, "before_eval_results": {"predictions": ["alloys", "Judgment City", "Lemmon", "\"Spanish Ladies\"", "Charley", "Microsoft", "Beethoven", "Latin", "high jump", "Nicole Kidman", "cork", "Fort Sumter", "Bourn2", "the Sahara", "Happy Days", "Mentor", "Vanna White", "Morris West", "trod", "Frank Sinatra", "Green Lantern", "infrared", "Pompey", "the Nome Nugget", "Jungle Jim", "St Martin", "Jerry Reed", "Jean-Paul Sartre", "Michelle Kwan", "a taco", "Baal", "Westinghouse Electric Corporation", "gastrocnemius muscle", "prophets", "Blue laws", "an integrated plan of production established by a state or other organizational body that controls the factors of production", "Anna Faris", "charbagh", "Shirley Mae Jones", "Barry Bonds", "Colonel Tom Parker", "Princess Margaret Rose of York", "three", "Mad Hatter", "Barbarella", "pentecost", "Nissan", "Australia", "nuclear weapons", "Atlas ICBM", "William Bradford", "Tim Allen", "Rochdale, North West England", "postmodern schools", "Netherlands", "Romeo", "UC Irvine Medical Center,", "misdemeanor assault charges", "education", "12-1", "working with the National Guard in Fargo to fill those sandbags", "Kris Allen,", "north-south highway", "one-shot victory"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6428254585326953}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 0.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-12170", "mrqa_searchqa-validation-9180", "mrqa_searchqa-validation-12784", "mrqa_searchqa-validation-3016", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-15982", "mrqa_searchqa-validation-16034", "mrqa_searchqa-validation-613", "mrqa_searchqa-validation-13204", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-1941", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-2478", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-212", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-2858"], "SR": 0.578125, "CSR": 0.521832191780822, "EFR": 1.0, "Overall": 0.6986633133561645}, {"timecode": 73, "before_eval_results": {"predictions": ["Terry McAuliffe", "an armadillo", "Daniel Boone", "Bigfoot", "the rupee", "Cold Mountain", "Frisbee", "neighbor", "an artistic gymnastics apparatus", "Queen Victoria", "a cave space", "Newton", "Ramayana", "Beauty and the Beast", "the cherry juice", "The Laughstack fast", "a lime", "The French Line", "an interval", "the Old Manse", "the Caribbean", "Tadpoles", "Fermium", "an accomplice-witness", "an aerie", "Woody Guthrie", "the Arawak Indians", "the Clydesdales", "fontanel", "Churchill", "defense", "Herman Melville", "upon a military service member's retirement", "1924", "1926", "the Veterans Committee", "Dumont d'Urville Station", "Gwendoline Christie", "Cambridge May Ball scene", "the British colonial government", "China", "\"His Holiness.\"", "east coast", "peter", "davy", "Bodhidharma", "Lincolnshire", "Marion Cotillard", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "England", "the villanelle poetic form", "7 November", "1966", "Paper", "New York Shakespeare Festival", "Ribhu Dasgupta", "12 hours in jail.", "Islamic republic's alleged efforts to acquire nuclear weapons", "potential revenues from oil and gas", "Stephen Tyrone Johns", "around 3.5 percent of global greenhouse emissions.", "Elin Nordegren,", "Tuesday", "Ben Roethlisberger"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6089337269853974}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3478260869565218, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.125, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.45454545454545453, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352941, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11850", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-15440", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3770", "mrqa_searchqa-validation-12722", "mrqa_searchqa-validation-1486", "mrqa_searchqa-validation-3732", "mrqa_searchqa-validation-12588", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-3524", "mrqa_naturalquestions-validation-923", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3399", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-95", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3887"], "SR": 0.5625, "CSR": 0.5223817567567568, "EFR": 0.9285714285714286, "Overall": 0.6844875120656371}, {"timecode": 74, "before_eval_results": {"predictions": ["Pacific Grove", "Phillip Paley", "`` Entropy ''", "classical architecture", "Darlene Cates", "Michael Madhusudan Dutta", "an active supporter of the League of Nations", "16 November 2001", "September 19 - 22, 2017", "November 1999", "on the inner wall of the pedestal", "mining", "April 1, 2016", "after World War II", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Spanish missionaries, ranchers and troops", "Gare du Nord", "John Travolta", "1980", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located )", "Butter Island off North Haven, Maine in the Penobscot Bay", "December 1886", "the end of the year on 29 December 1999", "the information and experiences that are directed towards an end - user or audience", "Nalini Negi", "a type of appropriations legislation", "Lesley Gore's", "a series of newsreel films depicting multiple alternative realities rather than a novel", "September 8, 2017", "a series of punitive laws", "American production duo The Chainsmokers", "British rock band Procol Harum", "Tahrir Square", "Heather Stanning", "bonny", "Slim Whitman", "ely", "dioscuri", "genus Castor", "gasoline", "Hanford Site", "Edward Trowbridge Collins Sr.", "over 1.6 million", "Paul W. S. Anderson", "Roscoe Lee Browne", "dice", "Clara Petacci", "Charice", "Citizens", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "off Somalia's coast.", "Daniel Radcliffe", "Pakistan", "in a motel,", "his mother, Katherine Jackson, his three children and undisclosed charities.", "intention to set up headquarters in Dublin.", "Winston Churchill", "Ricky Martin", "Mark Twain", "San Salvador", "Amelia Earhart", "ethanol", "8039 Beach Blvd.", "the pituitary gland"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6796454112011178}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.6153846153846154, 1.0, 1.0, 0.2, 0.16666666666666669, 1.0, 0.17391304347826086, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-138", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-9986", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-2653", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-299", "mrqa_triviaqa-validation-283", "mrqa_triviaqa-validation-3294", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-5521", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-192", "mrqa_searchqa-validation-14807"], "SR": 0.578125, "CSR": 0.5231250000000001, "EFR": 0.9629629629629629, "Overall": 0.6915144675925926}, {"timecode": 75, "before_eval_results": {"predictions": ["North Atlantic Ocean", "Dmitri Mendeleev", "Tagore", "charbagh", "Amerigo Vespucci", "2018", "at the head of Lituya Bay in Alaska", "0.30 in ( 7.6 mm )", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "the breast or lower chest", "May 2017", "Brian Steele", "around 2011", "19 June 2018", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "Sumitra", "sport utility vehicles", "If a vehicle towing a trailer skids", "Todd Griffin", "Julie Adams", "FIGG Bridge Engineers", "Turducken", "synovial joint", "Hallertau in Germany", "Billy Idol", "2017", "2020", "August Darnell", "International System of Units", "Stevie Wonder", "a political ideology", "New York City", "Trimdon, County Durham", "zelle", "green", "49", "charles", "eros", "Robinson", "Chile", "1875", "Brendan O'Brien", "33 of the 100 seats", "London", "Mitsubishi Motors", "John McClane", "Adelaide", "Exit 82", "attracted some U.S. senators", "380,000", "Spc. Megan Lynn Touma,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "Lindsey Vonn", "to do more to stop the Afghan opium trade", "Cal Ripken Jr.", "comments he made last night at the Annual Caddy Awards dinner in Shanghai,\"", "Byron", "a pythons", "Odin", "Pop art", "Greed", "a blubber", "Kinsey Millhone", "Romanov"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6489814247070439}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.2105263157894737, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-1955", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6288", "mrqa_triviaqa-validation-2195", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-404", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-620", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-4241", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-15046"], "SR": 0.5625, "CSR": 0.5236430921052632, "EFR": 0.9285714285714286, "Overall": 0.6847397791353383}, {"timecode": 76, "before_eval_results": {"predictions": ["several weeks,", "involvement during World War II in killings at a Nazi German death camp in Poland.", "the Ku Klux Klan", "Filippo Inzaghi", "The National Infrastructure Program,", "Silicon Valley.", "Booches Billiard Hall,", "Liverpool,", "Tribune Media Services", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "The Ministry of Defense said the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Afghan National Security Forces", "Nechirvan Barzani,", "planned attacks", "$273 million", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "77-year-old Oscar winner", "Brian Smith", "Brazil", "she supports the two-state solution to the Mideast conflict,", "$3 billion,", "The Stooges", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "Dan Brown", "because the Indians were gathering information about the rebels to give to the Colombian military.", "mental health", "Annie Duke", "570 billion pesos ($42 billion)", "India", "response to a civil disturbance call,", "November 17, 1800", "legislation", "concerned with all legal affairs", "Giancarlo Stanton", "A status line", "David Ben - Gurion", "18", "TC", "antelopes", "Mikhail S. Gorbachev", "14", "Ravi Shankar", "Baton Rouge", "Abe Reles", "a pommel horse", "Mussolini", "the Hebrides", "August 21, 1995", "1966", "Algernod Lanier Washington", "Highwaymen", "Michael Rispoli", "Middlesbrough", "Brenton Thwaites", "adhesion", "a crumpet", "the Potomac", "dinoflagellates", "Double Indemnity", "glengarry", "The _______ Family", "a penny"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6120177113102325}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.23076923076923078, 0.8947368421052632, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.5, 0.125, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1711", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-1452", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-4623", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-2003", "mrqa_searchqa-validation-3417", "mrqa_searchqa-validation-12186", "mrqa_searchqa-validation-711"], "SR": 0.53125, "CSR": 0.5237418831168832, "EFR": 0.9666666666666667, "Overall": 0.69237858495671}, {"timecode": 77, "before_eval_results": {"predictions": ["Dr. Jennifer Arnold and husband Bill Klein,", "David Beckham", "Alexandre Caizergues,", "jobs", "Cadillac, Buick and GMC.", "al Qaeda member", "debris", "1959,", "lower house of parliament,", "Six members", "East Java", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "psychotropic drugs", "1", "NASCAR", "executive director of the Americas Division of Human Rights Watch,", "jazz", "Mark Fields", "Jeanne Tripplehorn's", "FBI Special Agent Daniel Cain,", "the commissions are OK, \"provided that they are properly structured and administered.\"", "Peshawar", "Daytime Emmy Lifetime Achievement Award.", "Jewish", "President Obama's", "Woosuk Ken Choi,", "forcibly injecting them with psychotropic drugs", "Almost all", "Filippo Inzaghi", "Greeley, Colorado,", "building bombs,", "\"E! News\"", "Sophia Akuffo", "North Carolina", "Los Lonely Boys", "LED illuminated display", "a pagan custom, namely, the winter solstice which in Europe occurs in December", "1966", "1985 -- 1993", "sedimentary rock", "sedge", "Chiropractic", "Polish", "To Kill a Mockingbird", "eukharisti\u0101", "Samuel Johnson", "makes", "two", "the Knight Company", "Ferdinand Magellan", "23 March 1991", "the 34th President of the United States", "154 days", "seven", "\"I, D'oh- Bot\"", "Scottish", "Maksim Chmerkovskiy", "Captain Nemo", "Salvador Dali", "Arnold Schwarzenegger", "James Watt", "Mercury and Venus", "snakes", "the Scripps National Spelling Bee"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6703590029761906}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.125, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6875000000000001, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.09999999999999999, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-2933", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-3749", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2281", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-3659", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12187"], "SR": 0.5625, "CSR": 0.524238782051282, "EFR": 0.9642857142857143, "Overall": 0.6920017742673993}, {"timecode": 78, "before_eval_results": {"predictions": ["a micronutrient-rich diet", "Smart, stubborn and willful", "August 14, 1848", "American Horror Story", "Daniel Craig", "The Zebras", "c. c. Beck", "Mechanicalte Navstrechu", "Gainsborough Trinity", "Carlos Santana", "DreamWorks Animation", "40 Acres and a Mule Filmworks", "Fort Bragg", "Division I Football Bowl Subdivision", "Bob Iger", "The Gang", "A play-by-post role-playing game", "Newcastle upon Tyne, England", "Stephen Ireland", "Rooster", "27 January 1974", "second cousin once removed", "James Dean", "Jennifer Taylor", "lost in a raft for 47 days after his bomber crash landed in the ocean during World War II", "Illinois", "Twitch Interactive, a subsidiary of Amazon.com", "1853", "Coalwood", "Sarajevo", "Gareth Barry", "The Ryukyuan people", "in the west by the east coast of Queensland, thereby including the Great Barrier Reef, in the east by Vanuatu ( formerly the New Hebrides ) and by New Caledonia", "Samantha Jo `` Mandy '' Moore", "a patronymic surname, which arose separately in England and Wales", "Duisburg", "in the middle of the 15th century, in Yemen's Sufi monasteries", "the 4th century", "between 1765 and 1783", "President", "Another Day in Paradise", "Ronnie Carroll", "North by Northwest", "umbrellas", "Paris", "stomach", "wren", "ArcelorMittal Orbit", "near Georgia, from the east and Stevens county, to the west in Paulding and Carroll and Douglas, and in Cherokee in northwest Georgia.", "82 passengers", "appealed against the punishment for the player", "Graeme Smith", "then-presidential candidate Barack Obama,", "It has not been identified.", "in the Bronx and grew up in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "eco videos", "cosmetology", "the Knickerbacker name", "\"How the firebrand Shi'ite cleric became a major...", "Kentucky", "Tigger", "Giovanni Bertati", "Ariel Sharon", "\"What a joy to breathe the balmy air of Grosvenor Square\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5568492965367965}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 1.0, 0.8, 1.0, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.3333333333333333, 0.0, 0.0909090909090909, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4909", "mrqa_hotpotqa-validation-4476", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4791", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-106", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-2138", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-6", "mrqa_searchqa-validation-10111", "mrqa_searchqa-validation-6928", "mrqa_searchqa-validation-9708", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-13464"], "SR": 0.453125, "CSR": 0.5233386075949367, "EFR": 1.0, "Overall": 0.6989645965189875}, {"timecode": 79, "before_eval_results": {"predictions": ["St. Louis", "Warrington, Florida", "Pope Sergius III", "Muskogean", "Lt. Col. Masahiko Takehita", "English", "Carlos Santana", "Dana Fox", "7 January 1936", "The Onion", "Portsea", "Lapland", "1698", "Cuban descent", "2010", "California", "is a well-known historical figure in 16th-century Irish history.", "Ouse and Foss", "Thrushcross Grange", "Marijus Adomaitis", "My Boss, My Teacher", "York County", "Grave Digger", "a mermaid", "Europop", "Humberside", "Viaport Rotterdam", "Yasir Hussain", "566", "Cookstown", "YIVO", "Kairi", "Database - Protocol driver ( Pure Java driver )", "the head coach of the Philadelphia Eagles of the National Football League ( NFL )", "Phoebe ( MacKenzie Mauzy )", "Malayalam", "1991", "Lyndon B. Johnson", "a maritime signal, indicating that the vessel flying it is about to leave", "18 September", "a lancehead", "frottage", "polo", "gogglebox", "white", "12", "the Tower of London", "ptolemy Philadelphus", "38", "hanged in 1979", "Karen Floyd", "Canada.", "Vivek Wadhwa,", "South Carolina Republican Party Chairwoman Karen Floyd", "the \"fusion teams,\"", "another high tide", "out of body", "an F-22", "Wyoming", "a ghost", "the Hudson River", "Peter the Great", "the Taliban", "James Naismith"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6841031925590748}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-946", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-2012", "mrqa_hotpotqa-validation-4847", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8603", "mrqa_triviaqa-validation-201", "mrqa_triviaqa-validation-6426", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-3825", "mrqa_newsqa-validation-4058", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-1570"], "SR": 0.609375, "CSR": 0.5244140625, "EFR": 1.0, "Overall": 0.6991796875}, {"timecode": 80, "UKR": 0.66015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9073", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3317", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.787109375, "KG": 0.42421875, "before_eval_results": {"predictions": ["Parthenon", "mrs peter bowker", "6", "mrs mrs vii", "bougament", "Venice", "architect", "bobby mrs mrsrs", "Chief Inspector of Prisons", "Some Like It Hot", "mrs", "fire-in-effigy", "russia", "mrs aniley", "kirkies", "Norway", "mrs riken", "1890", "mrs mrsrs", "peter mcckelson", "mrs mrs", "mrs mccellen", "mrs mrs gracknell", "Camellia sinensis", "binky", "olivology", "Bolivia", "gold", "mrs mrs Plato believed that the elements were shaped like the platonic solids", "Denver", "mrs and guests would sit at a large dinner table to have a leisurely talk and discuss the latest events while drinking hot tea", "lilliputians", "Peter Brooks", "Mike Nesmith", "Gloria ( Lisa Stelly )", "client that was first developed and popularized by the Israeli company Mirabilis in 1996", "two", "pigs", "Janie Crawford, an African - American woman in her early forties", "while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "$60 million", "Alan David Sokal", "Square Enix", "1955", "Alexandre Dumas", "peter & Gordon", "Little Dixie", "8-track cartridge, an audio tape system", "$17,000", "there is not a mechanism at the federal level to ensure that drivers comply.", "think we have to rely on having a clinical breast exam once a year at a health care provider and doing your self-breast exam on a monthly basis.", "forgery and flying without a valid license,", "700", "38 feet Sunday afternoon.", "five years in jail", "bicycles", "ethanol", "Florida", "Massachusetts", "UFOs", "Voltaire", "the Dan", "aperire", "Prince William"], "metric_results": {"EM": 0.375, "QA-F1": 0.4312222933316683}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0909090909090909, 0.0, 0.0, 0.0, 0.8, 0.125, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.15384615384615385, 0.07142857142857144, 0.2857142857142857, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-6806", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-5545", "mrqa_triviaqa-validation-1980", "mrqa_triviaqa-validation-259", "mrqa_triviaqa-validation-2530", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-3793", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-2658", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-2083", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-1801", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-1142", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1742", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-15708"], "SR": 0.375, "CSR": 0.5225694444444444, "EFR": 0.95, "Overall": 0.6688107638888889}, {"timecode": 81, "before_eval_results": {"predictions": ["a catalytic converter", "c Cecil Beaton", "Clara wieck", "Xenophon", "carpathia", "Portugal", "Warren Commission", "perfume", "bennington", "Hall of Faith", "throw", "Wars of the Roses", "Lew hoad", "albert reynolds", "al jazeera", "m69", "Bayern", "thrombophlebitis", "Lorelei", "mrs spillane", "sir Walter Scott", "Shayne Ward", "French Guiana", "Vienna", "Amsterdam", "pickled peppers", "troposphere", "Paris", "Spain", "Arizona Diamondbacks", "little jack Horner", "mayflower", "the earlier national arms", "Have I Told You Lately", "in the New Testament", "Woody Paige", "17th Century", "February 27, 2015", "glycine and arginine", "the right", "Jacksonville", "January 2004", "The More", "Adelaide Lightning", "Sun Valley, Idaho", "Donald Sterling", "Richard Arthur", "Retina display", "the giant mega-yacht 'Wally Island'", "an \"unnamed international terror group\"", "her decades-long portrayal of Alice Horton on", "the death of a pregnant soldier", "iCloud service", "nearly $162 billion in war funding", "\"Mad Men\"", "$273 million", "a battery", "plaque", "Brownsville", "the two first", "Frank Sinatra", "Tasmania", "the Bodleian", "Ivy Dickens"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7736742424242424}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-402", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-2787", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-686", "mrqa_hotpotqa-validation-1220", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-162", "mrqa_searchqa-validation-10324", "mrqa_searchqa-validation-8274", "mrqa_searchqa-validation-9043"], "SR": 0.703125, "CSR": 0.5247713414634146, "EFR": 0.9473684210526315, "Overall": 0.6687248275032093}, {"timecode": 82, "before_eval_results": {"predictions": ["buxton", "mikado", "Yellowstone", "genesis hanks", "livery", "almond", "Salvation Army", "bali", "0", "vignale", "john of gaunt", "Towcester", "the Benedictine Order", "Trinity College, Cambridge", "evolution", "Cuba", "1", "15", "1964", "Tennessee", "architecture", "hiva", "Howard Keel", "lanka", "USS Missouri", "o", "Lithium", "king edward", "Colleen McCullough", "ilie nastase", "crian chiles", "mike", "1439", "Africa", "Amanda Leighton", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "the coccygeal nerve", "delta basin", "36 months", "Holly Marie Combs", "Charles Otto Puth Jr.", "Al Horford", "Wonder Woman", "Greek Revival", "\u00f7", "Cannes Film Festival", "Dominican", "World War I", "health care reform", "resources", "genesis of Guns N' Roses, REM, The Red Hot Chili Peppers and The Sex Pistols.", "Kandi Burruss,", "\"stressed and tired force\" made vulnerable by multiple deployments,", "Somali-based", "sanctions", "Stanford University,", "blimps", "House", "a nakd", "a root", "the southern cross", "Indonesia", "Homo erectus", "Athol Fugard"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5454788267288266}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-1556", "mrqa_triviaqa-validation-3401", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-5081", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-2255", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-989", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-4649", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-2236", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-10249"], "SR": 0.46875, "CSR": 0.5240963855421688, "EFR": 0.9705882352941176, "Overall": 0.6732337991672572}, {"timecode": 83, "before_eval_results": {"predictions": ["Luigi Pirandello", "Len Deighton", "cotton", "tartarus", "albert Tyrol", "tartan", "Mark Darcy", "Uranometria", "m\u00fcley hac\u00e9n", "Nasdaq", "The Great Gatsby", "Poland", "geoffrey", "Operation (game)Operation", "\"Rarely is the question asked, is our children learning?\"", "mike Gatting", "australia", "Massachusetts", "Eldorado", "william p Powell", "Armageddon", "Purple Rain", "Sinclair Lewis", "Herman Wouk", "Silverstone Circuit near the village of Silverstone in Northamptonshire in England.", "k Keswick", "a toad", "runic", "blue", "john Nash", "roffrey plant", "Pocahontas", "during the 1890s Klondike Gold Rush", "5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Baltimore -- Washington metropolitan area", "Massillon, Ohio", "Robin", "while studying All My Sons by Arthur Miller", "1975", "wavelength \u03bb", "June 2, 2008", "Ronald Ralph \" Ronnie Schell\" Schell", "Estadio de L\u00f3pez Cort\u00e1zar", "George Adamski", "Bangor Air National Guard Base", "Gwyneth Paltrow, Ewan McGregor, Olivia Munn, Paul Bettany and Jeff Goldblum", "The Swatch Group", "Campbellsville", "the territory simmer because of British oil companies' efforts to drill off the northern coast of the islands.", "Alejandro Peralta Alvarez,", "Christopher Columbus", "1-1", "Philippines", "three out of four", "the surgical anesthetic propofol", "murder in connection with the death of a woman", "an avian influenza virus", "Aquitaine", "Bohr", "the Sicilian pizza", "the funny bone", "an ancestor chart", "Paul Crewe", "the Tigris"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6259019414451827}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.32558139534883723, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.125, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-2603", "mrqa_triviaqa-validation-4160", "mrqa_triviaqa-validation-6864", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-1903", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-837", "mrqa_searchqa-validation-4860", "mrqa_searchqa-validation-81", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-8422", "mrqa_searchqa-validation-9720"], "SR": 0.53125, "CSR": 0.5241815476190477, "EFR": 1.0, "Overall": 0.6791331845238096}, {"timecode": 84, "before_eval_results": {"predictions": ["Tony Blair", "green", "sam Mendes", "Operation Dynamo", "garfield series", "john martin", "spain", "candies-Bergere", "Messenger", "bikav\u00e9r", "Granada", "rugby", "oliver Twist", "whiskey", "alien", "Big Dipper", "ape", "Luigi Pirandello", "spain", "horseshoes", "Annie Leibovitz", "atomic kitten", "Venus", "Tommy Roe", "cuticle", "spain", "rings", "spain girls", "garfield school of architecture", "Sherlock Holmes", "maxilla", "Suez Canal", "Presley Smith", "in the blood to the liver", "The Impalas", "in southern Turkey, dividing the Mediterranean coastal region of southern Turkey from the central Anatolian Plateau", "Henry Selick", "Frankel", "Tulsa, Oklahoma", "full '' sexual intercourse", "Linda Ronstadt", "British romantic comedy", "Bonkyll Castle", "Peoria, Illinois", "Outstanding Lighting Design", "North Atlantic Conference", "12", "George M. Cohan", "\"The Beatles' music received a bit of a facelift for the show and we go 'Whoa, listen to that,'", "Taliban", "\"That's something I sensed. Deep in the core of Johnny there's a toughness.\"", "Tim Clark, Matt Kuchar and Bubba Watson", "over the", "early detection and helping other women cope with the disease.\"", "Michelle Rounds", "28", "400th anniversary", "London", "a hump", "Big Brother", "Tom Cruise", "a short circuit", "Katherine Heigl", "a chimp"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6763415404040404}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.1, 1.0, 0.2222222222222222, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5562", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-6077", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-754", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-5458", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-8950", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4123", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-2915", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-15145"], "SR": 0.59375, "CSR": 0.525, "EFR": 0.9615384615384616, "Overall": 0.6716045673076924}, {"timecode": 85, "before_eval_results": {"predictions": ["Los Ticos", "\"probably from NORAD,\"", "\"I think she's wacko.\"", "Chevron", "Monday night", "citizenship to a man", "The Bronx County District Attorneys Office", "the 1950s,", "closure of Guant Bay prison and CIA \"black site\" prisons,", "flooding and debris", "tens of thousands of new voters", "tried", "45 minutes, five days a week.", "Nigeria,", "Los Alamitos Joint Forces Training Base", "10 below", "Mad Men's", "40 below", "Asashoryu", "she was held hostage by a still unidentified group of bandits.", "at checkposts and military camps in the Mohmand agency,", "China", "President Obama", "Mashhad", "Washington State's decommissioned Hanford nuclear site,", "Bollywood superstar", "16", "prisoners at the South Dakota State Penitentiary", "Thursday and Friday", "iPods", "3-2", "you love the environment and hate using fuel,\"", "AMX - 30", "The International System of Units ( SI )", "Nashville", "Chelsea ( 2009 -- 10 )", "her castle", "BC Jean and Toby Gad", "Brenda", "China", "Emily Davison", "Pompey", "new zealand", "bacterium", "jesse Charlie Hebdo attack", "business cycle", "Spanish", "stop motion effects", "FIFA Women's World Cup", "gamecock", "Alexandre Dimitri Song Billong", "40 million", "26,000", "1911", "Singha", "Dirk Werner Nowitzki", "Austin Powers", "Louisiana", "the iris", "actress", "Bob Dylan", "Princess Diana", "credit", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5839136904761905}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.2, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.08, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-341", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2395", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-9436", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-2511", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4680", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-1467", "mrqa_searchqa-validation-5024", "mrqa_searchqa-validation-14366"], "SR": 0.515625, "CSR": 0.524890988372093, "EFR": 0.9354838709677419, "Overall": 0.6663718468679669}, {"timecode": 86, "before_eval_results": {"predictions": ["australia", "heel", "pink", "jesie", "sam Cooke", "bette davis", "faversham", "coalbrookdale", "spain", "australia", "Mediterranean", "looking glass", "golf", "will carling", "Jonathan Swift", "spain feinstein", "tanzania", "ken Russell", "crows", "Enrico Caruso", "tara", "Morgan Spurlock", "spain", "spain", "Saturn", "castle of spaince", "Mi", "1879", "lizards", "Cosmos: A spacetime Odyssey", "time", "The West Wing", "Kimberlin Brown", "6 - 6", "activates a relay", "Andy Warhol", "Johnny Darrell", "a living prokaryotic cell ( or organelle )", "alpaca fiber and mohair from Angora goats", "Roentgenium ( Rg )", "President Obama", "1937", "bioelectromagnetics", "Len Wiseman", "Dutch", "Nathan Bedford Forrest", "Castle is a \u2018Z\u2019 plan fortalice dating from no later than 1618 and possibly founded as early as 1580", "Jim Carrey", "Six", "Mashhad", "issued his first military orders as leader of North Korea", "more than 200.", "test scores and graduation rates", "Nothing", "several weeks,", "digging", "The Beverly Hillbillies", "Airbus", "Kenya", "frank", "David Beckham", "The Mousetrap", "a guardian angel", "Herodotus"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5939867424242424}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-6109", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-5689", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-1587", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-2548", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-2670", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-77", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-12949"], "SR": 0.5625, "CSR": 0.525323275862069, "EFR": 0.9285714285714286, "Overall": 0.6650758158866995}, {"timecode": 87, "before_eval_results": {"predictions": ["\"Three's Company\"", "The Los Angeles Dance Theater", "Northern Ireland", "Richard Street", "1983 Summer Universiade", "The Missouri Tigers", "fourth-largest", "Buddha's delight", "Natalie Chandler", "Toshi Ichiyanagi", "1988", "Reinhard Heydrich", "A bass", "2004 Paris Motor Show", "Jeffrey Chiang", "Hanford Site", "December 19, 1998", "Mickey's PhilharMagic", "Centers for Medicare & Medicaid Services", "Donald Sterling", "Sada Carolyn Thompson", "Matt Groening", "All That", "comedy-drama series \"Glee\".", "Danish", "Portal", "Invader (Invasor)", "July 25 to August 4", "2015", "810", "San Diego County Fair", "blackpool Football Club is a professional association football club based in the seaside town of Blackpool, Lancashire, England.", "from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "Iowa", "sacroiliac joint", "Stefanie Scott", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "erosion", "Kerris Lilla Dorsey", "Joseph Sherrard Kearns", "spark", "Jack Lemmon", "france", "gulf of Aden", "massively multiplayer online", "jape", "oliver", "Tina Turner", "Roger Federer", "five", "public toilets and playgrounds.", "haitians", "Ferrari", "fritter his cash away on fast cars, drink and celebrity parties.", "Les Bleus", "228", "Frederick II", "your", "Vietnam", "centigrade", "Boston", "Baltimore", "Howard Hughes", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.625, "QA-F1": 0.7034902597402597}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5177", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-3545", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-2495", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5266", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-8417", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-715", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-761", "mrqa_searchqa-validation-1818", "mrqa_searchqa-validation-256", "mrqa_searchqa-validation-280", "mrqa_searchqa-validation-12610", "mrqa_searchqa-validation-10796"], "SR": 0.625, "CSR": 0.5264559659090908, "EFR": 0.9583333333333334, "Overall": 0.6712547348484849}, {"timecode": 88, "before_eval_results": {"predictions": ["Sunday,", "severe flooding", "Sicily", "cell phones", "I have tried to activate a \"kill switch\"", "be silent.", "body bags", "Chinese", "Taliban", "served in the military,", "Prague is a city of romance, of incredible architecture and history.", "suicide car", "The Intertropical Convergence Zone,", "Sheikha Lubna Al Qasimi,", "\"Quiet Nights,\"", "17-month", "Asashoryu", "managing", "Los Ticos", "John Rizzo,", "London's", "Tom Hanks,", "A growing percentage of the Somali population", "former World Trade Center's \"archaeological heart,\"", "Dodi Fayed,", "four", "Operation Pipeline Express.", "District Attorney Larry Abrahamson", "her child's piano lessons.", "the equator,", "Michael Partain,", "Nigeria,", "Abid Ali Neemuchwala", "the president", "Buddhism", "Waylon Jennings", "Bart Cummings", "You are a puzzle", "Lulu", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "john Tyler", "end of March 1939", "centaur", "mj\u00f6llnir", "france", "swindon town", "sisyphus", "gypsum", "Andr\u00e9 Ch\u00e9nier", "La Familia Michoacana", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Central Avenue", "Quentin Coldwater", "the Lewis and Clark Expedition", "Nippon Professional Baseball", "band Green Day", "James", "John Donne", "boar", "Joan of Arc", "lilac", "the Library of Congress", "Smacks", "R.E.M."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5790179551728465}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.1142857142857143, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.10000000000000002, 0.8, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.3636363636363636, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-577", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3978", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-5125", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-1425", "mrqa_hotpotqa-validation-5355", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-12633"], "SR": 0.515625, "CSR": 0.5263342696629214, "EFR": 0.9032258064516129, "Overall": 0.6602088902229069}, {"timecode": 89, "before_eval_results": {"predictions": ["38,", "have a smile on her face when her kids were around.", "Pakistan's combustible Swat Valley,", "education about rainforests.", "it is not just $3 billion of new money into the economy.", "\"extremely weak\" and said he weighs barely 100 pounds in a court document filed this week, but he walked on his own during the 45 minutes he was at the ceremony.", "Africa", "Bollywood superstar", "NATO", "Wigan Athletic", "technology experts Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "Africa", "Osama", "between Pyongyang and Seoul", "Saturday's Hungarian Grand Prix.", "\"Empire of the Sun,\"", "Germany", "Chinese President Hu Jintao", "cars", "4,000", "1 million", "Tomas Olsson,", "documents to recognize the legal right to freedom from tyranny,", "64,", "jazz", "last surviving British soldier from World War I", "Pixar's", "Miami Beach, Florida,", "\"The Screening Room\" takes a look at some of the best stunt ever pulled off -- and a few that didn't end so well.", "near Pakistan's border with Afghanistan", "his business dealings for possible securities violations", "five", "12.9 - kilometre ( 8 mi )", "Pakistan", "1945", "Wisconsin", "to prevent any contaminants in the sink from flowing into the potable water system by siphonage and is the least expensive form of backflow prevention", "the `` 0 '' trunk code", "Vancouver", "at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "truro truro", "president Gerald Ford", "hippety Hopper", "mike", "sevin peter", "potato", "annette Crosbie", "Group IIB Elements", "Knoxville, Tennessee", "Coalwood", "Sun Records founder Sam Phillips", "Westminster system", "Allan A. Goldstein", "\"Europop\".", "blood pudding", "northeastern", "T. S. Eliot", "Elie Wiesel", "Koninginnedag", "neurons", "shalom", "Nixon", "the Mongols", "a packer"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6106663535686142}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.1702127659574468, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.8571428571428571, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.5, 0.5, 0.8148148148148148, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-2682", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-4166", "mrqa_naturalquestions-validation-6949", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-5601", "mrqa_triviaqa-validation-3605", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-3687", "mrqa_hotpotqa-validation-1662", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-8966", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13619", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-16575"], "SR": 0.46875, "CSR": 0.5256944444444445, "EFR": 0.9411764705882353, "Overall": 0.667671058006536}, {"timecode": 90, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12633", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7012", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.82421875, "KG": 0.496875, "before_eval_results": {"predictions": ["Apple Inc.", "second", "\"Wolfman,\"", "using recreational drugs", "the annual White House Correspondents' Association dinner Saturday,", "CNN", "Rwanda", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "scored a hat-trick", "Apple employees", "80,", "five", "The donor will remain anonymous until the family gives permission for the name to be made public,", "Kenneth Cole", "off the north coast of Puerto Rico.", "psychotropic drugs", "He pleaded with a judge in March to stop Noriko Savoie from being able to travel to Japan for summer vacation.", "The killing of the officer is absolutely abhorrent, but also, Mr. White was presumed innocent and deserved his day in court just like any other citizens,\"", "learn in safer surroundings.", "southern city of Naples", "July in the Philippines", "abducting each other for ransoms or retribution.", "Kim Clijsters", "Colorado prosecutor", "in a post-Hosni Mubarak era during a second day of parliamentary elections,", "at Sea World in San Antonio,", "Saturday", "Kenneth Cole", "Republicans", "dismissed all charges", "NATO fighters", "Airbus A330-200", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "The Crescent City", "two", "the government", "John Smith", "eight", "Kenny Anderson", "Akhenaten's wife, Nefertiti", "paddington bear", "spain", "salford", "The Hague", "Tombstone", "dromedary", "Grail", "kim Smith", "T. R. M. Howard", "\"Orchard County\"", "The Bye Bye Man", "Oregon", "35,124", "Phil Collins", "Valhalla Highlands Historic District", "Can't Be Tamed", "koma", "Howard Hughes", "the tapir", "pep", "Meyer Lansky", "Russians", "Brave New World", "Paul Revere"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7423856695455959}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8235294117647058, 0.07407407407407407, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-827", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-11572", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-1049"], "SR": 0.671875, "CSR": 0.5273008241758241, "EFR": 0.9047619047619048, "Overall": 0.6912562957875459}, {"timecode": 91, "before_eval_results": {"predictions": ["francs", "tulle", "Count Basie", "English", "Pose", "The Naked Gun", "the Clean Air Act", "Mercury and Venus", "Opera on the Beach", "Stephen King", "Dunkin' Donuts", "the Vikings", "opera", "Raven Symone", "the Clark bar", "an open door", "The Devil\\'s Advocate", "The Big Red One", "The Cricket on the Hearth", "FDR", "echinacea", "the peripheral vision", "The Police", "Halley\\'s comet", "\"Just say no\"", "Namibia", "Kilimanjaro", "a companion", "Dresden", "Magnolia", "Tennessee", "Big Ben", "Michael Edwards", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "naturalization law for the United States, the Naturalization Act of 1790", "Tim McGraw and Kenny Chesney", "the Persian Gulf, the Red Sea and the Gulf of Mannar", "May 2010", "a liquid crystal on silicon ( LCoS ) ( based on an L CoS chip from Himax ), field - sequential color system, LED illuminated display", "the Mayor's son", "Kentucky Derby", "benevento", "La Boh\u00e8me", "surtsey", "switzerland", "bobby", "mmorpgs", "Herman G\u00f6ring", "Robert Digges Wimberly Connor", "about 3,000 inhabitants scattered in a dozen fishing villages when it was occupied by the United Kingdom in the First Opium War.", "1901", "member of the Executive Council of the General Anthroposophical Society at the Goetheanum in Dornach, Switzerland", "Dunlop Tyres", "Charles Hastings Judd", "1963", "Washington", "Wigan Athletic", "Democrats", "not", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "this week,\"", "Mokotedi Mpshe, head of the National prosecutoring Authority,", "Secretary of State Hillary Clinton,", "2nd Lt. Holley Wimunc."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6876193863035969}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.2105263157894737, 0.5714285714285715, 0.0, 1.0, 0.918918918918919, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.761904761904762, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.0, 0.33333333333333337, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1061", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-5690", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-1991", "mrqa_searchqa-validation-5935", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9604", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-4204", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-169", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1392"], "SR": 0.546875, "CSR": 0.5275135869565217, "EFR": 0.9310344827586207, "Overall": 0.6965533639430286}, {"timecode": 92, "before_eval_results": {"predictions": ["mexico", "sash", "moon landing", "queen", "meglise du Dome", "white house", "prince harry", "france", "david himquith", "joan b b bbrolli", "joania fraser", "george carlin", "testicles", "dumbo", "chromium", "staple singers", "baffin island", "antilles", "brudie fusiliers", "kia", "hard Times", "mexico", "aikido", "the Archive of American Folk Song", "four", "operation", "niece", "meuston", "pyrotechnic", "mem Sorry, I'll Read That Again", "mexico", "Q", "Afghanistan", "more of one good could be produced only by diverting resources from the other good, resulting in less production of it", "August 15, 1971", "December 24, 1836", "November 27, 2013", "a vertebral column ( spine )", "Rachel Kelly Tucker", "21 February", "\"boundary river\".", "Piper", "Westland", "Dennis Potter", "Ben Savage", "\"the most powerful argument for the newly instigated worldwide ban on whaling.\"", "Southern State Parkway", "David Pajo", "40", "give detainees greater latitude in selecting legal representation", "used luxury cars", "BBC's central London offices", "Fayetteville, North Carolina,", "Eintracht Frankfurt", "100 to 150", "March 24,", "Ted", "piano", "sultan", "anthrax", "table tennis", "jade", "Tunisia", "Don Quixote"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6070312499999999}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-4802", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-7661", "mrqa_triviaqa-validation-5934", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6764", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-5112", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-3208", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-9400", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1003", "mrqa_searchqa-validation-802"], "SR": 0.546875, "CSR": 0.5277217741935484, "EFR": 0.9310344827586207, "Overall": 0.6965950013904338}, {"timecode": 93, "before_eval_results": {"predictions": ["Colombia", "wigan Warriors", "Ty Hardin", "south dakota", "Thames", "nia comaneci", "John Masefield", "\u00e9dith piaf", "peter", "sesame Street", "a burrow", "joule", "gerry adams", "m65", "britishtennis.com", "paul anka", "the keeper of the Longstone (Fame Islands) lighthouse", "romania", "Thomas de quincey", "romania", "trwaites", "cauliflower", "Ambassador Bridge", "hrogate", "seven Wonders", "Sony Interactive Entertainment", "beetle", "bauxite", "peter", "naboth", "plum", "sandstone Trail", "a form of fixed - mobile convergence", "eusebeia", "Part 2", "2013", "The Blind Boys of Alabama", "Etienne de Mestre", "in the 1935 court case Schechter Poultry Corp. v. United States", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house", "19th", "Blue", "Johnny D. Bright", "saloon-keeper", "5320 km", "Hordaland", "Bancroft Shed", "Flushed Away", "women.", "18", "American Civil Liberties Union", "Michelle Obama", "Egypt", "People Against Switching Sides", "$8.8 million", "Manmohan Singh's Congress party,", "the Etruscans", "Judi Dench", "Westminster Abbey", "a final contest", "a chipmunk", "Richard Nixon", "Mexico", "Nebraska"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6171085858585859}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0909090909090909, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-229", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-432", "mrqa_triviaqa-validation-5097", "mrqa_triviaqa-validation-4954", "mrqa_triviaqa-validation-5647", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1718", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-1312", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-5718", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-3461", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-11414"], "SR": 0.5625, "CSR": 0.5280917553191489, "EFR": 1.0, "Overall": 0.7104621010638298}, {"timecode": 94, "before_eval_results": {"predictions": ["Gulf of Mexico", "PC", "dandy", "tintoretto", "Vitcos", "kunigunde Mackamotski", "Blofeld", "massive stars", "black", "jess Hardy Amies", "Central line", "bERlioz", "edmund", "mony", "tintoretto", "9", "Stephen Potter", "12", "monyricht treaty", "Femoral", "Lilacs", "george krajicek", "g Gordon Wallace", "cosmos", "little women", "king's college", "Richard Curtis,", "sabal palmetto", "croquet", "john donne", "espresso", "pops", "for the red - bed country of its watershed", "September 4, 2000", "the northern bluegrass band the Greenbriar Boys", "CBS Television City, studios 41 and 43 in Hollywood", "Australia", "Abigail Hawk", "Lewis Carroll", "a member of the family Sturnidae ( starlings and mynas ) native to Asia", "1937", "The conversation", "five", "26,000", "London", "republican", "Michelle Anne Sinclair", "Matt Groening", "citizenship", "Cash for Clunkers", "\"A Lion Among Men,\"", "the underprivileged.", "prisoners at the South Dakota State Penitentiary", "1,500 Marines", "Pakistani officials,", "Steven Green", "Chevy Chase", "the Statue of Liberty", "House", "fog", "Prado Museum", "( Boss) Tweed", "a repo man", "Graceland"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6643147363220304}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0, 1.0, 0.08695652173913042, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-947", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-3442", "mrqa_triviaqa-validation-4375", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-4043", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5687", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-2597", "mrqa_hotpotqa-validation-963", "mrqa_searchqa-validation-10824", "mrqa_searchqa-validation-14018"], "SR": 0.578125, "CSR": 0.5286184210526316, "EFR": 1.0, "Overall": 0.7105674342105264}, {"timecode": 95, "before_eval_results": {"predictions": ["Hollywood headquarters", "five", "last week,", "\"The Orchid thief\"", "Afghan lawmakers", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "2,000 euros ($2,963)", "Afghanistan's restive provinces", "prostate cancer,", "Karthik Rajaram,", "kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "Robert Barnett,", "MBA in finance", "Samoa", "Jacob Zuma,", "severe famine", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "off the coast of Dubai", "an \"unnamed international terror group\"", "Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "Bhola district", "38,", "U.S. State Department and British Foreign Office", "Jaipur", "$40 billion during the operations phase.", "$55.7 million", "11:30 p.m. Tuesday,", "a student who admitted to hanging a noose in a campus library,", "9 a.m.", "40 years of service", "Afghanistan and India have previously accused Pakistan of involvement", "Frank Ricci,", "16 June", "Lisa Stelly", "Procol Harum", "the largest financial inflows to developing countries", "Mahatma Gandhi", "twice", "Merrimen and his gang", "Sri Lanka Podujana Peramuna, led by former president Mahinda Rajapaksa, secured the most seats and local authorities", "atoine de caunes", "Carson City", "krak\u00f3w", "anton leibovitz", "black", "at Wembley", "technetium", "acetone", "\"Histoires ou contes du temps pass\u00e9\"", "Lord Chancellor of England", "Mathew Sacks", "Easy", "1932", "Centennial Olympic Stadium", "February 14, 1859", "Protestant Christian", "Somerset", "black", "gold", "a punching Bull", "a tuna", "a lampoon", "Tommy Franks", "the Principality of Liechtenstein"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6527100503663004}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2849", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2442", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10510", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-216", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4007", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-12584", "mrqa_searchqa-validation-4818"], "SR": 0.546875, "CSR": 0.52880859375, "EFR": 1.0, "Overall": 0.7106054687500001}, {"timecode": 96, "before_eval_results": {"predictions": ["the \"surge\" strategy he implemented last year.", "immediate release", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "19-year-old woman", "genocide", "Rodong Sinmun", "two satellites", "near the Somali coast", "James Whitehouse,", "Hundreds", "Eleven people died and 36 were wounded", "near his home in Peshawar", "12-1", "Aung San Suu Kyi", "July", "\"wider relationship\"", "bartering", "civilians,", "41,280", "The Tinkler.", "suicides", "Charlotte Gainsbourg and Willem Dafoe", "urgently to be rescued,", "1940's", "\"a striking blow to due process and the rule of law.\"", "September 23,", "the first five Potter films have been held", "Gospel Today,", "because a new model is simply out of their reach.", "Vice's broadband television network.", "AbdulMutallab", "71 percent of Americans consider China an economic threat to the United States,", "American rock band R.E.M.", "Achal Kumar Jyoti", "Kenny Anderson", "7000301604928199000", "September 2, 1945", "President", "Washington metropolitan area", "Divyanka Tripathi and Karan Patel", "tiff and Co", "heath Ledger", "Elizabeth Taylor", "jesse", "dodo", "honshu", "paul", "peterona", "Nicolas Vanier", "two", "Karen O", "Tunisian", "Rage Against the Machine", "MGM Resorts International", "4,530", "Krypto Report", "Hawaii", "the Earth", "Danube", "Fiddler", "Intrigue", "Redcliffe", "Grand Central Station", "shrewd"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6676651626559714}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.4, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.7272727272727273, 0.0, 0.9411764705882353, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-4063", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-2182", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-5730", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-3428", "mrqa_searchqa-validation-4231", "mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-13697"], "SR": 0.578125, "CSR": 0.5293170103092784, "EFR": 1.0, "Overall": 0.7107071520618558}, {"timecode": 97, "before_eval_results": {"predictions": ["Kindle Fire", "President Obama and Britain's Prince Charles", "that students often know ahead of time when and where violence will flare up on campus.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "homeless veterans and their entire family,\"", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "the iconic Hollywood headquarters of Capitol Records,", "Trevor Rees,", "next year", "Lisa Brown", "Filippo Inzaghi", "there is not a mechanism at the federal level to ensure that drivers comply.", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "There were no reports of ground strikes or interference with aircraft in flight,", "Illness", "apartment near Fort Bragg in North Carolina.", "free laundry service.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "18th", "the government.", "15,000", "Tokyo", "Top Gun", "the son decided to leave al Qaeda.", "57-year old male", "\"Z Zimbabwe cannot be British, it cannot be American. Yes, it is African,\"", "a delegation of American Muslim and Christian leaders", "Barack Obama", "autonomy", "Jeddah, Saudi Arabia,", "1831", "a \"procedure on her heart,\"", "September 1980", "1905", "Amerigo Vespucci", "the forex market", "Chinese Exclusion Act", "statistical", "111th", "Rory McIlroy", "lago de Nicaragua", "macbeth", "From Russia with Love", "santiago", "y Yahoo!", "in July 1961", "rounders", "car car", "University of Southern California", "Lucille D\u00e9sir\u00e9e Ball", "once", "Cheshire County", "Atlantic Ocean", "tomato", "odd-eyed", "agnus", "a spoonful", "Qwerty", "Austria", "garlic", "Argentina", "Repent ye: for the kingdom of heaven is at hand", "C Daryl Chessman", "Paul"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6881850961538462}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.08, 1.0, 0.8333333333333333, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.4, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5000", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-6942", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-1273", "mrqa_searchqa-validation-2609", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15672"], "SR": 0.5625, "CSR": 0.5296556122448979, "EFR": 1.0, "Overall": 0.7107748724489796}, {"timecode": 98, "before_eval_results": {"predictions": ["downslope", "James Blunt", "katherine parr", "air", "Lancashire", "shark", "nellie melba", "hannibal heyes and kid curry", "ubba", "lighthouse keeper", "george", "Elgar cello", "trombone", "egypt", "marriage", "Adrian Cronauer", "Thomas Jefferson", "romania", "Eric coates", "Dublin", "Welcome Stranger", "yellow", "Violin", "nippon Sangyo", "north carolina", "time machine", "Jack Nicholson", "stomachache", "new Hampshire", "annie wrayburn", "an arrowhead", "hand gun", "Schadenfreude", "Kelly Reno", "9 September 2018", "Ed Sheeran", "Lexie", "local organization of businesses whose goal is to further the interests of businesses", "Andrea Brooks", "The Impalas", "Cleveland Browns", "edith cavell", "Rick and Morty", "Tom Shadyac", "Jena Malone", "Red and Assiniboine Rivers", "Illinois", "Field of Dreams", "Former U.S. soldier Steven Green", "Iraq", "surgical anesthetic propofol", "70,000 in Sri Lanka's war zone.", "Port-au-Prince harbor where his fleet of trucks used to pick up cargo.", "$250,000 for Rivers' charity: God's Love We Deliver.", "antihistamine and an epinephrine auto-injector for emergencies,", "John and Elizabeth Calvert", "War of 1812", "Valium", "Wilbur and Orville Wright", "pie", "Waterloo", "Pan", "Qantas", "Anna Leonowens"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6428789790372671}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.08695652173913045, 0.2608695652173913, 1.0, 0.8, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6214", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-3614", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-7732", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2342", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-5509", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1700", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-4208", "mrqa_searchqa-validation-7820", "mrqa_searchqa-validation-15766", "mrqa_searchqa-validation-3710"], "SR": 0.546875, "CSR": 0.5298295454545454, "EFR": 1.0, "Overall": 0.7108096590909091}, {"timecode": 99, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3209", "mrqa_hotpotqa-validation-3351", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-38", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5235", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7499", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13632", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2816", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9772", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3442", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4204", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.814453125, "KG": 0.48046875, "before_eval_results": {"predictions": ["caracas", "Richard Marx", "cats", "lodges", "forstall", "\"Moon River\"", "Malcolm Turnbull", "fish", "Charlie Cairoli", "Gene Autry", "Addis Ababa", "jupiter", "Donald Sutherland", "alligator", "\"Holiday Inn\"", "ethiopia", "mexico", "tennessee", "piccadilly line", "ethiopian", "paul Gauguin", "Michael Caine", "jaws", "Wordsworth", "QPR", "south-flowing river", "\"Raging Bull.\"", "yellow", "Cypress", "cycling", "24", "Helen Clark", "India", "more than a million members ( including 195,000 youth members )", "1994", "the foreign exchange market ( FX )", "March 11, 2016", "drizzle", "31 January 1934", "Jackie Prenger", "Clarence Nash", "English", "September 21, 2014", "\"Kitty Hawk\"", "Kew Gardens", "Roger Jason Stone Jr.", "\"Si Da Ming Bu\"", "James William McCutcheon", "1975", "Hamas,", "Nazi war crimes suspect", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "MEND", "Swiss art heist", "South Africa", "Oxbow,", "Wet Seal", "Auguste Rodin", "Red Button", "the College of William", "Walt Whitman", "\"Time\"", "a kiwi", "quarks"], "metric_results": {"EM": 0.625, "QA-F1": 0.7007339015151515}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-6686", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-4056", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-4464", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-1083", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-3358", "mrqa_searchqa-validation-14625"], "SR": 0.625, "CSR": 0.53078125, "EFR": 0.9166666666666666, "Overall": 0.6894895833333333}]}