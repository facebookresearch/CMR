{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 3920, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["UK", "cnidarians", "seven", "religious", "Mnemiopsis", "TEU articles 4 and 5", "Denver", "public-key encryption", "1939", "communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Christian", "a painting", "2012", "Bruno Mars", "economic liberalism", "Einstein", "The Shah's army was split by diverse internecine feuds", "Lunar Roving Vehicle", "SoCal", "Block II spacesuit", "Dr. George E. Mueller", "a multi-party system", "attacks on teachers in Welsh schools which reached an all-time high between 2005 and 2010", "cartels", "Tony Hawk", "Broncos", "Stromatoveris", "Sonderungsverbot", "Vince Lombardi Trophy", "the need for alliances", "1525\u201332", "wealth", "a phylum of animals that live in marine waters worldwide", "The United Methodist Church", "A piece of paper", "concrete", "one", "opportunistic", "Duke Kent- Brown", "1910\u20131940", "Museum of Manufactures", "Downtown Los Angeles", "south", "1980s", "the Main Quadrangles", "the government and the National Assembly and the Senate", "48 hours", "Edgar Atheling", "Rhine knee", "materia medica", "three years", "An attorney", "11 million", "The Scotland Act 1998", "24 March 1879", "Albert Einstein", "an attack on New France's capital, Quebec", "the Common Core", "1851", "office of Patriarch", "life expectancy", "seven", "Jonathan Stewart", "Manakin Town"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8046130952380952}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9086", "mrqa_squad-validation-8065", "mrqa_squad-validation-166", "mrqa_squad-validation-664", "mrqa_squad-validation-7400", "mrqa_squad-validation-6163", "mrqa_squad-validation-8316", "mrqa_squad-validation-1886", "mrqa_squad-validation-2513", "mrqa_squad-validation-4534", "mrqa_squad-validation-2476", "mrqa_squad-validation-6945", "mrqa_squad-validation-2634", "mrqa_squad-validation-2314"], "SR": 0.78125, "CSR": 0.78125, "EFR": 0.9285714285714286, "Overall": 0.8549107142857143}, {"timecode": 1, "before_eval_results": {"predictions": ["The necessity defense", "The BBC was given the highlights of most of the matches, while BSkyB paying \u00a3304m for the Premier League rights", "their main method of locomotion", "Albert of Mainz", "nine", "bachelor's degree", "Duisburg", "Latin Mass", "#P", "soap sponge", "semantical problems", "The worst-case is when the input is sorted or sorted in reverse order", "force", "Bermuda 419 turf", "1975", "ABC Sunday Night Movie", "Chicago Theological Seminary", "The Daleks (a.k.a. The Mutants)", "comedies and family-oriented series", "The label Huguenot was purportedly first applied in France to those conspirators (all of them aristocratic members of the Reformed Church) involved in the Amboise plot of 1560", "Chinatown", "almost half", "2,000", "$2 million", "the head of government of a country", "Gods of Egypt", "the next day", "six", "turbine type", "late 14th-century", "Wittenberg", "Stanford", "427,652", "four days", "Thomas Edison", "1979", "a tool of the devil", "high voltage", "the warmest months from May through September, while the driest months are from November through April", "Dublin", "Madison Square Garden", "indulgences for the living", "domestic Islamists", "1891", "God", "Henry Cole", "the Marburg Colloquy", "actions-oriented", "western", "The special was one of several special 3D programmes the BBC produced at the time, using a 3D system that made use of the Pulfrich effect requiring glasses with one darkened lens", "T. J. Ward", "George Stephenson", "wiring the walls of a schoolroom and, \"saturating [the schoolroom] with infinitesimal electric waves vibrating at high frequency.\"", "\"blurring of theological and confessional differences in the interests of unity.\"", "cantatas", "a Qutb", "John Fox", "feigned retreat to break enemy formations and to lure small enemy groups away from the larger group", "biologically contained", "case law by the Court of Justice", "296", "it was made by this man who died in 1933", ",, The prospect of successful fur trade prompted the States General, the governing body of the Dutch Republic, to issue a statement on March 27", "1960"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7843769639265963}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.06666666666666667, 1.0, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2925", "mrqa_squad-validation-1719", "mrqa_squad-validation-6024", "mrqa_squad-validation-3048", "mrqa_squad-validation-6807", "mrqa_squad-validation-2165", "mrqa_squad-validation-7211", "mrqa_squad-validation-1456", "mrqa_squad-validation-2423", "mrqa_squad-validation-9744", "mrqa_squad-validation-7835", "mrqa_squad-validation-1531", "mrqa_squad-validation-2419", "mrqa_squad-validation-6254", "mrqa_squad-validation-8896", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-5692"], "SR": 0.734375, "CSR": 0.7578125, "EFR": 1.0, "Overall": 0.87890625}, {"timecode": 2, "before_eval_results": {"predictions": ["the mouth of the Monongahela River", "Rutherford Grammar School", "trying to recover market share", "Clair Cameron Patterson", "a tool of the devil", "Venom", "one MSP", "European Union", "700", "Peanuts", "Charles Darwin", "1906", "Protestantism", "Scottish rivers", "trust God's word", "Latin Rhenus", "the rediscovery of \"Christ and His salvation\"", "form business partnerships with physicians or give them \"kickback\" payments", "Troupes de la Marine and Indians", "(leptin, pituitary growth hormone, and prolactin", "The Tomorrow People", "2005", "Karakorum", "Paris", "Ukraine", "the policies of major powers, or simply, general-purpose aggressiveness", "Energiprojekt AB", "high inequality", "the introduction of the traditional Mongolian script and the creation of the Ikh Zasag (Great Administration)", "when the present amount of funding cannot cover the current costs for labour and materials", "Marconi successfully transmitted the letter S from England to Newfoundland", "Chuck Howley", "theatre", "the increase in tea drinking", "Istanbul", "Scotland Act 1998", "a stolen Mark I Type 40 TARDIS", "DC traction motor", "17 seconds", "George Westinghouse", "Omnicare, Kindred Healthcare and PharMerica", "14th century", "Robert Koch and Emil von Behring", "Guglielmo Marconi", "the first Thursday in May", "article 49", "60 minutes", "the Cobham\u2013Edmonds thesis", "the nobles", "Mork & Mindy", "Los Angeles Kings", "Arabic", "the steam escapes", "Word and Image Department", "6 miles (9.7 km)", "Wisconsin v. Yoder", "Industrial", "three of his ribs were broken", "(IV, vii. 6^)", "(with Pictures)", "( (...) (Louis) (especially Welsh) retain many traces of what appears to... (  Romance languages)", "(Israel)", "(CPG Sec. 550.550 Maraschino Cherries)", "step up"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7470441017316016}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4291", "mrqa_squad-validation-7051", "mrqa_squad-validation-9247", "mrqa_squad-validation-10203", "mrqa_squad-validation-6622", "mrqa_squad-validation-7637", "mrqa_squad-validation-9807", "mrqa_squad-validation-6333", "mrqa_squad-validation-635", "mrqa_squad-validation-7370", "mrqa_squad-validation-9569", "mrqa_squad-validation-478", "mrqa_squad-validation-5473", "mrqa_squad-validation-7049", "mrqa_squad-validation-1625", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-10983", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-2676"], "SR": 0.6875, "CSR": 0.734375, "EFR": 0.95, "Overall": 0.8421875}, {"timecode": 3, "before_eval_results": {"predictions": ["Genghis Khan", "Rutherford Grammar School", "Thomas Edison", "tidal currents", "32", "Newcastle University's student's union building", "11 July 1934", "15,000", "CBS", "4k + 3", "the aboral organ", "The Wojciech Bogus\u0142awski Theatre", "an antigen from a pathogen", "Maling", "NBC", "the level of the top tax rate", "October 16, 2012", "the Metro Light Rail system", "staying home", "The European Commission", "pastors and teachers", "The time and space hierarchy theorems", "(static discs)", "A contract", "US$100,000", "Cretaceous\u2013Paleogene extinction", "the red algal endosymbiont's original cell membrane", "Saturn V", "60%", "Litton's Weekend Aventure", "Happy Days", "seven-eighths", "1916", "Jonathan Stewart", "Ron Grainer", "Roone Arledge", "Scottish Government", "The Huguenots adapted quickly and often married outside their immediate French communities", "8 mm cine film", "MODES", "a mainline Protestant Methodist denomination", "the Golden Gate Bridge", "a new magma", "31 October", "Necessity-based", "\"Guilt implies wrong-doing\"", "Gallifrey", "twelve", "EU law", "ctenophores", "relationship contracting", "Mitochondria", "The Talons of Weng-Chiang", "discipline problems with the Flight Director's orders", "6.7+", "a poet and diplomat Paul Claudel", "a World War II military airfield complex", "a Roman mile of 1000 paces", "Sir Isaac Newton FRS", "a platypus", "Brussels", "Saint Helena", "cabbage", "zoology of mammals"], "metric_results": {"EM": 0.796875, "QA-F1": 0.820610119047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.28571428571428575, 0.0, 0.33333333333333337, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-2337", "mrqa_squad-validation-9487", "mrqa_squad-validation-3069", "mrqa_squad-validation-6913", "mrqa_squad-validation-4060", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-13132", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-16512", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-5998"], "SR": 0.796875, "CSR": 0.75, "EFR": 0.9230769230769231, "Overall": 0.8365384615384616}, {"timecode": 4, "before_eval_results": {"predictions": ["\u00a334m per year", "11.5 inches", "patient care rounds drug product selection", "494,665", "1,160,000 square miles", "highest", "Frederick W. Mote", "Plasmodium falciparum", "720p high definition", "many other herbs", "an orbital scientific instrument package", "Hasar, Hachiun, and Tem\u00fcge", "Hamburg merchants and traders", "Ikh Zasag", "remote sensing", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "Building construction", "rapid expansion in telecommunication and financial activity", "Vampire bats", "theNP-complete Boolean satisfiability problem", "Rose", "socialist realism", "270,000", "not in Wittenberg", "2005\u20132010", "environmental determinism", "Samuel Phillips", "pulmonary fibrosis", "45\u201360 nanometers", "Four thousand", "70", "A fundamental error", "multiplicative inverse", "Charly", "The Secret Life of Pets", "thousands", "1966", "Apollo 13", "silicon dioxide", "2007", "160 kPa", "use the potential energy stored in an H+ or hydrogen ion gradient", "NFL owners", "Roger Goodell", "the packets may be delivered according to a multiple access scheme", "regeneration", "with the help of the military", "Genghis Khan", "national courts", "color confinement", "ca. 2 million", "males", "a B.S. in aeronautical engineering", "W. C. Fields", "Helen Hayes", "Rooster Cogburn", "a mathematician", "a port", "FDR", "George S. Kaufman", "a horse", "a raven", "Indianapolis", "an album"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8466145833333334}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4383", "mrqa_squad-validation-1862", "mrqa_squad-validation-8789", "mrqa_squad-validation-1640", "mrqa_squad-validation-9047", "mrqa_squad-validation-600", "mrqa_squad-validation-3664", "mrqa_squad-validation-8902", "mrqa_squad-validation-3040", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-14679"], "SR": 0.78125, "CSR": 0.75625, "EFR": 1.0, "Overall": 0.878125}, {"timecode": 5, "before_eval_results": {"predictions": ["the Uighurs of the Kingdom of Qocho", "the oceans and seas", "1562", "San Jose State", "John Sutcliffe", "a military coup d'\u00e9tat", "the weekly screenings of all available classic episodes", "Manned Spacecraft Center", "dislodge the French", "July", "The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated", "Yale", "the middle of the 20th century", "1852", "uniting warring tribes", "private", "1815", "the mid-Cambrian period", "the Bayeux Tapestry", "specialised education and training", "US$100,000", "Ollie Treiz", "1599", "John Fox", "a stronger, tech-oriented economy", "Venom", "eliminate the accusing law", "the Scotland Parliament", "Narrow alleys", "reciprocating steam engines", "Evaporative cooling tower", "Japan", "NP-intermediate problems", "1542", "remote sensing", "\"The Neutral Zone\"", "a number of stages", "a bill in one of the areas under its remit", "suburban", "Higher Real Gymnasium", "the school's financial endowment", "Asia", "The Skirmish of the Brick Church", "the Ten Commandments", "the local church", "six", "Jurassic Period", "Lenin", "seven", "the 17th century", "Olivia Newton - John and Cliff Richard", "Fortnite has up to four players cooperating on various missions on randomly - generated maps to collect resources", "fluoride concentration in saliva to about 0.04 mg / L several times during a day", "18", "The PNS is to connect the CNS to the limbs and organs", "Pyeongchang County, Gangwon Province, South Korea", "In 1954, language FORTRAN was invented at IBM", "Sam Crow", "the biblical Book of Exodus", "There are a total of 538 electors, corresponding to the 435 representatives and 100 senators, plus the three electors for the District of Columbia", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "the Marx Brothers", "TNT", "The Man"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7240543954650827}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.48275862068965514, 1.0, 0.4, 0.23529411764705882, 0.8571428571428571, 0.09523809523809523, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-802", "mrqa_squad-validation-3860", "mrqa_squad-validation-1075", "mrqa_squad-validation-2835", "mrqa_squad-validation-9492", "mrqa_squad-validation-3369", "mrqa_squad-validation-3403", "mrqa_squad-validation-9452", "mrqa_squad-validation-6919", "mrqa_squad-validation-2457", "mrqa_squad-validation-10041", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5396", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-10267"], "SR": 0.640625, "CSR": 0.7369791666666667, "EFR": 1.0, "Overall": 0.8684895833333334}, {"timecode": 6, "before_eval_results": {"predictions": ["net force", "October 12, 1943", "by qualified majority", "non-self molecules", "1893", "Channel Islands", "Doctor Who \u2013 The Ultimate Adventure", "preventable diseases like malaria, HIV/AIDS, pneumonia, diarrhoea and malnutrition", "US$10 a week", "Einstein", "1988", "pyrenoid and thylakoids", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "GM", "Inalchuq", "51.6%", "NBC Red Network", "quantitative", "John 8:7", "rain", "Christ Church Hall", "short-tempered and even harsher", "theology and philosophy", "KRFX", "the Bay Area's technology, culinary creations, and cultural diversity", "Metro: All Change", "General Samuel C. Phillips", "Brad Nortman", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Anheuser-Busch InBev", "Inflammation", "1313", "Rice University", "Dignity Health", "six", "December 1878", "force", "ordained by many individuals", "Start Here", "Karluk Kara-Khanid ruler", "send aid and sometimes to go themselves to fight for their faith", "judicial branch", "the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "inequitable taxes", "dangerous enemies", "peer tuitions and the school's financial endowment", "Albert Einstein", "Vespa", "Jerry Seinfeld", "South Africa", "pitched", "Antony", "Ronnie", "4", "Francis Matthews Dies", "11,034", "Lord Snooty", "Leon Baptiste", "Star Wars: The Force Unleashed", "\"Piggy\u201d Worth", "the UK", "Derry City", "H. R. Haldeman", "consulting"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7168154761904761}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.47619047619047616, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5697", "mrqa_squad-validation-4074", "mrqa_squad-validation-1272", "mrqa_squad-validation-1550", "mrqa_squad-validation-8690", "mrqa_squad-validation-3751", "mrqa_squad-validation-6161", "mrqa_squad-validation-2486", "mrqa_squad-validation-519", "mrqa_squad-validation-2315", "mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-1164", "mrqa_triviaqa-validation-3388", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-632"], "SR": 0.640625, "CSR": 0.7232142857142857, "EFR": 0.9565217391304348, "Overall": 0.8398680124223603}, {"timecode": 7, "before_eval_results": {"predictions": ["through confirmation and sometimes the profession of faith", "latent heat", "3,837", "Canada", "UK", "487", "Westinghouse Electric", "1957", "soap sponge", "Dynasty", "torn down", "by store and forward switching", "a university or college", "Charles River", "October 2006", "arrows", "The Victorian Alps", "Louis Comfort Tiffany and \u00c9mile Gall\u00e9", "evolution of the German language and literature", "silent", "The Greens", "weak labor movements", "gas supplied through oxygen masks", "one way streets", "oppidum Ubiorum", "middle", "public-Private Partnering", "must have an address in the member state", "left", "1901", "the poor application of well-established", "Industrial", "100% oxygen", "Madame de Pompadour", "Disney Media Networks", "The Saxon Garden", "political figures", "modern art by Polish and international artists", "Neoclassical economics", "Starch", "immune surveillance", "pharmacakeia", "in commerce, schooling and government", "commensal flora", "must have", "The Battle of Bunker Hill", "Odisha", "Geoffrey Hutchings", "Dublin", "j Jesper Myrfors", "from 1986 to 2013", "Dave Brubeck", "M Rookie Blaylock", "Martha Coolidge", "the Rose Garden", "the Royal Air Force", "Viacom Media Networks", "trans-Pacific", "Oklahoma City", "Lions for Lambs", "in late - 17th century New England", "South Africa", "poems", "jerry"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7860119047619047}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.0, 0.1111111111111111, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4891", "mrqa_squad-validation-5168", "mrqa_squad-validation-3686", "mrqa_squad-validation-9298", "mrqa_squad-validation-4287", "mrqa_squad-validation-6779", "mrqa_squad-validation-4460", "mrqa_squad-validation-8547", "mrqa_squad-validation-6207", "mrqa_squad-validation-6814", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2943", "mrqa_naturalquestions-validation-3893", "mrqa_searchqa-validation-10900"], "SR": 0.703125, "CSR": 0.720703125, "EFR": 1.0, "Overall": 0.8603515625}, {"timecode": 8, "before_eval_results": {"predictions": ["extremely high", "Elton Rule", "those who already hold wealth", "Protestant clergy to marry", "Italian and French Renaissance", "Robert of Jumi\u00e8ges", "two Japanese research teams", "1905", "gambling", "Mount Bogong", "Mark Twain", "1573", "nine", "enthusiasm", "Roman law meaning 'empty land'", "a double displacement loop", "attempting to reconcile electromagnetic theory with two observations", "WBT-FM (99.3 FM)", "state-secondary (non-tertiary) schools", "antiforms", "accepting Jesus as your personal Lord and Savior", "the Compromise of 1850", "Keraite", "the end of the Pleistocene (~11,600 BP)", "draw British resources away from North America and the European mainland", "1724", "Hendrix v Employee Insurance Institute", "500", "heat and pressure", "guerrilla warfare campaign", "temperance", "Christian", "Oirads", "local area networks to be established ad hoc without the requirement for a centralized modem or server", "Golden Gate Bridge", "Larger Catechism", "D", "2014", "Conan O'Brien", "The issues of conflicting territorial claims between British and French colonies", "1700", "its humoral system", "1812", "wrestler", "\"Spring Sonata\" (\"Fr\u00fchlingssonate\"", "Golden Gate National Recreation Area", "1938", "The LA Galaxy", "Edward Robert Martin Jr.", "July 10, 2017", "reigning monarch of the United Kingdom", "punk rock", "Konstant\u012bns Raudive", "Rob Reiner", "Alain Robbe-Grillet", "Jeffrey Perry", "Stern-Plaza", "1969 until 1974", "2009", "Sri Lanka Freedom Party", "Penelope Garcia", "Carbon", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma", "red & white"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7640384984639017}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.761904761904762, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9032258064516129, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-271", "mrqa_squad-validation-9761", "mrqa_squad-validation-10489", "mrqa_squad-validation-6981", "mrqa_squad-validation-5111", "mrqa_squad-validation-9989", "mrqa_squad-validation-10294", "mrqa_squad-validation-10027", "mrqa_squad-validation-4789", "mrqa_squad-validation-3991", "mrqa_squad-validation-10168", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-5170", "mrqa_newsqa-validation-3485", "mrqa_searchqa-validation-7920"], "SR": 0.703125, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 9, "before_eval_results": {"predictions": ["gilt bronze", "Charles I", "Christian Whiton", "39", "hymn-writer", "two", "MPEG-4", "1724", "\"grace alone\" more fully", "NewcastleGateshead", "famous musicians", "Immunology", "Jesus Christ", "temperature-dependent", "his friendship", "shipping toxic waste", "118", "2:45 a.m.", "Charles L. Hutchinson", "NFL Mobile", "English and Swahili", "a bachelor's degree", "Alvaro Martin", "Belgrade", "no French regular army troops were stationed in North America", "Hadrian's Wall", "American Association of University Women", "uncertain", "Charlesfort", "1650", "William Hartnell's poor health", "Three's Company", "October 6, 2004", "Dutch East India Company", "1060s", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "coastal beaches and the game reserves", "10 July 1856", "Leonardo da Vinci's", "his lab and elsewhere", "Japan Airlines Flight 123", "13\u20133", "Sir Hiram Stevens Maxim", "riders are turned upside-down and then back upright", "47,818", "Humvee", "Schutzstaffel", "The 1984 South Asian Games", "2012", "Hoosick, Rensselaer County", "Shenae Grimes-Beech", "Chris Evans", "Seacoast Region", "McLean, Virginia", "Dame Harriet Walter", "Fainaru Fantaj\u012b Tuerubu", "Pan Am Railways", "Taoiseach", "paternalistic policies enacted upon Native American tribes", "Jason Marsden", "Lanzarote", "Sudanese nor orphans", "Abundant in fruits, vegetables, and grains", "tsuzumi"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7750760740995116}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0625, 0.5, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2305", "mrqa_squad-validation-2329", "mrqa_squad-validation-3554", "mrqa_squad-validation-7885", "mrqa_squad-validation-5537", "mrqa_squad-validation-1613", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-8586"], "SR": 0.703125, "CSR": 0.7171875, "EFR": 1.0, "Overall": 0.85859375}, {"timecode": 10, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-384", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7845", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-424", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10267", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-13132", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2676", "mrqa_searchqa-validation-3109", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-8586", "mrqa_searchqa-validation-9529", "mrqa_squad-validation-10018", "mrqa_squad-validation-10022", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10062", "mrqa_squad-validation-10078", "mrqa_squad-validation-10136", "mrqa_squad-validation-10168", "mrqa_squad-validation-10191", "mrqa_squad-validation-10203", "mrqa_squad-validation-10294", "mrqa_squad-validation-10337", "mrqa_squad-validation-10338", "mrqa_squad-validation-1035", "mrqa_squad-validation-10370", "mrqa_squad-validation-10406", "mrqa_squad-validation-10474", "mrqa_squad-validation-1068", "mrqa_squad-validation-1075", "mrqa_squad-validation-1110", "mrqa_squad-validation-1113", "mrqa_squad-validation-1131", "mrqa_squad-validation-1148", "mrqa_squad-validation-1182", "mrqa_squad-validation-1243", "mrqa_squad-validation-1272", "mrqa_squad-validation-1301", "mrqa_squad-validation-1320", "mrqa_squad-validation-1343", "mrqa_squad-validation-1349", "mrqa_squad-validation-135", "mrqa_squad-validation-1355", "mrqa_squad-validation-1371", "mrqa_squad-validation-1404", "mrqa_squad-validation-1456", "mrqa_squad-validation-1467", "mrqa_squad-validation-1478", "mrqa_squad-validation-1531", "mrqa_squad-validation-1548", "mrqa_squad-validation-1556", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-1625", "mrqa_squad-validation-1639", "mrqa_squad-validation-1640", "mrqa_squad-validation-1656", "mrqa_squad-validation-166", "mrqa_squad-validation-1684", "mrqa_squad-validation-1719", "mrqa_squad-validation-1755", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1811", "mrqa_squad-validation-1862", "mrqa_squad-validation-1886", "mrqa_squad-validation-1889", "mrqa_squad-validation-195", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-1977", "mrqa_squad-validation-2073", "mrqa_squad-validation-2077", "mrqa_squad-validation-2079", "mrqa_squad-validation-2106", "mrqa_squad-validation-2112", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2133", "mrqa_squad-validation-2165", "mrqa_squad-validation-2165", "mrqa_squad-validation-2169", "mrqa_squad-validation-218", "mrqa_squad-validation-2283", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2305", "mrqa_squad-validation-2315", "mrqa_squad-validation-2323", "mrqa_squad-validation-2326", "mrqa_squad-validation-2329", "mrqa_squad-validation-2337", "mrqa_squad-validation-237", "mrqa_squad-validation-2375", "mrqa_squad-validation-2390", "mrqa_squad-validation-2399", "mrqa_squad-validation-2423", "mrqa_squad-validation-2457", "mrqa_squad-validation-2476", "mrqa_squad-validation-2486", "mrqa_squad-validation-2489", "mrqa_squad-validation-2513", "mrqa_squad-validation-2540", "mrqa_squad-validation-2544", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2607", "mrqa_squad-validation-2616", "mrqa_squad-validation-2634", "mrqa_squad-validation-2648", "mrqa_squad-validation-2667", "mrqa_squad-validation-2709", "mrqa_squad-validation-271", "mrqa_squad-validation-2723", "mrqa_squad-validation-2736", "mrqa_squad-validation-2785", "mrqa_squad-validation-2798", "mrqa_squad-validation-2810", "mrqa_squad-validation-2835", "mrqa_squad-validation-2885", "mrqa_squad-validation-2911", "mrqa_squad-validation-2923", "mrqa_squad-validation-2925", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3010", "mrqa_squad-validation-3028", "mrqa_squad-validation-3040", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3081", "mrqa_squad-validation-3097", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-318", "mrqa_squad-validation-32", "mrqa_squad-validation-3207", "mrqa_squad-validation-3225", "mrqa_squad-validation-3233", "mrqa_squad-validation-3324", "mrqa_squad-validation-3345", "mrqa_squad-validation-3369", "mrqa_squad-validation-340", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3432", "mrqa_squad-validation-3554", "mrqa_squad-validation-3623", "mrqa_squad-validation-3636", "mrqa_squad-validation-3640", "mrqa_squad-validation-3641", "mrqa_squad-validation-3664", "mrqa_squad-validation-3751", "mrqa_squad-validation-381", "mrqa_squad-validation-3816", "mrqa_squad-validation-3836", "mrqa_squad-validation-3853", "mrqa_squad-validation-3860", "mrqa_squad-validation-3887", "mrqa_squad-validation-3921", "mrqa_squad-validation-3925", "mrqa_squad-validation-3937", "mrqa_squad-validation-3942", "mrqa_squad-validation-3965", "mrqa_squad-validation-3991", "mrqa_squad-validation-402", "mrqa_squad-validation-4023", "mrqa_squad-validation-4024", "mrqa_squad-validation-4057", "mrqa_squad-validation-4060", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-415", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4187", "mrqa_squad-validation-419", "mrqa_squad-validation-4267", "mrqa_squad-validation-4287", "mrqa_squad-validation-4291", "mrqa_squad-validation-4307", "mrqa_squad-validation-4311", "mrqa_squad-validation-4348", "mrqa_squad-validation-4358", "mrqa_squad-validation-4383", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4437", "mrqa_squad-validation-4442", "mrqa_squad-validation-4477", "mrqa_squad-validation-4490", "mrqa_squad-validation-453", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4575", "mrqa_squad-validation-462", "mrqa_squad-validation-4694", "mrqa_squad-validation-4724", "mrqa_squad-validation-4755", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4891", "mrqa_squad-validation-4983", "mrqa_squad-validation-4986", "mrqa_squad-validation-5081", "mrqa_squad-validation-5134", "mrqa_squad-validation-5156", "mrqa_squad-validation-5162", "mrqa_squad-validation-5168", "mrqa_squad-validation-5179", "mrqa_squad-validation-519", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5257", "mrqa_squad-validation-5269", "mrqa_squad-validation-530", "mrqa_squad-validation-5322", "mrqa_squad-validation-5351", "mrqa_squad-validation-537", "mrqa_squad-validation-5373", "mrqa_squad-validation-5396", "mrqa_squad-validation-5400", "mrqa_squad-validation-5400", "mrqa_squad-validation-5412", "mrqa_squad-validation-5423", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-5489", "mrqa_squad-validation-5502", "mrqa_squad-validation-5533", "mrqa_squad-validation-5537", "mrqa_squad-validation-5555", "mrqa_squad-validation-5557", "mrqa_squad-validation-5593", "mrqa_squad-validation-5620", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5697", "mrqa_squad-validation-5754", "mrqa_squad-validation-5760", "mrqa_squad-validation-5846", "mrqa_squad-validation-5891", "mrqa_squad-validation-5939", "mrqa_squad-validation-5966", "mrqa_squad-validation-5967", "mrqa_squad-validation-6007", "mrqa_squad-validation-6024", "mrqa_squad-validation-6031", "mrqa_squad-validation-6034", "mrqa_squad-validation-604", "mrqa_squad-validation-6040", "mrqa_squad-validation-6086", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6142", "mrqa_squad-validation-6161", "mrqa_squad-validation-6163", "mrqa_squad-validation-619", "mrqa_squad-validation-6214", "mrqa_squad-validation-6221", "mrqa_squad-validation-6222", "mrqa_squad-validation-6227", "mrqa_squad-validation-6254", "mrqa_squad-validation-626", "mrqa_squad-validation-6284", "mrqa_squad-validation-629", "mrqa_squad-validation-6318", "mrqa_squad-validation-6330", "mrqa_squad-validation-6333", "mrqa_squad-validation-6349", "mrqa_squad-validation-635", "mrqa_squad-validation-6373", "mrqa_squad-validation-6412", "mrqa_squad-validation-6447", "mrqa_squad-validation-652", "mrqa_squad-validation-6541", "mrqa_squad-validation-6606", "mrqa_squad-validation-6628", "mrqa_squad-validation-664", "mrqa_squad-validation-6653", "mrqa_squad-validation-6694", "mrqa_squad-validation-6718", "mrqa_squad-validation-6718", "mrqa_squad-validation-6738", "mrqa_squad-validation-6777", "mrqa_squad-validation-6807", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6814", "mrqa_squad-validation-6831", "mrqa_squad-validation-686", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6919", "mrqa_squad-validation-6919", "mrqa_squad-validation-6945", "mrqa_squad-validation-696", "mrqa_squad-validation-7046", "mrqa_squad-validation-7049", "mrqa_squad-validation-7094", "mrqa_squad-validation-7111", "mrqa_squad-validation-7135", "mrqa_squad-validation-7165", "mrqa_squad-validation-7198", "mrqa_squad-validation-7203", "mrqa_squad-validation-7204", "mrqa_squad-validation-721", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7263", "mrqa_squad-validation-7267", "mrqa_squad-validation-7312", "mrqa_squad-validation-7322", "mrqa_squad-validation-7327", "mrqa_squad-validation-7354", "mrqa_squad-validation-7369", "mrqa_squad-validation-7370", "mrqa_squad-validation-7400", "mrqa_squad-validation-7424", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-751", "mrqa_squad-validation-7547", "mrqa_squad-validation-7580", "mrqa_squad-validation-7637", "mrqa_squad-validation-7646", "mrqa_squad-validation-7649", "mrqa_squad-validation-7663", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7733", "mrqa_squad-validation-7752", "mrqa_squad-validation-7821", "mrqa_squad-validation-7835", "mrqa_squad-validation-7851", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7975", "mrqa_squad-validation-7979", "mrqa_squad-validation-7991", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8073", "mrqa_squad-validation-8092", "mrqa_squad-validation-8163", "mrqa_squad-validation-820", "mrqa_squad-validation-823", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8301", "mrqa_squad-validation-831", "mrqa_squad-validation-8316", "mrqa_squad-validation-8319", "mrqa_squad-validation-8337", "mrqa_squad-validation-8341", "mrqa_squad-validation-8401", "mrqa_squad-validation-8418", "mrqa_squad-validation-8455", "mrqa_squad-validation-8465", "mrqa_squad-validation-8466", "mrqa_squad-validation-8527", "mrqa_squad-validation-8547", "mrqa_squad-validation-8599", "mrqa_squad-validation-8654", "mrqa_squad-validation-8664", "mrqa_squad-validation-8682", "mrqa_squad-validation-8690", "mrqa_squad-validation-8700", "mrqa_squad-validation-878", "mrqa_squad-validation-8837", "mrqa_squad-validation-8837", "mrqa_squad-validation-8902", "mrqa_squad-validation-9041", "mrqa_squad-validation-9047", "mrqa_squad-validation-9082", "mrqa_squad-validation-9086", "mrqa_squad-validation-9110", "mrqa_squad-validation-9192", "mrqa_squad-validation-9276", "mrqa_squad-validation-929", "mrqa_squad-validation-9298", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9427", "mrqa_squad-validation-9452", "mrqa_squad-validation-9456", "mrqa_squad-validation-9492", "mrqa_squad-validation-9543", "mrqa_squad-validation-9543", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-967", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9709", "mrqa_squad-validation-9719", "mrqa_squad-validation-9744", "mrqa_squad-validation-9770", "mrqa_squad-validation-9773", "mrqa_squad-validation-980", "mrqa_squad-validation-9807", "mrqa_squad-validation-9837", "mrqa_squad-validation-9844", "mrqa_squad-validation-9857", "mrqa_squad-validation-9889", "mrqa_squad-validation-9900", "mrqa_squad-validation-9940", "mrqa_squad-validation-9989", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1289", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6283", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7276"], "OKR": 0.896484375, "KG": 0.43671875, "before_eval_results": {"predictions": ["5 to 15 years", "Chagatai", "a conjunction of three planets", "Gottfried Fritschel", "1960s", "a supervisory church body", "The Emperor presented the final draft of the Edict of Worms", "2000", "Budget cuts", "Sky Digital", "carbohydrates", "powerful high frequency currents set up in them", "for the light reactions", "1518\u201331", "an epidemiological account of the plague", "Doritos", "Stanford Stadium", "Sava Kosanovi\u0107", "A diaspora of French Australians", "the region's diversified technology and industrial base", "ABC Inc.", "Super Bowl City", "ESPN Deportes", "Article 102", "1973", "Africa", "to elect and appoint bishops", "seven", "Beyonc\u00e9", "modular exponentiation", "Wade Phillips", "when the immune system is less active than normal, resulting in recurring and life-threatening infections", "domestic legislation of the Scottish Parliament", "Aaron Spelling", "the Emmerich Rhine Bridge", "music", "three", "unsustainable monetary stimulation", "orchestr the first movement piano sketch", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "Them", "Armada", "Boyd Gaming", "Little Big League", "October 25, 1881", "\"A Charlie Brown Christmas\"", "Quasimodo", "Canada's first train robbery", "1933", "Patrick Dempsey", "16 April 1986", "Ian Rush", "The Number Twelve looks Like You", "A. R. Rahman", "ice hockey", "Lincoln Memorial University", "Vincent Anthony Guaraldi", "Romas Kalanta", "French", "May 1, 2018", "Charles Dickens", "to put a lid on the marking of Ashura", "a 30-day full month", "his father's parenting skills"], "metric_results": {"EM": 0.671875, "QA-F1": 0.713971688034188}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4875", "mrqa_squad-validation-2209", "mrqa_squad-validation-1474", "mrqa_squad-validation-8873", "mrqa_squad-validation-4953", "mrqa_squad-validation-3063", "mrqa_squad-validation-2708", "mrqa_squad-validation-103", "mrqa_squad-validation-6442", "mrqa_squad-validation-7387", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-4950", "mrqa_newsqa-validation-1058", "mrqa_searchqa-validation-8755"], "SR": 0.671875, "CSR": 0.7130681818181819, "EFR": 0.9523809523809523, "Overall": 0.7583242018398269}, {"timecode": 11, "before_eval_results": {"predictions": ["the courts of member states", "on a phased basis", "Ukraine", "different viewpoints and political parties", "raised his arm", "1888", "Academy Awards", "August 1914", "1962", "the Chinese", "eastwards", "Genghis", "detention", "the main porch", "Imperial", "Michelle Gomez", "Education in Wales", "New England Patriots", "San Jose Marriott", "2016", "AD 14", "encourage growth", "Buddhism", "c1600", "a supervisory church body", "\"The Snowmen\"", "the northern Mokot\u00f3w", "to avoid trivialization", "1948", "article 49", "the butcher Market", "the highest terrace", "San Mateo", "Waterlogged", "(firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project", "4 August 1915 until November 1918", "Queens, New York", "Maryland", "Francis Egerton", "1901", "eclectic mix of musical styles", "Super Bowl XXIX", "Hugh Christopher Edmund Fearnley-Whittingstall", "Vancouver", "1978", "Martin Scorsese", "Groupe PSA", "850 m", "50JJB Sports Fitness Clubs", "Nia Kay", "Massachusetts", "125 lb (57 kg)", "16 January 1856", "Ericsson", "a classroom specialist", "1949", "Obergruppenf\u00fchrer", "Soha Ali Khan", "1985", "a cigarette", "in the west African nation", "that they feel worse during the winter", "Addis Ababa", "The Merry Wives of Windsor"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7511837121212122}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666665, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-2080", "mrqa_squad-validation-2468", "mrqa_squad-validation-7758", "mrqa_squad-validation-1042", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-2763", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-8175", "mrqa_newsqa-validation-3429", "mrqa_searchqa-validation-10721"], "SR": 0.703125, "CSR": 0.7122395833333333, "EFR": 1.0, "Overall": 0.7676822916666666}, {"timecode": 12, "before_eval_results": {"predictions": ["less than $1.25", "Brazil", "Oahu", "turbine", "Pegasus satellites", "1708", "Zh\u00e8ng", "captured enemies", "US", "Paul Samuelson", "Construction", "baptism", "1227", "President", "Stockton and Darlington Railway", "Venus", "Thomas Edison", "Santa Clara Marriott", "Maria Sk\u0142odowska-Curie Institute of Oncology", "from January 1964", "Thomas Davis", "Blaydon Race", "45,000 pounds", "1972", "Between 1975 and 1990", "Denver Broncos", "Jacksonville", "the Song dynasty", "two", "the United States", "HAMAS (\"zeal\")", "arranged marriages", "Algeria", "the innate immune system versus the adaptive immune system", "three", "Gustav's top winds", "Vernon Forrest", "50,000", "persistent pain", "22", "women", "Karl Kr\u00f8yer", "prevention our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia", "$1.5 million", "Dennis Davern", "American", "the tune", "$250,000", "Kaka", "Laura Ling and Euna Lee", "1995", "\"disagreements\" with the Port Authority of New York and New Jersey", "because the Indians were gathering information about the rebels", "three", "recall notices", "a president who understands the world today", "Jan Brewer", "15,000", "Mato Grosso", "Ricky Gervais", "Argand lamp", "Malcolm Guthrie", "Charles \"Lucky\" Luciano", "John Ford"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7214607007575757}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.625, 0.8, 0.5, 0.18181818181818182, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6279", "mrqa_squad-validation-3854", "mrqa_squad-validation-1050", "mrqa_squad-validation-1", "mrqa_squad-validation-9599", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-4145", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-1871", "mrqa_searchqa-validation-4954", "mrqa_hotpotqa-validation-3124"], "SR": 0.671875, "CSR": 0.7091346153846154, "EFR": 1.0, "Overall": 0.767061298076923}, {"timecode": 13, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "gilt copper", "that faith and reason were antithetical", "young and the elderly", "because it has survived many wars, conflicts and invasions", "common flagellated", "anti-colonial movements", "five million", "John Pell, Lord of Pelham Manor", "the expendable nature of the worker in relation to his or her particular job", "five-year", "3%", "1080i HD", "their Annual Conference", "\"spectacular\"", "electric", "aspirational consumption", "Vistula River", "390 billion", "red algae red", "other ctenophores.", "to protect the King's land in the Ohio Valley from the British.", "Department of Justice", "Barbara Walters", "Chicago Theological Seminary", "May", "1885", "William S. Paley", "six", "Iran", "shopping", "an extensive, electrified, passenger system", "southern California coast", "1989", "Cathy Dennis and Rob Davis", "1971 album, What's Going On", "Louis Mountbatten", "in a geographical coordinate system at which longitude is defined to be 0 \u00b0", "Detective Abigail Baker", "159 beats per minute", "50 states of the United States of America", "The eighth and final season", "as the home", "Hugh S. Johnson", "$75,000", "Rigg", "NFL coaches, general managers, and scouts", "Bill Condon", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "On November 6, 2017, it was announced that the revival season would premiere on March 11, 2018", "in the cell nucleus, where the DNA is held", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "georgia", "total cost", "Michael Christopher McDowell", "in China", "8 January 1999", "Hungarian : Magyarorsz\u00e1g z\u00e1szlaja", "Argentina", "professional footballer, and a current manager.", "safety issues in the company's cars", "turkey vulture", "DNA sequencer.", "Cucumber Tree"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6919863230379871}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.4, 0.0, 0.16666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.3157894736842105, 0.6, 0.33333333333333337, 0.33333333333333337, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.33333333333333337, 0.33333333333333337, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2461", "mrqa_squad-validation-1039", "mrqa_squad-validation-4415", "mrqa_squad-validation-8840", "mrqa_squad-validation-9568", "mrqa_squad-validation-2585", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-10347", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-5578", "mrqa_newsqa-validation-247", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-5681"], "SR": 0.546875, "CSR": 0.6975446428571428, "EFR": 0.9310344827586207, "Overall": 0.7509502001231526}, {"timecode": 14, "before_eval_results": {"predictions": ["Worldvision Enterprises", "the computational model", "16,000 rpm", "These scholars", "it infringed on democratic freedoms.", "a \"racket\"", "the Yuan dynasty", "The Five Doctors", "a job where there are many workers willing to work a large amount of time (high supply)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "the International Stanis\u0142aw Moniuszko Vocal Competition, the Mozart Festival, and the Festival of Old Music", "platyctenids", "increased blood flow into tissue", "Deformational", "a kleptoplast", "organic solvents", "three to five", "1996", "half", "antelope", "Brad Nortman", "Super Bowl XLIV", "San Jose State", "12 to 15 million", "the Carmichael numbers", "BPP, ZPP and RP", "Genghis Khan", "Advanced Steam movement", "because Dutch law said only people established in the Netherlands could give legal advice.", "Alsace", "Vistula River", "Genoa and Lucca", "George W. Bush", "sarsaparilla", "Idaho State", "the heart", "Navajo Code Talkers", "Tennessee", "the focal point", "James R. Garfield", "the United States Navy", "Jack Nicholson", "Enchanted", "prostate", "New Brunswick, Canada", "Green Mashed Potatoes", "Willie Mays", "1933", "that career change to Fascist leader of Italy", "Stripes (1981)", "Israel", "Boo Boo", "the Arctic Ocean", "Wyoming", "Crayola", "the total angular momentum of a system cannot change", "pommel horse", "four", "Riemannian geometry and geometric topology", "Elizabeth River", "Stratfor", "Joseph Heller", "fealty and filial piety", "14 December"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6412632761437909}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9865", "mrqa_squad-validation-8428", "mrqa_squad-validation-7792", "mrqa_squad-validation-7407", "mrqa_squad-validation-5226", "mrqa_squad-validation-680", "mrqa_squad-validation-8677", "mrqa_squad-validation-10316", "mrqa_squad-validation-6044", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-9604", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-12609", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-15249", "mrqa_searchqa-validation-6365", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-10017", "mrqa_triviaqa-validation-6244", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-5897"], "SR": 0.546875, "CSR": 0.6875, "EFR": 0.9655172413793104, "Overall": 0.755837823275862}, {"timecode": 15, "before_eval_results": {"predictions": ["7.8%", "Manakin Town", "three", "the city council", "the Monarch", "filaments", "growth in the service sector", "Accountants", "Jim Nantz and Phil Simms", "phagolysosome", "Westinghouse", "October 16, 2012", "February 9, 1832", "a VideoGuard UK", "June 4, 2014", "26", "an average of $5 million", "trade unions", "Ronnie Hillman", "Chris Keates", "the murder of Christ", "to provide a standardized interface into and out of packet networks", "days, weeks and months", "complexity", "CBS", "prices", "random noise", "sorcery or even poison", "an Islamic state", "Hendrix v Employee Insurance Institute", "a tone raised a half step in pitch", "the Boston Marathon", "Home Alone", "Tainted Love", "\"Suspicious Minds\"", "humans", "Saratoga Race Course", "the Duran Duran", "the fragging grenade", "cinnamon", "Bo Schembechler", "PT-109", "Uranus", "insulin", "Afrikaans", "right whales", "Edgar Allan Poe", "Casey Stengel", "the bus", "( Burns)", "Cy Young", "the King", "the parrot", "genes", "the Firth of Forth", "The Pirates of Penzance", "Zachary Taylor", "his frustration with the atmosphere in the group", "a drum part", "Australia", "the Internet", "86,112", "a pioneer", "Sweden, Norway and Denmark"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7377367424242425}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-413", "mrqa_squad-validation-8899", "mrqa_squad-validation-8401", "mrqa_squad-validation-92", "mrqa_squad-validation-4968", "mrqa_squad-validation-6933", "mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-2002", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-1993", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-13960", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-6370", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-16371", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-5415"], "SR": 0.671875, "CSR": 0.6865234375, "EFR": 1.0, "Overall": 0.7625390624999999}, {"timecode": 16, "before_eval_results": {"predictions": ["middle eastern scientists", "an electrical generator", "force", "4,404.5", "time or space", "International Association of Methodist-related Schools, Colleges, and Universities", "ideological", "Jason Bourne", "48.8 \u00b0C", "0.52/sq mi", "Jamukha", "journalist", "the current Doctor Who", "3 million", "eight", "2014", "Ugali with vegetables, sour milk, meat, fish or any other stew", "1285", "interleukins", "Parliament", "1978", "not designed to fly through the Earth's atmosphere or return to Earth", "Hughes Hotel", "North Sea in the Netherlands", "The Emperor", "Josh Norman", "government officials and climate change experts", "Thomas Commerford Martin", "Jack Russell", "Stephen King", "parliaments", "Tiberius", "Carly Simon", "parliaments", "Ida Noddack", "Japan", "George Alexander Louis", "Uganda", "parliaments", "France", "Taiwan", "Liverpool", "God We Trust", "Budapest", "DC", "parliaments", "parliaments", "Jupiter", "Chuck Hagel", "Victoria", "a sharp", "Tina Turner", "Margaret Thatcher", "parliaments", "hypopituitarism", "Prime Minister John Prescott", "boudin", "a bar", "Bardney", "4 meters (13 feet) high)", "Black Sea", "Todd Griffin", "1908", "balsam"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6745907738095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-601", "mrqa_squad-validation-7877", "mrqa_squad-validation-72", "mrqa_squad-validation-9482", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-7612", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-2320", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-3542", "mrqa_naturalquestions-validation-2462", "mrqa_newsqa-validation-1214", "mrqa_naturalquestions-validation-1704"], "SR": 0.640625, "CSR": 0.6838235294117647, "EFR": 1.0, "Overall": 0.7619990808823529}, {"timecode": 17, "before_eval_results": {"predictions": ["life expectancy", "eleven", "1937", "biomass", "to avoid being targeted by the boycott", "Walter Reed", "September 1944", "two", "new form", "an area of science where our scientific understanding is rapidly changing, this has been raised as a serious shortcoming in a body which is widely regarded as the ultimate authority on the science", "12%", "six to nine percent", "The European Court of Justice", "2010", "that the law is no longer to be taught to Christians but belonged only to city hall", "Ardabil Carpet", "US", "the Fifth, Sixth and Seventh Doctors", "land and housing", "Pons Aelius", "difference between a problem and an instance", "slash and burn", "he was at least partly the product of a declining state of mind", "131", "Hugh Downs", "Dillon, Read & Co.", "relationship of the number to its corresponding value of Euler's totient function or the sum of divisors function", "Julian Fellowes", "arthur", "2", "Adam Werritty", "four", "Ipswich Town", "Margaret Thatcher", "South Korea", "hard", "Lesotho", "India", "arthur", "Papua New Guinea", "arraf al Omari", "her skills", "Ramadan", "arles", "arthur", "nUT", "Mongol Empire", "South Africa", "Edward de Bono", "aron Ralston", "antonyu", "arctic", "Andes Mountains of Chile and Argentina", "arthur", "arthur", "Nelson Mandela", "an", "above the light source and under the sample", "9", "Sunday", "arthur", "1960s", "Ahmad Givens ( Real )", "its absolute temperature"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5684953703703703}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5333333333333333, 0.0, 1.0, 1.0, 0.8148148148148148, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4799999999999999, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8470", "mrqa_squad-validation-2473", "mrqa_squad-validation-7859", "mrqa_squad-validation-1670", "mrqa_squad-validation-2523", "mrqa_squad-validation-9416", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-5219", "mrqa_triviaqa-validation-3748", "mrqa_triviaqa-validation-2246", "mrqa_naturalquestions-validation-4132", "mrqa_hotpotqa-validation-893", "mrqa_searchqa-validation-2851", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-365"], "SR": 0.515625, "CSR": 0.6744791666666667, "EFR": 0.967741935483871, "Overall": 0.7536785954301075}, {"timecode": 18, "before_eval_results": {"predictions": ["violence", "the Swiss-Austrian border", "Michael P. Millardi", "in the autumn of 1991", "MetroCentre", "force", "packet switching", "high wages", "Buena Vista International Television", "three hundred sixty", "patents", "the Alps", "Marco Polo", "January 2003", "1972", "the 1965\u201366 season", "Luther", "Storybook", "1981", "1264", "monophyletic", "complete the modules", "1981", "glowed even when turned off", "21 October 1512", "gobi", "the rapeseed family", "the people", "the Nokia tune", "proustian", "lady", "gobi", "gobi", "\"Born to be Wild\"", "pakistan", "in the city of Leicester", "the wrist", "Devon", "The Red Lion", "shmoop", "pakistan", "Isle of Wight", "puffins", "pakistan", "douglas", "dans", "omid Djalili", "the Hound of the Baskervilles", "mjollnir", "The Crab with the Golden Claws", "pakistan", "the transfusion of blood from an animal to a human", "masantiaGO", "S\u00e8vres", "lady in Boots", "The World is Not Enough", "The Romantics", "2020", "The Pirate", "1968", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "the widows of John Lennon and George Harrison", "The Gateway Arch", "The University of Exeter"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5849578373015873}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08333333333333333, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2921", "mrqa_squad-validation-9071", "mrqa_squad-validation-2093", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-6625", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-3465", "mrqa_hotpotqa-validation-774", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2128", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-10897"], "SR": 0.53125, "CSR": 0.6669407894736843, "EFR": 1.0, "Overall": 0.7586225328947368}, {"timecode": 19, "before_eval_results": {"predictions": ["\u03b2-defensins", "10.0%", "Josh Norman", "the smallest subfield", "Andrew Alper", "1990s", "CBS", "2003", "seven", "southern China to Daidu in the north", "3.6%", "to the park", "Book of Discipline", "optimisation", "Neil Shubin and Paul Sereno", "Colorado Springs", "more convenient and private method", "Several procedures", "on Fresno's far southeast side", "antigenic variation", "c1110", "glaucophyte", "700,000", "Cestida", "Baltimore", "Copenhagen", "Peter Firmin", "cricket", "between Seventh Avenue and Broadway", "geologic", "john hahnast", "Coldplay", "fern", "Rafael Nadal", "Western Australia", "liza Barth", "Florence", "Thailand", "Porridge", "strangeways", "Uranus", "Phil Redmond", "Benito Mussolini", "John Madden", "roman numerals", "Montmorency", "South african", "clarence", "Boyle", "Mozambique Channel", "ytterby", "li'l abner", "four", "mohair", "doe", "south african", "3D modeling", "February 7, 2018", "Mary Bonauto, Susan Murray, and Beth Robinson", "850", "Hugo Chavez", "Michoacan Family", "Parkinson's disease", "crust"], "metric_results": {"EM": 0.625, "QA-F1": 0.6973011363636363}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-225", "mrqa_squad-validation-8103", "mrqa_squad-validation-1631", "mrqa_squad-validation-6282", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-1884", "mrqa_naturalquestions-validation-4879", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-2897", "mrqa_searchqa-validation-5795"], "SR": 0.625, "CSR": 0.66484375, "EFR": 0.9583333333333334, "Overall": 0.7498697916666666}, {"timecode": 20, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3099", "mrqa_hotpotqa-validation-3115", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-384", "mrqa_hotpotqa-validation-3861", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-8654", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9684", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-15884", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-23", "mrqa_searchqa-validation-2676", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-4220", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-4997", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9604", "mrqa_searchqa-validation-9885", "mrqa_squad-validation-10009", "mrqa_squad-validation-10018", "mrqa_squad-validation-10022", "mrqa_squad-validation-10027", "mrqa_squad-validation-10062", "mrqa_squad-validation-10087", "mrqa_squad-validation-101", "mrqa_squad-validation-10168", "mrqa_squad-validation-10191", "mrqa_squad-validation-10206", "mrqa_squad-validation-10252", "mrqa_squad-validation-10264", "mrqa_squad-validation-10294", "mrqa_squad-validation-10316", "mrqa_squad-validation-10338", "mrqa_squad-validation-10383", "mrqa_squad-validation-10406", "mrqa_squad-validation-1042", "mrqa_squad-validation-10474", "mrqa_squad-validation-1048", "mrqa_squad-validation-1068", "mrqa_squad-validation-1075", "mrqa_squad-validation-1081", "mrqa_squad-validation-1110", "mrqa_squad-validation-1148", "mrqa_squad-validation-1156", "mrqa_squad-validation-1243", "mrqa_squad-validation-1272", "mrqa_squad-validation-1371", "mrqa_squad-validation-1394", "mrqa_squad-validation-1404", "mrqa_squad-validation-1441", "mrqa_squad-validation-1547", "mrqa_squad-validation-1550", "mrqa_squad-validation-1556", "mrqa_squad-validation-1557", "mrqa_squad-validation-1577", "mrqa_squad-validation-159", "mrqa_squad-validation-1613", "mrqa_squad-validation-1631", "mrqa_squad-validation-1639", "mrqa_squad-validation-167", "mrqa_squad-validation-1719", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1862", "mrqa_squad-validation-1882", "mrqa_squad-validation-1889", "mrqa_squad-validation-192", "mrqa_squad-validation-195", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-1993", "mrqa_squad-validation-2033", "mrqa_squad-validation-2079", "mrqa_squad-validation-2112", "mrqa_squad-validation-2122", "mrqa_squad-validation-2140", "mrqa_squad-validation-2169", "mrqa_squad-validation-2191", "mrqa_squad-validation-225", "mrqa_squad-validation-2292", "mrqa_squad-validation-2329", "mrqa_squad-validation-2337", "mrqa_squad-validation-237", "mrqa_squad-validation-2375", "mrqa_squad-validation-2399", "mrqa_squad-validation-2411", "mrqa_squad-validation-2423", "mrqa_squad-validation-2453", "mrqa_squad-validation-2461", "mrqa_squad-validation-2467", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-2523", "mrqa_squad-validation-2540", "mrqa_squad-validation-256", "mrqa_squad-validation-2570", "mrqa_squad-validation-258", "mrqa_squad-validation-2585", "mrqa_squad-validation-2616", "mrqa_squad-validation-2625", "mrqa_squad-validation-2634", "mrqa_squad-validation-2648", "mrqa_squad-validation-2708", "mrqa_squad-validation-2736", "mrqa_squad-validation-2778", "mrqa_squad-validation-2834", "mrqa_squad-validation-2885", "mrqa_squad-validation-2900", "mrqa_squad-validation-2909", "mrqa_squad-validation-2923", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3028", "mrqa_squad-validation-305", "mrqa_squad-validation-3063", "mrqa_squad-validation-3081", "mrqa_squad-validation-3097", "mrqa_squad-validation-3125", "mrqa_squad-validation-3165", "mrqa_squad-validation-317", "mrqa_squad-validation-3193", "mrqa_squad-validation-32", "mrqa_squad-validation-3204", "mrqa_squad-validation-3225", "mrqa_squad-validation-3233", "mrqa_squad-validation-3233", "mrqa_squad-validation-3267", "mrqa_squad-validation-327", "mrqa_squad-validation-3324", "mrqa_squad-validation-3345", "mrqa_squad-validation-340", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3469", "mrqa_squad-validation-3661", "mrqa_squad-validation-3686", "mrqa_squad-validation-3740", "mrqa_squad-validation-3751", "mrqa_squad-validation-3759", "mrqa_squad-validation-3802", "mrqa_squad-validation-3806", "mrqa_squad-validation-381", "mrqa_squad-validation-3836", "mrqa_squad-validation-3860", "mrqa_squad-validation-3902", "mrqa_squad-validation-3925", "mrqa_squad-validation-3991", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-415", "mrqa_squad-validation-419", "mrqa_squad-validation-4267", "mrqa_squad-validation-4366", "mrqa_squad-validation-4383", "mrqa_squad-validation-4429", "mrqa_squad-validation-4429", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4575", "mrqa_squad-validation-4607", "mrqa_squad-validation-462", "mrqa_squad-validation-4662", "mrqa_squad-validation-4670", "mrqa_squad-validation-468", "mrqa_squad-validation-4694", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-5003", "mrqa_squad-validation-5010", "mrqa_squad-validation-5012", "mrqa_squad-validation-5054", "mrqa_squad-validation-5077", "mrqa_squad-validation-5134", "mrqa_squad-validation-5162", "mrqa_squad-validation-5179", "mrqa_squad-validation-5185", "mrqa_squad-validation-519", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5269", "mrqa_squad-validation-5282", "mrqa_squad-validation-5319", "mrqa_squad-validation-5351", "mrqa_squad-validation-5363", "mrqa_squad-validation-5382", "mrqa_squad-validation-5400", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-552", "mrqa_squad-validation-5537", "mrqa_squad-validation-5556", "mrqa_squad-validation-5557", "mrqa_squad-validation-5600", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5718", "mrqa_squad-validation-5754", "mrqa_squad-validation-5797", "mrqa_squad-validation-5840", "mrqa_squad-validation-5846", "mrqa_squad-validation-5853", "mrqa_squad-validation-5889", "mrqa_squad-validation-5903", "mrqa_squad-validation-5927", "mrqa_squad-validation-5939", "mrqa_squad-validation-5948", "mrqa_squad-validation-596", "mrqa_squad-validation-6024", "mrqa_squad-validation-6026", "mrqa_squad-validation-6034", "mrqa_squad-validation-604", "mrqa_squad-validation-6086", "mrqa_squad-validation-6142", "mrqa_squad-validation-6151", "mrqa_squad-validation-6161", "mrqa_squad-validation-6209", "mrqa_squad-validation-6214", "mrqa_squad-validation-6254", "mrqa_squad-validation-6268", "mrqa_squad-validation-6278", "mrqa_squad-validation-6279", "mrqa_squad-validation-6284", "mrqa_squad-validation-6349", "mrqa_squad-validation-6415", "mrqa_squad-validation-6447", "mrqa_squad-validation-6457", "mrqa_squad-validation-6474", "mrqa_squad-validation-6492", "mrqa_squad-validation-6517", "mrqa_squad-validation-6541", "mrqa_squad-validation-6622", "mrqa_squad-validation-6624", "mrqa_squad-validation-6766", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6807", "mrqa_squad-validation-6831", "mrqa_squad-validation-686", "mrqa_squad-validation-6879", "mrqa_squad-validation-6919", "mrqa_squad-validation-6933", "mrqa_squad-validation-696", "mrqa_squad-validation-7005", "mrqa_squad-validation-7026", "mrqa_squad-validation-7046", "mrqa_squad-validation-7051", "mrqa_squad-validation-7111", "mrqa_squad-validation-7135", "mrqa_squad-validation-7165", "mrqa_squad-validation-7170", "mrqa_squad-validation-7198", "mrqa_squad-validation-7203", "mrqa_squad-validation-7204", "mrqa_squad-validation-7208", "mrqa_squad-validation-7211", "mrqa_squad-validation-725", "mrqa_squad-validation-7260", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7292", "mrqa_squad-validation-7370", "mrqa_squad-validation-7387", "mrqa_squad-validation-7400", "mrqa_squad-validation-7417", "mrqa_squad-validation-7435", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-748", "mrqa_squad-validation-7500", "mrqa_squad-validation-7508", "mrqa_squad-validation-751", "mrqa_squad-validation-7637", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7758", "mrqa_squad-validation-7794", "mrqa_squad-validation-7851", "mrqa_squad-validation-7859", "mrqa_squad-validation-7885", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7949", "mrqa_squad-validation-7975", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8092", "mrqa_squad-validation-8123", "mrqa_squad-validation-8151", "mrqa_squad-validation-8171", "mrqa_squad-validation-8180", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8250", "mrqa_squad-validation-8301", "mrqa_squad-validation-8316", "mrqa_squad-validation-8338", "mrqa_squad-validation-8341", "mrqa_squad-validation-8349", "mrqa_squad-validation-835", "mrqa_squad-validation-8394", "mrqa_squad-validation-8401", "mrqa_squad-validation-8418", "mrqa_squad-validation-8428", "mrqa_squad-validation-8457", "mrqa_squad-validation-8478", "mrqa_squad-validation-8527", "mrqa_squad-validation-8599", "mrqa_squad-validation-862", "mrqa_squad-validation-8664", "mrqa_squad-validation-8677", "mrqa_squad-validation-8682", "mrqa_squad-validation-8690", "mrqa_squad-validation-8694", "mrqa_squad-validation-8700", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8839", "mrqa_squad-validation-8840", "mrqa_squad-validation-8902", "mrqa_squad-validation-9036", "mrqa_squad-validation-9041", "mrqa_squad-validation-9047", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-929", "mrqa_squad-validation-9317", "mrqa_squad-validation-9456", "mrqa_squad-validation-9487", "mrqa_squad-validation-9492", "mrqa_squad-validation-9504", "mrqa_squad-validation-9524", "mrqa_squad-validation-9653", "mrqa_squad-validation-967", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9699", "mrqa_squad-validation-9761", "mrqa_squad-validation-9770", "mrqa_squad-validation-9787", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9971", "mrqa_squad-validation-9995", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1111", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1780", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2137", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3748", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4590", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5181", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5660", "mrqa_triviaqa-validation-5780", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6026", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7450", "mrqa_triviaqa-validation-7612", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-828"], "OKR": 0.888671875, "KG": 0.4421875, "before_eval_results": {"predictions": ["Cuba", "The General Conference of the United Methodist Church", "Satya Nadella", "24\u201310", "achieving crime control via incapacitation and deterrence", "Cosgrove Hall", "eleven", "issues under their jurisdiction", "article 49", "Corliss", "Doctor of Pharmacy (Pharm. D.)", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "the father of the house when in his home", "Battle of Dalan Balzhut", "Since the 1970s, numerous geoglyphs have been discovered on deforested land dating between AD 0\u20131250", "higher returns", "US$10 a week raise over Tesla's US$18 per week salary", "1698", "Urgench", "clinical pharmacists", "exceeds any given number", "Enric Miralles", "178", "Franklin, Tennessee", "volkswagen", "two", "robert boyle", "cell phones are valuable contraband,", "March 24", "CEO of an engineering and construction company", "Olivia Newton-John", "Fernando Gonzalez", "July 4", "Nazi Germany", "269,000", "taking a medicine that contained the banned substance cortisone", "long domestic relay in Olympic history", "collaborating with the Colombian government,", "250,000", "Ricardo Valles de la Rosa", "Sporting Lisbon", "Tuesday", "leftist Workers' Party", "al-Shabaab", "Arthur E. Morgan III", "head for Italy", "behind the counter", "Sheikh Sharif Sheikh Ahmed", "Derek Mears", "United Arab Emirates based companies", "host the Olympic Games in Rio de Janeiro", "Jacob", "strife in Somalia", "insurgency", "revolutionized the world of music downloads", "two years", "2026", "Harold Godwinson", "humps", "australia", "every Rose Has Its Thorn", "every aspect of public and private life", "Tokyo", "cVS"], "metric_results": {"EM": 0.625, "QA-F1": 0.73989898989899}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 0.6, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 0.8, 1.0, 1.0, 0.9090909090909091, 0.9333333333333333, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4298", "mrqa_squad-validation-2318", "mrqa_squad-validation-4304", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2620", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-10604", "mrqa_triviaqa-validation-3916"], "SR": 0.625, "CSR": 0.6629464285714286, "EFR": 1.0, "Overall": 0.7557924107142858}, {"timecode": 21, "before_eval_results": {"predictions": ["half", "specific indications, effectiveness of treatment regimens, safety of medications (i.e., drug interactions)", "Vince Lombardi", "CALIPSO", "1989", "an electrical generator.", "festivals", "potential drug interactions, adverse drug reactions, and assess patient drug allergies", "international", "the spread of diseases from Europe,", "Gilgamesh of Uruk and Atilla the Hun", "The Prospect Studios", "good, clear laws,", "Warsaw Uprising Museum", "some extra costs are levied.", "Warsaw University of Technology", "Stanford University", "1978", "Citadel Media", "abolish the state of Israel.", "2001", "continental European liberalism", "Good Neighbors", "an ornamental figure or illustration", "Paul Anka", "Wings", "hael", "Donatello", "seven", "a tin star.", "1982", "54-Marquesses", "George Clooney", "The Falkland Islands", "origami", "Wisconsin", "Norman Mailer.", "the NOW Magazine", "How to express your thanks in numerous different languages, and  how to reply when someone thanks you.", "AMNESTY", "Parsnip", "\"Archer.", "Justin Bieber", "The Merry Wives of Windsor", "Lingerie", "26", "dominoes", "Hadrian", "phylloxera", "Java", "changed the name of the country", "The Seven Year Itch", "Anita Brookner", "inflation", "Frogmore Estate", "copper", "September 4, 2000 to February 25, 2003", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "U.S. Marshals", "Thomas Jefferson", "\"Hillbilly Handfishin'\"", "22-10.", "arizonensis", "the ceiling"], "metric_results": {"EM": 0.65625, "QA-F1": 0.732373903508772}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6361", "mrqa_squad-validation-8021", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2069", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-7375", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-4815", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-1722", "mrqa_hotpotqa-validation-2937", "mrqa_searchqa-validation-9630"], "SR": 0.65625, "CSR": 0.6626420454545454, "EFR": 1.0, "Overall": 0.7557315340909091}, {"timecode": 22, "before_eval_results": {"predictions": ["steam", "a freshwater lake", "a family member", "Rhin", "increased local producer prices", "Queen Bees", "coal", "the LGBT community", "When the Methodists in America were separated from the Church of England", "imperial and colonial powers", "CBS", "differences in value added by labor, capital and land.", "Battle of Fort Bull", "Vince Lombardi", "Treaty provisions", "Albert Einstein", "X-ray", "as soon as they enter into force", "eighteenth century", "light", "one (or more)", "Bangladesh", "shark River Park", "1994", "adult reality show", "a face-to-face interview", "The Rosie Show", "Addis Ababa", "a remote part of northwestern Montana", "next year", "pirates", "greece", "a man who was raised at Camp Lejeune", "hung a noose", "2,700-acre", "Copts", "the deployment of 30,000 additional U.S. troops to Afghanistan", "to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "suicide bombing", "Red Cross", "Jacob", "Battlefield helicopter crews", "380,000", "Larry Ellison", "an Airbus A320-214", "$50", "a head injury.", "Venus Williams", "1912", "prostate cancer", "Larry Zeiger", "pro-democracy activists", "baron", "Two United Arab Emirates based companies", "nighttime", "Hot Wings", "transmitted these messages over military telephone or radio communications nets", "Lois", "Newton", "Tony Burke", "47,818", "Dante", "Corinthian", "baron"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6259539072039071}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.22222222222222224, 0.1904761904761905, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.07692307692307693, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8325", "mrqa_squad-validation-9730", "mrqa_squad-validation-5764", "mrqa_squad-validation-1407", "mrqa_squad-validation-8771", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-3412", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-932", "mrqa_newsqa-validation-1286", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2863", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5431", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-7611", "mrqa_searchqa-validation-4084"], "SR": 0.546875, "CSR": 0.6576086956521738, "EFR": 1.0, "Overall": 0.7547248641304348}, {"timecode": 23, "before_eval_results": {"predictions": ["Brompton district of the Royal Borough of Kensington and Chelsea", "Golden Gate Bridge", "Wars of Religion", "winter of 1973\u201374", "steam", "festivals", "15 February 1763", "major cities", "hotel room", "Oxygen therapy", "Tibetan Buddhism", "transubstantiation", "Swahili", "plantar fasciitis", "necessity", "Ancient Egypt", "catechism", "Court of Justice of the European Union (CJEU)", "tenfold", "San Joaquin Light & Power Building", "Crips", "Overland Park, Kansas", "White Horse", "XVideos", "that Fama and French's research is period dependent.", "16 March 1987", "south", "twenty-three", "beer and soft drinks", "skiing and mountaineering", "east", "Patricia Arquette", "Robert Paul \"Robbie\" Gould III", "Fred Derry", "The Onion", "Mary Bonauto, Susan Murray", "Malayalam fantasy comedy", "Edith Cavell", "end of the 17th century", "Jenn Brown", "League of the Three Emperors", "Viacom Media Networks", "Oakland, California", "1993", "Randal Keith Orton", "Sullenberger III", "Dziga Vertov", "Bill Clinton", "Martin Scorsese", "Florence Foster Jenkins", "Haitian Revolution", "filibuster and scathing rhetoric,", "60", "Nossa Senhora de F\u00e1tima, Macau", "Kalahari Desert", "$1.84 billion", "in 2008", "Tornado", "Harold Wilson", "Ronald Cummings", "curfew in Jaipur", "Sioux City", "Luxor", "Arnold Schoenberg"], "metric_results": {"EM": 0.625, "QA-F1": 0.7248139880952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.25, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.0, 0.28571428571428575, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3053", "mrqa_squad-validation-629", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5030", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-583", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-4134", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-8696", "mrqa_newsqa-validation-1616"], "SR": 0.625, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.7544531250000001}, {"timecode": 24, "before_eval_results": {"predictions": ["The largest Tesco store in the United Kingdom is located in Kingston Park on the edge of Newcastle", "Thermochemical", "was lost in the 5th Avenue laboratory fire of March 1895", "Orthodox Christians", "July 23, 1963", "twelve", "gain support from China for a planned $2.5 billion railway", "local church lay servant, who serve in and through their local churches, in other churches, and through district or conference projects and programs", "Hitler's secret police demanded to know if they were hiding a Jew in their house.\"", "The league", "Wednesdays", "17.5 million", "104 \u00b0F (40 \u00b0C)", "blue-green algae", "Acute oxygen toxicity", "New England Patriots", "chemical bonds", "Harrods", "extra-legal", "HSH Nordbank Arena", "The two-hour finale", "Tuesday afternoon.", "When people gathered outside as the conference in the building ended.", "The new star is next to the iconic Hollywood headquarters of Capitol Records,", "first-degree murder", "to share personal information", "David Beckham", "sharia law", "\"She was focused so much on learning that she didn't notice,\"", "three out of four", "Barack Obama", "Department of Homeland Security Secretary Janet Napolitano", "The contraband cell phones were found behind bars or in transit to Texas inmates in 2008.", "Climatecare", "Venus Williams", "Graziano Transmissioni", "April 2010", "The charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "Iran's parliament speaker", "Asashoryu", "Tigris and Euphrates", "The minister later apologized, telling CNN his comments had been taken out of context.", "The man was refused citizenship because he was depriving his wife of the liberty to come and go with her face uncovered,", "Bangladesh", "forgery and flying without a valid license", "plastic surgery research", "Four Americans", "Purvis", "\"Slumdog Millionaire\"", "34", "It was \"deeply saddened\" by the accident.\"", "Another billion people worldwide", "in the cellar", "6-1", "former U.S. secretary of state.", "Hope Alice Williams", "900 rib", "Leo", "Scorpius", "George A. Romero", "\"50 best cities to live in.\"", "Whitehorse", "The Viga", "Tomorrowland"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6846381532188357}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true], "QA-F1": [0.10526315789473685, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.08, 0.9032258064516129, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.08, 1.0, 0.4444444444444445, 1.0, 1.0, 0.23529411764705882, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.6206896551724138, 1.0, 0.2857142857142857, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5010", "mrqa_squad-validation-8452", "mrqa_squad-validation-10080", "mrqa_squad-validation-6998", "mrqa_squad-validation-170", "mrqa_squad-validation-7162", "mrqa_squad-validation-3599", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-806", "mrqa_naturalquestions-validation-3206", "mrqa_naturalquestions-validation-6074", "mrqa_triviaqa-validation-7252", "mrqa_searchqa-validation-12546"], "SR": 0.546875, "CSR": 0.651875, "EFR": 1.0, "Overall": 0.753578125}, {"timecode": 25, "before_eval_results": {"predictions": ["more equality in the income distribution", "Lunar Module Pilot", "Apollo 11", "few British troops.", "linebacker", "unicellular organisms", "high demand", "evenly round the body", "BBC UKTV", "the solution", "stout man with a \"double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck.\"", "Sweden", "macrophages and lymphocytes", "14,000", "Tony Hawk", "C. J. Anderson", "the biologically important energy-carrying molecules ATP and ADP", "average workers", "brigadier general, major general, and lieutenant general", "Mycological Bulletin", "psilocybin, psilocin and baeocystin", "Scotland", "Raimond Gaita", "Andrew Preston", "the Haitian Revolution", "saloon-keeper and Justice of the Peace", "July", "Wayman Tisdale", "Massachusetts", "26,000", "August 14, 1848", "The Dressmaker", "punk rock", "Dark Heresy", "North Holland", "James Brolin", "1874 until 1994", "1698", "UFC 50: The War of '04", "6", "1987", "Mwabvi river", "John M. Dowd", "Amber Laura Heard", "the 2011 Pulitzer Prize in General Nonfiction", "Reverend Timothy \"Tim\" Lovejoy", "1888-1954", "Patterns of Sexual Behavior", "George Raft", "UFC Fight Pass", "McLean, Virginia", "Nicole Kidman", "\"Odorama\"", "February 22, 1968", "Allen County", "Long Island", "Jacques Cousteau", "Bodhidharma", "Joe Hart", "Awa", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "salt", "Afghanistan", "1879"], "metric_results": {"EM": 0.5, "QA-F1": 0.5941612462292609}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 0.0, 0.5, 0.3076923076923077, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6453", "mrqa_squad-validation-7408", "mrqa_squad-validation-7791", "mrqa_squad-validation-2599", "mrqa_squad-validation-3624", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-2293", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1381", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-649", "mrqa_naturalquestions-validation-5143", "mrqa_triviaqa-validation-310", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1848", "mrqa_searchqa-validation-4623", "mrqa_naturalquestions-validation-7387"], "SR": 0.5, "CSR": 0.6460336538461539, "EFR": 1.0, "Overall": 0.7524098557692308}, {"timecode": 26, "before_eval_results": {"predictions": ["gender roles and customs", "80,000", "complexity classes", "9 March 1508", "reduce consumer costs", "2020", "their greatest common divisor is one", "Sophocles", "low latitude", "1726", "environmental factors", "backing for the uprising", "around 28,000", "The Reconstruction of Religious Thought in Islam", "July", "Harvey Martin", "water pump", "state senators who will decide whether to remove him from office", "Islamabad's foreign ministry,", "it", "Nearly all of Britain's troops in Iraq", "Susan Atkins", "almost 100 vessels", "40 militants and six Pakistan soldiers dead", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "the United States", "\"The elections are expected to provide Mugabe with the toughest challenge yet in his nearly 28 years of rule.", "Cambodian territory", "Monday night", "the 45-year-old future president", "one out of every 17 children under 3 years old in America", "some of our competitors", "Georgia", "his grandfather was a \"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of terror hate organizations\"", "Welshman Ncube", "in 1995", "Turkey", "President Obama", "piers Morgan", "an Omani national", "East Java", "protective shoes", "Thaksin Shinawatra", "gasoline", "diplomatic relations", "auxiliary lock", "prostate cancer", "The ACLU", "fight back against Israel in Gaza", "soldiers", "off the coast of Dubai", "cancer", "five minutes before commandos descended from ropes that dangled from helicopters,", "benzodiazepines", "retinal ganglion cell axons", "cotton", "Samson", "Jack Ridley", "bioelectromagnetics", "Scandinavian", "p.m.", "thunderstorms"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6328874869086266}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 0.8, 0.923076923076923, 0.18181818181818182, 0.1904761904761905, 0.888888888888889, 1.0, 0.1111111111111111, 1.0, 0.6666666666666666, 0.0, 0.23529411764705882, 0.0, 0.0, 0.10810810810810811, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.625, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7041", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2349", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1292", "mrqa_naturalquestions-validation-3368", "mrqa_hotpotqa-validation-2944", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-11260"], "SR": 0.515625, "CSR": 0.6412037037037037, "EFR": 0.967741935483871, "Overall": 0.744992252837515}, {"timecode": 27, "before_eval_results": {"predictions": ["Milka, Angelina and Marica", "Working Group chairs", "nine", "Stanley Steamer", "nerves", "rediscovery of \"Christ and His salvation\"", "a program of coordinated, evolving projects sponsored by the National Science Foundation", "nervous breakdown", "NewcastleGateshead", "cholecalciferol", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "government and the National Assembly and the Senate", "vocational subjects", "four", "an eight-year term", "Shoushi Li", "the Scilly Isles", "Happy Birthday to You", "The Simpsons", "Brazil", "gravlax", "Annabel Croft", "D.\u00a0W. Griffith", "flowering plants", "James Bond", "Chester", "a skunk", "James Bowie", "the University of Cambridge", "Argentina", "penny", "Jackie Chiles", "\"Oriver Twist\"", "cella", "Morten Skovsby", "Mesopotamia", "West Ham", "Las Vegas", "Charlie Brown", "Joanne Harris", "a leopard seal", "On the Braden Beat", "parsnip", "port", "Day 1", "steam turbines", "a duck", "Walter Hagen", "Dirty Dancing", "his most important contribution is \u2018Butskellism\u2019", "Raspberry", "the Bank of England", "Texas", "Tuscaloosa", "1936", "35 to 40 hours per week", "neuro-orthopaedic", "one child, Lisa Brennan-Jobs", "Bhutto died Thursday after a suicide bombing at a political rally in Rawalpindi, Pakistan.", "17 in New York,", "Camillo", "over 500000", "Douglas MacArthur", "flavonoids"], "metric_results": {"EM": 0.5, "QA-F1": 0.5609471006144393}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4285714285714285, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4846", "mrqa_squad-validation-6426", "mrqa_squad-validation-8234", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-6416", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-3167", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-7193", "mrqa_naturalquestions-validation-6770", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1072", "mrqa_searchqa-validation-16770", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-7657"], "SR": 0.5, "CSR": 0.6361607142857143, "EFR": 0.96875, "Overall": 0.7441852678571429}, {"timecode": 28, "before_eval_results": {"predictions": ["quantum electrodynamics", "Khans of various ethnicities as part of the Mongol Empire", "westerns and detective series", "one astronaut", "The NBC Blue Network", "Friday", "CD40", "The \"Big Five\"", "high", "\"exterminate\" all non-Dalek beings", "all of the citizens of Warsaw", "a second Bachelor's degree", "Miocene", "Rhin", "in the kingdom", "James Bond", "Madagascar", "shoulders", "miliel Spark", "Japan", "John Peel", "argument form", "Christine Keeler", "\"Comedy Playhouse\u201d", "leopard", "frogs", "The Verruckt", "o-s-c-a-are", "weather", "stewarda pulao", "antelope", "robson", "bridge", "Jersey", "fluorine", "the Northern line", "harold harold wilson", "london", "british broadcasting corporation", "\"black\"", "Venezuela", "water", "saxophone", "herb", "high noon", "1925", "Bosnian", "narwhals", "eagle", "one to three inches", "The Blind Beggar pub", "Vatican Crypt", "tiger", "The Wooden Prince", "Andrew Garfield", "leonais", "Delilah Luke", "Tianhe Stadium", "Guinea, Myanmar, Sudan and Venezuela", "10 a.m.", "Phantom", "bond", "Washington", "Mineola"], "metric_results": {"EM": 0.5, "QA-F1": 0.5810763888888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6352", "mrqa_squad-validation-5906", "mrqa_squad-validation-7778", "mrqa_squad-validation-1051", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-756", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-515", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-4045", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-6264", "mrqa_triviaqa-validation-1848", "mrqa_naturalquestions-validation-19", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2580", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-7462"], "SR": 0.5, "CSR": 0.6314655172413793, "EFR": 0.96875, "Overall": 0.7432462284482759}, {"timecode": 29, "before_eval_results": {"predictions": ["ten times their own weight", "nine", "a year", "2003", "Africa", "Nederrijn", "lack of understanding of the legal ramifications, or due to a fear of seeming rude", "Elie Metchnikoff", "a global", "50%", "7\u20134\u20132\u20133 system", "Connectional Table", "2nd century BCE", "1060s", "George Washington Carver", "\"The Pelican Brief\"", "panther", "Viet Nam", "Kyoto", "alia", "pannier", "dollar", "225", "oven", "diphthong", "henry higgins", "FLORIDA", "Austria", "oscar", "Seventy-six trombones", "red", "bennett", "ER", "crustacean", "hen", "Robert Bork", "Luzon", "vainberg", "The Big Sleep", "quid pro quo", "bismarck", "the chalk cliffs", "Prague", "notes", "vanya", "Harlem", "Yitzhak Rabin", "tICA", "sugarfoot", "Normandy", "bumblebee", "weasel", "tariq jahan", "oven", "central plains", "49", "the Big Bang", "higgins", "southwestern", "Jenji Kohan", "public opinion", "a monthly allowance", "Via Vai", "Black Abbots"], "metric_results": {"EM": 0.5625, "QA-F1": 0.609375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8490", "mrqa_searchqa-validation-13985", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-8718", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-4467", "mrqa_searchqa-validation-2393", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-1650", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-893", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-14246", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-14606", "mrqa_searchqa-validation-1761", "mrqa_searchqa-validation-11380", "mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-6317", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-1871", "mrqa_newsqa-validation-3833"], "SR": 0.5625, "CSR": 0.6291666666666667, "EFR": 1.0, "Overall": 0.7490364583333333}, {"timecode": 30, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2293", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3502", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-3861", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5729", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-774", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9986", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-2912", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3412", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-542", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-868", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-12609", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-14606", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-320", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-3841", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7611", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7920", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-917", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9604", "mrqa_squad-validation-10009", "mrqa_squad-validation-10018", "mrqa_squad-validation-10025", "mrqa_squad-validation-10027", "mrqa_squad-validation-10064", "mrqa_squad-validation-101", "mrqa_squad-validation-10264", "mrqa_squad-validation-10294", "mrqa_squad-validation-10370", "mrqa_squad-validation-10474", "mrqa_squad-validation-10491", "mrqa_squad-validation-10500", "mrqa_squad-validation-1068", "mrqa_squad-validation-1081", "mrqa_squad-validation-1148", "mrqa_squad-validation-1156", "mrqa_squad-validation-1272", "mrqa_squad-validation-1273", "mrqa_squad-validation-1355", "mrqa_squad-validation-1371", "mrqa_squad-validation-1550", "mrqa_squad-validation-159", "mrqa_squad-validation-167", "mrqa_squad-validation-1684", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1889", "mrqa_squad-validation-195", "mrqa_squad-validation-1977", "mrqa_squad-validation-1980", "mrqa_squad-validation-1996", "mrqa_squad-validation-2140", "mrqa_squad-validation-2146", "mrqa_squad-validation-2169", "mrqa_squad-validation-2191", "mrqa_squad-validation-2329", "mrqa_squad-validation-237", "mrqa_squad-validation-2411", "mrqa_squad-validation-2423", "mrqa_squad-validation-2453", "mrqa_squad-validation-2461", "mrqa_squad-validation-2467", "mrqa_squad-validation-2468", "mrqa_squad-validation-2473", "mrqa_squad-validation-2523", "mrqa_squad-validation-2540", "mrqa_squad-validation-256", "mrqa_squad-validation-258", "mrqa_squad-validation-2616", "mrqa_squad-validation-2634", "mrqa_squad-validation-2709", "mrqa_squad-validation-2778", "mrqa_squad-validation-2834", "mrqa_squad-validation-2885", "mrqa_squad-validation-2900", "mrqa_squad-validation-2909", "mrqa_squad-validation-2994", "mrqa_squad-validation-3028", "mrqa_squad-validation-3048", "mrqa_squad-validation-3063", "mrqa_squad-validation-3081", "mrqa_squad-validation-3125", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-317", "mrqa_squad-validation-318", "mrqa_squad-validation-319", "mrqa_squad-validation-32", "mrqa_squad-validation-3207", "mrqa_squad-validation-3233", "mrqa_squad-validation-3264", "mrqa_squad-validation-3267", "mrqa_squad-validation-327", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3469", "mrqa_squad-validation-3522", "mrqa_squad-validation-3641", "mrqa_squad-validation-3740", "mrqa_squad-validation-3751", "mrqa_squad-validation-3791", "mrqa_squad-validation-3837", "mrqa_squad-validation-3860", "mrqa_squad-validation-3921", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-4144", "mrqa_squad-validation-415", "mrqa_squad-validation-4267", "mrqa_squad-validation-4303", "mrqa_squad-validation-4325", "mrqa_squad-validation-4366", "mrqa_squad-validation-4429", "mrqa_squad-validation-4437", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4662", "mrqa_squad-validation-4670", "mrqa_squad-validation-472", "mrqa_squad-validation-4795", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-4958", "mrqa_squad-validation-5003", "mrqa_squad-validation-5010", "mrqa_squad-validation-5054", "mrqa_squad-validation-5077", "mrqa_squad-validation-5078", "mrqa_squad-validation-5134", "mrqa_squad-validation-5162", "mrqa_squad-validation-5179", "mrqa_squad-validation-5185", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5319", "mrqa_squad-validation-5351", "mrqa_squad-validation-5382", "mrqa_squad-validation-5400", "mrqa_squad-validation-5457", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-5557", "mrqa_squad-validation-5600", "mrqa_squad-validation-5607", "mrqa_squad-validation-5609", "mrqa_squad-validation-5611", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5754", "mrqa_squad-validation-5760", "mrqa_squad-validation-5840", "mrqa_squad-validation-5906", "mrqa_squad-validation-5939", "mrqa_squad-validation-5948", "mrqa_squad-validation-596", "mrqa_squad-validation-5966", "mrqa_squad-validation-6024", "mrqa_squad-validation-604", "mrqa_squad-validation-6086", "mrqa_squad-validation-6142", "mrqa_squad-validation-6161", "mrqa_squad-validation-6224", "mrqa_squad-validation-6227", "mrqa_squad-validation-6254", "mrqa_squad-validation-6268", "mrqa_squad-validation-6278", "mrqa_squad-validation-6284", "mrqa_squad-validation-6292", "mrqa_squad-validation-6349", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6380", "mrqa_squad-validation-6415", "mrqa_squad-validation-643", "mrqa_squad-validation-6434", "mrqa_squad-validation-6447", "mrqa_squad-validation-6457", "mrqa_squad-validation-6474", "mrqa_squad-validation-6492", "mrqa_squad-validation-654", "mrqa_squad-validation-6622", "mrqa_squad-validation-6624", "mrqa_squad-validation-6650", "mrqa_squad-validation-6744", "mrqa_squad-validation-6766", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6807", "mrqa_squad-validation-6812", "mrqa_squad-validation-6879", "mrqa_squad-validation-6941", "mrqa_squad-validation-7005", "mrqa_squad-validation-7026", "mrqa_squad-validation-7049", "mrqa_squad-validation-7051", "mrqa_squad-validation-708", "mrqa_squad-validation-7162", "mrqa_squad-validation-7170", "mrqa_squad-validation-7198", "mrqa_squad-validation-7208", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7260", "mrqa_squad-validation-7263", "mrqa_squad-validation-7284", "mrqa_squad-validation-7370", "mrqa_squad-validation-7387", "mrqa_squad-validation-7400", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-7492", "mrqa_squad-validation-7500", "mrqa_squad-validation-7508", "mrqa_squad-validation-7550", "mrqa_squad-validation-7562", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7794", "mrqa_squad-validation-7851", "mrqa_squad-validation-7859", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7949", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8123", "mrqa_squad-validation-8171", "mrqa_squad-validation-8180", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8234", "mrqa_squad-validation-8289", "mrqa_squad-validation-83", "mrqa_squad-validation-8319", "mrqa_squad-validation-8325", "mrqa_squad-validation-8338", "mrqa_squad-validation-8394", "mrqa_squad-validation-8418", "mrqa_squad-validation-8428", "mrqa_squad-validation-8476", "mrqa_squad-validation-8478", "mrqa_squad-validation-8527", "mrqa_squad-validation-8599", "mrqa_squad-validation-862", "mrqa_squad-validation-8652", "mrqa_squad-validation-8664", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8771", "mrqa_squad-validation-8839", "mrqa_squad-validation-8840", "mrqa_squad-validation-8902", "mrqa_squad-validation-9020", "mrqa_squad-validation-9036", "mrqa_squad-validation-9243", "mrqa_squad-validation-9247", "mrqa_squad-validation-9247", "mrqa_squad-validation-9486", "mrqa_squad-validation-9492", "mrqa_squad-validation-9504", "mrqa_squad-validation-9653", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9761", "mrqa_squad-validation-9762", "mrqa_squad-validation-9770", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-999", "mrqa_squad-validation-9995", "mrqa_triviaqa-validation-1111", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-122", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2618", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-310", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3471", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3620", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-4045", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4590", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5318", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6006", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7375", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-987"], "OKR": 0.865234375, "KG": 0.47890625, "before_eval_results": {"predictions": ["the mid-Eocene", "2004", "yes", "lives of their disciples", "data sampling is biased away from the center of the Amazon basin", "downward", "Westinghouse Electric", "captured Fort Beaus\u00e9jour in June 1755", "Saxon chancellery", "Advanced Steam movement", "July 23, 1963", "Stanford University", "the coast of Mexico", "the disappearance of a strong ally and counterweight to British expansion", "an alphabet", "Elijah Muhammad", "bamboo", "Benjamin Franklin", "Guatemala", "Moscow", "Good Times", "Mount Hood", "The Backstreet Boys", "the hand", "Red", "a magnetic compass", "the Black Maria", "Walt Kelly", "the evaporator", "the QWERTY", "mitsumata", "The Trial", "Raytheon", "red", "a burn", "Thomas Hardy", "Signs", "the British", "the plunger", "cream", "adultery", "Sunday, November 6,", "Syracuse", "the Native American (Indian) Tribes", "an NCO", "the breath", "Toulouse", "dualism", "(Dan) Griffith", "French Guiana", "a Dutchboy", "a fruitcake", "viola", "(Mary) Wheaton", "McFerrin, Robin Williams, and Bill Irwin", "George Harrison", "perfumer", "Horns", "the 1999 Odisha cyclone", "15,000", "two remaining crew members", "a war-torn nation.", "any records showing that Barlow and the girl were married and any evidence of them having a child.", "the United States, NATO member states, Russia and India"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5324048913043479}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.17391304347826086, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4390", "mrqa_squad-validation-10273", "mrqa_squad-validation-10300", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-4646", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-12239", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-5735", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-14007", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-3933", "mrqa_searchqa-validation-5043", "mrqa_searchqa-validation-9552", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-4605", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-2901", "mrqa_searchqa-validation-8256", "mrqa_searchqa-validation-6586", "mrqa_naturalquestions-validation-3325", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5310", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-775"], "SR": 0.484375, "CSR": 0.6244959677419355, "EFR": 1.0, "Overall": 0.7464616935483871}, {"timecode": 31, "before_eval_results": {"predictions": ["Maria Fold and thrust Belt", "OpenTV", "E.I. du Pont", "a shortage of male teachers", "water", "John D. Rockefeller", "a lot of waste", "Subutai", "Gaelic", "New Orleans", "five", "half as much", "John Pell, Lord of Pelham Manor", "Cher", "a doses", "Treasure Island", "San Antonio", "Cantor", "Hindu", "the Amistad", "The Usual Suspects", "malaria", "Chicago", "Versailles", "Shakira", "Yokohama", "Stan Avery", "Morse Code", "the Merida Cable Car", "Pete Rose", "Hannibal", "Mercury", "Naples", "Iran", "Oahu", "standard", "Lincoln", "Chicago", "the Blue Nile", "insulin", "Newt Gingrich", "Alexander Hamilton", "the Great Wall", "a sugar-sweetened breakfast cereal", "Nepal", "Ernest Hemingway", "Stephen Hawking", "center", "guru", "a Lamb Souvlaki", "Beverly Hills", "the bumblebee", "Roosevelt", "Hermes", "during his first trip to Seattle", "the spectroscopic notation", "Switzerland", "Henry IV", "Giuseppe Verdi", "South Africa", "refused to refer the case of Mohammed al-Qahtani to prosecutors", "most devices carry few security risks.", "the heads of federal executive departments who form the Cabinet of the United States", "peace between two entities"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6476799242424243}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8181818181818181, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-4528", "mrqa_squad-validation-6128", "mrqa_squad-validation-341", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-460", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-10333", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-12500", "mrqa_searchqa-validation-3737", "mrqa_searchqa-validation-6966", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4343", "mrqa_searchqa-validation-15563", "mrqa_searchqa-validation-16250", "mrqa_searchqa-validation-7816", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-585", "mrqa_hotpotqa-validation-3953", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2370", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-645"], "SR": 0.546875, "CSR": 0.6220703125, "EFR": 1.0, "Overall": 0.7459765625}, {"timecode": 32, "before_eval_results": {"predictions": ["O2", "a peer-to-peer network architectures", "the Kenyan Coast", "SyFy", "1530", "the temperance movement", "12 December 1964", "18", "the British troops", "Harvard Yard", "all war", "Mongol peace", "the thymus", "red", "rani", "the endive", "the 18th century", "Flamenco", "Nero", "an Old Manse", "( Kenneth) Gorelick", "Rio de Janeiro", "the earth", "Tonto", "( Helen) Hayes", "New Revised Standard Version", "Bluetooth", "Ma Barker", "the St. Valentine's Day Massacre", "Ratatouille", "Spain", "(Louis) Tiffany", "the afrikaans", "Jericho", "(John) Cabot", "Orwell", "( Friedrich) Nietzsche", "eggs", "A Christmas Story", "Raymond", "Hawaii", "rhythmic gymnastics", "a falcon", "a prayer", "Sicily", "The Lord of the Rings", "Georgia O'Keeffe", "Athens", "Roxanne", "the family plot", "(Jos) Olmedo", "saturated fat", "Caspar Weinberger", "the Ninja Turtles", "Amerigo Vespucci", "1979", "an eclipse", "I Will survive", "Girls' Generation", "Guangzhou, China", "\"Dance Your Ass Off\"", "\"ocean sprawl\"", "$627,", "blew up an ice jam"], "metric_results": {"EM": 0.625, "QA-F1": 0.7036458333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3457", "mrqa_squad-validation-4676", "mrqa_squad-validation-8369", "mrqa_searchqa-validation-16221", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-7410", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-1105", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-11804", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2889", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-16241", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-6217", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3453"], "SR": 0.625, "CSR": 0.6221590909090908, "EFR": 1.0, "Overall": 0.7459943181818182}, {"timecode": 33, "before_eval_results": {"predictions": ["the Colony of Victoria Act", "1,345,598", "Cestum veneris", "the traditional Mongolian aristocracy", "exaptations", "colonizing empires", "to coordinate the response to the embargo", "the Pliocene period", "Apollo Program Director", "yellow fever outbreaks", "the thylakoid network", "fish larvae", "Saturn", "Nyasaland", "a Maltese", "Ramakant Desai", "(Max) Immelmann", "Mel Brooks", "Maltese", "the Archive of American Folk Song", "315", "the marsupials", "Florence", "Rodgers and Hammerstein", "Washington", "Malt", "South Africa", "British Airways", "Hopper", "divorce", "three", "Peter Paul Rubens", "( Goran) Ivanisevic", "cabbage", "June", "Judy Garland", "Mel Brooks", "Solo", "Tina Turner", "Angevin", "Hydrogen", "Tesla", "magnesium", "David Frost", "(Mary) Pickford", "Edward VIII", "(Saffron)", "Hercules", "Sousa", "New Zealand", "the giraffe", "the Crusades", "(Sue) Hill", "big house", "Kevin Garnett", "1980", "drummer", "Marco Hietala", "Zed", "$60 billion", "Maltese", "Gone Home", "Portugal", "5 liters"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6255580357142856}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2857142857142857]}}, "before_error_ids": ["mrqa_squad-validation-7151", "mrqa_squad-validation-8806", "mrqa_squad-validation-4319", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-497", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2686", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-350", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-1956", "mrqa_naturalquestions-validation-2008", "mrqa_naturalquestions-validation-2732", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-15800", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-4054"], "SR": 0.546875, "CSR": 0.6199448529411764, "EFR": 1.0, "Overall": 0.7455514705882353}, {"timecode": 34, "before_eval_results": {"predictions": ["powerful", "an inauspicious typhoon", "municipal building inspector", "Battle of Hastings", "gold", "Victoria", "western European", "Muslim and Chinese siege engines", "four", "the town council", "tea or porridge", "tell them the truth about why you broke up!", "January 24, 2006.", "William Shakespeare", "George Lucas", "The Da Vinci Code", "the Russian air force,", "10 below zero", "18th", "byproducts emitted during the process of burning and melting raw materials.", "london, England", "\"It was more of an event held by a radio station.\"", "misdemeanor assault charges", "wings", "More than 22 million people in sub-Saharan Africa", "Mumbai", "nirvana", "New Haven, Connecticut, firefighter", "I went to stay with my Uncle Jack in Miami Beach.", "he spent the first night in his car.\"", "Nazi Party members", "Empire of the Sun", "genocide", "her husband", "\"It feels good for me to talk about her,\"", "killed six young people at a house party in Crandon, Wisconsin,", "about 3,000 kilometers (1,900 miles)", "Australia", "burned over 65 percent of his body", "attempted murder", "Intensifying violence, food shortages and widespread drought", "robert williams", "India", "Hakeemullah Mehsud", "LEDs", "preserved corpses having sex", "Cambodian territory", "\"Horehound\"", "red", "more than 2.5 million copies,", "commitment to equality,", "two Lebanese soldiers,", "British", "his brother, who died in action in the United States Army", "Karina Smirnoff", "lyonesse", "football", "Merovingian", "Chicago, Illinois", "First Amendment to the United States Constitution", "portier", "richard nixon", "venus connery", "venus williams"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4420042293233083}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.08, 0.0, 0.0, 0.0, 0.7368421052631579, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.25, 0.5, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6968", "mrqa_squad-validation-6253", "mrqa_squad-validation-8517", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1336", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3889", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-2930", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1674", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-7046", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7531"], "SR": 0.34375, "CSR": 0.6120535714285714, "EFR": 1.0, "Overall": 0.7439732142857143}, {"timecode": 35, "before_eval_results": {"predictions": ["4,222,000", "1271", "17", "Falls are one of the most common causes of fatal and non-fatal injuries among construction workers", "Highly combustible", "Engineering News- Record (ENR)", "single-tape Turing machines", "Pitt", "\u00a330m", "On the Councils and the Church", "Honorary freemen", "Congress", "he didn't know if Woods' wife, Elin Nordegren, would appear with her husband.", "Sri Lanka's Tamil rebels", "daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.\"", "passengers on the Miva Marmara", "37th", "Liverpool Street Station", "Black and Hispanic students", "soldiers are coming home, as a result of our policy called 'Return on Success.'", "one Iraqi soldier,", "Department of Homeland Security Secretary Janet Napolitano", "glamour and hedonism", "This will be the second time since the 1990s that the army has been sent in to combat Mafia crime in southern Italy,", "Math teacher Mawise Gumba", "Former U.S. soldier Steven Green", "more than 78,000 parents of children ages 3 to 17.", "March 22, the ACLU said.", "Afghanistan and India", "Tuesday's iPhone 4S news,", "we seek a new way forward, based on mutual interest and mutual respect.\"", "two", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "environmental", "Little Rock Central High School in Arkansas.\"", "NATO's International Security Assistance Force", "U.S. senators", "summer", "Bob Johnson", "Kim Il Sung", "63", "$250,000", "Communist Party of Nepal (Unified Marxist-Leninist)", "Diego Maradona", "April 22.", "She said Cain suggested meeting over dinner, then tried to reach up her skirt after the meal -- and when she protested, he told her, \"You want a job, right?\"", "She must now enter calculations for the French Open", "MS Columbus,", "Former U.S. President Bill Clinton", "Sonia Sotomayor", "Rambosk", "15-month investigation,", "a president who understands the world today, the future we seek and the change we", "tRNAs are a necessary component of translation, the biological synthesis of new proteins in accordance with the genetic code", "The original building was completed in 1800", "Barry Humphries", "James Chadwick", "Patrick Dempsey and Amanda Peterson", "Kristy Lee Cook", "Clint Eastwood", "Italy", "Sarah Newlin", "General Motors", "Doctor of Philosophy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6206882833920877}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.125, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.782608695652174, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.5, 0.4444444444444445, 0.375, 0.09523809523809523, 0.6666666666666666, 0.5714285714285715, 0.7777777777777777, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.1081081081081081, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-7006", "mrqa_squad-validation-3490", "mrqa_squad-validation-6693", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-234", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-187", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-5297"], "SR": 0.484375, "CSR": 0.6085069444444444, "EFR": 1.0, "Overall": 0.743263888888889}, {"timecode": 36, "before_eval_results": {"predictions": ["new form", "Greg Brady", "the mid-sixties", "less than a year", "average", "the 7th century,", "William Tyndale", "Henry Cole", "released Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists.", "military action,", "mixed martial arts", "Rick and Morty", "Ringo Starr", "Don Juan", "Resorts World Genting", "World of Wonder", "Vernier, Switzerland", "the Ruul", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "Springfield, Massachusetts", "musician", "2004 Paris Motor Show", "Jonathan Daniel Hamm", "Abdul Razzak Yaqoob", "Vilnius Old Town", "Mr. Tumnus", "elders", "Gerard Marenghi", "Bolton, England", "Conservative", "1966", "Indiana,", "Portsmouth", "romantic comedy", "Mani", "Muslim", "people working in film and the performing arts,", "Nicolas Vanier", "Antigua & Barbuda, Argentina, South Africa, Mauritius, Mayotte, R\u00e9union, Seychelles,", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Bob Mould", "German", "electric currents and magnetic fields", "Oklahoma Sooners", "books, films and other", "Galway", "Razor Ramon", "illnesses", "land area", "Veneto region of Northern Italy", "Bisexuality", "400 MW", "Germany", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "IB Diploma Program", "the Caribbean Chili Of Choice", "Eva Per\u00f3n", "15", "to break up ice jams.", "the Mississippi River", "pelota", "Michael Jackson and Lionel Richie", "George Halas", "Florida"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5671296296296295}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.33333333333333337, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7000000000000001, 0.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1701", "mrqa_squad-validation-9567", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-1059", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-4931", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-4473", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-1283", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-6179", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3460", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-2977", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-5468"], "SR": 0.484375, "CSR": 0.605152027027027, "EFR": 1.0, "Overall": 0.7425929054054055}, {"timecode": 37, "before_eval_results": {"predictions": ["not having a residence permit", "Torchwood", "the p-adic norm", "sedimentary", "Doritos", "30", "the dinophyte nucleus", "six", "Blum complexity axioms", "electromagnetic force", "15,000 people", "Boston, Massachusetts", "Mudvayne", "\"Aloha \u02bbOe\"", "Jeffrey Adam \"Duff\" Goldman", "69.7 million litres", "the stars, like Serena Williams, Colin Montgomerie and Ben Ainslie", "an English Grand Prix motorcycle road racer and Formula One driver", "Kait Parker", "Revolution Studios", "Lord's Resistance Movement", "June 26, 1970", "Dorothy", "2008", "Cookstown", "1978", "Colonel", "Giuseppe Verdi", "madonna", "North Carolina", "Vogue", "Arthur Freed", "a military coup", "the lead roles of Timmy Sanders", "Maine", "Oklahoma Sooners", "the University College of North Staffordshire, England", "more than 230", "Northern Irish", "since 1736", "Joachim Trier", "Michael Burger", "Washington, D.C.", "Andrzej Go\u0142ota", "Patrick Dempsey", "Flaw", "president", "Derry City F.C.", "My Gorgeous Life", "1,382 inhabitants", "Nana Patekar", "KBS2", "2005", "Jack Gleeson", "18th century", "the Thomas Cup", "Caernarfon", "Barack Obama", "madonna", "San Antonio de Valero Mission", "operatic", "Robert Mugabe", "More than 150,000", "3.5 percent of global greenhouse emissions."], "metric_results": {"EM": 0.578125, "QA-F1": 0.688056734931735}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.5, 1.0, 0.9090909090909091, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.923076923076923]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_squad-validation-8958", "mrqa_squad-validation-10483", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5779", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4196", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-923", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-5052", "mrqa_triviaqa-validation-5021", "mrqa_newsqa-validation-3529", "mrqa_searchqa-validation-2", "mrqa_searchqa-validation-11958", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-2507"], "SR": 0.578125, "CSR": 0.6044407894736843, "EFR": 0.9629629629629629, "Overall": 0.7350432504873294}, {"timecode": 38, "before_eval_results": {"predictions": ["deforestation", "21 to 11", "a commune (gmina)", "lower lake", "the Yuan dynasty", "a quantity surveyor", "Squillace", "chloroplasts", "the wedding banquet", "Starlite", "David Yates", "New Zealand food writer", "the end of the Roman Republic", "265 million", "the County of York", "Lev Ivanovich Yashin", "Germanicus", "Leonard", "Edmonton, Alberta", "Brooklyn", "Symphony No. 7", "champion dancer", "Indianapolis Motor Speedway", "Central University of India, Aligarh Muslim University", "on Boxing Day, 2004", "an additional conservation law for total energy, of which kinetic energy of motion is one element", "Detective Stan \"Wojo\" Wojciehowicz", "Daniel Sturridge", "supernatural psychological horror", "Philippe of Belgium", "Ardeth Bay", "Hans Rosenfeldt", "January 15, 1975", "I Am Furious", "Nikolai Trubetzkoy", "books, films and other media", "1770", "John Meston", "Tottenham", "Operation Neptune", "The Future", "Ready Player One", "Cecily Legler Strong", "\"My Own Worst Enemy\"", "Gillian Leigh Gibson", "SKUM", "Tom Ewell", "the western end of the National Mall in Washington, D.C., across from the Washington Monument", "1976", "Vice President George Mifflin Dallas", "the Sun", "1987", "Thorgan", "quarterback", "hairpin turn", "an antipasto, appetizer, table bread, or snack", "ABBA", "Fareed Zakaria:", "torture and indefinite detention", "Indiana", "the Nixon scandal", "Vice President, Speaker of the House of Representatives, President pro tempore of the Senate, and then the heads of federal executive departments who form the Cabinet of the United States", "May 30, 2017", "Mahatma Gandhi"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7005005411255412}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 0.25, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.2857142857142857, 1.0, 0.7499999999999999, 1.0, 1.0, 0.4, 0.25, 0.0, 0.2857142857142857, 1.0, 1.0, 0.8, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-973", "mrqa_squad-validation-1028", "mrqa_squad-validation-8792", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3973", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-3064", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-9979", "mrqa_triviaqa-validation-3529", "mrqa_newsqa-validation-1110", "mrqa_searchqa-validation-3074", "mrqa_naturalquestions-validation-5537"], "SR": 0.5625, "CSR": 0.6033653846153846, "EFR": 1.0, "Overall": 0.742235576923077}, {"timecode": 39, "before_eval_results": {"predictions": ["11\u201313th century AD", "nearly 75% of voters in the proposed Territory of Colorado", "the tolerance of civil disobedience", "Ersatzschulen", "1543", "Edward the Confessor", "July 2013", "Warszawa", "the University of Northumbria at Newcastle", "the Great Fire of London", "a Zen monastery", "George Glenn Jones", "arch", "Life of Pi", "the Great Circus Parade", "Nixon", "the endangered pacarana", "Judy Garland", "The English Patient", "Chicago Cubs", "the Militia Act", "Stoke-on-Trent", "August Wilson", "an accordion", "Volkswagen Passat", "Mad About You", "a promissory note", "James Patterson", "Great Britain", "the Voice of the Century", "Maria Sharapova", "Daniel Defoe", "\"The Secrets of a Fire King\"", "Picuelo Streets", "National History of New Orleans", "the AmericanATODAY.com", "the Minotaur", "The Man Who Mistook His wife for a Hat", "bleach", "\"Good Times Roll\"", "sculptor Leonard Craske", "the Bordeaux region of France", "George Washington", "the White Rabbit", "a worthless thing or endeavor", "Tila Tequila", "Chris O'Donnell", "Emancipation Proclamation", "the British Flag", "Billy Joel", "Wuthering Heights", "the Caribbean Sea", "the Catholic Church", "1963", "Stephen Graham", "Leeds", "bitter liqueurs", "pop music and popular culture", "5,656", "almost 100", "in a hotel near Fort Bragg", "Ojekero Tweya", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "dorsally on the forearm"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5113181089743589}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6, 0.0, 0.3076923076923077, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2739", "mrqa_squad-validation-5337", "mrqa_searchqa-validation-4961", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-4136", "mrqa_searchqa-validation-4117", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-15899", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-116", "mrqa_searchqa-validation-13589", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-16699", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-2218", "mrqa_searchqa-validation-14815", "mrqa_searchqa-validation-7777", "mrqa_searchqa-validation-4513", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-2794", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-4986", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-13468", "mrqa_triviaqa-validation-6197", "mrqa_hotpotqa-validation-4527", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1391", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-4351"], "SR": 0.4375, "CSR": 0.59921875, "EFR": 0.9444444444444444, "Overall": 0.730295138888889}, {"timecode": 40, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3630", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4987", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-4532", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9778", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2084", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1184", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16699", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2218", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2794", "mrqa_searchqa-validation-2795", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4117", "mrqa_searchqa-validation-4228", "mrqa_searchqa-validation-4467", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5043", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-6365", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7575", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7777", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9784", "mrqa_searchqa-validation-9894", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10203", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-103", "mrqa_squad-validation-10316", "mrqa_squad-validation-10489", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1311", "mrqa_squad-validation-1343", "mrqa_squad-validation-1379", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1481", "mrqa_squad-validation-1506", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-157", "mrqa_squad-validation-1640", "mrqa_squad-validation-167", "mrqa_squad-validation-1670", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-1977", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2208", "mrqa_squad-validation-225", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2354", "mrqa_squad-validation-2375", "mrqa_squad-validation-2628", "mrqa_squad-validation-2648", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-2739", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3097", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-3146", "mrqa_squad-validation-3193", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3420", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-3653", "mrqa_squad-validation-3661", "mrqa_squad-validation-3664", "mrqa_squad-validation-3713", "mrqa_squad-validation-3725", "mrqa_squad-validation-3740", "mrqa_squad-validation-3759", "mrqa_squad-validation-3791", "mrqa_squad-validation-3853", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3941", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4023", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4267", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4415", "mrqa_squad-validation-4429", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4446", "mrqa_squad-validation-4478", "mrqa_squad-validation-4490", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-472", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4846", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5012", "mrqa_squad-validation-5077", "mrqa_squad-validation-5185", "mrqa_squad-validation-5256", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5373", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5846", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6128", "mrqa_squad-validation-6224", "mrqa_squad-validation-6242", "mrqa_squad-validation-6279", "mrqa_squad-validation-6292", "mrqa_squad-validation-630", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6520", "mrqa_squad-validation-6541", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6871", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-696", "mrqa_squad-validation-7006", "mrqa_squad-validation-7136", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7663", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7871", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8065", "mrqa_squad-validation-8103", "mrqa_squad-validation-820", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-8505", "mrqa_squad-validation-8554", "mrqa_squad-validation-8561", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8969", "mrqa_squad-validation-8977", "mrqa_squad-validation-9020", "mrqa_squad-validation-9068", "mrqa_squad-validation-9102", "mrqa_squad-validation-9145", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-9744", "mrqa_squad-validation-9787", "mrqa_squad-validation-980", "mrqa_squad-validation-9800", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9944", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-6033", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6912", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-91"], "OKR": 0.8359375, "KG": 0.484375, "before_eval_results": {"predictions": ["England", "Jimmy Kimmel", "an invasion of Western Europe", "\"The Day of the Doctor\"", "mannerist architecture", "n > 3", "the same message routing methodology as developed by Baran", "Jonathan Stewart", "two", "Balvenie Castle", "Vernier, Switzerland", "1966", "Selden", "Henry J. Kaiser", "841", "a creek", "20 March to 1 May 2003", "Newfoundland and the coast of Labrador", "9 November 1967", "Westchester County", "French Canadians", "Towards the Sun", "the SIS", "Portal A Interactive", "the Windigo legend of North America", "aging issues", "1949", "churros", "Tom Kartsotis", "Terrence Jones", "Orson Welles", "Thomas Allen", "HC Davos", "Tampa Bay Lightning", "Vishal Bhardwaj", "15", "University of Vienna", "four", "KlingStubbins", "the fifteenth season", "pornographystar", "torpedoes", "1951", "Saint Motel", "BBC Formula One coverage", "1995", "1689", "David Villa S\u00e1nchez", "Champion Jockey", "Google Calendar, Drive, Docs", "Russell Humphreys", "He served as director of the Saint Petersburg Conservatory", "Lauren Lane", "Sons of Anarchy Motorcycle Club", "Mel Tillis", "Baseball", "sand", "a pregnancy", "terrorism convictions for a Yemeni cleric and his personal assistant", "3800", "a mirror", "anlam", "Fred Astaire", "Douglas MacArthur"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7178199404761905}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9362", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-553", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-4007", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-4288", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-815", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-10901"], "SR": 0.609375, "CSR": 0.5994664634146342, "EFR": 1.0, "Overall": 0.7327839176829268}, {"timecode": 41, "before_eval_results": {"predictions": ["c1600", "absolute value", "4 vector equations", "James Hutton", "force-free magnetic fields", "six", "vocational youth/village polytechnic", "BBC Wales", "bass", "books, films and other media", "Sindbad", "Ben Ainslie", "the Pac-12 Conference", "Rochdale", "Jay Park", "the remake by George A. Romero", "11 November 1918", "Fort Bragg", "The Drudge Report", "Homebrewing", "an anvil", "U.S. Senator, Justin Smith Morrill who authored the Morrill Land-Grant Acts of 1862 and 1890", "Andy Garc\u00eda", "Eielson Air Force Base", "1 December 1948", "Polar Bear", "David Dunn", "The WB supernatural drama series \"Charmed\"", "White Horse", "a postal delivery company", "Europop", "private", "Leofric", "Germanic", "Prince Sung-won", "voicing Liquid Snake", "Schaffer", "the Cold Spring Historic District", "San Antonio, Texas", "Wal-Mart Canada Corp.", "southwestern", "1964", "Richard Arthur", "Presbyterian Church", "CD Castell\u00f3n", "left-hand or right-hand batsman", "the German Empire", "three", "River Welland", "Ben Stokes", "Canada's first train robbery", "Donald Wayne Johnson", "Austro-Hungarian Army", "13 February", "in place on the microscope's stage by slide clips, slide clamps or a cross-table", "Frederick", "Downton Abbey", "February 2008", "Daniel Radcliffe", "Nostradamus", "a reptile", "Leo", "Pablo Picasso", "Kosovo"], "metric_results": {"EM": 0.625, "QA-F1": 0.7037250905797102}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5217391304347826, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10416", "mrqa_squad-validation-8472", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-1201", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-1845", "mrqa_newsqa-validation-1275", "mrqa_searchqa-validation-4054"], "SR": 0.625, "CSR": 0.6000744047619048, "EFR": 0.9583333333333334, "Overall": 0.7245721726190476}, {"timecode": 42, "before_eval_results": {"predictions": ["Oxygen therapy", "a series of New York hotels", "energy content", "Baiju", "July 24", "S-IVB-200", "over half", "New Orleans, Louisiana", "German", "GE Appliances", "Taipei", "John Travolta", "co-authorship", "National Football Conference", "Lucy Maud Montgomery", "25 August 1949", "over 20 million records worldwide", "The Captain Matchbox Whoopee Band", "Sippin' on Some Syrup", "40 million albums", "22,500 acres", "Citizens for a Sound Economy", "Woodsy owl", "Hellenism", "the robbery itself", "Mauritian", "LA Galaxy", "Rowan Atkinson", "first baseman and third baseman", "Rymill Park", "Alonso L\u00f3pez", "Tampa Bay Storm", "the Runaways", "Las Vegas", "House of Habsburg-Lorraine", "Tian Tan Buddha", "Hopi", "astronomer and composer of German and Czech-Jewish origin", "Martin Truex Jr.", "Secretary of Defense", "twice", "Logar Province", "1998", "Lionel Hollins", "Pamelyn Ferdin", "the Blue Ridge Parkway", "bioelectromagnetics", "VH1", "Kentucky", "Dra\u017een Petrovi\u0107", "SAS AB", "Hawaii", "Will Smith, Jared Leto, Margot Robbie, Joel Kinnaman, Viola Davis, Jai Courtney, Jay Hernandez, Adewale Akinnuoye-Agbaje, Ike Barinholtz", "Isabella Palmieri as Rachel Kapowski", "July 1, 1923", "radionuclides", "behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity", "Marines", "comments he made after his new boss, Golfer Adam Scott, defeated Woods at the Bridgestone Invitational in Ohio", "a bangers", "the 17th", "60 Minutes", "Nixon", "jollyroger"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5877976190476191}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.5, 0.28571428571428575, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5714285714285715, 0.5, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1247", "mrqa_squad-validation-3891", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-3300", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-5542", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7704", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2811", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-14021", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-7720"], "SR": 0.484375, "CSR": 0.5973837209302326, "EFR": 0.9696969696969697, "Overall": 0.7263067631254405}, {"timecode": 43, "before_eval_results": {"predictions": ["19", "Creon", "unbalanced torque", "refill bulk liquid oxygen storage containers,", "April 1959", "the water level", "the Daleks, the Cybermen, and the Master", "94", "Lana Del Rey", "Teri Hatcher as Mel Jones, Coraline's mother, and the Beldam / Other Mother, the ruler of the Other World", "Joanna Page", "Brazil and Paraguay", "Adam", "Bryan Cranston", "1987", "Kanawha River", "December 8, 2013", "observing the magnetic stripe `` anomalies '' on the ocean floor", "Bart Cummings", "Hermann Ebbinghaus", "optic chiasma", "New Mexico", "The centuries - old Jedi Grand Master of an unknown species", "James Zeebo", "The decision effectively overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "Jos Plateau", "Spike", "1975", "Van Halen", "Parker's pregnancy", "Aibak", "1951", "season five", "the east coast of North America", "Abid Ali Neemuchwala", "Lord Banquo", "1998", "early 20th century", "March 31, 2017", "The sales area is primarily concentrated in the Southern United States, and has been sold as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "Ernest Hemingway", "about 15 metres ( 49 feet ) per year", "Sara Gilbert", "Jesse Frederick James Conaway", "hair / fur ( including wool ) and feathers", "Hongwu Emperor of the Ming Dynasty", "Stan and Cartman accidentally destroy a dam, causing the town of Beaverton to be destroyed", "Elena Anaya", "Jennifer O'Neill as Hermie's mysterious love interest, and Katherine Allentuck and Christopher Norris as a pair of girls whom Hermie and Oscy attempt to seduce", "an Aldabra giant tortoise", "a chimera", "bulging shape", "late to mid-2000s", "Wilkie Collins", "an aircraft carrier", "Vyto Ruginis", "Archie Andrews", "five", "two tickets to Italy", "(John) Shaft", "The Laughing Cavalier", "Saturday Night Live", "Bono", "The Royal Ballet"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6434707908513055}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.4, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.8, 0.1818181818181818, 0.4, 0.5, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.35, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.15384615384615385, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3688", "mrqa_squad-validation-7615", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-8908"], "SR": 0.546875, "CSR": 0.5962357954545454, "EFR": 0.9310344827586207, "Overall": 0.7183446806426332}, {"timecode": 44, "before_eval_results": {"predictions": ["a green algal derived chloroplast", "bilaterians", "Ward", "The Swiss cities", "the goals he receives from his superior", "7 January 1900", "education", "Lexus, Lincoln, Infiniti", "10 years in prison", "that a UH-60 Blackhawk helicopters crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen.", "millions of Muslims around the globe, including the growing number of American Muslims,", "arson", "about 5:20 p.m.", "Citizens", "a music video", "at least nine", "on Drugs and Crime shows the export value of this year's poppy harvest stood at around $4 billion, a 29 per cent increase over 2006.", "forgery and flying without a valid license", "The 19-year-old woman", "amazed at their band's amazing impact in an interview Tuesday on CNN's \"Larry King Live.\"", "in a canyon", "recanted her claims that she was lured to a dorm and assaulted in a bathroom stall.", "Ronaldinho", "shock, quickly followed by speculation about what was going to happen next,\"", "Illness", "Utah Valley Regional Medical Center", "a number of calls, and those calls were intriguing, and we're chasing those down now,\"", "Frank Ricci", "200.", "striker Tevez remained on the bench despite a rousing reception when he went on a touchline warm-up during the game.\"", "African National Congress", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "the document to recognize the legal right to freedom from tyranny", "dozens more children and young women", "New Year's Day", "South Africa", "Unseeded Frenchwoman Aravane Rezai", "in Belfast", "after nine years.", "Tutsi ethnic minority and the Hutu majority", "the island's dining scene", "sovereignty over them.", "Kgalema Motlanthe", "1918-1919.", "Newcastle", "\"disagreements\" with the Port Authority of New York and New Jersey,", "German Chancellor Angela Merkel", "EU naval force", "Genocide Prevention Task Force", "President Obama", "U.S. military bases in the Pacific Ocean territory of Guam", "Hong Kong and Shenzhen, a city in mainland China", "201-262-2800.", "Neuropsychology", "Rumplestiltskin", "Saturn", "Hyundai", "South America", "WB Television Network", "Richard Nixon", "parody", "satirical cartoons, drawings, stage design", "decorate", "Jack Johnson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5812160062160062}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.057142857142857134, 0.2857142857142857, 0.3636363636363636, 0.0, 0.4444444444444445, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.1111111111111111, 0.14285714285714285, 0.0, 0.5714285714285715, 1.0, 0.4, 0.6666666666666666, 0.0, 0.2222222222222222, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-844", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1307", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-1704", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-263", "mrqa_naturalquestions-validation-2839", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-4612"], "SR": 0.453125, "CSR": 0.5930555555555556, "EFR": 0.9714285714285714, "Overall": 0.7257874503968254}, {"timecode": 45, "before_eval_results": {"predictions": ["Jane Kim", "BBC National Orchestra of Wales", "Tien Shan mountains", "in Algeria", "to the Qara Khitai.", "algorithm is said to be a polynomial time algorithm.", "Robert Goulet", "piano", "Ramen", "Herbert Francis Peyser", "oregano", "Patrick Henry", "The Police", "Bligh", "the National Mall", "Se sitcom", "The Tournament of Kings", "James Garner", "hot dog", "Marlon Brando", "The Hogan Family", "zoology", "Italian", "Judges", "A ground level effort", "Howard Hughes", "Lake Michigan", "The Unity of Religion", "Carl Jung", "Indira Gandhi", "Teddy Roosevelt", "Southeast Asia", "The Otis family", "Lance Armstrong", "It's a marvelous Life", "The Passing of Arthur", "Cewek123", "The Winds of War", "Pizze Napoletane", "the Hundred Years' War", "Wellington", "Bangkok", "Castle Rock", "carbon fiber", "Women in Love", "California", "Quizlet", "Italian media", "a helicopter", "an ace", "The Beard of Love", "Pearl", "Marshall Sahlins", "42", "Leo Arnaud", "Jane Eyre", "Debbie Rowe", "Carmen", "extended play", "Sulfur mustard", "Iceal \"Gene\" Hambleton", "\"Top Gun.\"", "fake his own death by crashing his private plane into a Florida swamp.", "\"Iran's Green Movement of protesters against the regime indicates that waiting could have its benefits, Zakaria said."], "metric_results": {"EM": 0.46875, "QA-F1": 0.570501893939394}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6125", "mrqa_squad-validation-9680", "mrqa_squad-validation-6192", "mrqa_squad-validation-1710", "mrqa_searchqa-validation-7347", "mrqa_searchqa-validation-10923", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-9674", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-15808", "mrqa_searchqa-validation-1698", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-3040", "mrqa_searchqa-validation-4957", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-5644", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-338", "mrqa_searchqa-validation-3427", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-4954", "mrqa_searchqa-validation-11073", "mrqa_searchqa-validation-9532", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-6692", "mrqa_triviaqa-validation-134", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-3528", "mrqa_newsqa-validation-729"], "SR": 0.46875, "CSR": 0.5903532608695652, "EFR": 0.9705882352941176, "Overall": 0.7250789242327366}, {"timecode": 46, "before_eval_results": {"predictions": ["Ed McCaffrey", "oxygen-16", "December 1517", "a blue British police box", "Georgia", "Four thousand", "Mirzapur, Uttar Pradesh", "Rich Mullins", "master carpenter Anthony Mayfield", "Elsie Ross", "Sara Gilbert", "1986", "the Christian biblical canon", "1971", "Butter Island", "2018", "between the stomach and the large intestine", "Cheap Trick", "such famous figures as Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "2017 / 18 Divisional Round", "Dunedin", "2018", "Cyndi Grecco", "Elia", "Catholic Monarchs of Castile and Aragon funded Christopher Columbus's plan to sail west to reach the Indies by crossing the Atlantic", "starch", "Grand Inquisition", "Electoral College", "Christopher Jones", "Lynne", "June 8, 2009", "fertilization", "management team", "The Enchantress", "Americans", "Herod", "Gibraltar", "13 February", "Bill Russell", "Brenda Epperson", "Salamis Bay", "the start of the seventeenth series", "the uppermost layer of the dermis", "Total Drama World Tour", "18", "food and clothing", "Empiricism", "senators", "in the blood, brain, and other tissues", "March 29, 2018", "acid rain", "UN General Assembly", "South Pacific", "Kenya", "vanilla", "1999", "John Snow", "March 13, 2013", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Polo", "1,073 immigration detainees", "Elingrid Chavez", "a Purple Heart", "Caltech"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6256718975468976}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.20000000000000004, 1.0, 0.2666666666666667, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-5008", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-692", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4472", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-14323"], "SR": 0.53125, "CSR": 0.5890957446808511, "EFR": 0.9333333333333333, "Overall": 0.7173764406028369}, {"timecode": 47, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "Sainte Foy in Quebec", "Arley D. Cathey", "1930", "the current Doctor Who logo", "stagnant wages for the working class amidst rising levels of property income for the capitalist class", "nine hours from Coordinated Universal Time ( UTC \u2212 09 : 00 )", "three levels", "New England Patriots", "The claims process starts at noon Eastern Time and ends 24 hours later", "Brooklyn Heights, New York", "hydrogen", "http://www.example.com/index.HTML", "Massachusetts", "on the table", "Daniel A. Dailey", "the leaves of the plant species Stevia rebaudiana", "Captain Jones", "Freddie Highmore", "Squamish, British Columbia, Canada", "Jack Barry", "Jason Marsden", "Tandi, in Lahaul", "1912", "Etienne de Mestre", "1877", "Lauren Tom", "two - third of the total members present and voted in favour of the bill with more than 50 % of the number members of a house", "Miami Heat", "The Intolerable Acts", "accommodationism", "pilgrimages to Jerusalem and a desire to reproduce Via Dolorosa", "Shirley Mae Jones", "Welsh pirate Edward Kenway", "937 total weeks", "the head of the Imperial Family and the traditional head of state of Japan", "frontal lobe", "Frederik Barth in his review of this system of social stratification in Pakistan", "Lizzy Greene", "Z n 2 + ( \\ displaystyle Zn ^ ( 2 + ) molecules", "Walter Brennan", "Tabaqui", "Keeley Clare Julia Hawes", "end when they qualify as a medical practitioner following graduation with a Bachelor of Medicine", "Georgia Groome as Georgia Nicolson", "1773", "Dr. Rajendra Prasad", "Elizabeth Mitchell", "Mahatma Gandhi", "predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Jonathan Goldstein", "Malvolio", "formic acid", "transuranic", "the western Caribbean Sea", "The final of 2011 AFC Asian Cup", "Stephen Crawford Young", "Great Northern Railway transcontinental railway line", "The Stooges comedic farce entitled \"Three Little Beers,\"", "South America and Africa.", "seeking an end, once and for all, to illegal immigration on its southern border.", "Brazil", "rodeo", "William Safire"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6527560615554036}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.11111111111111112, 0.5714285714285715, 0.6666666666666666, 1.0, 0.2857142857142857, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.21428571428571427, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.1, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7971", "mrqa_squad-validation-7875", "mrqa_squad-validation-7188", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-2605", "mrqa_triviaqa-validation-1912", "mrqa_hotpotqa-validation-3417", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-1444"], "SR": 0.578125, "CSR": 0.5888671875, "EFR": 1.0, "Overall": 0.7306640625}, {"timecode": 48, "before_eval_results": {"predictions": ["6,000 square kilometres (2,300 sq mi)", "about 10,000", "1998", "materials melted near an impact crater", "through the Rhine Gorge,", "the nearest White Castle in New Brunswick,", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "3D modeling ( or three - dimensional modeling )", "Kevin McKidd", "Abanindranath Tagore CIE", "The Osmonds", "arm", "2017", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "a pop ballad", "the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "the person compelled to pay for reformist programs", "China in American colonies without paying any taxes", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "art of the book and architecture", "Buddhism", "a greeting", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "bicameral Congress", "1984", "Geoffrey Zakarian", "in Rome in 336", "2014", "eleven", "The Fixx", "ancient Rome", "Buffalo Lookout", "Charles Carson", "July 2012", "Spanish / Basque", "a beeline", "March 15, 1945", "September 1972", "a usually red oxide formed by the redox reaction", "the statue of Athena housed in the naos", "SURFACE AREA OF ROOTS", "movement of the Earth's continents relative to each other, thus appearing to `` drift '' across the ocean bed", "a competitor or team in a sport or other tournament", "78 %", "Sean Connery", "Vital Records Office of the states", "endocrine ( hormonal )", "the Royal Air Force ( RAF )", "It plays a key role in chain elongation in fatty acid biosynthesis and polyketide biosynthesis", "Sharyans Resources", "Manchester United Football Club", "caribbean", "kermadec Islands", "Gloucestershire", "Ely", "Chad", "Johnnie Ray", "William Corcoran Eustis", "\"it is impossible to turn back the tide of globalization.\"", "Afghan homes and compounds,", "Booches Billiard Hall,", "thunder", "to twirl a baton", "man"], "metric_results": {"EM": 0.5, "QA-F1": 0.6149529682068688}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.33333333333333337, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.11764705882352941, 0.14634146341463414, 0.6666666666666666, 0.0, 1.0, 0.13333333333333333, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3026", "mrqa_squad-validation-8989", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9361", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-4922", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-4531", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-4240", "mrqa_newsqa-validation-1973", "mrqa_searchqa-validation-10457"], "SR": 0.5, "CSR": 0.5870535714285714, "EFR": 0.9375, "Overall": 0.7178013392857142}, {"timecode": 49, "before_eval_results": {"predictions": ["Leonard Bernstein", "Boolean circuits", "Yale University", "Wednesdays", "with the help of the military.", "on cavities and surfaces of blood vessels and organs throughout the body", "alveolar process", "the court from its members for a three - year term", "a focal point", "provinces along the Yangtze River and in provinces in the south", "420 mg", "W. Edwards Deming", "Fa Ze Banks", "membranes of the body's cells, and is abundant in the brain, muscles, and liver", "cartilage", "`` Nearer, My God, to Thee ''", "freedmen joined the ranks of farmers and the urban working class", "Eurasian Plate", "Ukraine", "near the end of their main sequence lifetime", "2007", "2018", "Barbara Eve Harris", "Walmart", "Celtic", "Ra\u00fal Eduardo Esparza", "1890s", "C\u03bc and C\u03b4", "medical care provided on an outpatient basis, including diagnosis, observation, consultation, treatment, intervention, and rehabilitation services", "Justice Harlan", "2017 season", "February 7 in Los Angeles, California", "Travis Tritt and Marty Stuart", "Missi Hale", "1939", "The main pulmonary artery begins at the base of the right ventricle", "Acts passed by the Congress of the United States and its predecessor, the Continental Congress", "Columbia Pictures and Warner Bros.", "Bart Howard", "Jason Flemyng", "Brad Dourif", "Steve Mazzaro & Missi Hale", "represents a set of related data", "Kyla Pratt", "Ariana Clarice Richards", "glucose", "pathology", "Lionel Hardcastle", "Debbie Gibson", "on the bank's own funds and signed by a cashier", "2017", "Tavares", "Dombey and Son", "bison", "The Duchess", "Puente Hills Mall", "Adam Levine", "Westchester", "\"It should stay that way.\"", "more than 200.", "humans", "doberman pinscher", "Chuck Schumer", "Elie Wiesel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5891296203796204}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 1.0, 0.2222222222222222, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-2402", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6052", "mrqa_triviaqa-validation-2988", "mrqa_hotpotqa-validation-745", "mrqa_newsqa-validation-2655", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-5647"], "SR": 0.515625, "CSR": 0.5856250000000001, "EFR": 1.0, "Overall": 0.7300156250000001}, {"timecode": 50, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1245", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-88", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-662", "mrqa_naturalquestions-validation-6987", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7942", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9029", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9684", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1184", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-13419", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14635", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-578", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7297", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9442", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9820", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10160", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-103", "mrqa_squad-validation-10316", "mrqa_squad-validation-10416", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1247", "mrqa_squad-validation-1311", "mrqa_squad-validation-1379", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1481", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-157", "mrqa_squad-validation-1640", "mrqa_squad-validation-167", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2648", "mrqa_squad-validation-267", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-3146", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-366", "mrqa_squad-validation-3661", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3941", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4478", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5077", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5337", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5846", "mrqa_squad-validation-5857", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6125", "mrqa_squad-validation-6128", "mrqa_squad-validation-6224", "mrqa_squad-validation-6279", "mrqa_squad-validation-6292", "mrqa_squad-validation-630", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6520", "mrqa_squad-validation-6541", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-7006", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7615", "mrqa_squad-validation-7627", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7864", "mrqa_squad-validation-7871", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8065", "mrqa_squad-validation-8103", "mrqa_squad-validation-8143", "mrqa_squad-validation-820", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-844", "mrqa_squad-validation-8472", "mrqa_squad-validation-8554", "mrqa_squad-validation-8561", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8792", "mrqa_squad-validation-8958", "mrqa_squad-validation-8977", "mrqa_squad-validation-9068", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9680", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-973", "mrqa_squad-validation-9787", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9944", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867"], "OKR": 0.78515625, "KG": 0.44140625, "before_eval_results": {"predictions": ["Rankine cycle", "1849", "the Bureau of Buddhist and Tibetan Affairs (Xuanzheng Yuan)", "More than 1 million", "October 16, 2012", "Liverpool Bay", "\"Little Dixie\"", "Sadar Bazaar", "Burning Man", "David Mitchell", "Revengers Tragedy", "was not born in the United States", "Jessica Lange", "Boeing EA-18G Growler", "thermal shielding material", "County Executive", "Urijah Faber", "1980", "Ben R. Guttery", "Wiz Khalifa", "Matt Kemp", "January 15, 1975", "Gibraltar", "Candace Cameron Bure", "German Shepherd", "March 19, 2017", "Albert", "2000 Summer Olympics", "Mikoyan design bureau", "Vyd\u016bnas", "Greg Hertz", "seven", "1999", "a few minutes", "Venus", "the 924", "\"The Process\"", "16 November 1973", "Baugur Group", "Duval County, Florida", "William Allen White", "Tel Aviv University", "Stephen Crawford Young", "performed under the mononym Charice until his gender transition to male", "94", "Rabat", "Brenton Thwaites", "Centers for Medicare and Medicaid Services (CMS)", "2013", "Romeo Montague", "Mark Neveldine and Brian Taylor", "Canyon", "in Middlesex County", "2018", "bachata", "the Treaty of Utrecht", "EMI", "Charley", "in a Nazi concentration camps,", "Paul Ryan", "his business is shattered, no one in his family died in the quake.", "Enigmata", "Pink Floyd", "Guadalajara"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6150580171639954}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.5, 0.0, 1.0, 0.0, 0.2727272727272727, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.08695652173913043, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8351", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-188", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-4862", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-453", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1961", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-2743", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-7018", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2853", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-13144"], "SR": 0.484375, "CSR": 0.5836397058823529, "EFR": 1.0, "Overall": 0.7061810661764706}, {"timecode": 51, "before_eval_results": {"predictions": ["356 \u00b1 47 tonnes per hectare", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs", "more than half", "5K", "an all-Gemini veteran crew", "Euna Lee,", "strawberry", "Chinese President Hu Jintao", "$3 billion,", "Basel", "9-1", "Afghanistan's restive provinces", "middlevina Dean, is trying to raise money so she can stay in the nursing home she prefers.", "Steven Green", "Filippo Inzaghi", "public opinion in Turkey.\"", "Honduras", "\"learn how to dance and feel sexy,\"", "45 minutes, five days a week.", "six", "Days of our Lives", "\"Quiet Nights,\"", "military personnel", "Frank Ricci", "Stoke City.", "Australian officials", "Kim Il Sung", "Indonesian", "Karen Floyd", "a \" Happy ending\" to the case.", "Brett Cummins", "a body from underneath the rubble of a collapsed apartment building", "Nearly eight in 10", "Anil Kapoor", "MS Columbus", "late Thursday to form a government of national reconciliation.", "Tennis Channel", "inability to \"turn it off\"", "Thursday.", "Nineteen", "a freighter seized by pirates off east  Africa", "tried to fake his own death by crashing his private plane into a Florida swamp.", "kite surfers and wind surfers", "Mugabe's opponents", "$24.1 million", "Kaka", "south-central Washington, an area roughly half the size of Rhode Island,\"", "543", "Kim Jong Il.", "maintain an \"aesthetic environment\" and ensure public safety", "FBI", "Rolling Stone", "Branford College", "St. John's, Newfoundland and Labrador", "September 24, 2012", "Erinyes", "the liver", "Ty Ganol", "a South Australian town located 21 kilometres south-east of Adelaide, in the Adelaide Hills.", "St James's Palace", "Salisbury", "a panda", "maroochi nut", "marcellus"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6143261709051182}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.07692307692307693, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8421052631578948, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.8333333333333333, 1.0, 1.0, 0.2, 0.923076923076923, 0.0, 1.0, 0.5, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-844", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3239", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-5096", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-2306", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-3324", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-15795"], "SR": 0.515625, "CSR": 0.5823317307692308, "EFR": 0.967741935483871, "Overall": 0.6994678582506204}, {"timecode": 52, "before_eval_results": {"predictions": ["education, sanitation, and traffic control within the city limits", "\u00a343,389 per annum,", "Genghis Khan", "Emergency Highway Energy Conservation Act", "Training Day", "Havenhurst", "given fair weather", "1 April 1985", "George Washington Bridge", "Red Rock West", "Larry Eustachy,", "third baseman and shortstop", "Stephen King", "Sports Illustrated", "Chicago Bears", "43rd", "\"The School Boys\"", "In Pursuit", "British", "Memphis", "Fort Snelling", "Martin Lee Truex Jr.", "The Onion", "\"Let's Make Sure We Kiss Goodbye,\"", "1993", "Saint Louis County", "Democratic", "Anno 2053", "the Wabanaki Confederacy", "Marty Ingels", "New York Islanders", "The Division of Cook", "a simian-like creature", "\"The King of Chutzpah\"", "$10\u201320 million", "Julianne Moore", "beer", "Francis the Talking Mule", "Mexico", "model", "16,776", "Confessions of a Teenage Drama Queen", "superhero roles as the Marvel Comics", "October 13, 1980", "HBO World Championship Boxing", "College Football Scoreboard", "November 1978", "Buffalo", "John Monash", "Godiva", "science fiction", "1978", "December 9, 2017", "the Deathly Hallows", "Atticus Finch", "Muhammad Ali", "Elvis Presley", "Bangladesh, Sri Lanka, the Malabar Coast of India, and Burma.", "April 6, 1994", "Texas and Oklahoma to points east, with 8 to 10 inches of snow possible in some locales,", "Brown-Waite", "Abuja", "Cuba", "Judo"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6490449134199134}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 0.5, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_squad-validation-6820", "mrqa_squad-validation-6150", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-1316", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5155", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-2802", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-7461", "mrqa_triviaqa-validation-5506", "mrqa_triviaqa-validation-6224", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-1016"], "SR": 0.53125, "CSR": 0.5813679245283019, "EFR": 0.9666666666666667, "Overall": 0.6990600432389937}, {"timecode": 53, "before_eval_results": {"predictions": ["Johannes Bugenhagen and Philipp Melanchthon", "Renaissance", "Warszawa", "Independence Day: Resurgence", "Levon Helm", "three", "Zack Snyder", "the Qin dynasty", "University of Vienna", "University of Texas at Austin", "the Ducks", "1970s and 1980s", "1983", "The Next Step", "enshrined at Dayton, Ohio, in the National Aviation Hall of Fame class of 2001", "Gerard Marenghi", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies.", "University of the District of Columbia", "University of Southern California", "20", "an invoice, bill or tab", "October 13, 1980", "Russell T Davies", "nearly 8 km", "Tony Aloupis", "London", "Antonio Salieri", "USS Essex", "Lonestar", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Merck Sharp & Dohme", "Dundalk", "A123 Systems, LLC", "Northern Ireland", "shock cavalry", "Robert L. Stone", "American", "Flushed Away", "Unibet", "four", "The Division of Cook", "Macau, Macau", "New York Shakespeare Festival", "Princess Muna al-Hussein", "October 25, 1881", "Jan Kazimierz", "Mary-Kay Wilmers", "2001", "1994", "Art of Dying", "Robert \"Bobby\" Bunda", "RAF Mount Pleasant", "Akshay Kumar", "Dr. Sachchidananda Sinha", "a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "Flemish", "elvis", "kenya", "to do jobs that Arizonans wouldn't do.", "Robert Barnett", "more than a million residents who have been displaced by", "Mark Twain", "peameal", "Covington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6619137806637806}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5555555555555556, 0.5, 0.45454545454545453, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.5, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-4900", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-392", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1999", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-9008"], "SR": 0.53125, "CSR": 0.5804398148148149, "EFR": 1.0, "Overall": 0.705541087962963}, {"timecode": 54, "before_eval_results": {"predictions": ["\u00a341,004", "environmental degradation", "1876", "Standard Model", "The United States of America", "50 JJB Sports Fitness Clubs", "The Cherokee Nation", "Gatwick Airport", "Berea College", "Daniel Espinosa", "Bill Boyd", "General Sir John Monash", "1860", "25 November 2015", "Trilochanpala", "Charles Edward Stuart", "Martin Truex Jr.", "Objectivism", "Ronald Joseph Ryan", "Hennepin County", "Samuel Barclay Beckett", "Barbara Bush", "Carl David Tolm\u00e9 Runge", "James Weldon Johnson", "Tom Jones", "sarod", "Bank of China Tower", "two", "Enemy", "40,400 members", "more than 100", "for crafting and voting on legislation", "Flaw", "237", "Matthieu Vaxivi\u00e8re", "puzzle", "Jennifer Taylor", "Sam Tick", "Nye County", "The Tempest", "Orlando Predators", "Charlie Wilson's War", "The King of Chutzpah", "Afghanistan", "Girls' Generation", "Division I", "1968", "Vernier, Switzerland", "Park Hyung-Sik", "Britain", "Newell Highway", "11 September 1996", "104 colonists", "My Summer Story", "Samantha Jo `` Mandy '' Moore", "6", "china", "Ben Watson", "Three aid workers", "Mexico", "to \"wipe out\" the United States if provoked.", "Stones from the River", "a sneer", "bacon"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6451522435897437}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2072", "mrqa_squad-validation-7629", "mrqa_squad-validation-5540", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-4584", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-4696", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1892", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-2990", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-14967", "mrqa_searchqa-validation-3400"], "SR": 0.5625, "CSR": 0.5801136363636363, "EFR": 1.0, "Overall": 0.7054758522727272}, {"timecode": 55, "before_eval_results": {"predictions": ["The Earth's mantle", "deep spiritual despair", "Three", "the printing press", "made 109", "Pixar's", "Eden Park", "Brazil", "9 percent", "led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver", "reduce the cost of auto repairs", "his father", "\"release\" civilians,", "1940's", "The oceans", "228", "23 years.", "\"Nazi Party members digging up American bodies at Berga.\"", "Sharon Bialek", "15,000", "cell phones", "The sole survivor of the crash that killed Princess Diana", "one American diplomat", "declared the charity of kidnapping the children and concealing their identities.", "North Korea intends to launch a long-range missile in the near future,", "21", "\"The Hills Have Eyes II.\"", "two Israeli soldiers,", "illegal immigrants", "150", "1994", "Al-Shabaab, the radical Islamist militia that controls the city", "The situation has been exacerbated by drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "U.S. Secretary of State Hillary Clinton", "those traveling near the Somali coast", "Basilan", "a skilled hacker", "on supporting full marriage equality,\"", "the college campus.", "President Sheikh Sharif Sheikh Ahmed", "20,000", "returning combat veterans", "fighting charges of Nazi war crimes for well over two decades.", "12-1", "second", "criminals", "Mashhad", "543", "the intersection where the crash took place,", "love the environment and hate using fuel", "the Russian air force", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "Kimberlin Brown", "Philippe Petit", "the year 2026", "Hugh Hefner", "Switzerland", "Ladee-Lo", "1982", "bronze", "Mary Astor", "Tuesday", "the blue wildebeest", "Parris Island"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6547629989495798}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.5882352941176471, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.125, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.25, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.7058823529411764, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1190", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-2850", "mrqa_triviaqa-validation-6942", "mrqa_hotpotqa-validation-5657", "mrqa_searchqa-validation-16545"], "SR": 0.546875, "CSR": 0.5795200892857143, "EFR": 1.0, "Overall": 0.7053571428571429}, {"timecode": 56, "before_eval_results": {"predictions": ["thyroid hormone activity", "microscopic analysis of oriented thin sections of geologic samples", "$41 trillion", "Eisleben, Saxony,", "The EU naval force", "King Birendra,", "a head injury.", "Haitians", "by the time the Presidents Day holiday weekend is over.", "Harrison Ford", "Kenyan Defense Minister Yusuf Haji,", "Don Draper", "against meat consumption by covering themselves in fake blood and lying in human-sized meat packages.", "near Grand Ronde, Oregon.", "President Sheikh Sharif Sheikh Ahmed", "two satellites", "Sgt. John Auer,", "150", "the hiring of hundreds of foreign workers for a construction project", "CNN polling director Keating Holland", "Amitabh Bachchan", "\"It's an inconvenience, but we are grateful that it has worked as well as it has.\"", "Pakistan security officials", "\"Nothing But Love\"", "(Saturn owners)", "French army helicopter taking off from French frigate Nivose,", "five", "the job bill's controversial millionaire's surtax,", "Los Angeles County Fire Department", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "one count of attempted murder in the second degree in the October 12 attack", "it would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "because a new model is simply out of their reach.", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Wigan Athletic", "Stephen Vance", "Kris Allen", "Chester Stiles,", "President Bush", "1,073 immigration detainees", "James Newell Osterberg", "\"A Lion Among Men,\"", "Monday and Tuesday", "The discovery of millions of extra ballots", "Ernesto Bertarelli", "84-year-old", "Leo Frank,", "Robert Hawkins from going on a murderous rampage at an Omaha, Nebraska, shopping mall", "The soluble fiber ispaghula husk,", "In Group 1, Tim Cahill scored twice as Australia came from behind to beat Japan 2-1", "Jaime Andrade", "Body Works", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "14 December 1972 UTC", "24", "triathlon", "Thomas Chippendale", "Andy Murray", "Martin \"Marty\" McCann", "Captain B.J. Hunnicutt", "Jackson Storm", "arsenic", "a typewriter", "(NSW)"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5906994216761051}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.25, 1.0, 0.0, 0.8571428571428571, 0.33333333333333337, 1.0, 0.3333333333333333, 1.0, 0.8, 0.0, 1.0, 0.07142857142857144, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.2857142857142857, 0.9411764705882353, 0.9411764705882353, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6649", "mrqa_squad-validation-5038", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-3231", "mrqa_naturalquestions-validation-290", "mrqa_hotpotqa-validation-2702", "mrqa_searchqa-validation-10114", "mrqa_searchqa-validation-6692"], "SR": 0.453125, "CSR": 0.5773026315789473, "EFR": 0.9714285714285714, "Overall": 0.6991993656015038}, {"timecode": 57, "before_eval_results": {"predictions": ["Sunday Times University of the Year", "probabilistic", "d'Hondt", "Gene Barry", "Muhammad", "Djokovic", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "Around 1200", "Atlanta, Georgia", "Jessica Simpson", "Kevin Spacey", "Bart Cummings", "Jodie Foster", "Tenochtitlan", "Andrew Michael Harrison", "virtual reality simulator", "Edward Seton", "Lake Powell", "September of that year", "two", "Lori Rom", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "a stray wandering the streets of Moscow", "Daniel A. Dailey", "Antigonon leptopus", "James Chadwick", "green", "United Nations", "Mason Alan Dinehart", "skeletal muscle and the brain", "depicting multiple alternative realities rather than a novel", "Lou Rawls", "2017", "New York City", "Pat McCormick", "the season ten episode ``", "Michael Phelps", "Seattle, Washington", "six", "HTML", "Steve Russell", "Steve Hale", "Philippe Petit", "France fashioned a semi-independent State of Vietnam, within the French Union, with B\u1ea3i C\u00e1ch Ru\u1ed9ng \u0110\u1ea1i as Head of State", "October 28, 2007", "the top 50 accounts", "post translational modification", "Hon July Moyo", "Audrey II ''", "Mankombu Sambasivan Swaminathan", "1994", "Chris Rea", "Smeagol", "Tim Peake", "a knife", "Sir Seretse Goitsebeng Maphiri Khama", "Waltham Abbey", "Kentucky River", "about 50", "Israel", "five female pastors", "Jehovah", "Narcissus", "a husband"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6664133832283987}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09090909090909091, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3076923076923077, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5211", "mrqa_triviaqa-validation-5598", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-323", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-1213", "mrqa_searchqa-validation-3021"], "SR": 0.59375, "CSR": 0.5775862068965517, "EFR": 0.8846153846153846, "Overall": 0.6818934433023872}, {"timecode": 58, "before_eval_results": {"predictions": ["Tomingaj, near Gra\u010dac.", "Class II MHC molecules", "2011", "fall 2010", "Frankie Muniz", "`` G - Baby '' ( De Wayne Warren )", "the thirteen American colonies regarded themselves as a new nation, the United States of America, and were no longer part of the British Empire", "Tom Brady", "1997", "in 2016, the series began expanding to more stadiums, first to Twickenham Stadium, London ( 2016 -- 18 ) and to Estadio Azteca, Mexico City (", "plant food", "modern random - access memory ( RAM )", "Andaman and Nicobar Islands", "W. Edwards Deming", "LED illuminated display", "1804", "end of 1066", "Billie Jean King", "`` Great G minor symphony ''", "a major fall in stock prices", "1940", "Ben Findon", "Anna Faris", "$19.8 trillion", "Walter", "six degrees", "frontal lobe", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "MGM Resorts International", "Davos, in the eastern Alps region of Switzerland", "asphyxia '' ( cutting off the oxygen supply )", "4.5 pounds or 2.04 kg", "Hans Christian Andersen", "January 1, 1976", "Bill Pullman", "July 4, 1776", "`` burqa ban ''", "a theory", "eight", "production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "Athens", "1958", "a customer asks a bank for a cashier's check", "Germany's defeat and widespread popular discontent triggered the German Revolution of 1918 -- 19 which overthrew the monarchy and established the Weimar Republic", "a mark that reminds of the Omnipotent Lord, which is formless", "two installments", "loosely on Eminem's actual upbringing", "the Chicago metropolitan area", "Chris Martin", "Professor Eobard Thawne", "Natya Shastra", "NBC's", "Triumph", "an abbreviation of either Schnellbahn, Stadtbahn", "fibrous", "1964", "Epic Records", "French mathematician and physicist", "Kerstin Fritzl,", "gasoline", "\"Well, about time.\"", "ticks", "Biden", "Germany"], "metric_results": {"EM": 0.34375, "QA-F1": 0.49467179539610007}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.18181818181818182, 0.0, 1.0, 0.0, 0.1904761904761905, 0.888888888888889, 0.8, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6666666666666666, 0.12500000000000003, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.8695652173913044, 1.0, 0.7777777777777778, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6153846153846153, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1227", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-3246", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7751", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-1822", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-509", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-7402"], "SR": 0.34375, "CSR": 0.5736228813559322, "EFR": 0.9523809523809523, "Overall": 0.6946538917473769}, {"timecode": 59, "before_eval_results": {"predictions": ["\u2212F", "100", "productivity gap", "Toby Keith", "John Adams", "Abbot Suger", "senators", "1966", "Justice Lawrence John Wargrave", "Isaiah Amir Mustafa", "Michelle", "MGM Resorts International", "Joudeh Al - Goudia family", "The United Kingdom", "`` Wellington's Chamber ''", "538", "American comedy - drama", "Turner Layton", "rupees", "Richard Stallman", "Ferm\u00edn Francisco de Lasu\u00e9n", "to manage the characteristics of the beer's head", "Sean O' Neal", "Indonesia", "950 pesos ( approximately $ 18 )", "Best Art Direction, Best Makeup, and Best Visual Effects", "The U.S. Army Chaplain insignia", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "`` Gollaher's Diet Beer,", "Representative in Congress", "B cells ( for humoral, antibody - driven adaptive immunity )", "Miami Heat", "Terry Kath", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "pathology", "South Africa", "Sarah Silverman", "Coriolis force", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Warren Zevon", "produced with constant technology and resources per unit of time", "amino acids glycine and arginine", "1917", "a regulatory site", "`` Jocelyn Flores ''", "October 27, 2017", "Lauren", "1912", "Zilphia Horton", "Bryan Cranston", "July 2, 1776", "Sachin Tendulkar", "Christian Dior", "b\u0259l\u025bs b\u0251\u02c8l\u0251/  Spell  a radical Shi\u02bfite Muslim organization in Lebanon engaged in guerrilla warfare against Israel", "Field Marshal Lord Gort", "841", "VH1", "September 21.", "Sen. Barack Obama", "Robert", "Wendell, North Carolina", "cinnamon", "lily"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6469581582633054}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307692, 0.4, 0.0, 0.8, 0.5, 0.0, 0.23076923076923078, 0.35294117647058826, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-10333", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-6091", "mrqa_triviaqa-validation-4198", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1976", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-10417"], "SR": 0.53125, "CSR": 0.5729166666666667, "EFR": 0.9666666666666667, "Overall": 0.6973697916666667}, {"timecode": 60, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1245", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1806", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3649", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-88", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-412", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-662", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7942", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-853", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8870", "mrqa_naturalquestions-validation-9029", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9931", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2474", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14635", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5647", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-578", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9442", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9820", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10160", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-10309", "mrqa_squad-validation-10316", "mrqa_squad-validation-10416", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1311", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-167", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2648", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3112", "mrqa_squad-validation-3146", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-3661", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5077", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5337", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5857", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6125", "mrqa_squad-validation-6128", "mrqa_squad-validation-6150", "mrqa_squad-validation-6224", "mrqa_squad-validation-6279", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-7006", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7615", "mrqa_squad-validation-7627", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8103", "mrqa_squad-validation-8143", "mrqa_squad-validation-820", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8792", "mrqa_squad-validation-8958", "mrqa_squad-validation-9068", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9643", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-973", "mrqa_squad-validation-9787", "mrqa_squad-validation-9796", "mrqa_squad-validation-9837", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5506", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867"], "OKR": 0.84765625, "KG": 0.48359375, "before_eval_results": {"predictions": ["1963", "The Judicial Council", "Sports Night", "April 1917", "Kim Basinger", "Fonzworth Bentley", "24", "Shenzi", "1840s", "R2E Micral CCMC", "Authors William Strauss and Neil Howe", "Edward Kenway ( Matt Ryan )", "2018", "IFN - \u03b1", "seawater pearls", "the spinal cord", "1999", "The Charter, however, granted new powers to the courts to enforce remedies that are more creative and to exclude more evidence in trials", "Montreal Canadiens", "March 9, 2018", "Malina Weissman", "49", "2014", "Felix Baumgartner", "2019", "Chelsea", "Ra\u00fal Eduardo Esparza", "Shawn Wayans", "1800", "eleven", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "Lady Gaga", "Mitch Murray", "May 2017", "during the day", "arrives in central Nigeria in July bringing with it high humidity, heavy cloud cover and heavy rainfall", "Barbara Windsor", "comic", "Paul Esteben, L. Ruch, J.P. Rouede, W. Maylie, S. Firmberg, B. Beaubay, William", "hydrogen", "Madison, Wisconsin, United States", "Starscream", "to transform Hitler's government into a legal dictatorship", "Macon Blair", "8 December 1985", "a hostname", "1998", "Alex Ryan", "Eukarya", "the ARPANET project", "a heart rate that exceeds the normal resting rate", "a liquid crystal on silicon ( LCoS ) ( based on an LCo S chip from Himax ), field - sequential color system, LED illuminated display", "\u201cDance Stance\u201d", "british", "hurdles", "1991", "Urijah Christopher Faber", "1993 to 1996", "Spanish Davis Cup hero Fernando Verdasco,", "Teen Patti", "called Chu \"uniquely suited to be our next secretary of energy\" for his work on new and cleaner forms of energy.", "The Rio Grande", "the kidneys", "the Cobb salad"], "metric_results": {"EM": 0.5625, "QA-F1": 0.658538555511508}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5217391304347826, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.918918918918919, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.09302325581395349, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8368", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-5997", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-754", "mrqa_triviaqa-validation-5198", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2359", "mrqa_hotpotqa-validation-2625", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-674"], "SR": 0.5625, "CSR": 0.5727459016393442, "EFR": 0.9285714285714286, "Overall": 0.7161228410421545}, {"timecode": 61, "before_eval_results": {"predictions": ["Charles Darwin", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "interacting and working directly with students", "Burma", "the chameleons", "Red Bank NJ", "Diana Ross", "an overcast day outside you should pay attention to the kind of clouds in the sky", "Edward of Woodstock KG (15 June 1330  8 June 1376)", "Lost in America", "Louis XV", "a mustard", "Nunavut, Canada", "a monkey", "the sea HD vide. 2015", "a turquoise", "a marble sculpture of Michelangelo", "Carousel", "the mug", "Charles Ogle", "a barrel", "linda evans", "a chakra (Sesli Szlk)", "the plague", "Wall Street", "Tiger Woods", "Samuel Beckett", "Colorado", "Wall Street", "Eastman Kodak Company", "Juicy Couture", "a skirting board", "Beautiful", "Istanbul", "Port Washington, New York", "an obelisk", "Brad Paisley", "a dallas", "Maharishi Mahesh Yogi", "the Green Mountain range", "Theodore Roosevelt", "a violin", "The Lamb", "Stephen F. Austin", "Elvis Presley", "The Washington Post", "Adam Jolles", "a bass trombone", "Antigone", "the Bay of Biscay", "Charles Dickens", "Gene Krupa", "Gorakhpur Junction", "about 1500 BC", "`` Reveille ''", "Spain", "peter", "Katy Perry", "\u00c6nima (\"ON-ima\")", "The entity", "Mark Helfrich", "Steven Green", "Saturday's Hungarian Grand Prix.", "People Against Switching Sides (PASS)"], "metric_results": {"EM": 0.421875, "QA-F1": 0.51973079004329}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.8, 0.14285714285714288, 0.18181818181818182, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6324", "mrqa_searchqa-validation-9701", "mrqa_searchqa-validation-9357", "mrqa_searchqa-validation-7311", "mrqa_searchqa-validation-10839", "mrqa_searchqa-validation-9796", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-14750", "mrqa_searchqa-validation-12163", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-7733", "mrqa_searchqa-validation-2006", "mrqa_searchqa-validation-1041", "mrqa_searchqa-validation-2346", "mrqa_searchqa-validation-8006", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-764", "mrqa_searchqa-validation-14742", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-8059", "mrqa_searchqa-validation-174", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-8268", "mrqa_searchqa-validation-3659", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-4354", "mrqa_hotpotqa-validation-2654", "mrqa_newsqa-validation-1733"], "SR": 0.421875, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.7299218749999999}, {"timecode": 62, "before_eval_results": {"predictions": ["Saul Alinsky", "in the spring of 1329", "to protect their tribal lands from commercial interests", "Legally Blonde", "Equus", "Eagle Eye", "the Teamsters Union", "New Zealand", "a microwave oven", "FDR", "Budapest", "feminism", "John Hersey", "tofu", "a roof", "Queen Victoria", "Judge Advocate General's Corps", "(Green Day)", "the Ottoman Empire", "the Narnia", "Vermont", "a rolling stone", "ex", "Austria", "Dante", "a piccolo", "Kosher Wines", "Austin", "Marian Anderson", "All About Eve", "U2", "Laos", "St Andrew", "\"Walden\"", "\"The Color Purple\"", "Manfred von Richthofen", "an artist, dancer and choreographer", "Canada", "Vladimir Lenin", "Bots", "a carpet", "a lampoon", "hydrogen", "the Centaur", "India", "treasury bonds", "necropolis", "Israel", "Justine", "the baroque", "Belgium", "the Vietnam War", "10,000 BC", "Jewish audiences", "October 6, 2017", "Japan", "Fred Gwynne", "San Francisco", "TD Garden", "Mitsubishi Motors", "zoonotic", "seeking help", "Zimbabwe President Robert", "a fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.5, "QA-F1": 0.6044591647387699}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 0.3157894736842105, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-8158", "mrqa_squad-validation-4314", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-7735", "mrqa_searchqa-validation-2017", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-13817", "mrqa_searchqa-validation-2666", "mrqa_searchqa-validation-13068", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-7093", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-6169", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-9729", "mrqa_searchqa-validation-8773", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-13426", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-12122", "mrqa_searchqa-validation-12255", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-11763", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-11854", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-620", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-85"], "SR": 0.5, "CSR": 0.5691964285714286, "EFR": 0.96875, "Overall": 0.7234486607142857}, {"timecode": 63, "before_eval_results": {"predictions": ["Montpellier", "39", "easier credit", "Pedro Calomino", "May 5, 2015", "Julia Verdin", "Dennis Potter", "1909 Cuban-American Major League Clubs Series", "Martin Ingerman", "\"The Godfather Part II\" (1974)", "seal hunting", "October 21, 2016", "Memphis", "William Corcoran Eustis", "queen consort of Hanover", "The Oregon Ducks", "The club", "December 24, 1973", "Christianity", "Polihale State Park", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "its riverside location", "322,520", "North Greenwich Arena", "Ringo Starr", "Designed by KlingStubbins", "the Swiss national team", "four", "Elsie Audrey Mossom", "Rice", "Albert Bridge", "Alfred Graf von Schlieffen", "the Mexican War on Drugs", "Erreway", "Leinster", "Oracle", "Katherine Harris", "Hong Kong", "Ice Princess", "Hellenism", "Iceland", "Alfred Edward Housman", "Cyclic Defrost", "\"Losing My Religion\"", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "1998", "Boston University", "dachshund", "Frank Sinatra", "Woking, England", "Army & Navy", "The 2016\u201317 NHL season", "Kate Walsh", "November 3, 2007", "Article 300 - A", "Cybill Shepherd", "Pamplona", "the narwhal", "$40 billion during the operations phase.", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks", "Tim Kaine", "Haile Selassie", "the cherry", "Gettysburg"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6649925595238095}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3034", "mrqa_squad-validation-639", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-4814", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5855", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3544", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-1756", "mrqa_naturalquestions-validation-2242", "mrqa_triviaqa-validation-51", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1832", "mrqa_searchqa-validation-14467"], "SR": 0.578125, "CSR": 0.5693359375, "EFR": 1.0, "Overall": 0.7297265625}, {"timecode": 64, "before_eval_results": {"predictions": ["six", "Six-time", "rear - view mirror", "German rock band Scorpions", "Fighter Command", "to fit in", "Kyrie Irving", "over 300,000", "Canada", "Jesse Frederick James Conaway", "B.R. Ambedkar", "1987", "a Malibu, California beach intercut with scenes of them driving an orange campervan", "during the American Civil War", "moist temperate climates", "twelve", "American singer and songwriter Lana Del Rey", "Nicole Gale Anderson", "TLC -", "Charles Perrault", "Djokovic", "alternative rock", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "declared neutrality", "in 1986", "between 1765 and 1783", "annuity", "Anthony Caruso as Johnny Rivers", "Michael Crawford", "Inequality of opportunity was higher in the transition economies of Central and Eastern Europe and Central Asia", "the New Testament canon ( although there are short apocalyptic passages in various places in the Gospels and the Epistles )", "4 September 1936", "the Dolby Theatre in Hollywood, Los Angeles, California", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "tax purposes", "Scheria", "flytrap", "Master Christopher Jones", "RMS Titanic", "the right side of the heart to the lungs", "all within the Pittsburgh metropolitan area", "the sea witch character who appears in the fairy tale `` The Little Mermaid '' by Hans Christian Andersen", "15 February 1998", "Library of Congress", "Arthur Chung ( January 10, 1918 -- June 23, 2008 )", "advisory speed signs are classified as warning signs, not regulatory signs", "March 31, 2013", "semi-automatic", "1987", "Rajendra Prasad", "Donna", "December 1800", "bacall", "Western Samoa", "british", "a micronutrient-rich diet", "A.P. M\u00f8ller Foundation", "December 13, 1920", "raping and killing a 14-year-old Iraqi girl.", "verify the authenticity of the voice on the tape.", "$627,", "A Portrait of the Artist", "Crete", "a brownie"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6398271775738882}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222218, 0.6666666666666666, 0.6666666666666666, 0.4, 0.16666666666666669, 0.5714285714285715, 1.0, 0.3157894736842105, 0.3157894736842105, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8333333333333333, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-571", "mrqa_naturalquestions-validation-8275", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-2989", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-10294", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-3505", "mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-5688", "mrqa_searchqa-validation-1924"], "SR": 0.484375, "CSR": 0.5680288461538461, "EFR": 0.9696969696969697, "Overall": 0.7234045381701631}, {"timecode": 65, "before_eval_results": {"predictions": ["1856", "CALIPSO", "Matt Flinders", "2026 -- the centenary of Gaud\u00ed's death", "the pouring rain at a rest stop", "to combine keys which are usually kept separate", "Ra\u00fal Eduardo Esparza", "1962", "April 1979", "1792", "Charles Carson", "November 5, 2017", "about a relationship", "1832", "8,850", "blood plasma and lymph in the `` intravascular compartment '' ( inside the blood vessels and lymphatic vessels )", "O'Meara", "uniform established proportions", "September 19, 2017", "almost all officeholders annually", "Rashida Jones", "Nodar Kumaritashvili", "Elis", "2000", "J.P. Zenger High", "Georges Auguste Escoffier", "After Shawn's kidnapping", "1904", "the American Kennel Club", "Liam Cunningham", "before the first year begins", "Amanda Bynes", "Woody Paige", "Albert Einstein", "the hypothalamus", "St. Theodosius Russian Orthodox Cathedral", "David Ben - Gurion", "T.J. Miller", "1995", "toys or doorbell installations", "Dirk Benedict", "New Croton Reservoir in Westchester and Putnam counties", "the source of the donor organ", "to prevent further offense by convincing the offender that their conduct was wrong", "Bill Henderson", "thunderstorms", "Roger Federer", "1981", "Paris", "St. John's, Newfoundland and Labrador", "Elvis Presley", "southern Anatolia", "Hugh Laurie", "All Things Must Pass", "Toronto", "Salvatore Testa", "Newark, New Jersey at the Prudential Center", "Washington, D.C.", "75.", "Grand Ronde, Oregon.", "1994", "Buddha", "Oslo", "Coretta Scott King"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5930544845779221}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0625, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.08, 0.0, 0.0, 0.7000000000000001, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-4228", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-9383", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-4896", "mrqa_hotpotqa-validation-5725", "mrqa_searchqa-validation-3274"], "SR": 0.515625, "CSR": 0.5672348484848485, "EFR": 0.9354838709677419, "Overall": 0.7164031188905181}, {"timecode": 66, "before_eval_results": {"predictions": ["24 March 1879", "shopping destinations in Newcastle", "saliva", "Robert Louis Stevenson", "South African", "Ben Whishaw", "Neighbours", "Alan B'Stard", "Wawrinka", "Vienna", "Dennis", "Brocks Hill", "Aphrodite Hills", "The Nutcracker", "climates of the earth", "Take That", "predominantly Muslim", "Cole", "carburetors", "on the island Lindisfarne", "Denmark", "conduction", "Willy Russell", "beets", "Celsius", "Henkel", "\"Li'l Abner,\"", "robot cop", "Aromatherapy", "Lieutenant", "barba", "Fenn Street School", "Fran\u00e7ois Hollande", "Johannesburg", "The Great Leap", "Vernon Kay", "Madrid", "The Danelaw", "Statue of Liberty", "wine director/Sommelier", "pasta", "blue", "Johnny Cash", "Ned Sherrin", "The Pillow Book", "Sally Ride", "table tennis", "the caterpillar", "1992", "Billy Fury", "California", "Anton Chekhov", "after the Spanish -- American War in the 1898 Treaty of Paris", "The Third Five - year Plan", "`` Killer Within ''", "Shepardson Microsystems", "Nan Britton", "Venice", "Haleigh Cummings", "growing crowded,", "Capitol Hill, Indiana,", "the bark", "the tonka bean", "the Fertile Crescent"], "metric_results": {"EM": 0.5, "QA-F1": 0.5505208333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5009", "mrqa_triviaqa-validation-355", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4013", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-951", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-6481", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-4790", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-598", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-3673", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4915", "mrqa_triviaqa-validation-4946", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-376", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-2926", "mrqa_searchqa-validation-11467"], "SR": 0.5, "CSR": 0.5662313432835822, "EFR": 0.96875, "Overall": 0.7228556436567164}, {"timecode": 67, "before_eval_results": {"predictions": ["1850", "Ed McCaffrey", "Dutch", "the realization of the first integrated circuit", "Abu Dhabi, United Arab Emirates", "Spain, Mexico and France", "Humberside Airport", "Jeff Meldrum", "Central Avenue", "1977", "Martin Truex Jr.", "compact car", "Frank Ocean", "Germanicus", "Detroit, Michigan", "924", "Birmingham, Alabama", "Sean", "Brian Friel", "Paul W. S. Anderson", "heavy metal", "Nebraska Cornhuskers women's basketball", "Frank Fertitta, Jr.", "beer and soft drinks", "Muriel", "Awake", "Helena Sternlicht", "British", "Nickelodeon", "2,615", "McG", "\"Invader (Invasor)\"", "oregon", "Orpheus and Eurydice", "Sean Yseult", "Nicholas Civella", "34 days", "Paris", "1907", "Jenji Kohan", "Collins", "Beatrice Tinsley", "fashion designers", "Crossed: Dead or Alive", "singer, songwriter, actress, and radio and television presenter", "1903", "Warsaw", "Slaughterhouse-Five", "2016", "The Spiderwick Chronicles", "Mediterranean", "Dave Schultz", "ty Rooney", "on the left hand ring finger", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "pope", "The Battle of Austerlitz", "ballet", "rocket", "Pixar's \"Toy Story\"", "Anonymous", "horror", "cellulopterus", "China"], "metric_results": {"EM": 0.5, "QA-F1": 0.6257688492063492}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.5714285714285715, 0.7499999999999999, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9852", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-529", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-355", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-4195", "mrqa_triviaqa-validation-1630", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-2617", "mrqa_searchqa-validation-7217"], "SR": 0.5, "CSR": 0.5652573529411764, "EFR": 0.96875, "Overall": 0.7226608455882353}, {"timecode": 68, "before_eval_results": {"predictions": ["the time of the Miocene", "observer status", "Moton Field, the Tuskegee Army Air Field", "O'Meara", "digitization of social systems", "vehicles inspired by theJeep that are suitable for use on rough terrain", "posthumously in 1890 in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "North Dakota ( 21.5 % )", "Ali Daei", "Russia", "Herbert Hoover", "elected or appointed by means of a commission ( letters patent ) to keep the peace", "The Bellamy Brothers", "the 1980s", "Hellenismos", "Batman", "Wakanda and the Savage Land", "August Darnell", "Lori Rom", "Nodar Kumaritashvili", "T - Bone Walker", "New Mexico", "The final venues were confirmed, along with the tournament's schedule, on 2 May 2013", "FIGG Bridge Engineers", "scrolls", "architecture", "Emmett Lathrop `` Doc '' Brown", "c. 1000 AD", "Baker, California, USA", "majority of members present at that time", "the passing of the year", "Christopher Allen Lloyd", "The Abbott and Costello Show episode `` The Actor's Home ''", "Moscazzano", "the Lower Mainland in Vancouver", "Gare du Nord", "the 9th century", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Wednesday, September 21, 2016", "turlough", "Glenn Close", "a substance that fully activates the receptor that it binds to ) while under other conditions, behaves as an antagonist", "Tommy Shaw", "All Hallows'Day", "the east African coast across the Indian Ocean", "The mixing of sea water and fresh water", "March 2018 in North America and Europe", "American singer Elvis Presley", "The pulmonary circulation", "Raja Dhilu", "1881", "1997", "Moon River", "the Prussian 2nd Army", "Loch Ness", "Boyd Gaming", "Guardians of the Galaxy Vol. 2", "Love Actually", "five", "Washington", "managing his time.", "Robin Hood", "Lafayette", "Virgin Atlantic"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6696906271566597}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.8695652173913043, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.72, 1.0, 0.28571428571428575, 0.0, 0.4444444444444445, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-9749", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-6843", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-2065", "mrqa_hotpotqa-validation-4345", "mrqa_newsqa-validation-4073", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11403"], "SR": 0.515625, "CSR": 0.5645380434782609, "EFR": 0.967741935483871, "Overall": 0.7223153707924264}, {"timecode": 69, "before_eval_results": {"predictions": ["Labor", "the hosts have the responsibility to ensure orderly delivery of packets", "the western part of the Republic of Botswana in southern Africa", "a minor basilica", "\"The Independent\"", "Royal College of Music", "Sullivan University College of Pharmacy", "Archbishop of Canterbury", "stagecoach robbery", "13 May 2018", "1941", "Kentucky", "Hard Workin' Man", "Kennedy Road", "August 1973", "Hawaii County", "the Summer Olympic Games", "Minnesota Timberwolves", "David Kossoff", "a priest", "England", "coaxial", "Tony Ducks", "striker", "Colin Vaines", "Indraneil S Penguinpta", "win world titles in four weight classes", "Dark Heresy", "Texas Tech University", "actress", "Scott Mosier", "Paradise, Nevada", "Adelaide", "Erinsborough", "interstate commerce", "Darci Kistler", "\"Northern Lights\"", "the Southern rock group Molly Hatchet", "\"Alceste\"", "Prada", "the Isles", "Mary Bonauto, Susan Murray", "David Wells", "Nayvadius DeMun Wilburn", "Macomb County", "wine", "Boston, Massachusetts", "Juventus", "7 June 1954", "Matthew Abraham \"Matt\" Groening", "Dan Crow", "Lucius Cornelius Sulla Felix", "1923", "Harrison Ford", "Bill Pullman", "setts", "nkrumah", "a docked yacht", "draquila -- Italy Trembles.", "positive signal", "a section of the roof", "the Bears", "pikes peak", "James Watt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6412202380952381}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4777", "mrqa_hotpotqa-validation-4134", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-5041", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-622", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-2768", "mrqa_searchqa-validation-9505"], "SR": 0.546875, "CSR": 0.5642857142857143, "EFR": 0.9655172413793104, "Overall": 0.7218199661330049}, {"timecode": 70, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1522", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2306", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4579", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1263", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-10839", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12507", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-1761", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-3828", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-893", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9626", "mrqa_searchqa-validation-9729", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10338", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1048", "mrqa_squad-validation-10491", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1655", "mrqa_squad-validation-1769", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2112", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-237", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3554", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-3965", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4286", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4615", "mrqa_squad-validation-4755", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5256", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6266", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-664", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-6924", "mrqa_squad-validation-6959", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7203", "mrqa_squad-validation-7211", "mrqa_squad-validation-7357", "mrqa_squad-validation-7407", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7793", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-7979", "mrqa_squad-validation-823", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-8566", "mrqa_squad-validation-8664", "mrqa_squad-validation-867", "mrqa_squad-validation-8837", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9567", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-9944", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1625", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-679", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.826171875, "KG": 0.47890625, "before_eval_results": {"predictions": ["the Song dynasty", "manage the pharmacy department", "$106,482,500", "a president who understands the world today, the future we seek and the change we", "police were trying to cover up the truth behind her daughter's murder, and that they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case.", "45 minutes,", "The forward's lawyer", "Michael Jackson", "over 1000 square meters in forward deck space, allowing for such features as a full garden and pool, a tennis court, or several heli-pads.", "Iran's claim that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "a bag", "the procedures", "free laundry service", "The son of Gabon's former president was declared the winner of the country's presidential elections", "Kingdom City", "\"An undated photo of Alexandros Grigoropoulos,", "in Section 60.", "\"build a fortress around America; to stop trading with other countries, shut down immigration, and rely on old industries", "the challenges a pregnancy", "The EU naval force", "Bob Bogle", "composer of \"Phantom of the Opera\" and \"Cats\"", "19-12", "$273 million", "O2 Arena.", "that the Richmond students did nothing because of the \"bystander effect\"", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and", "Lucky Dube", "Unseeded", "the bill", "82", "150", "burns", "In funky neighborhoods of Tokyo customers", "Ennis, County Clare", "Jacob", "\"The finding of the body \"has really cut the legs out of the defense,\"", "flooding and debris", "a little more than 5,600", "the fact that the teens were charged as adults.", "13", "Iraqi Prime Minister Nouri al-Maliki", "Mugabe's opponents", "Kenyan and Somali", "snowstorm", "Stephen Johns reportedly opened the door for the man police say was his killer.", "Kenner, Louisiana", "Benazir Bhutto", "\"Watchmen\"", "U.S. Agency for International Development,", "a building in downtown Nairobi.", "1998.", "Tom Tucker", "Indirect rule", "Kryptonite", "Southern Carolina", "giant", "James Stewart", "Bobby Hurley", "Sulla", "a Gondorian soldier", "Howard Hughes", "Harry Truman", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6491807858995359}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.37837837837837834, 0.5714285714285715, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.22222222222222224, 0.5, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2297", "mrqa_naturalquestions-validation-10523", "mrqa_triviaqa-validation-55", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-10796"], "SR": 0.546875, "CSR": 0.5640404929577465, "EFR": 1.0, "Overall": 0.7144487235915493}, {"timecode": 71, "before_eval_results": {"predictions": ["playing cards", "the divinity of Jesus", "Rocky Down Mexico Way", "St Mark", "Romeo & Juliet", "Augello Cook", "debts", "Cape Town", "the FHA", "yodeling", "The Criminal Justice Information Services Division,", "Spartacus", "white", "sphinx", "Prince Rogers Nelson", "Saddam", "the Senate", "Texas A&M", "Dance Duet", "anaconda", "the Dead Sea, and chiefly south of the river Arnon", "Stanford", "a relationship with a man who proves to", "1927", "Transportation", "Mexico City", "atolls", "cat", "Bosnia", "Christopher Columbus", "ton", "the inquisition", "\"pan rabbit,\"", "the hyoid horns", "Joan of Arc", "Maternity", "Mulan", "Namibia", "Abraham Lincoln", "Alaska", "President Bush", "Jim Thorpe", "a wombat", "the phi phenomenon", "Pushkin", "dessert sized portions", "the Norman conquest of England", "An Officer and a Gentleman", "Elna", "Summer Was employed as a life guard", "Monopoly", "Volkswagen", "Lulu", "Miami Heat of the National Basketball Association ( NBA )", "Jurchen Aisin Gioro clan", "a black Ferrari", "the Netherlands", "Harvard", "1978", "1866", "276,170", "New Haven firefighter", "stand down.", "Wednesday."], "metric_results": {"EM": 0.53125, "QA-F1": 0.5760664682539682}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1151", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-5826", "mrqa_searchqa-validation-16238", "mrqa_searchqa-validation-6742", "mrqa_searchqa-validation-5494", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-16635", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-14499", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-14020", "mrqa_searchqa-validation-402", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-3840", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-1027", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-10857", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-55", "mrqa_newsqa-validation-2710"], "SR": 0.53125, "CSR": 0.5635850694444444, "EFR": 1.0, "Overall": 0.7143576388888889}, {"timecode": 72, "before_eval_results": {"predictions": ["reflects the structure and substance of his questions and answers concerning baptism in the Small Catechism", "three", "Montpelier", "City of Hope", "New Orleans", "a router", "the United States dollar", "Herbert Hoover", "crawfish", "a Doric order structure", "Emil Von Behring", "Senator", "a yeast", "Lewis and Clark", "Mount Everest", "saxophones", "a Sketch", "Peter Paul Rubens", "grapefruit", "Fermi", "a monarch caterpillar", "Jerusalem", "the Juilliard School", "Ralph Lauren", "Mary Poppins", "molecules of... but are pressed together and act as a shield between the water and the paper", "Kansas City", "the Godfather", "a pickle chip", "Franklin Pierce", "a calculating machine", "magnesium", "4", "occipital", "Mao Zedong", "Coretta Scott", "the uterus", "Men\\'s Hockey", "a Champagne producer", "clouds", "a hog's thigh", "Spanish-American War", "George Rogers Clark", "Jack Palance", "a dose", "Columbus Day", "Southern", "Congo", "a squash", "a calves", "a person", "Prince William and Kate Middleton", "Michigan State Spartans", "Todd Bridges", "August 18, 1998", "the fusion of two or more digits of the feet", "Dave", "Berlin", "Tybalt", "evangelical Christian", "Windermere", "6-2 6-1", "Kaka", "last year's"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5411458333333334}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false], "QA-F1": [0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2448", "mrqa_searchqa-validation-16859", "mrqa_searchqa-validation-6176", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-4928", "mrqa_searchqa-validation-16619", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-13383", "mrqa_searchqa-validation-12215", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-1352", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-11217", "mrqa_searchqa-validation-11175", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-4301", "mrqa_searchqa-validation-2531", "mrqa_searchqa-validation-14033", "mrqa_searchqa-validation-13169", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-15708", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-449", "mrqa_hotpotqa-validation-1680", "mrqa_newsqa-validation-1750"], "SR": 0.4375, "CSR": 0.5618578767123288, "EFR": 1.0, "Overall": 0.7140122003424658}, {"timecode": 73, "before_eval_results": {"predictions": ["IgG", "the green chloroplast lineage", "George", "Beauty", "Discworld", "prometheus", "Hulk Hogan", "Genre Grandeur", "Rodgers & Hammerstein", "Jamaica", "Wawrinka", "Marillion", "Giuseppe Verdi", "Roy Keane", "a second-year", "Sarah Vaughan", "goats", "Didier Drogba", "Monaco", "Chaplin", "Cary Grant", "Mozart", "Mary Quant", "Tina Turner", "Culloden", "Ishmael", "the Swordfish", "Buenos Aires", "the Porteous Riots", "Santa Cruz", "Julian Fellowes", "the cornea", "motocross", "John Nash", "Batman & Robin", "alexmarcelo", "Lauren Bacall", "Thom Yorke", "Hokkaido", "a sister and brother", "milk", "Some Like It Hot", "The Life and Opinions of Tristram Shandy", "Mahatma Gandhi", "Minette Walters", "Erik Thorvaldsson", "Churchill Downs", "Honda", "multiplayer", "the narwhal", "Prophet Joseph Smith, Jr.", "Adidas", "Benzodiazepines", "in southern Anatolia", "Akbar the Great", "Awki", "Fade Out: The Calamitous Final Days of MGM", "Fountains of Wayne", "debate preparation.", "four decades", "snow, sleet, freezing drizzle", "milk", "Helena", "dachshund"], "metric_results": {"EM": 0.5, "QA-F1": 0.5489583333333334}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-4342", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-1384", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-6119", "mrqa_naturalquestions-validation-6482", "mrqa_hotpotqa-validation-1327", "mrqa_newsqa-validation-1019", "mrqa_searchqa-validation-6441", "mrqa_searchqa-validation-9755", "mrqa_searchqa-validation-5816"], "SR": 0.5, "CSR": 0.5610219594594594, "EFR": 0.875, "Overall": 0.6888450168918919}, {"timecode": 74, "before_eval_results": {"predictions": ["ships", "$10\u201320 million", "London", "1944", "Alfonso Cuar\u00f3n", "FC Bayern Munich", "DeskMate", "J. K. Rowling", "1967", "Anandji Virji Shah", "jena Malone", "\"Le Divorce\"", "Conservatorio Verdi", "The Bye Bye Man", "Esteban Ocon", "The fictional character Spider-Man", "Ben Elton", "400 MW", "\"personal earnings\" (such as salary and wages)", "1991", "Patricia Arquette", "Robins Air Force Base", "1993", "belte midler", "Newcastle upon Tyne, England", "the Recording Industry Association of America", "Zero Mostel", "Donald Duck", "2 March 1972", "Aiden English", "Big Machine Records", "Matt Groening", "Jean Acker", "1938", "Norse mythology", "National Hockey League", "Vincent Landay", "political", "The Province of Syracuse", "dachshund", "cricket fighting", "Venus", "Ford Field in Detroit, Michigan", "largest country", "\"We'll Burn That Bridge\"", "Summerlin, Nevada", "gamecock", "1995", "\"The Ones Who Walk Away from Omelas\"", "Philip K. Dick", "The Clinchfield Railroad", "`` Have I Told You Lately ''", "159", "an undisclosed location", "Via Appia", "kimbe", "Bruce Alexander", "punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "boyhood experience in a World War II internment camp", "Lillo Brancato Jr.", "eye", "bugsy siegel", "penny Lane", "two"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7564383715699505}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.23076923076923078, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-517", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-84", "mrqa_triviaqa-validation-4157", "mrqa_newsqa-validation-1759", "mrqa_searchqa-validation-9976", "mrqa_triviaqa-validation-1193"], "SR": 0.703125, "CSR": 0.5629166666666667, "EFR": 1.0, "Overall": 0.7142239583333334}, {"timecode": 75, "before_eval_results": {"predictions": ["counterflow", "Colonel Robert E. Lee", "Duck", "the red - bed country of its watershed", "a best - of - seven playoff", "Seven", "the 4th century", "Macon Blair", "Carolyn Sue Jones", "President pro tempore of the Senate", "Geoffrey Dyson Palmer", "John Smith", "July 2010", "economic recession", "requiring all non-U.S. ('foreign') financial institutions", "the breast or lower chest of beef", "Edward V", "Presley Smith", "flour and water", "The Gupta Empire", "the times sign or the dimension sign is the symbol \u00d7", "Dr. Rajendra Prasad", "1986", "1900", "Tandi", "France", "Richard Parker", "Ryan Pinkston", "in the Texas Panhandle", "differential erosion", "Terrence Howard", "2018", "lead", "a reference to a fictional character, le roi d'irelande, in the courtly legend cycle of Tristan", "Bill Ray", "his cousin D\u00e1in", "relieves the driving motor from the load of holding the elevator cab", "the Speaker", "in the 1970s", "9 or 10 national ( significant ) numbers after the `` 0 '' trunk code", "Siddharth Arora / Vibhav Roy", "the epidermis", "in the books of Exodus and Deuteronomy", "Archduke Franz Ferdinand of Austria", "blighted ovum or anembryonic gestation", "China", "85 %", "Tony Orlando", "Lou Rawls", "William", "a crust of mashed potato", "the Forth Bridge", "\"Wooden Heart (Muss I Denn)", "Billy", "Golden Globe Award", "Laurence Olivier", "port of Baltimore west to Sandy Hook", "bribing other wrestlers to lose bouts,", "\"Aboud El Zomor, the leader of Al Gamaa al-Islamiyya,", "Another high tide", "Labor Day", "David Beckham", "Ear", "Sarajevo"], "metric_results": {"EM": 0.40625, "QA-F1": 0.60630919784872}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.5, 0.923076923076923, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.0, 1.0, 0.24000000000000002, 0.8, 0.3333333333333333, 1.0, 0.35294117647058826, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.125, 0.0, 1.0, 0.16666666666666666, 0.0, 0.3333333333333333, 0.18181818181818182, 1.0, 1.0, 0.2857142857142857, 1.0, 0.8333333333333333, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 0.6666666666666666, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-10260", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-6109", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10616", "mrqa_triviaqa-validation-5282", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-6266", "mrqa_hotpotqa-validation-1727", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-914", "mrqa_searchqa-validation-177"], "SR": 0.40625, "CSR": 0.5608552631578947, "EFR": 0.9473684210526315, "Overall": 0.7032853618421052}, {"timecode": 76, "before_eval_results": {"predictions": ["The South Florida/Miami area", "2010", "1901", "1992", "the president", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "convergent plate boundary", "Debbie Gibson", "Polly Walker", "President", "employment in which a person works a minimum number of hours defined as such by his / her employer", "Castleford", "James Chadwick", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Mark Lowry", "Portuguese and Spanish - French origins", "Andrew Lloyd Webber", "in 1991, 1994, 2002, 2004 and 2010 in Switzerland, Austria, France and Germany", "To capitalize on her publicity", "In December 1971", "her attractive tattoosed neighbour ( Holden Nowell ) as he is working on his lawn", "grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ), 3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "The courts", "actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169 ( also known as the NLRA and the Wagner Act", "October 27, 1904", "The Parlement de Bretagne", "Vijaya Mulay", "Mark Humphrey as Frank Hogan", "Thomas Jefferson", "Hans Raffert 1988", "Robert Duvall", "whether they wish to collect a jackpot prize in cash or annuity", "a young girl ( an illustration by Everest creative Maganlal Daiya back in the 1960s )", "July 21, 1861", "Lead and lead dioxide", "Notts County ( 1894 )", "Mary Chapin Carpenter", "November 2016", "Hellenic Polytheism", "Russian defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "2002", "weekly Torah portion ( Hebrew : \u05e4\u05b8\u05bc\u05e8\u05b8\u05e9\u05b7\u05c1\u05ea \u05d4\u05b7\u05e9\u05b8\u05b7 \u202c Parashat ha - Shavua )", "Divyanka Tripathi", "save, rescue, savior", "1906", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "modestly and cover their breasts and genitals", "the government - owned Panama Canal Authority", "tectonic", "Middle Eastern alchemy", "the investment bank Friedman Billings Ramsey", "the Hooded Claw", "Culloden", "The Comitium", "McLaren-Honda", "Alexander Alexeyevich Gorsky", "Lev Yashin", "August 4, 2000", "Turkey,", "the Southeast,", "biological", "king of Spain", "high jump", "Jane Addams"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6440647035720839}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.22222222222222224, 0.0, 0.2666666666666667, 0.25, 1.0, 0.8656716417910448, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.08333333333333333, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 0.3846153846153846, 0.4, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-172", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-2011", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-3865", "mrqa_hotpotqa-validation-2464", "mrqa_searchqa-validation-1499"], "SR": 0.53125, "CSR": 0.5604707792207793, "EFR": 0.9333333333333333, "Overall": 0.7004014475108226}, {"timecode": 77, "before_eval_results": {"predictions": ["lay servants", "Randy Newman", "Daman and Diu", "Melanie Lynskey", "Ann Gillespie", "A turlough, or turlach", "Angad Bedi", "the 1830s", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "by the early 3rd century", "John Young", "Wilt Chamberlain and LeBron James", "March 18, 2005", "Sir Rowland Hill", "Spain", "one", "a mountainous, peninsular mainland jutting out into the Mediterranean Sea at the southernmost tip of the Balkans, and two smaller peninsulas projecting from it : the Chalkidice and the Peloponnese", "Chris Rea", "Jodie Foster", "honey bees", "Cress", "Narin Niruttinanon", "his brother", "XXXX", "Ian Tucker", "scrolls dating back to the 12th century", "1546", "Emily Perkins", "Atlantic Ocean", "Joseph M. Scriven", "ex-England cricketer Phil Tufnell", "c. 1000 AD", "a `` house edge '', a statistical advantage for the casino that is built into the game", "in the original Star Wars film in 1977", "sovereignty", "MacFarlane", "Marie Van Brittan Brown", "an individual noticing that the person in the photograph is attractive, well groomed, and properly attired", "Richie Cunningham", "16,801", "Alex, Meredith, Lexie and Jackson", "The Beatles", "the final episode of the series", "the summer of 1979", "10 May 1940", "December 14, 2017", "in 2014", "May 2010", "the pyloric valve", "Rome, Italy", "Aegisthus", "the Golden Hind", "Joyce Carol Oates", "seal", "political correctness", "professional wrestler, actor, and hip hop musician", "1944", "The city,", "on a Taurus XL rocket from Vandenberg Air Force Base in California", "The opposition group, also known as the \"red shirts,\" is demanding that the prime minister dissolve the parliament within 15 days.", "Java", "radius", "Sahara", "Carisa Cunningham,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6385975340719678}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.5, 0.0, 1.0, 0.7741935483870968, 0.28571428571428575, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.5531914893617021, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5833333333333334, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-9322", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8491", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4605", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2674", "mrqa_searchqa-validation-9180"], "SR": 0.515625, "CSR": 0.5598958333333333, "EFR": 0.9354838709677419, "Overall": 0.700716565860215}, {"timecode": 78, "before_eval_results": {"predictions": ["Ernest Gimson, Edward William Godwin, Charles Voysey, Adolf Loos and Otto Wagner", "Emily Perkins", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Frank Oz", "Mickey Rourke", "Thomas Jefferson", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "Kryptonite", "facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "Hundreds or even thousands", "Iowa", "the Ramones", "The Massachusetts Compromise", "Gabrielle - Suzanne Barbot de Villeneuve", "Johannes Gutenberg", "Cress", "rocks and minerals", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed by a destroyer's railgun called in by Simmons", "Lord's, on 15 July 2004 between Middlesex and Surrey", "1979", "Buddhism", "2014", "1974", "gravitation", "Supplemental oxygen was used in the past for most people with chest pain but is not needed unless the oxygen saturations are less than 94 % or there are signs of respiratory distress", "Nancy Jean Cartwright", "the heart", "the coffee shop Monk's", "increased productivity, trade, and secular economic trends", "1975", "2018", "1960", "Garbi\u00f1e Muguruza", "In 1674, Dutch navy captain Jurriaen Aernoutsz also briefly captured two forts in the French colony of Acadia, which he claimed as Dutch territory the new colony of New Holland", "the Sunni Muslim family", "3", "Ali Daei", "the intersection of Del Monte Blvd and Esplanade Street", "the primal rib", "Abigail Hawk", "Bed and breakfast", "J.P. Zenger High", "September 24", "in Super Bowl LII", "Billy Gibbons", "a donor molecule", "Massachusetts", "The season seven premiere", "October 29 - 30, 2012", "In Britain followed the rest of the world in decimalising its currency, the Mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "Service / Crown personnel serving in the UK or overseas in the British Armed Forces or with Her Majesty's Government", "Carpathia", "Scharnhorst", "Tasmania", "Prussia", "Gregg Harper", "Daphnis et Chlo\u00e9", "Mogadishu", "Linda Hogan, 49,", "Hakeemullah Mehsud", "Joplin", "credit card debt", "Tolkien", "hate crime"], "metric_results": {"EM": 0.625, "QA-F1": 0.7095820998854088}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false], "QA-F1": [0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6363636363636364, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7619047619047621, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-5599", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-3881", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-6610"], "SR": 0.625, "CSR": 0.5607199367088608, "EFR": 0.9166666666666666, "Overall": 0.6971179456751055}, {"timecode": 79, "before_eval_results": {"predictions": ["in collenchyma tissue", "Dollywood", "Chicken Run", "a Fender Stratocaster", "Portugal", "Roger Williams", "Amherst", "Annika Sorenstam", "breakfast", "Christmas", "John Keats", "Ford", "Lindsay Davenport", "Vanessa Hudgens", "50 million cells per litre (quart)", "Starsky and Hutch", "John Locke", "Canterbury", "the Ottoman Empire", "Phil of the Future", "12", "Nacho Libre", "India", "Mork & Mindy", "867-5309", "Twin-lens reflex camera", "a backfire", "the Rhine & the Main", "Mentor", "Virgin", "Angel Gabriel", "American Graffiti (1973)", "Danny Elfman", "a complementary angle", "the Horn of Africa", "captain", "driving Miss Daisy", "hippopotamus", "paulaner singapore clarke quay", "Pope John Paul II", "Viggo Mortensen", "pufferfish", "E. A. Poe Society of Baltimore", "pimm\\'s Cup", "Aston Martin", "the House of Commons", "Edward R. Murrow", "Mahatma Gandhi", "Houston Rockets", "fief", "COPIER- Xerox", "Harrys", "American country music artist Toby Keith", "advisory speed signs are classified as warning signs, not regulatory signs", "Thailand", "Walt Whitman", "otto von Bismarck", "Dire Straits", "Wynantskill, New York", "motor ships", "Iran's parliament speaker", "Cannes Film Festival,", "Tuesday in Los Angeles.", "Help!"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6982886904761905}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-5773", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-15298", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-766", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16351", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9466", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-649", "mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-4209", "mrqa_searchqa-validation-15443", "mrqa_searchqa-validation-1559", "mrqa_naturalquestions-validation-922", "mrqa_hotpotqa-validation-5724", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-1275"], "SR": 0.59375, "CSR": 0.5611328125, "EFR": 1.0, "Overall": 0.7138671875}, {"timecode": 80, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4349", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-10377", "mrqa_searchqa-validation-10784", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-14131", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9626", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1769", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-237", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3554", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4286", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4755", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6266", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7793", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-855", "mrqa_squad-validation-8664", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-9944", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1625", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.81640625, "KG": 0.49453125, "before_eval_results": {"predictions": ["the wedding banquet", "(David) Copperfield", "Adolf Hitler", "Indiana Jones", "(S Sweeney) Todd", "Nirvana", "Cosmopolitan", "changelings", "right", "Erin Brockovich", "Shamgar", "Biggie Smalls", "a coil", "Sherlock Holmes", "Ring", "green", "Vietnam", "Star Trek", "the white cliffs", "silkworm", "Bangkok", "the northern", "the Puget Sound", "Winnie the Pooh", "the Erie Canal", "1942", "the Middle Kingdom", "(Joseph) Ingraham", "vowel", "Dorothy Parker", "Passover", "the Pooh", "Austin Powers", "offensive", "the manatee", "Shampoo", "the X-Files", "(VUS.7d)", "West Virginia", "La Salle", "the President", "King John", "China", "a pigeon", "Vyacheslav Molotov", "(Paul) McCartney", "comedian", "(Jackie) Robinson", "hurt Tammy Wynette", "an electron", "Zbigniew Brzezinski", "1986", "Catherine Tramell", "2018", "belt", "(cueball)", "mongoose", "United States Food and Drug Administration (FDA)", "Ben Johnston", "Austria-Hungary", "Mexico", "gingerbread cookies.", "a dozen", "a cloak"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6010416666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15304", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-16217", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-5808", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-4805", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-12464", "mrqa_triviaqa-validation-1379", "mrqa_triviaqa-validation-6859", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-4669", "mrqa_newsqa-validation-3124", "mrqa_triviaqa-validation-2123"], "SR": 0.578125, "CSR": 0.5613425925925926, "EFR": 1.0, "Overall": 0.7197685185185185}, {"timecode": 81, "before_eval_results": {"predictions": ["Rembrandt", "South Africa", "Germany", "the Red Sea", "Ping Pong", "in America", "a howitzer", "the Teen Titan", "Orwell", "New Zealand", "the Blue Meanies", "drop", "the submediant", "the \"Fisherman\\'s ring", "Iris Murdoch", "Coleridge", "tabby", "St. Francis of Assisi", "a porter waiting", "Cathy", "Pilsner malt", "Joe Louis", "The Flying Dutchman", "Madison", "Iraq", "Canada", "a dewclaw", "Honolulu", "Hootoo", "lion", "Dolphins", "King Solomon", "Dallas", "a satrap", "Virginia Woolf", "high jump", "a title", "Poseidon", "Cesare Borgia", "Urdu", "lamb", "man", "hot springs", "Advil", "Guy Arcizet", "Schoenberg", "glassware", "Joel Osteen", "Ambrose Bierce", "Paris", "the Manila Galleons", "Bill Hayes", "4 September 1936", "at birth", "Blackfriars", "Sedbergh", "Renzo Piano", "Sydney", "34 days", "Woon Swee Oan", "as many as 250,000 unprotected civilians", "A 22-year-old college student in Boston, Massachusetts,", "\"Oprah: A Biography,\"", "Hungary"], "metric_results": {"EM": 0.515625, "QA-F1": 0.604584478021978}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.30769230769230765, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-671", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-10497", "mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-9487", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-14534", "mrqa_searchqa-validation-7447", "mrqa_searchqa-validation-14275", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-3172", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-4659", "mrqa_searchqa-validation-531", "mrqa_searchqa-validation-750", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-13942", "mrqa_naturalquestions-validation-9799", "mrqa_triviaqa-validation-5511", "mrqa_hotpotqa-validation-5146", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-834"], "SR": 0.515625, "CSR": 0.5607850609756098, "EFR": 1.0, "Overall": 0.7196570121951219}, {"timecode": 82, "before_eval_results": {"predictions": ["between June and September", "six", "1910", "northwestern", "Spanish", "100 million", "Walcha", "Duchess Eleanor of Aquitaine", "DJ Scotch Egg", "122,067", "1971", "Ian Rush", "May 4, 2004", "Jan Kazimierz", "1844", "Theodore Robert Bundy", "October 16, 2015", "Buffalo", "21 July 2015", "as", "128", "2", "the E Street Band", "Hindi", "an organ", "the Innviertel region of western Upper Austria", "\" Rogue One\"", "\"God and the just cause\"", "Trilochanapala", "Deputy F\u00fchrer", "James Victor Chesnutt", "\"good character\"", "constant support from propaganda campaigns", "Westchester", "\"The Worm\"", "New York", "Northampton, England", "Suspiria", "Russian", "Laurel, Mississippi", "India Today", "MTV Russia", "McLaren-Honda", "January 28, 2016", "Debbie Reynolds", "Blender", "Stage Stores", "two", "between the 8th and 16th centuries", "Jerry Michael Glanville (born October 14, 1941)", "Hindi", "beneath the liver", "Golden Gate Bridge", "The Intolerable Acts", "Dik Browne", "a cigarette", "The Perfect Storm", "Afghan security forces", "Libreville, Gabon.", "planning processes are urgently needed", "profondo", "a reel", "Kentucky", "December"], "metric_results": {"EM": 0.546875, "QA-F1": 0.604985119047619}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1766", "mrqa_hotpotqa-validation-4216", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-43", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-6333", "mrqa_newsqa-validation-2178", "mrqa_searchqa-validation-10144", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-4061", "mrqa_naturalquestions-validation-2169"], "SR": 0.546875, "CSR": 0.5606174698795181, "EFR": 1.0, "Overall": 0.7196234939759036}, {"timecode": 83, "before_eval_results": {"predictions": ["Imperial Secretariat", "rural areas", "1985", "1851", "novelty songs", "Albert", "Norman", "1930s and 1940s", "Captain", "Mrs. Eastwood & Company", "November 23, 2011", "CBS", "The R-8 Human Rhythm Composer", "Joulupukki", "Clive Staples Lewis", "United Holy Salvation Army and Uganda Christian Army/Movement", "\"Mechte Navstrechu\"", "FEMSA", "Football Club Barcelona", "100 countries", "Colin Blakely", "The 2004 Nokia Sugar Bowl", "Jon Hamm", "Gian Carlo Menotti", "1883", "Dougray Scott", "January 15, 1975", "Tim Whelan", "Montreal", "Long Island", "the Kentucky Music Hall of Fame", "The Oklahoma Sooners men's basketball", "trans-Pacific flight", "The United States House of Representatives", "Anthony Lynn", "Alfred Graf von Schlieffen", "graffiti", "Justin Adler", "Empire of Japan", "Lieutenant Colonel Iceal E. \"Gene\" Hambleton", "VfB Stuttgart", "27 November 1956", "2015", "ESPN", "actor", "May", "Hillary Rodham Clinton", "1981", "Oakdale", "J\u00fcrgen M. Geissinger", "Gi Francesco Malipiero", "Inequality of opportunity", "`` If These Dolls Could Talk ''", "victory", "John Steinbeck", "in god we trust", "\"Special Forces Command\"", "\"A Whiter Shade of Pale\"", "Matthew Fisher", "$250,000", "Olivetti", "the EZ Spin Foam Frisbee", "a typewriter", "May 29, 2018"], "metric_results": {"EM": 0.734375, "QA-F1": 0.77421875}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8075", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-3880", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-572", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10512", "mrqa_triviaqa-validation-3097", "mrqa_searchqa-validation-1542"], "SR": 0.734375, "CSR": 0.5626860119047619, "EFR": 0.9411764705882353, "Overall": 0.7082724964985995}, {"timecode": 84, "before_eval_results": {"predictions": ["Apollo 17", "\"Kill Your Darlings\" (2012)", "Julian Dana William McMahon", "Ericsson (\"Telefonaktiebolaget L. M. Ericsson\")", "Casey Bond", "The S7 series", "John Ford", "Hallett Cove", "June", "Boston Celtics", "Hong Kong", "the Magic Band", "Sir Frank P. Lowy", "Colonel Jackie Lynwood \"Jack\" Ridley", "Southbank", "The Lost Boyz", "Guangzhou, China", "J.R. R. Tolkien", "World War II", "31 October 1783", "Japan", "largest Mission Revival Style building in the United States", "Skipton", "24 May 1965", "the special episode \"The Clash of Triton or To SquarePants\"", "Amon Leopold G\u00f6th", "Aqua", "Colonial colleges", "Premier League club Tottenham Hotspur and the England national team", "Linda Ronstadt", "Wilhelmus Simon Petrus Fortuijn", "Steven Selling", "Tottenham", "Vilyam \"Willie\" Genrikhovich Fisher", "Francisco Antonio Zea", "France", "Anne", "The Bruce Springsteen Band", "King Duncan", "Bardot", "On St. Patrick's Day in 1988", "1985", "40 million", "On March 27, 1977, two Boeing 747 passenger jets, KLM Flight 4805 and Pan Am Flight 1736, collided on the runway at Los Rodeos Airport (now Tenerife North Airport)", "General Motors", "singer", "Miami Gardens", "Marvel Comics", "the bounty hunter Jango Fett", "Hjernevask", "Austral L\u00edneas A\u00e9reas", "Madhouse", "The epidermis", "May 18, 2010", "the Carolina Parakeet", "parlophone", "caviar", "The Casalesi Camorra clan", "Jason Chaffetz", "Amsterdam, in the Netherlands,", "Anne Rice", "improvisation", "Jupiter", "538 electors"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6781411297036297}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.42857142857142855, 1.0, 1.0, 1.0, 0.5, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 0.15384615384615385, 0.0, 0.3333333333333333, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4991", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2451", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-3639", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-2329", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5501", "mrqa_naturalquestions-validation-7513", "mrqa_triviaqa-validation-3939", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-8988", "mrqa_naturalquestions-validation-5417"], "SR": 0.484375, "CSR": 0.5617647058823529, "EFR": 0.9393939393939394, "Overall": 0.7077317290552585}, {"timecode": 85, "before_eval_results": {"predictions": ["Solovay-Strassen tests", "liberalisation", "W. G. Harding", "Gus Grissom, Edward White, and Roger Chaffee", "Larry Bird", "Stan", "drogon", "gurus", "New York", "Spin", "happy", "Grand Central Liquor", "e Selamu", "The Devil Wears Prada", "heresy", "Cleveland", "Barrientos", "Iridill", "red sausage", "Newton", "Union Square", "a mirror", "The Sleeping Beauty", "Fresno", "the Rachel Carson State Office Building", "1066", "a tightrope", "WD-40", "Bonobos", "the sound barrier", "Hillary Clinton", "Adam", "a metronome", "the pupil", "Les Noces", "Larry Fortensky", "the 2005 holiday movie season", "3", "the Guantnamo Bay Naval Station", "is the 44th and current President of the United States of America.", "Yond Cassius", "Turkish", "True West", "The liver", "Anja Prson", "John Paul Jones", "a chalkboard", "a coat", "The Simpsons Movie", "\"Billy Budd, Billy Budd\"", "Utah", "Richie Cunningham", "April 1917", "1799", "weasel", "beard", "fractal geometry.", "Aamir Khan", "five times", "\"Hacksaw Ridge\"", "National Infrastructure Program, as he called it,", "he and Armento, 51, were drinking at a strip club when they decided to go hunt for valium.", "the Employee Free Choice act in Lafayette Square in Washington", "1960"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5093425671550672}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.4, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9025", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-3796", "mrqa_searchqa-validation-725", "mrqa_searchqa-validation-5299", "mrqa_searchqa-validation-11783", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-5370", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-4801", "mrqa_searchqa-validation-1215", "mrqa_searchqa-validation-9247", "mrqa_searchqa-validation-16204", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-3351", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-10218", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-11956", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-12376", "mrqa_searchqa-validation-2715", "mrqa_searchqa-validation-6776", "mrqa_searchqa-validation-14066", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-5379", "mrqa_searchqa-validation-15030", "mrqa_triviaqa-validation-899", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2344"], "SR": 0.4375, "CSR": 0.5603197674418605, "EFR": 1.0, "Overall": 0.7195639534883721}, {"timecode": 86, "before_eval_results": {"predictions": ["over 200 awards", "business pests", "the Cherokee Nation", "Danny Elfman", "a blank space", "Vladivostok", "Brian Cowen", "Spike the Vampire Slayer", "Linus", "Seurat", "Pompeii", "achaia", "a tuba", "a megaton", "Michigan", "the sternum", "September", "Madrid", "Roger Williams", "Washington", "the retina", "20 million", "dance", "\"The Life Times of John Barrymore\"", "Moldova", "Andrew Johnson", "\"The Professor's House\"", "a black hole", "Uranus", "gluttony", "a catalog", "Marx", "German", "words", "Baghdad", "Merlin", "The Love Song of J. Alfred Prufrock", "the fteedoDi", "\"The Bells\"", "George Washington Carver", "Tommy Franks", "dreams", "Tigger", "Songs of Innocence", "Ishmael", "Charles Schumer", "Jean", "Saudi Arabia", "the \" Dustbin lids\"", "Kufic", "focal point", "a living prokaryotic cell ( or organelle )", "Lady Gaga", "Mars Hill", "Portugal", "Puccini", "Rainbow", "Harry Booth", "Vilnius Old Town", "35,402", "700 guests", "Zimbabwe", "someone with a compatible organ died and their family asked that it be given to the singer, according to the organ procurement group that handled the donation.", "Orson Welles"], "metric_results": {"EM": 0.5, "QA-F1": 0.5971354166666666}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true], "QA-F1": [0.8, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.08333333333333333, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8000", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-5818", "mrqa_searchqa-validation-1912", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-15909", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-7736", "mrqa_searchqa-validation-16202", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-12862", "mrqa_searchqa-validation-1985", "mrqa_searchqa-validation-15649", "mrqa_searchqa-validation-9217", "mrqa_searchqa-validation-11033", "mrqa_searchqa-validation-8437", "mrqa_searchqa-validation-13025", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14277", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-765", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1276"], "SR": 0.5, "CSR": 0.5596264367816092, "EFR": 1.0, "Overall": 0.7194252873563218}, {"timecode": 87, "before_eval_results": {"predictions": ["\"colonial\" coloration", "peat moss", "linguistic analysis", "Oliver Parker", "Walt Disney", "mathematician and physicist", "Afghanistan", "novelty songs", "Barbados", "The Captain Matchbox Whoopee Band", "Guillermo del Toro", "Miracle", "\"Boston Herald\" Rumor Clinic", "Commissioner", "itty Hawk", "Scandinavian design", "Linda Maria Ronstadt", "\"Naked Killer\"", "the corner of North Avenue at Techwood Drive", "Michael Redgrave", "(Polish) \"Trzy kolory\"", "100 and 200 meters dash", "Scottish national team", "William Bradford", "200,167", "Boston, Providence", "Rose Theatre", "Columbine", "\"Pastime Paradise\"", "Sir William Collins", "G\u00e9rard Depardieu", "Lionsgate", "the 13th century", "his son Louis", "Zara Kate Bate", "Louisiana Tech University", "Clark County, Nevada", "around 8000 BC", "Rochdale", "Andrew Lloyd Webber and Tim Rice", "12", "1955", "8", "Henry Lau", "Terry the Tomboy", "Teenage Dream", "2015", "Federal Bureau of Prisons", "Mick Jackson", "Mayor Ed Lee", "two", "Matt Monro", "on the lateral side", "Jason Flemyng", "Elizabeth I", "Ghostbusters II", "Dennis Potter", "homicide", "its intention to set up headquarters in Dublin.", "arrested, arraigned and jailed,", "Bolivia", "\"to bring together\"", "Spain", "vomiting"], "metric_results": {"EM": 0.53125, "QA-F1": 0.7199872737556561}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 0.5714285714285715, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.20000000000000004, 0.0, 1.0, 1.0, 0.2857142857142857, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.28571428571428575, 0.0, 0.8, 0.8, 1.0, 1.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8413", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4226", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-648", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2344", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-5438", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-459", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-2770"], "SR": 0.53125, "CSR": 0.5593039772727273, "EFR": 0.9666666666666667, "Overall": 0.7126941287878787}, {"timecode": 88, "before_eval_results": {"predictions": ["Scottish rivers", "Heracles", "New Kids on the Block", "James A. Van Allen", "Duchess of Cornwall", "Sybil", "a scorpion", "Federal Reserve", "souvlaki", "Charles Dana Gibson", "Philip Seymour Hoffman", "ACTIVE", "chicken pox", "Japan", "chickens", "Saint Patrick", "Lebanon", "George Sand", "Over the hifls", "The Atchison Family", "the Holy Grail", "shalom", "the Cumberland Gap", "Belgium", "Hollaback", "Michigan Wolverines", "Poor Richard's Almanack", "Red", "Al Jolson", "violin", "Hestia", "\"We Have Met the Anime\"", "greatness", "Rand McNally", "Scrabble", "Martin Luther King III", "Duct tape", "Henry Cavendish", "confer", "the gap", "Bill & George Clinton", "Condoleezza Rice", "John Glenn", "The Bluest Eye", "Spinal Tap", "Janet Reno", "(Casey) Stengel", "Monticello", "Eric Clapton", "saguaro", "Louisiana", "Beijing", "Charlotte Hornets", "appellate courts", "Sweden", "Lucas McCain", "Auckland", "hunt", "1449", "35", "billboards with an image of the burning World Trade Center", "At least 14 bodies", "Fullerton, California,", "1936"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7831086601307189}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-862", "mrqa_searchqa-validation-3953", "mrqa_searchqa-validation-5433", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-13166", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-3306", "mrqa_searchqa-validation-12642", "mrqa_searchqa-validation-8571", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-10409", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10380", "mrqa_triviaqa-validation-6874", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-798"], "SR": 0.71875, "CSR": 0.5610955056179776, "EFR": 1.0, "Overall": 0.7197191011235955}, {"timecode": 89, "before_eval_results": {"predictions": ["white light", "Fame", "the prosciutto", "Lois Jourdan", "tuna", "John of Patmos", "bamboo", "the Stratosphere", "Harry Potter and the Goblet of Fire", "the Carolinas Gardener's Guide: Toby Bost: 0789172000796", "guru", "Mercury and Venus", "Exxon Corporation", "the sick", "Captain Hook", "magnesium", "Columbia", "chicken", "the Romanov dynasty", "Jeanette Rankin", "Dame Melba", "the 19th century", "Musee Grevin", "the Titanic", "George Harrison", "the foot", "Superbad", "Emily Post", "a drum", "Morris West", "New Zealand", "Europe and Asia", "Sacco and Vanzetti", "the Burning Bush", "Copenhagen", "Tennessee", "Mozart & Haydn", "a car", "Clinton", "the Ford Motor Company", "the inside", "academic", "the 44 elderly participants who could speak both Spanish and English", "the Homemaker", "a hood", "the Oakland Raiders", "the owl", "Julius Caesar", "the \"Caesar\"", "Elvis Presley", "Chicago", "the study of the interstellar medium ( ISM ) and giant molecular clouds ( GMC )", "Thomas Edison", "Terrell Suggs", "Benjamin Franklin", "James Christopher Bolam", "fluorine", "studied Arabic grammar", "University of California", "11 November 1918", "the Ku Klux Klan", "Sarah Siemionow", "a remote part of northwestern Montana", "the \"Father of Liberalism\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5739583333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8865", "mrqa_searchqa-validation-11947", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-10914", "mrqa_searchqa-validation-1068", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-12675", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-15046", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-10002", "mrqa_searchqa-validation-2698", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-8542", "mrqa_searchqa-validation-12099", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-8714", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-8388", "mrqa_searchqa-validation-16902", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-9117", "mrqa_searchqa-validation-16973", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-16659", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-3474", "mrqa_triviaqa-validation-1400", "mrqa_hotpotqa-validation-5637", "mrqa_newsqa-validation-1679"], "SR": 0.453125, "CSR": 0.5598958333333333, "EFR": 1.0, "Overall": 0.7194791666666667}, {"timecode": 90, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1768", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-10784", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12479", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-15583", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-162", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9562", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9599", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3898", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5520", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-855", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.806640625, "KG": 0.503125, "before_eval_results": {"predictions": ["nonviolent", "propellers", "Joseph", "Sideways", "clothes", "Thomas Wolfe", "Ford", "Snow White", "Missouri", "a pyramid", "Subway", "Tila Tequila", "protons and neutrons", "cells", "Bloomingdale\\'s", "John N. Mitchell", "Marie Osmond", "bumblebees", "loyal", "French", "canter", "Barney Miller", "the Constitution", "a bullseye", "Thomas Wolfe", "Lynette \"Squeaky\" Fromme", "the fourth Thursday", "robe", "Matthew", "Fenway Park", "Pennsylvania", "a bell", "Cardinal Richelieu", "Guinevere", "James Jeffords", "The Wachowski brothers", "a clef", "Brazil", "Montgomery", "Sindbad", "Dictates", "John Brown", "a piranha", "26.2 miles", "the pupils", "Amish TV", "Anthony Michael Hall", "Lord Baden-Powell", "Judo", "Don Knotts", "messenger", "Spencer Treat Clark", "beloved", "northwest Washington", "Charlie Harper", "\"Stairway to Heaven\"", "Sweden", "WB Television Network", "The Royal Navy", "Eric Edward Whitacre", "martial arts", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use as he arrived in Africa for his first visit to the continent as pope.", "Eleven", "Jackson Storm"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7364583333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-92", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8002", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-8352", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-10414", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-8459", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-5563", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-13537", "mrqa_searchqa-validation-13842", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-5235", "mrqa_naturalquestions-validation-10610", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3947", "mrqa_hotpotqa-validation-5559", "mrqa_newsqa-validation-1663"], "SR": 0.65625, "CSR": 0.5609546703296704, "EFR": 1.0, "Overall": 0.7249253090659341}, {"timecode": 91, "before_eval_results": {"predictions": ["1330 Avenue of the Americas", "25", "gossip Girl", "an angry mob.", "wars in Iraq and Afghanistan", "elections", "military trials", "Missouri", "A Colorado prosecutor", "hostile war zones,", "Authorities", "Zimbabwe", "Jacob, Ethan, Michael, Alexander, William, Joshua, Daniel, Jayden, Noah and Anthony.", "Illlinois.", "Blacks and Hispanics", "Mohammed Mohsen Zayed,", "Wednesday", "33-year-old", "3 to 17", "\"overcharged,\"", "Australian officials", "two", "\"I am sick of life -- what can I say to you?\"", "the equator", "police", "2004.", "Inter Milan", "Airbus A330-200", "her child was made to suffer as retribution.", "it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "The Washington Post", "hanged", "that a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U. S.-backed Iraqi forces and gunmen.", "her boyfriend,", "Argentine", "poems", "three empty vodka bottles,", "Harvard symbologist", "new cars", "george charlie chaplin, Cary Grant, Clark Gable, Gregory Peck, Carol Lombard and Hearst's mistress Marion Davies", "government soldiers and Taliban militants", "additional information", "federal officers' bodies", "sixth world title", "a violent government crackdown seeped out.", "Dr. Octopus", "a new model is simply out of their reach.", "costs $50 less, while the Nook has twice the storage space and a longer advertised battery life", "part", "dental work", "wings", "introverted Sensing ( Si )", "one season", "Lady Gaga", "the Quran", "enid", "Chile", "Club Deportivo Castell\u00f3n", "Bishop", "2000", "Chester", "suck face", "Billie Holiday", "st. Moritz"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5628234257749922}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.058823529411764705, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.08695652173913043, 1.0, 0.5, 0.9142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.35294117647058826, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-5972", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-223", "mrqa_triviaqa-validation-1532", "mrqa_hotpotqa-validation-1164", "mrqa_searchqa-validation-12610", "mrqa_searchqa-validation-10962", "mrqa_searchqa-validation-11844", "mrqa_triviaqa-validation-1028"], "SR": 0.453125, "CSR": 0.5597826086956521, "EFR": 0.9428571428571428, "Overall": 0.713262325310559}, {"timecode": 92, "before_eval_results": {"predictions": ["very influential", "Saturn", "a nuclear weapon", "150", "we seek a new way forward, based on mutual interest and mutual respect.", "Tim Clark, Matt Kuchar and Bubba Watson", "\"She was focused so much on learning that she didn't notice,\"", "\"utterly baseless.\"", "outlet mall", "Somalia's piracy problem was fueled by environmental and political events.", "safety issues in the company's cars", "innovative, exciting skyscrapers", "Kurdish militant group", "Two pages -- usually high school juniors who serve Congress as messengers", "Zetas and Gulf cartel", "Sunday", "strife in Somalia,", "put a lid on the marking of Ashura", "the Bronx", "Congress", "sculptures", "eight", "Kerstin Fritzl,", "Zimbabwe", "fled the region", "Stratfor", "potential revenues from oil and gas", "surrogate", "Mandi Hamlin", "\"The people kill him with the blocks,", "her mother", "in his 60s,", "to clean up Washington State's decommissioned Hanford nuclear site,", "Haiti", "four months ago,", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn, who was the executive producer of \"The Departed.\"", "it has not intercepted any", "acid attack", "Utah Valley Regional Medical Center", "he is committed to equality, citing the repeal of the military's \"don't ask, don't tell\" policy", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "22", "38", "South Africa", "iCloud service", "Don't Run", "Jared Polis", "eight", "CNN's \"Larry King Live,\" Fox and The Associated Press.", "May 2000,", "101", "Shalimar Gardens ( Lahore )", "boxing", "southeastern United States", "Scotland", "the Last Post", "Columbus", "Indian", "professional professor and writer", "\"Operation Julin\" series", "Kevin Sorbo", "the West Wing", "neurotransmitters", "the Carthaginians'Phoenician ancestry"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5800542492638081}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.9411764705882353, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.1, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.06666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4, 1.0, 0.0, 1.0, 0.13333333333333336]}}, "before_error_ids": ["mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-3928", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-561", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-9459", "mrqa_triviaqa-validation-2429", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-5647", "mrqa_searchqa-validation-16412", "mrqa_naturalquestions-validation-2042"], "SR": 0.515625, "CSR": 0.5593077956989247, "EFR": 0.967741935483871, "Overall": 0.7181443212365591}, {"timecode": 93, "before_eval_results": {"predictions": ["the Uighurs of the Kingdom of Qocho", "World Trade Center", "Olivia Olson in most appearances, by Ava Acres as a child and by Cloris Leachman as an old woman", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "England", "Michael Schumacher", "Sajjad Delafrooz", "one season", "Jonas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Sonya, Nick Choksi as Dolokhov, Shaina Taub as Mary", "Thespis", "eurozone", "Keith Timberwolvesodeaux", "2014 Winter Olympics in Sochi, Russia", "Dadra", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "the inner core", "Tatsumi", "Gaget, Gauthier & Co.", "six", "World War II", "at 11 : 40 p.m. ship's time", "Sheev Palpatine", "April 2001", "73", "the ulnar nerve", "the medulla oblongata", "Carole Landis, Dean Martin, and Ethel Merman", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Hellenismos", "three", "Spanish explorers", "statutory law", "Menorca", "20 - year period", "Lori McKenna", "infection, irritation, or allergies", "Eastern Ghats and the Bay of Bengal", "Mary Elizabeth ( Margaret Hoard )", "Nuevo Reino de Le\u00f3n", "International Orange", "in the duodenum", "October 6, 2017", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "the five states", "Pebe Sebert and Hugh Moffatt", "Arthur Chung", "semilunar pulmonary valve", "Lana Del Rey", "Mangal Pandey of the 34th BNI, angered by the recent actions of the East India Company", "October 3, 2013", "Charlton Heston", "faggot", "Parchman Farm", "SpongeBob", "the twelfth season of daytime version of \"Who Wants to Be a Millionaire\"", "848", "2004", "\"It was a wrong thing to say, something that we both acknowledge,\"", "jobs aid, family finances, competitiveness, infrastructure, and actions toward public spending that is more transparent and efficient.", "positive signal", "lizard", "Gone With the Wind", "Charles Chaplin", "the Papua New Guinea"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6034200850050792}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.42857142857142855, 0.21052631578947367, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.5, 0.2857142857142857, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9142857142857143, 0.11764705882352941, 1.0, 1.0, 0.18181818181818182, 1.0, 0.4761904761904762, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.625, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-10469", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-7457", "mrqa_hotpotqa-validation-455", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-3446", "mrqa_searchqa-validation-8023", "mrqa_searchqa-validation-16519"], "SR": 0.46875, "CSR": 0.558344414893617, "EFR": 0.9117647058823529, "Overall": 0.706756199155194}, {"timecode": 94, "before_eval_results": {"predictions": ["8", "country singer Mickey Gilley", "1952", "Long Island Rail Road", "The Keeping Hours", "John \"John\" Alexander Florence", "Brisbane", "1995 to 2012", "Los Angeles", "Cuban-American Major League Clubs", "1.23 million", "The interview", "Pan Am Railways", "102,984", "Lancashire", "321,520", "Mary Harron", "Newcastle upon Tyne, England", "Sunyani", "a reward for ability or finding an easy way out of an unpleasant situation by dishonest means", "University of Vienna", "1998", "capitol building", "Ericsson", "suburb", "William Bradford", "Tamaulipas", "highland regions of Scotland", "2015", "Richard Arthur", "Elizabeth", "Sheen Michaels Entertainment", "Nikita Sergeyevich Khrushchev", "Charlyn Marie \" Chan\" Marshall", "Europe", "Pound Puppies and the Legend of Big Paw", "November 8, 2016", "War Is the Answer", "drawings", "Philip K. Dick", "Ireland", "the junction with Interstate 95", "Cambridge", "Mineola", "Ministry of European Integration", "CBS", "The R-8 Human Rhythm Composer", "London", "North Carolina", "Steve Carell", "The entity", "Have I Told You Lately ''", "can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "1,350", "armada", "freedom of interaction that is necessary for the community to carry out its conversations", "cartoons", "Guam", "The Charlie Daniels Band", "a share in the royalties", "a macadamia", "Travertine", "Final Cut", "gentry"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6861607142857143}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.13333333333333333, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.10714285714285714, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-4404", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2488", "mrqa_hotpotqa-validation-285", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5791", "mrqa_naturalquestions-validation-4740", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-871", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-15161", "mrqa_triviaqa-validation-6256"], "SR": 0.546875, "CSR": 0.5582236842105264, "EFR": 0.9310344827586207, "Overall": 0.7105860083938295}, {"timecode": 95, "before_eval_results": {"predictions": ["YouTube", "soccer", "Rockbridge County", "Rawhide", "John Sullivan", "John Monash", "Ice Princess", "Antonio Lucio Vivaldi", "Terry Malloy", "Alfred Preis", "professional wrestler, actor, and hip hop musician", "2009", "Twitch Interactive, a subsidiary of Amazon.com", "(500) Days of Summer", "Accokeek, Maryland", "the self-immolation of a 19-year-old student named Romas Kalanta", "Cartoon Network Studios", "Love", "a parabolic reflector", "The Daily Stormer", "Keelung", "Adam Levine", "the Pennacook people", "Michael Crawford", "The Five", "The Lykan Hypersport", "Bundesliga", "The 133d Air Refueling Squadron (133 ARS)", "Honolulu", "from 1993 to 1996", "Hannaford", "John Lennon", "Big Fucking German", "Sophie Winkleman", "In 2017, Zaur Pachulia (Georgian: \u10d6\u10d0\u10d6) is a Georgian professional basketball player for the Golden State Warriors of the National Basketball Association (NBA)", "1954", "Tie Domi", "Harold Lipshitz", "American serial killer couple", "Target Corporation", "George A. Romero", "Nassau County Executive", "number five", "British racing driver", "26 September 1961", "British troops", "Spitsbergen in Svalbard, Norway", "1983", "Linux Format", "French Canadians (also referred to as Franco-Canadians or Canadiens; French: \"Canadien(ne)s fran\u00e7ais(es)\"", "Kew", "twice", "Meghalaya", "December 15, 2017", "le Fosche Notturne Spoglie", "Love Is All Around", "they are published at least once a week.", "12-1", "to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "\"This is not something that anybody can reasonably anticipate,\"", "parallel port", "apples", "the Triassic", "Greenland"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6821744227994228}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.09090909090909091, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.28571428571428575, 1.0, 0.0, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-5506", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-179", "mrqa_hotpotqa-validation-1886", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-3039", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-4407", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-11462"], "SR": 0.59375, "CSR": 0.55859375, "EFR": 1.0, "Overall": 0.7244531249999999}, {"timecode": 96, "before_eval_results": {"predictions": ["where", "won", "Thomas Jefferson", "Adam", "Jesus'birth", "Paul Rudd", "John F. Kennedy", "the end", "S", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Garfield Sobers", "stability, security, and predictability of British law and government", "Hal Derwin", "Madison, Wisconsin, United States", "Ireland", "at the hour of death or in the presence of the dying", "rum", "13 February", "specific brain regions", "Michael K. Williams", "Joseph M. Scriven", "Kiss", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "commercial at", "Pakistan", "Danny Veltri", "Jimmy Matthews", "Ole Einar Bj\u00f8rndalen", "Marie Fredriksson", "Amitabh Bachchan", "Twin Pines / Lone Pine Mall", "Del and Rodney", "Speaker of the House of Representatives", "538", "Robin", "March 2016", "1924", "Clarence Williams", "Siddharth Arora / Vibhav Roy", "Vinati Amar Pethawala", "4 January 2011", "U2", "Strother Martin ( as the Captain, a prison warden )", "an optional message body", "an optional message body", "Kida", "the following year", "remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "1961", "The person who has existence in two parallel worlds", "My Favorite Martian", "Perseus", "piscina", "Justin Adler", "Port Melbourne", "G\u00f6tene in Sweden", "The father of Haleigh Cummings, a Florida girl who disappeared in February,", "in Egypt.", "Stuart Gaffney,", "a cent", "an Liqueur", "Arthur Miller", "Renzo Piano"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6403519825268817}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.12500000000000003, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7741935483870968, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-3714", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-3184", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-260", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-9343"], "SR": 0.578125, "CSR": 0.5587951030927836, "EFR": 1.0, "Overall": 0.7244933956185567}, {"timecode": 97, "before_eval_results": {"predictions": ["23 November 1946", "Afghanistan", "C. J. Cherryh", "1938", "German", "1978", "Trey Parker and Matt Stone", "The Hawaii House of Representatives", "the late eighteenth century", "the \"Father of Liberalism\"", "Grave Digger", "1854", "Thriller", "various names", "March 23, 2017", "Vladimir Valentinovich Menshov", "Shery", "American", "The Colorado Rockies", "96", "the Bears", "EN World web site", "John Andr\u00e9", "Brian Keith Bosworth", "Lynwood", "Hawaii County, Hawaii", "Mel Blanc", "January 2016", "Philadelphia", "nine", "Finding Nemo", "1957", "House of Commons", "Terry the Tomboy", "1995 teen drama \"Kids\"", "Switzerland", "Polk County, Georgia", "Charles L. Clifford", "Taoiseach", "2018\u201319 UEFA Europa League group stage", "island of Spitsbergen", "Ronald Ryan", "2017", "10 October 2010", "Che Guevara", "Marigold Newey", "three", "Warsaw, Poland", "horror", "Virginia", "a region of Vietnam north of Hanoi that served as the Viet Minh's base of support during the First Indochina War (1946\u20131954)", "upon a military service member's retirement, separation, or discharge from active duty", "Strabo", "1922", "Bono", "a mosaic", "Indonesia", "\"procedure on her heart,\"", "British", "Briton Carl Froch", "Kosovo", "Oh Henry", "James Earl Ray", "le Carr\u00e9"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7974032971339007}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.2727272727272727, 0.6896551724137931, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2037", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-2442", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-840", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-1911", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-302"], "SR": 0.671875, "CSR": 0.5599489795918368, "EFR": 0.9047619047619048, "Overall": 0.7056765518707483}, {"timecode": 98, "before_eval_results": {"predictions": ["the plain of Marathon", "a clove hitch", "birds", "Samuel Rutherford", "Sheryl Crow", "Rich Girl", "circus wagons", "endive", "an obstetrician, or a doctor that specializes in", "Patricia Arquette", "One billion", "tuberculosis", "Judges", "Milan", "Abu Musab al-Zarqawi", "Sanford and Son", "repent", "New Mexico", "Henri Matisse", "1849", "Queensland", "Monarch", "Pluto", "Quisp Cereal", "1803", "The Prose Works of John Milton", "bloom", "a catalog", "Hydra", "The petroleum sector", "Tin lizzie", "Irish Coffee", "Rome", "According to Jim Crow Laws", "Jack Dempsey", "Vladivostok", "an earthquake", "Bizarro", "Burt Reynolds", "Sanction", "A 4.0 GPA", "New Kids on the Block", "Louis the German", "Cutpurse", "buttered cookie sheet", "Princeton", "Stephen Collins", "T.S Eliot", "San Francisco", "a kite", "State the Chemical Element", "Amartya Sen ( 1998, Economics )", "its genome", "1883", "drake", "(later Sir Arthur) Conan Doyle", "Newfoundland and Labrador", "Kentucky Wildcats", "MGM Resorts International", "North West England", "Sporting Lisbon", "Vicente Carrillo Leyva", "She knows who you are,", "Allison Janney"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6147073412698413}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14941", "mrqa_searchqa-validation-8747", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-12806", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-8848", "mrqa_searchqa-validation-2401", "mrqa_searchqa-validation-4343", "mrqa_searchqa-validation-14595", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-6005", "mrqa_searchqa-validation-1069", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-12932", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-9456", "mrqa_searchqa-validation-4943", "mrqa_searchqa-validation-5833", "mrqa_searchqa-validation-2890", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-3985", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-730", "mrqa_hotpotqa-validation-5799", "mrqa_newsqa-validation-2960"], "SR": 0.53125, "CSR": 0.5596590909090908, "EFR": 1.0, "Overall": 0.7246661931818181}, {"timecode": 99, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1768", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-184", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2657", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4274", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-4654", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-1069", "mrqa_searchqa-validation-10772", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-12806", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-15583", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-162", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2372", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-4228", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7362", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9562", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9599", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-2390", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3898", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-5373", "mrqa_squad-validation-5520", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1532", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3057", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3947", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.837890625, "KG": 0.50703125, "before_eval_results": {"predictions": ["Albert Square", "Steely Dan", "manganese", "Toronto", "Goat Island", "Mase", "Adam and Eve", "Walter", "Rome", "Leander Club", "Tripoli", "a window", "Sir Winston Churchill", "Tom Hanks", "Jamaica", "the Hooded Claw", "Strangeways", "Paul Gauguin", "Floor-length", "Tim Brooke Taylor", "gestapo", "Shania Twain", "o'Sullivan", "the Black Sea", "J. M. W. Turner", "an ear", "wagner", "Taco Bell", "Malaysia", "Ford Motor Company", "mark", "paper sales company", "Dilbert", "horizon", "Pablo Picasso", "David Bowie", "24", "Ibrox", "brill Giants", "the Cheshire Cat", "Kent", "East of Eden", "palladium", "Agincourt", "the Greek War of Independence", "Sam Houston", "significant achievement", "Rodgers & Hammerstein", "Dr John Sentamu", "a dice game", "green", "Jenny Slate", "an unknown recipient", "Nicole Quartermaine ( Leslie Charleson )", "February 9, 1994", "Pantone Matching System", "13 October 1958", "Revolutionary Armed Forces of Colombia,", "Sheikh Sharif Sheikh Ahmed", "allegations that a dorm parent mistreated students at the school.", "Thurgood Marshall", "Inigo Montoya", "King of the Hill", "prevent any contaminants in the sink from flowing into the potable water system by siphonage and is the least expensive form of backflow prevention"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6063632246376812}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.17391304347826084]}}, "before_error_ids": ["mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-5717", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-5961", "mrqa_triviaqa-validation-4339", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-2308", "mrqa_naturalquestions-validation-10691", "mrqa_hotpotqa-validation-4558", "mrqa_searchqa-validation-11101", "mrqa_naturalquestions-validation-5297"], "SR": 0.546875, "CSR": 0.55953125, "EFR": 1.0, "Overall": 0.72034375}]}