{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4520, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Ed Asner", "arrows", "1st century BC", "Marburg Colloquy", "Brookhaven", "ca. 2 million", "the Hungarians", "Mercury", "19th Century", "Art Deco style in painting and art", "The ability to make probabilistic decisions", "impact process effects", "1999", "phagosome", "the mass of the attracting body", "the Association of American Universities", "three", "allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "freight services", "up to four minutes", "the Little Horn", "Muslim and Chinese", "intracellular pathogenesis", "Santa Clara, California", "1784", "George Low", "Annual Conference Cabinet", "three", "Students", "Atlantic", "2001", "1887", "Chicago Bears", "John Harvard", "increase its bulk and decrease its density", "literacy and numeracy", "Christmas Eve", "the state", "Paris", "gender roles and customs", "outdated or only approproriate", "soy farmers", "United States", "Albert Einstein", "the number of social services that people can access wherever they move", "Tesco", "ABC Inc.", "1776", "wireless", "an electric current", "Warszowa", "the courts of member states", "supervisory church body", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church", "Manakin Episcopal Church", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Westminster", "Von Miller", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "Khwarezmia", "Queen Elizabeth II", "CBS", "Pittsburgh Steelers", "The chloroplast peripheral reticulum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8875363542546205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212122, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1826", "mrqa_squad-validation-4874", "mrqa_squad-validation-4283", "mrqa_squad-validation-1802", "mrqa_squad-validation-6210", "mrqa_squad-validation-3650", "mrqa_squad-validation-7430", "mrqa_squad-validation-6136"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["the Inland Empire", "New Zealand", "Jacksonville", "Newton's First Law", "the ability to pursue valued goals", "May 1888", "lecture theatre", "more than 28 days", "elliptical", "Boston", "Wednesdays", "Orange", "three", "Lampea", "San Jose State", "March 29, 1883", "between AD 0\u20131250", "Pleurobrachia", "eleven", "punts", "the Solim\u00f5es Basin", "1474", "Arizona Cardinals", "Julia Butterfly Hill", "Orange", "Doctor in Bible", "left Graz", "waldzither", "over $40 million", "14th century", "8.0", "the end of the 19th century", "peace", "$40,000", "the Cloth of St Gereon", "time and space", "7,000", "elementary particles", "indigenous", "about 3.5 billion", "New York City O&O WABC-TV and Philadelphia O&o WPVI-TV", "John Fox", "architectural", "Prime ideals", "Normant", "Leonardo da Vinci", "2003", "modern buildings as well as structures dating from the 15th\u201318th centuries", "Charles River", "KOA", "a disaster", "no contest", "Latin", "Manakin Town", "40,000", "After liberation", "\"winds up\" the debate", "10%", "The Daily Mail is mentioned in The Beatles\u2019 hit single Paperback Writer  The Yorkshire Post was the first British newspaper to report on The Abdication Crisis", "Uncle Tom's Cabin", "the liver", "No man", "Martina Hingis", "Ukraine does not have real established and ratified borders with Russia"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8738095238095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669]}}, "before_error_ids": ["mrqa_squad-validation-4458", "mrqa_squad-validation-2798", "mrqa_squad-validation-1775", "mrqa_squad-validation-7552", "mrqa_squad-validation-1001", "mrqa_squad-validation-696", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-7750", "mrqa_naturalquestions-validation-646"], "SR": 0.84375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 2, "before_eval_results": {"predictions": ["$155 million", "CBS", "San Jose State", "Half", "the evolution of the German language and literature", "the Qur'an", "the Brotherhood", "high demand", "Tolui (1190\u20131232)", "human law", "the object's weight", "50%", "1960s", "two months", "his friends", "1805", "Elders", "30\u201375%", "45,000 pounds", "self molecules", "lege-Khitays (Khitans)", "1960", "Captain America: Civil War", "political divisions", "linear", "Alta California", "Book of Common Prayer", "14", "Charleston, South Carolina", "fear of their lives", "hot winds", "intracellular pathogenesis", "Safari Rally", "10,005,721", "Philip Segal", "legon-complete knapsack problem", "1965", "how or whether this connection is relevant on microscales", "German Te Deum", "Stanford, California", "Jin", "Trevathan", "Doctor Who", "1206", "clinical services", "CRISPR sequences", "Queen Elizabeth II", "zero", "1992", "food", "plasmas", "their low ratio of organic matter to salt and water", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "brown", "guardian", "guardian", "black", "50 feet"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5884943181818182}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7408", "mrqa_squad-validation-6099", "mrqa_squad-validation-6641", "mrqa_squad-validation-4405", "mrqa_squad-validation-2547", "mrqa_squad-validation-8360", "mrqa_squad-validation-2577", "mrqa_squad-validation-8747", "mrqa_squad-validation-2809", "mrqa_squad-validation-5893", "mrqa_squad-validation-2906", "mrqa_squad-validation-9015", "mrqa_squad-validation-1860", "mrqa_squad-validation-10427", "mrqa_squad-validation-164", "mrqa_squad-validation-6178", "mrqa_squad-validation-6405", "mrqa_squad-validation-8402", "mrqa_squad-validation-1435", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11930", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-12889", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3857"], "SR": 0.515625, "CSR": 0.7447916666666667, "EFR": 1.0, "Overall": 0.8723958333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["10 employees", "1624", "Hangzhou", "in committee", "19th century", "1962", "dealing with patients' prescriptions and patient safety issues", "a group that included priests, religious leaders, and case workers as well as teachers", "Vistula River", "1290", "21 October 1512", "427,652 people", "a double membrane", "August 1967", "German", "a single-step, 5-cylinder engine (no compound) with superheated steam", "four", "50 fund", "Arizona Cardinals", "Peanuts", "\"ctenes\" or \"comb plates\"", "the steroid hormone calcitriol", "Krak\u00f3w", "time", "since at least the mid-14th century", "the mitochondrial double membrane", "Mike Figgis", "in an adult plant's apical meristems", "isopentenyl pyrophosphate synthesis", "Associating forces with vectors", "Prime ideals", "The Three Doctors", "Malik Jackson", "four", "the Koori", "1910\u20131940", "pressure swing adsorption", "Johann Tetzel", "English", "a lack of remorse", "the fundamental means by which forces are emitted and absorbed", "A1 (Gateshead Newcastle Western Bypass", "Sun Life Stadium", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "John Houghton", "February 2015", "draftsman", "a tiny snail", "Orestes", "Galapagos Islands", "the U.S. government", "a gas (like water vapor) changes state to become a liquid", "the Forty-Davidson Museum", "a cocoa favorite", "a paint, ink or a plastic item represents a major raw", "the Mycenaean civilization", "a circadian clock", "the Belasco Theatre", "before", "a bank robber", "fibula", "The Anvil Chorus", "the South Pole", "the East Coast Main Line"], "metric_results": {"EM": 0.625, "QA-F1": 0.6654040404040404}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6937", "mrqa_squad-validation-6345", "mrqa_squad-validation-2192", "mrqa_squad-validation-4724", "mrqa_squad-validation-3347", "mrqa_squad-validation-3673", "mrqa_squad-validation-6737", "mrqa_squad-validation-10310", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-8509", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-10694", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-7003"], "SR": 0.625, "CSR": 0.71484375, "EFR": 0.9583333333333334, "Overall": 0.8365885416666667}, {"timecode": 4, "before_eval_results": {"predictions": ["Germany and Austria", "Centrum", "blue", "to spearhead the regeneration of the North-East", "R\u0113nos", "gambling", "the wing of the secular powers", "applied mathematics", "Zhongtong", "11.1%", "1538", "Deacons", "New Testament", "experience, ideology, and weapons", "25", "October 2016", "Torchwood", "capturing prey", "c4", "livestock pasture", "Victorian", "1,300,000", "passing between carbon dioxide and oxygen", "20,000", "eight", "computational problem", "WZZM and WOTV", "Orange", "tentilla", "gender roles and customs", "social unrest and violence", "Woodward", "1745", "Battle of Olustee", "observer", "50-yard line", "3D printing technology", "The Malkin Athletic Center", "24\u201310", "6.7+", "empires", "the domestic legislation of the Scottish Parliament", "sentence", "Michael Bloomberg", "Environmental Defense Fund", "innovative, exciting skyscrapers", "a cancerous tumor", "the war years", "the computer processing unit (CPU) market", "Matt Kuchar and Bubba Watson", "fastest circumnavigation of the globe", "the man facing up, with his arms out to the side", "the BBC's central London offices", "buddhism", "Manchester", "Noriko Savoie", "three", "change course", "Tsvangirai", "Wicked", "January 2", "honey", "Chelsea Lately", "Luxembourg"], "metric_results": {"EM": 0.625, "QA-F1": 0.7062003968253969}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2, 0.0, 0.8333333333333333, 0.4, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7606", "mrqa_squad-validation-9250", "mrqa_squad-validation-2982", "mrqa_squad-validation-7626", "mrqa_squad-validation-8874", "mrqa_squad-validation-4258", "mrqa_squad-validation-2984", "mrqa_squad-validation-8832", "mrqa_squad-validation-4572", "mrqa_squad-validation-7094", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-862", "mrqa_naturalquestions-validation-47", "mrqa_hotpotqa-validation-547"], "SR": 0.625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["a cadre of influential loyalists", "Doritos", "4000", "$37.6 billion", "Anglo-Saxons", "seven", "the Golden Gate Bridge", "Southwest Fresno", "divergent boundaries", "a double membrane", "QuickBooks", "surface condensers", "clinical pharmacists", "a seal", "Philip Howard", "Duke Richard II", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "three", "French", "gold-trimmed logos", "the constitutional traditions common to the member states", "a placebo effect", "Huguenots", "10\u20137", "Polish Academy of Sciences", "spherical", "Nurses", "New England Patriots", "Time", "Class II MHC", "two", "George Westinghouse", "by disrupting their plasma membrane", "reciprocating (piston) steam engines", "a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei)", "gurus, mullahs, rabbis, pastors/youth pastors and lamas", "B cells", "a substantial number of deaths, injuries, and structural collapses", "at the Sweden-based photojournalism agency Kontinent, for which the two men work,", "a British teenage girl in Goa", "How I Met Your Mother", "France's famous Louvre museum", "Leo Frank", "the capital city", "Graziano Transmissioni", "opposition leaders", "204,000", "United's", "the release of the four men", "putting a personal and human face on the issue", "Ed McMahon", "at least $20 million to $30 million", "the Democratic VP candidate", "Friday", "Ali Larijani", "ballots", "Sodra nongovernmental organization", "sodium dichromate", "promotes fuel economy and safety", "a resting heart rate over 100 beats per minute", "heavy breeds", "Denmark", "Cincinnati", "Rossif Sutherland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5830233134920635}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.28571428571428575, 0.0, 0.25, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9691", "mrqa_squad-validation-3087", "mrqa_squad-validation-353", "mrqa_squad-validation-3610", "mrqa_squad-validation-3075", "mrqa_squad-validation-827", "mrqa_squad-validation-6644", "mrqa_squad-validation-3263", "mrqa_squad-validation-10444", "mrqa_squad-validation-1852", "mrqa_squad-validation-2531", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-3935", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-4043", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-4171", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-2465"], "SR": 0.53125, "CSR": 0.6692708333333333, "EFR": 1.0, "Overall": 0.8346354166666666}, {"timecode": 6, "before_eval_results": {"predictions": ["18 February 1546", "11", "it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefit.", "the University of Chicago Press", "$2 million", "2015", "1762", "biased against Genghis Khan", "Warsaw Stock Exchange", "to increase the chloroplast's surface area for cross-membrane transport", "a computational resource", "to denote unknown or unexplored territory", "Nicholas Stone", "early Lutheran hymnals", "a straight line", "the autumn of 1991", "William Smith", "William Pitt", "KREEP", "Theory of the Earth to the Royal Society of Edinburgh", "Japan", "the Tuesday afternoon prior to the game", "the Working Group chairs", "laws of physics", "Denver's Executive Vice President of Football Operations and General Manager", "noisiest", "independent of each other", "issues under their jurisdiction", "unsuccessful", "a human", "a community drugstore", "a modified version of the sieve that considers 1 as prime would eliminate all multiples of 1", "nerves", "1700", "the Reichstag", "aldehydes and ketones", "DNC", "the heart, blood & blood vessels", "troggs", "six literal ordinary days", "Slovakia", "Diana the Princess", "the illegal slave trade.", "vena cava", "Virginia's", "bull's Eye", "Tartarus", "hor\u017fe", "Yul Brynner", "the Persian Achaemenid Empire", "the daily grind", "the German Count Ferdinand von Zeppelin", "San Francisco", "George IV", "aeolian process", "Edward R. Egan", "hor\u017fe", "the survivors of Oceanic Flight 815", "a comic book series by Robert Kirkman, Tony Moore, and Charlie Adlard", "Love Is All Around", "\"Everybody Comes to Rick's\"", "the International Maritime Bureau", "a form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,\"", "18"], "metric_results": {"EM": 0.5, "QA-F1": 0.5545296717171717}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11111111111111112, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6773", "mrqa_squad-validation-6230", "mrqa_squad-validation-5588", "mrqa_squad-validation-5054", "mrqa_squad-validation-464", "mrqa_squad-validation-383", "mrqa_squad-validation-10398", "mrqa_squad-validation-6337", "mrqa_squad-validation-9064", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4830", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-8993", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-1637", "mrqa_searchqa-validation-12770", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-10057", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1064"], "SR": 0.5, "CSR": 0.6450892857142857, "EFR": 1.0, "Overall": 0.8225446428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["extinction of the dinosaurs", "oxygen", "reduce growth", "K-9 and Company", "9.1 million", "U.S. economy consistently affords a lower level of economic mobility than all the continental European countries", "School corporal punishment", "cattle", "Mongol", "a maze of semantical problems and grammatical niceties", "five", "the \"gold standard\" of religion in minds of some or many Muslims.", "British", "Finsteraarhorn", "Abilene", "white", "The Sierra Freeway", "Thanksgiving", "Jacksonville", "Two thirds", "the Privy Council", "the nineteenth century", "the human development approach", "Daily Mail", "San Mateo", "Spanish", "300,000", "cryptomonads", "The Swahili", "hymn-writer", "starch", "Bryant", "Earth", "Pulitzer", "Rodeo", "hog", "Barack Obama", "Kenny G", "Demitasse", "a peacock", "Annapolis", "spring", "krukhit", "bark", "Allah", "Humans", "Pythons", "Cecil B. DeMille", "Nicole Kidman", "Faith Hill", "Ben Affleck", "Sandy", "the \"unconquerable will of the occupied territories\"", "particle", "Yardbird", "Kiruna", "Indonesia", "94", "Alexandria", "Perfume", "New Jersey Economic Development Authority", "Georgetown", "Essex Eagles", "Alzheimer's"], "metric_results": {"EM": 0.515625, "QA-F1": 0.559375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7399", "mrqa_squad-validation-6220", "mrqa_squad-validation-9588", "mrqa_squad-validation-4562", "mrqa_squad-validation-7473", "mrqa_squad-validation-8189", "mrqa_squad-validation-3072", "mrqa_squad-validation-7565", "mrqa_searchqa-validation-47", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-7048", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-8990", "mrqa_searchqa-validation-4050", "mrqa_searchqa-validation-15487", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10916", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-5178", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-12687", "mrqa_naturalquestions-validation-10073", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3468"], "SR": 0.515625, "CSR": 0.62890625, "EFR": 1.0, "Overall": 0.814453125}, {"timecode": 8, "before_eval_results": {"predictions": ["shocked", "lymphocytes", "producers", "BSkyB", "Kawann Short", "Daidu", "silent", "22", "the park", "1965", "tidal currents", "combustion", "Ma Jianlong", "Demaryius Thomas", "Lake Constance", "the main opposition party", "Ireland", "Red", "around 1700", "ITT", "1966", "masses", "Linebacker", "high art and folk music", "four", "with six series of theses", "Body of Proof", "seven-eighths", "Richelieu", "the Atlas Mountains", "Madrid", "the Danube", "eagles", "leather", "George Pullman", "red", "the Messiah", "Sappho", "possession", "the tonka bean", "the divisor", "Nyx", "Texas", "the Rooty Tooty Fresh 'N Fruity", "alzheimer", "Bill of Rights", "SAT", "Brasilia", "Henry David Thoreau", "alzheimer", "Dick Cheney", "3.9", "Gustave Eiffel", "Edward Hopper", "Central Intelligence Agency", "d'Artagnan", "the Rotunda", "1985", "alzheimer", "its air-cushioned sole", "13", "Fort Worth", "Agent 99", "private"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6056857638888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7803", "mrqa_squad-validation-3478", "mrqa_squad-validation-8421", "mrqa_squad-validation-866", "mrqa_squad-validation-9837", "mrqa_squad-validation-2474", "mrqa_squad-validation-5926", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-15182", "mrqa_searchqa-validation-523", "mrqa_searchqa-validation-15584", "mrqa_searchqa-validation-9386", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-10315", "mrqa_searchqa-validation-14478", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-9179", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4461"], "SR": 0.546875, "CSR": 0.6197916666666667, "EFR": 1.0, "Overall": 0.8098958333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["\"Provisional Registration\"", "August 15, 1971", "Levi's Stadium", "the main international treaty on climate change", "Inflammation", "Brown v. Board of Education of Topeka", "15 May 1525", "The Walt Disney Company", "Dundee", "61", "Second World War", "the integer factorization problem", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence", "exploration is still continuing to determine if there are more reserves", "prep schools", "its soft power", "a strong Islamist outlook", "lengthening rubbing surfaces of the valve", "$32 billion", "the most characteristic musical instrument in the region", "the Dutch Republic", "Alex Haley", "three", "the helmeted honeyeater", "4:51", "Khrushchev", "Hera", "the plague", "Ohky Chateau", "Cuba", "the Battle of Thermopylae", "the Khazars", "Kroc", "cricket", "white", "Washington State", "Carmen", "Genoa", "12", "tarn", "the novel", "the bison", "Anne Widdecombe", "the Equilateral triangle", "the Old Kent Road", "Tuesday", "the hydrates", "Ab Fab", "Massachusetts", "Cumbria", "California", "the Preamac River", "Kajagoogoo", "a berry", "Singapore", "Wigan", "Davos", "eight", "the \"eternal outsider, the sardonic drifter,\" someone who rebels against the social structure", "the Home Rule Party", "Janet Napolitano", "a powerful brand", "Oh, Oh,", "Bea Benaderet"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5886351495726496}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8784", "mrqa_squad-validation-9552", "mrqa_squad-validation-8273", "mrqa_squad-validation-9870", "mrqa_squad-validation-5376", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-5332", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2533", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-4737", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-1203", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2672", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1553", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-7509"], "SR": 0.515625, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.8046875}, {"timecode": 10, "before_eval_results": {"predictions": ["tenggis", "environmental determinism", "4 August 2010", "King George III", "radio", "the 'Lord's Enclosure' (Mongolian: Edsen Khoroo)", "League of Augsburg", "Duarte Barbosa", "the Inner Mongolia region", "Roman Catholic", "Amazonia: Man and Culture in a Counterfeit Paradise", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "Sydney", "five", "January 18, 1974", "Spanish", "NFL", "extremely difficult", "student populations", "Catholic", "United Kingdom", "296", "ghent", "a mulberry", "vitis", "yuri", "Ken Russell", "Dan Dare", "yuri", "Smiths", "Mike Tyson", "yuri", "Passover", "yuri", "a\u00e7aleidoscope", "Uranus", "a\u00e7ollon", "yuri Carlin", "crimea", "Sydney", "Los Angeles", "the Underground Railroad", "Robin Goodfellow", "blue", "a pomegranate", "Portugal", "football", "yuri Carey", "63 to 144 inches", "Titanic", "The Marriage Contract", "Christian Dior", "snail", "Mendip", "Wichita", "the Last Supper", "New Croton Reservoir", "andr\u00e9 3000", "Democritus", "Stephen King", "Venus Williams", "firefighter", "yuri", "yuri"], "metric_results": {"EM": 0.546875, "QA-F1": 0.599222132034632}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6811", "mrqa_squad-validation-9391", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-5319", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-5874", "mrqa_triviaqa-validation-4926", "mrqa_triviaqa-validation-4884", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-2265", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-7138", "mrqa_hotpotqa-validation-2340", "mrqa_newsqa-validation-2710", "mrqa_searchqa-validation-3397", "mrqa_searchqa-validation-8589"], "SR": 0.546875, "CSR": 0.6036931818181819, "EFR": 1.0, "Overall": 0.8018465909090909}, {"timecode": 11, "before_eval_results": {"predictions": ["homebound", "salvation", "jugs", "eurhamphaea vexilligera", "zaju", "uninterested in administration", "Chivas USA", "Edinburgh", "The Pink Triangle", "the dot", "Magdalen Tower", "a large public network", "public service", "Guy de Lusignan", "a \"tiger team\"", "t cells", "The European Commission", "the completed (or local) fields", "a force is required to maintain motion, even at a constant velocity", "Mongol and Turkic", "hez-bah-lah", "three", "puck", "morocco", "juliusi, Volterra, Cortona, Arezzo, Fiesole", "black light", "purple", "Pluto", "chromium", "purple", "Hague", "Vancouver Island", "puck", "george smiley", "nizhny Novgorod", "brown trout", "Beyonce", "Wordsworth", "morocco", "New Zealand", "Samuel Johnson", "Conrad Murray", "Mary Poppins", "brian Cerf", "purple leaf", "puck", "the Volga-Don canal", "Snow White and the Seven Dwarfs", "morocco", "lions", "purple flowers", "julma Kelly", "morocco", "Shanghai", "julie Forbush", "car", "Snake River Valley", "17 October 2006", "beer", "Hoover Dam", "Saturday's Hungarian Grand Prix", "\"The Ass of the Capuchins\"", "Edgar Allan Poe", "blue and twos"], "metric_results": {"EM": 0.375, "QA-F1": 0.4429315476190476}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6338", "mrqa_squad-validation-5460", "mrqa_squad-validation-4510", "mrqa_squad-validation-8135", "mrqa_squad-validation-4870", "mrqa_squad-validation-6530", "mrqa_squad-validation-10338", "mrqa_squad-validation-8226", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-3550", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-2204", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-3846", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6189", "mrqa_triviaqa-validation-6760", "mrqa_triviaqa-validation-3023", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-890", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-8473"], "SR": 0.375, "CSR": 0.5846354166666667, "EFR": 1.0, "Overall": 0.7923177083333334}, {"timecode": 12, "before_eval_results": {"predictions": ["a gift from God", "Greenland", "1724 to 1725", "he broadened the foundations of the Reformation placing them on prophetic faith", "1.25 million", "720p high definition", "five", "Maria Goeppert-Mayer", "the International Association of Methodist-related Schools, Colleges, and Universities", "one", "a majority in Parliament", "President Mahmoud Ahmadine", "Newcastle Eagles", "cholera", "other senior pharmacy technicians", "relative units of force and mass", "AD 14", "orogenic wedges", "Daniel Boone", "The Handmaid's Tale", "the pygmy chimpanzee", "The Fault in Our Stars", "car", "a puzzle video game", "1898", "the reservoir of the Bui Dam", "Total Nonstop Action Wrestling", "galt\u00fcr", "Archbishop of Canterbury", "1861", "the Walt Disney World Resort in Lake Buena Vista, Florida", "David Villa", "Red and Assiniboine Rivers", "Alpine, New Jersey", "the Continental Army", "Robert Norton Noyce", "Ryan Guno", "Umar S. Israilov", "Corey Scott", "1933", "What's Up", "the Baudot code", "1959", "1887", "The Governor of Minnesota", "Marvel Comics", "The Weeknd", "Nick Cassavetes", "Lamar Hunt", "Sarah Winnemucca", "Jean Baptiste Point", "England", "Queen Catherine Parr", "a basilica", "1994", "Ricky Nelson", "in Wakanda and the Savage Land", "mercury", "gymnophobia", "opium", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "andrew johnson", "a spider", "in pre-Columbian times"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5968141233766233}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.0, 0.04761904761904762, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2249", "mrqa_squad-validation-4958", "mrqa_squad-validation-5889", "mrqa_squad-validation-9658", "mrqa_squad-validation-10428", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-2706", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-383", "mrqa_naturalquestions-validation-6015", "mrqa_triviaqa-validation-2685", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-7025", "mrqa_naturalquestions-validation-8227"], "SR": 0.53125, "CSR": 0.5805288461538461, "EFR": 1.0, "Overall": 0.7902644230769231}, {"timecode": 13, "before_eval_results": {"predictions": ["\u00a341,004", "Salamanca", "Jochi", "lower lake", "Gospi\u0107, Austrian Empire", "since 2001", "impossible", "Southwest Fresno", "5,000", "Greek mathematician", "ABC News Now", "demolished by Boldt to make the land a more viable real estate asset", "\u00c9mile Girardeau", "Brownlee", "The Five Doctors", "The average fee is around \u20ac5,000 annually for most schools", "the Gulf South Conference (GSC)", "Earl Sinclair", "psycho-physical awareness", "Las Vegas", "Ranulf de Gernon", "2017", "downtown Dallas", "\"Love at First Sting\"", "Shrek", "Lucille Ball", "\"Grimjack\" (from First Comics)", "16\u201321", "Vince Guaraldi", "Jack Boyd", "Michael Redgrave", "6th", "Highlands Course", "Hawaii", "a moth of the Gracillariidae family", "Marquis de Lafayette", "Bharatiya Janta Party (BJP)", "three", "Winter Haven Mall", "four", "\"The Process\"", "Mindy Kaling", "Surrey", "Claudio L\u00f3pez", "\"Uptown Funk\"", "FCI Danbury", "a few", "the \"Home of the Submarine Force\"", "Las Vegas", "Pope John X", "2013", "Anna Mae Aquash", "The Rwandan genocide", "Larnelle Harris", "Commander in Chief of the United States Armed Forces", "2009", "1982", "rod", "Chris Robinson", "\"CBTs\"", "shock waves", "\"I don't know who Bob Beamon is\"", "the Egyptian Goddess of Creation", "Richie Unterberger"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5458333333333334}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3176", "mrqa_squad-validation-6126", "mrqa_squad-validation-3287", "mrqa_squad-validation-1488", "mrqa_squad-validation-7792", "mrqa_squad-validation-7036", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3556", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-3990", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-16181", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-469"], "SR": 0.46875, "CSR": 0.5725446428571428, "EFR": 1.0, "Overall": 0.7862723214285714}, {"timecode": 14, "before_eval_results": {"predictions": ["10,000", "quarter of the object", "half of all Americans combined", "December 1963", "1970", "religious freedom in the Polish\u2013Lithuanian Commonwealth", "it then travelled along the Silk Road, reaching Crimea by 1343", "a kilogram-force", "ten times their own weight", "Quaternary", "1887", "ctenophores", "a symbiotic relationship", "quantifying the amount of resources needed to solve them", "Vistula", "quarter", "a handful of tweaks to Apple's iconic music-Player line", "a dorm parent mistreated students", "March 8", "Democrats and Republicans", "the Catholic League", "1,000 pounds", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "Friday", "Ameneh Bahrami", "stories of different women coping with breast cancer in five vignettes", "there were problems with the well and he should move his ship away", "a smile", "a quarter of bread", "Lance Cpl. Maria Lauterbach", "Hyundai Steel", "London", "more than 4,000 commercial farmers", "Val d'Isere, France", "a chaplain", "a municipal building in Baghdad's Sadr City,", "$14.1 million", "1616", "Saturn", "Chile", "some of the most gigantic pumpkins in the world", "Buddhism", "J.Crew", "U.S. Secretary of State Hillary Clinton", "\"I sort of had a fascination with John Dillinger", "boyhood experience in a World War II internment camp", "suppress the memories and to live as normal a life as possible", "about 4 meters (13 feet)", "the Irish capital", "Republican", "Utah County sheriff's spokesman Lt. Dennis Harris", "Allred", "Islamabad", "quarter a.m.", "March 26, 1973", "Indian Ocean", "quarter", "throw", "Sevens", "England", "Yemen", "el Greco", "dreams", "silver"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4049175650738151}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.3, 1.0, 0.0, 0.7777777777777778, 0.05, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 0.0, 0.26666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10400", "mrqa_squad-validation-7456", "mrqa_squad-validation-7770", "mrqa_squad-validation-4856", "mrqa_squad-validation-10458", "mrqa_squad-validation-1672", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-3798", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-1329", "mrqa_naturalquestions-validation-6733", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-7587", "mrqa_triviaqa-validation-3839", "mrqa_triviaqa-validation-740"], "SR": 0.3125, "CSR": 0.5552083333333333, "EFR": 0.9772727272727273, "Overall": 0.7662405303030303}, {"timecode": 15, "before_eval_results": {"predictions": ["Prospect Park", "Khanbaliq", "Quaternary", "1870", "Mercury", "prime", "50 fund", "the Camisards", "over $40 million through sponsors", "GTE", "1,100", "spin", "Oligocene,", "Melodie Rydalch,", "Darwin", "Little Rock military recruiting center", "March 24,", "the Beatles", "Jesus Christ", "Adriano", "Two of the dead were police officers.", "2007", "an unprecedented wave of buying amid the elections.", "\"She had a smile on her face, like she always does when she comes in here,\"", "19,600", "National Football League", "\"The Lost Symbol\"", "one of its diplomats", "vitamin injections that promise to improve health and beauty.", "two soldiers", "Atlanta", "resources", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "two Emmys", "\"I have a very, very good family that the American people believe in you, support you and are 100 percent behind you, and we thank God every day that you have our back.", "\"Three Little Beers,\"", "Rwanda", "75.", "to kill members of the Zetas cartel from the state of Veracruz,", "closing these racial gaps", "a bond hearing Friday,", "President Bush", "Amstetten,", "African National Congress Deputy President Kgalema Motlanthe,", "\"It's hard for everyone... I thought it was better for me here,\"", "\"He lost his bid to Francisco de Narvaez, who leads a rival Peronist party, Union PRO, by a tally of 34.6 percent to 32.1 percent.", "\"I've always been fascinated by the political process ever since I was a kid.", "a strict interpretation of the law,", "saying Chaudhary's death was warning to management.", "Prime Minister Benjamin Netanyahu", "20% tax credit", "July 23.", "70,000", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.", "Robert Remak", "Tim McGraw", "Prussian", "cabbage", "game", "Beno\u00eet Jacquot", "blue", "John George Stewart", "The Left Book Club", "holography"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48280836640211633}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444444, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.057142857142857134, 0.0, 0.29629629629629634, 0.0, 0.0, 1.0, 1.0, 0.1111111111111111, 0.2962962962962963, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3293", "mrqa_squad-validation-9016", "mrqa_squad-validation-427", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2727", "mrqa_naturalquestions-validation-7158", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-1335", "mrqa_triviaqa-validation-6296"], "SR": 0.421875, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.7734375}, {"timecode": 16, "before_eval_results": {"predictions": ["two thousand", "address information", "the high risk of a conflict of interest and/or the avoidance of absolute powers.", "to look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system.", "Thames River", "British East Africa (as the Protectorate was generally known) and German East Africa", "several hundred thousand, some 30% of the city \u2013 herded into the Warsaw Ghetto.", "the Tower District", "Ted Ginn Jr.", "Catch Me Who Can", "Gary Kubiak", "housing bubble", "a total of 183 people, including 137 children, have been taken away since law enforcement officers raided the compound Thursday night,", "Adam Lambert and Kris Allen,", "Brian Smith", "\"Hillbilly Handfishin'\"", "Charles Lock", "voluntary manslaughter", "his enjoyment of sex and how he lost his virginity at age 14.", "his injuries,", "in 1979", "the armed robbery and kidnapping of another victim,", "next year", "to overhaul domestic policies,", "Noriko Savoie", "Anil Kapoor", "Afghanistan and India", "Dr. Albert Reiter,", "\"The Significance of the Iranian Threat,\"", "to step up attacks against innocent civilians", "Matthew Fisher", "cancer,", "Courtney Love,", "us to step up.", "\"Walk -- Don't Run\" and \"Haw Hawaii Five-O\"", "your own environmental videos", "two women", "Queen Elizabeth's", "Sunday and two others throughout the night, rescue crews were not able to find the pilot or the five passengers from the plane,", "male veterans", "Yusuf Saad Kamel", "Hokeriet,", "11 healthy eggs", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Fullerton, California,", "1918-1919.", "in the 1950s,", "at least four of them.", "Brazil has saved $1 billion alone by producing its own generic versions of HIV/AIDS medicines and negotiating discounts for imported drugs.", "vegan bake sales", "\"The Rosie Show,\"", "Dodi Fayed", "Oxbow,", "gastrocnemius", "Ed Sheeran", "\"clock\"", "Australia", "three-part", "2001", "parishes", "U.S.", "George Blake", "Bogota"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5759102876290376}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.9743589743589743, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 1.0, 0.27272727272727276, 0.22222222222222224, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.08333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.17142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-8570", "mrqa_squad-validation-914", "mrqa_squad-validation-626", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-837", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-2957", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-4647", "mrqa_searchqa-validation-10239", "mrqa_triviaqa-validation-6739"], "SR": 0.46875, "CSR": 0.5422794117647058, "EFR": 1.0, "Overall": 0.7711397058823529}, {"timecode": 17, "before_eval_results": {"predictions": ["low levels of development have relatively equal distributions of wealth", "a pharmacy practice residency and sometimes followed by another residency in a specific area.", "a method of imparting the basics of Christianity to the congregations", "wakes (sed vigilat) and experiences visions", "the \"Brompton Boilers\"", "12 January 1943,", "60,000", "Zagreus", "CBS", "17", "a temperate zone.", "Rod Blagojevich", "Mad Men", "St. Louis, Missouri.", "$50 less,", "Afghanistan's", "found his qualifications mean little as a refugee.", "collaborating with the Colombian government,", "Iran", "concerns about the missile defense system.", "Herman Cain", "Matthew Fisher,", "Peppermint oil, soluble fiber, and antispasmodic drugs can indeed help people with irritable bowel syndrome,", "in the north and west of the country,", "injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "introduce legislation Thursday to improve the military's suicide-prevention programs.", "$250,000", "first or second week in April.", "Derek Mears", "braving elements ranging from rain to wind and even one speeding ticket (which she says she talked her way out of).", "Gary Player", "Hamburg", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "fastest wind-powered boat on the planet is rapidly gaining momentum as speeds reach all-time highs.", "Virgin America", "Wellington, Florida,", "Daniel Wozniak", "22-year-old", "Osama bin Laden", "how health care can affect families.", "NATO", "U.S. Food and Drug Administration", "Casa de Campo International Airport", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.", "2002", "at checkposts and military camps in the Mohmand agency,", "its forces killed nine gunmen.", "Friday,", "\"Here Comes the Sun.", "crocodile eggs", "from Geraldine Ferraro to Bill Clinton.", "hid his money,", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "senators", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "Sky & Telescope", "Arlene Phillips", "23 July 1989", "Ry\u016bkyuan", "surrealism", "C.S. Lewis", "a number multiplied by itself will result in the", "sake", "Nova Scotia"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5113563149311988}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false], "QA-F1": [0.6666666666666666, 0.4, 0.2105263157894737, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.125, 1.0, 0.12500000000000003, 0.0, 1.0, 0.0, 0.3529411764705882, 0.45454545454545453, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.4444444444444444, 0.0, 1.0, 0.7692307692307693, 0.15384615384615388, 0.2857142857142857, 0.0, 0.20000000000000004, 1.0, 0.0, 1.0, 0.0, 0.0, 0.1, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7341", "mrqa_squad-validation-6359", "mrqa_squad-validation-2338", "mrqa_squad-validation-2408", "mrqa_squad-validation-5326", "mrqa_squad-validation-1570", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-1858", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-157", "mrqa_naturalquestions-validation-132", "mrqa_triviaqa-validation-1659", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-4857"], "SR": 0.40625, "CSR": 0.5347222222222222, "EFR": 1.0, "Overall": 0.7673611111111112}, {"timecode": 18, "before_eval_results": {"predictions": ["melatonin", "constant factors and smaller terms", "Shi Bingzhi", "Baron Dieskau", "linear", "Advanced Steam", "Defensive ends", "\"the dot\"", "chastity", "European Court of Justice", "a bronze medal in the women's figure skating final,", "that U.S. Ambassador to Zimbabwe James McGee would be expelled from the country if he \"persisted in meddling in Zimbabwe's electoral process,\"", "UK", "\"Gandhi,\"", "Argentina", "Congress", "28", "New Haven, Connecticut, firefighter Frank Ricci", "Iraq's autonomous region of Kurdish Regional Government", "\"terrifying.\"", "Bill & Melinda Gates Foundation", "$106.5 million", "because everybody around me likes Obama,\"", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "\"political and religious\"", "$163 million (180 million Swiss francs)", "Afghan lawmakers", "Saudi Arabia", "\"All of our stations are overcrowded,\"", "mammoth's fossil", "\"global security, prosperity and freedom.\"", "a disgraceful anti-immigration and pro-racial-profiling law,", "Molotov cocktails, rocks and glass.", "opening 101 new jobs to British workers,", "Ben Roethlisberger", "Dr. Christina Romete", "Ewan McGregor", "Brazil", "Meira Kumar", "next week.", "Hong Kong", "Vonn,", "in an interview Tuesday on CNN's \"Larry King Live,\"", "organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "\"Racism and racist conversations have no place today in America.", "Brazil", "a big shopping center", "trying to detonate an explosive device in his underwear aboard a Christmas 2009 flight to Detroit,", "two people", "40-year-old", "Pakistan's North West Frontier Province", "Casey Anthony,", "\"Let it Roll:", "Emma Watson and Dan Stevens", "2002", "a leak", "\"Taxman,\"", "Che Guevara", "Miller Brewing", "Elizabeth Tudor", "John Fogerty", "Garonne", "the giraffe", "cheese"], "metric_results": {"EM": 0.453125, "QA-F1": 0.528532586970087}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.12121212121212123, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.2857142857142857, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.2666666666666667, 0.19047619047619047, 1.0, 0.0, 0.16, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2702702702702703, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-2847", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9104", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-254", "mrqa_searchqa-validation-11385"], "SR": 0.453125, "CSR": 0.5304276315789473, "EFR": 0.9714285714285714, "Overall": 0.7509281015037594}, {"timecode": 19, "before_eval_results": {"predictions": ["1876", "1507", "safety Darian Stewart", "11", "killed through overwork", "Japanese", "Muqali,", "2011 and 2012,", "the Pittsburgh Steelers", "apartment building", "Aung San Suu Kyi", "Chuck Bass", "3rd District of Utah.", "that suggested returning combat veterans could be recruited by right-wing extremist groups.", "Stephen Johns", "three", "procedures", "an acid attack by a spurned suitor.", "those who managed to survive the incident hid in a boiler room and storage closets", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Courtney Love,", "33-year-old", "cell phones", "a book", "\"He's as healthy as he can be -- no health problems whatsoever,\"", "stand down.", "Ashley \"A.J.\" Jewell,", "at least 17", "home in Satsuma, Florida,", "to the southern city of Naples", "Hugo Chavez", "London's", "rural up in rural California,", "was killed in an attempted car-jacking as he dropped his children off at a relative's house,", "Old Trafford", "the area of the 11th century Preah Vihear temple", "a floating National Historic Landmark,", "design and produce the Haeftling range.", "home in the Pacific Ocean territory of Guam within striking distance,", "homicide", "The Ski Train", "Aniston, Demi Moore and Alicia Keys", "Robert De Niro", "intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "CNN's Campbell Brown", "$60 billion on America's infrastructure.", "a month of training to get used to wearing the shoes", "Roberto Micheletti,", "U.S. President-elect Barack Obama", "Burhanuddin Rabbani,", "that Clarkson  was depressed over a recent breakup, grabbed the gun and  took her own life.", "\"We wondered how can we protect our dogs' feet against glass,\"", "stone", "around 2.45 billion years ago", "Cambridge", "Colorado", "Bangor International Airport", "\"Grandmasters\"", "Suffragist", "Canterbury", "Tunisia", "Silver", "Bonnie and Clyde"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6241002185056945}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.8, 0.0, 1.0, 0.5714285714285715, 0.0, 0.10526315789473685, 0.07692307692307693, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.888888888888889, 1.0, 1.0, 0.5714285714285715, 0.1, 0.0, 0.0, 0.0, 0.0, 0.631578947368421, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.5714285714285715, 0.1818181818181818, 0.0, 1.0, 1.0, 0.35294117647058826, 0.15384615384615385, 0.0, 0.5, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-816", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-413", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-6758", "mrqa_hotpotqa-validation-2782"], "SR": 0.484375, "CSR": 0.528125, "EFR": 1.0, "Overall": 0.7640625}, {"timecode": 20, "before_eval_results": {"predictions": ["the Hostmen", "Greg Brady", "Norman Huguenots", "Hungarians", "De Materia Medica (Concerning medical substances)", "John D. Rockefeller", "seven professional schools", "sentence", "Beijing, China,", "Virgil Tibbs", "Thaddeus Rowe Luckinbill", "up to 100,000", "the United States, its NATO allies and others", "Virginia Dare", "JackScanlon", "Can't Get You Out of My Head", "94 by 50", "Lalo Schifrin", "MGM Resorts International", "16 August 1975", "seawater pearls", "1962", "Buddhist", "1978", "1927, 1934, 1938, 1956 ) and four since the advent of the Super Bowl ( Super Bowls XXI ( 1986 ), XXV ( 1990 ), XLII ( 2007 ),", "1969", "the eventual Super Bowl champion New England Patriots", "Joseph Heller", "90 \u00b0 N 0 \u00b0 W", "1,350", "Leonard Bernstein", "25 September 2007", "Wolfgang Hochstetter", "Yale University", "62", "the team", "November 2014", "Archduke Franz Ferdinand of Austria", "a central place in Christian eschatology", "October 1941", "peace between two entities ( especially between man and God or between two countries ), or to the well - being, welfare or safety of an individual or a group of individuals", "Shalimar Gardens", "Cee - Lo", "one of his kidnapper that he has a girlfriend named Abigail and that he wants to call her to say goodbye before they kill him", "the additives common to a complete tomato sauce and does not have the thickness of paste", "Conservative Party", "three times", "November 25, 2002,", "January 1, 2016", "Peter Greene", "31 March 1909", "Ed Sheeran", "two", "Alberto Salazar", "live animals", "American", "Hoosick,", "an engineering and construction company with a vast personal fortune.", "1.2 million", "Paul Galdone", "Robert Louis Stevenson", "Samoa", "Mugabe and China", "the remote area about 75 miles east of Yakima"], "metric_results": {"EM": 0.5, "QA-F1": 0.6298343794841639}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.6, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.25, 1.0, 0.5945945945945945, 0.0, 0.0, 0.0, 0.896551724137931, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3086", "mrqa_squad-validation-6314", "mrqa_squad-validation-8027", "mrqa_squad-validation-7929", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-5330", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-3886", "mrqa_hotpotqa-validation-2298", "mrqa_newsqa-validation-3687", "mrqa_searchqa-validation-13486", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-2446"], "SR": 0.5, "CSR": 0.5267857142857143, "EFR": 0.96875, "Overall": 0.7477678571428572}, {"timecode": 21, "before_eval_results": {"predictions": ["66 million years ago", "Spanish", "an attack on New France's capital, Quebec", "Fresno Traction Company", "Parliament of the United Kingdom", "blue-green algae", "four", "Sauron", "the Washington metropolitan area", "the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "the breast or lower chest of beef or veal", "Sargon II", "Tagalog or English", "around 1600 BC", "From 1976 to 1983", "Michael Phelps", "Rajendra Prasad", "Ren\u00e9 Georges Hermann - Paul", "Donna", "Keith Hernandez and Willie Stargell", "Orangeville, Ontario, Canada", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Janie Crawford,", "in the 2nd century", "in the pancreas", "Kanawha Rivers", "1961", "iOS, watchOS, and tvOS", "rocks and minerals", "Michael Schumacher", "the foreign exchange option", "1957", "1776", "1963", "President Friedrich Ebert", "2018", "Ireland", "Kit Harington", "Scarlett Johansson", "Transvaginal ultrasonography", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Baker, California, USA", "Sonepat, Panipat, Tilpat ( near Faridabad )", "1979", "Guy Berryman", "Tessa Virtue and Scott Moir", "Sophocles", "Tim Allen", "thick skin", "the middle Jaffa Cakes", "India", "Brazil, Turkey and Uzbekistan", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "cricket", "the Major General of the Navy", "Clayton Mark's", "14,000", "Iran could be secretly working on a nuclear weapon", "Honduras", "Pardon of Richard Nixon", "Ellen DeGeneres", "12 April 1961", "punk rock", "Westfield Old Orchard"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5633720395209365}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.30769230769230765, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.8, 0.0, 0.6666666666666666, 0.0, 0.7692307692307693, 1.0, 0.0, 0.0, 0.5, 1.0, 0.4, 0.7777777777777778, 1.0, 0.0, 0.0, 0.6666666666666666, 0.08333333333333333, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9392", "mrqa_squad-validation-2387", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-4641", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-1675", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-3883", "mrqa_hotpotqa-validation-427", "mrqa_hotpotqa-validation-3984"], "SR": 0.40625, "CSR": 0.5213068181818181, "EFR": 1.0, "Overall": 0.7606534090909091}, {"timecode": 22, "before_eval_results": {"predictions": ["at the mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania)", "Stanford University", "Stewart", "the Mongol and Turkic tribes", "1859", "Danny Lane", "John 6 : 67 -- 71", "new wave rock band The Fixx", "Andrew Johnson", "Hellenic polytheism", "Mark Jackson", "Manhattan Island", "British rock band", "In 2015", "breasts and genitals", "week 4", "L.K. Advani", "the House of the United States Congress", "Zachary John Quinto", "Sukhvinder Singh, Mahalaxmi Iyer", "Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton", "a single particle", "John Adams", "Lituya Bay in Alaska", "Manhattan, the Bronx, Queens", "Burbank, California", "Ricardo Chavira", "2014", "Yuzuru Hanyu", "Glenn Close", "Elk", "flawed democracy", "China", "Kirk Douglas", "Masha Skorobogatov", "on February 27, 2007", "Neil Patrick Harris", "8ft", "Owen Vaccaro", "food", "on the lateral side of the tibia", "Steve Trevor Jr.", "erosion", "90 \u00b0 N 0 \u00b0 W \ufeff / \ufffdrous 90 \u00b0N - 0 \u00b0 E", "London to Canterbury", "in the bloodstream or surrounding tissue", "2017", "February 28 or March 1", "1840s", "9 t ( 31.82 ft )", "Montgomery", "if he was not Romeo, then he would not be a Montague and she would be able to get married with no problem at all", "Hotel California", "Queen Elizabeth II", "\u00ef\u00bf\u00bd", "the American rock band Pearl Jam", "March 28, 1970", "Dan Tyminski", "British singer Robbie Williams", "in the southern port city of Karachi,", "at least nine", "Bashar al-Assad", "the Christian Bible", "ski and shoot"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4856171180184338}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.9473684210526316, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 0.14814814814814814, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.0, 1.0, 0.5, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8148148148148148, 0.5, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10228", "mrqa_squad-validation-807", "mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2079", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-7486", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-1046", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-3470", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-2101", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1458", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1295", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-4138", "mrqa_searchqa-validation-2112"], "SR": 0.359375, "CSR": 0.5142663043478262, "EFR": 0.9512195121951219, "Overall": 0.732742908271474}, {"timecode": 23, "before_eval_results": {"predictions": ["internal strife", "a new stage in the architectural history of the regions they subdued", "Fresno", "its many castles and vineyards", "below 0 \u00b0C", "Von Miller", "Kansas", "Thaddeus Rowe Luckinbill", "December 25", "2002", "The Mandate of Heaven", "Geoffrey Zakarian", "Christopher Allen Lloyd", "during prenatal development", "gaius caesar", "Tanvi Shah", "Article 1, Section 2", "the Constitution of India came into effect on 26 January 1950", "Richard Bremmer", "Dick Rutan and Jeana Yeager", "in sequence with each heartbeat", "Ren\u00e9 Descartes", "James P. Flynn", "detritus", "September 27, 2017", "Johnny Logan", "1978", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Tony Orlando", "on February 10, 2017", "Alex Skuby", "Virginia", "March 2016", "from 1922 to 1991", "Tony Curran", "Bacon", "in an explosion", "Heather Stebbins", "Redenbacher family", "two amino acids", "10 national", "April 1, 2016", "Friedman Billings Ramsey", "New York City", "a milling cutter", "Massachusetts Compromise", "Justin Timberlake", "Andrew Moray", "Alamodome and city of San Antonio", "asexually", "Anthony Caruso", "In 1871 A.D. Pt. Buddhiballav Pant", "eye", "The History Boys", "gaius caesar", "the White Knights of the Ku Klux Klan", "five", "Mot\u00f6rhead", "Kingman Regional Medical Center,", "Phillip A. Myers", "middle-class suburb about an hour outside Cairo, Egypt.", "Antarctica", "spinal cord", "gaius caesar"], "metric_results": {"EM": 0.5, "QA-F1": 0.6335689484126983}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.6666666666666666, 0.28571428571428575, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4444444444444445, 0.2857142857142857, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1129", "mrqa_squad-validation-8990", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-1207", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-3651", "mrqa_newsqa-validation-505", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-12624"], "SR": 0.5, "CSR": 0.513671875, "EFR": 0.96875, "Overall": 0.7412109375}, {"timecode": 24, "before_eval_results": {"predictions": ["circus", "no", "Stadtholder William III of Orange", "1933\u20131953", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Jim Thorpe", "the first hole of a sudden-death playoff with Kentucky native Kenny Perry", "the onset and progression of Alzheimer's disease", "Disco", "\"Regno di Dalmazia\"", "Anil Kumar", "New York", "Charles Whitman", "C. H. Greenblatt", "\"The Thing\"", "the A541 Mold-Denbigh road", "Corendon Airlines", "86 ft", "Minneapolis", "Loch Duich", "Fatih Ozmen", "U.S.", "Pacific Place", "political commentator", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "Westminster, London", "2016", "Steve Cuden", "New York", "Malcolm Mays", "\"Harper's Bazaar\"", "dementia", "on the Po di Volano, a branch channel of the main stream of the Po River, located 5 km north", "Guadalcanal Campaign", "Bishop's Stortford", "Starvation Is Motivation", "Barbara Niven", "Black Friday", "Archbishop of Canterbury", "Verizon Wireless Arena", "Vic Chesnutt", "the high priest Bruteno", "Australian", "Julie Taymor", "Easy", "World War I", "79 AD", "Musicology", "Portland, OR", "the Yoruba", "\"Lucky\"", "Charles Otto Puth Jr.", "2007", "2001", "1966", "Jeremy Clarkson", "Ryan Harris", "gaius caesar", "Joe Jackson", "a car claiming 280 miles per gallon", "2004", "genes", "Olive", "Stockholm"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5288776327838828}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.08333333333333334, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.30769230769230765, 0.6666666666666666, 0.4, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5531", "mrqa_squad-validation-7092", "mrqa_squad-validation-2153", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-4105", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1849", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-431", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-2659", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-5855", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2930"], "SR": 0.453125, "CSR": 0.51125, "EFR": 0.9714285714285714, "Overall": 0.7413392857142858}, {"timecode": 25, "UKR": 0.59765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-1123", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-4562", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3206", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-3654", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-11385", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11900", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12624", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-1335", "mrqa_searchqa-validation-13486", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14883", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-16181", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3783", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-4857", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8589", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-9756", "mrqa_squad-validation-10045", "mrqa_squad-validation-10069", "mrqa_squad-validation-10074", "mrqa_squad-validation-10086", "mrqa_squad-validation-10216", "mrqa_squad-validation-10228", "mrqa_squad-validation-10254", "mrqa_squad-validation-10310", "mrqa_squad-validation-10324", "mrqa_squad-validation-10338", "mrqa_squad-validation-10353", "mrqa_squad-validation-1036", "mrqa_squad-validation-10378", "mrqa_squad-validation-10477", "mrqa_squad-validation-1090", "mrqa_squad-validation-1320", "mrqa_squad-validation-1450", "mrqa_squad-validation-1603", "mrqa_squad-validation-1636", "mrqa_squad-validation-1672", "mrqa_squad-validation-1694", "mrqa_squad-validation-178", "mrqa_squad-validation-1802", "mrqa_squad-validation-1852", "mrqa_squad-validation-1855", "mrqa_squad-validation-1857", "mrqa_squad-validation-1938", "mrqa_squad-validation-1967", "mrqa_squad-validation-2040", "mrqa_squad-validation-2126", "mrqa_squad-validation-2153", "mrqa_squad-validation-2216", "mrqa_squad-validation-2289", "mrqa_squad-validation-2384", "mrqa_squad-validation-2400", "mrqa_squad-validation-2436", "mrqa_squad-validation-2460", "mrqa_squad-validation-2477", "mrqa_squad-validation-255", "mrqa_squad-validation-2577", "mrqa_squad-validation-2602", "mrqa_squad-validation-2619", "mrqa_squad-validation-268", "mrqa_squad-validation-2693", "mrqa_squad-validation-2773", "mrqa_squad-validation-2782", "mrqa_squad-validation-2798", "mrqa_squad-validation-282", "mrqa_squad-validation-2824", "mrqa_squad-validation-285", "mrqa_squad-validation-2929", "mrqa_squad-validation-3019", "mrqa_squad-validation-3041", "mrqa_squad-validation-3135", "mrqa_squad-validation-3185", "mrqa_squad-validation-320", "mrqa_squad-validation-3337", "mrqa_squad-validation-3476", "mrqa_squad-validation-353", "mrqa_squad-validation-3589", "mrqa_squad-validation-3709", "mrqa_squad-validation-383", "mrqa_squad-validation-3931", "mrqa_squad-validation-3948", "mrqa_squad-validation-3955", "mrqa_squad-validation-397", "mrqa_squad-validation-3993", "mrqa_squad-validation-4005", "mrqa_squad-validation-4079", "mrqa_squad-validation-4140", "mrqa_squad-validation-415", "mrqa_squad-validation-4181", "mrqa_squad-validation-427", "mrqa_squad-validation-4291", "mrqa_squad-validation-4305", "mrqa_squad-validation-4333", "mrqa_squad-validation-4338", "mrqa_squad-validation-4472", "mrqa_squad-validation-462", "mrqa_squad-validation-4686", "mrqa_squad-validation-4704", "mrqa_squad-validation-4835", "mrqa_squad-validation-4856", "mrqa_squad-validation-4870", "mrqa_squad-validation-5054", "mrqa_squad-validation-5088", "mrqa_squad-validation-5096", "mrqa_squad-validation-5154", "mrqa_squad-validation-5176", "mrqa_squad-validation-5238", "mrqa_squad-validation-5302", "mrqa_squad-validation-5326", "mrqa_squad-validation-5376", "mrqa_squad-validation-550", "mrqa_squad-validation-5537", "mrqa_squad-validation-5541", "mrqa_squad-validation-5588", "mrqa_squad-validation-5616", "mrqa_squad-validation-5672", "mrqa_squad-validation-5703", "mrqa_squad-validation-5767", "mrqa_squad-validation-5777", "mrqa_squad-validation-5913", "mrqa_squad-validation-60", "mrqa_squad-validation-60", "mrqa_squad-validation-607", "mrqa_squad-validation-6099", "mrqa_squad-validation-6126", "mrqa_squad-validation-6143", "mrqa_squad-validation-6178", "mrqa_squad-validation-6220", "mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6362", "mrqa_squad-validation-6395", "mrqa_squad-validation-6414", "mrqa_squad-validation-6564", "mrqa_squad-validation-660", "mrqa_squad-validation-6641", "mrqa_squad-validation-6737", "mrqa_squad-validation-6754", "mrqa_squad-validation-6782", "mrqa_squad-validation-68", "mrqa_squad-validation-6817", "mrqa_squad-validation-6915", "mrqa_squad-validation-696", "mrqa_squad-validation-7018", "mrqa_squad-validation-703", "mrqa_squad-validation-7069", "mrqa_squad-validation-707", "mrqa_squad-validation-7150", "mrqa_squad-validation-7161", "mrqa_squad-validation-7180", "mrqa_squad-validation-7198", "mrqa_squad-validation-7260", "mrqa_squad-validation-7399", "mrqa_squad-validation-754", "mrqa_squad-validation-7552", "mrqa_squad-validation-7597", "mrqa_squad-validation-7640", "mrqa_squad-validation-765", "mrqa_squad-validation-7678", "mrqa_squad-validation-7770", "mrqa_squad-validation-7782", "mrqa_squad-validation-7814", "mrqa_squad-validation-7856", "mrqa_squad-validation-7882", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-804", "mrqa_squad-validation-8056", "mrqa_squad-validation-8104", "mrqa_squad-validation-8115", "mrqa_squad-validation-8189", "mrqa_squad-validation-8226", "mrqa_squad-validation-8226", "mrqa_squad-validation-8285", "mrqa_squad-validation-8406", "mrqa_squad-validation-8480", "mrqa_squad-validation-8527", "mrqa_squad-validation-8629", "mrqa_squad-validation-8735", "mrqa_squad-validation-8760", "mrqa_squad-validation-8765", "mrqa_squad-validation-8832", "mrqa_squad-validation-884", "mrqa_squad-validation-8867", "mrqa_squad-validation-890", "mrqa_squad-validation-8957", "mrqa_squad-validation-898", "mrqa_squad-validation-9031", "mrqa_squad-validation-9066", "mrqa_squad-validation-9135", "mrqa_squad-validation-9186", "mrqa_squad-validation-9227", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9391", "mrqa_squad-validation-9392", "mrqa_squad-validation-9465", "mrqa_squad-validation-9504", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9652", "mrqa_squad-validation-9658", "mrqa_squad-validation-9771", "mrqa_squad-validation-979", "mrqa_squad-validation-9818", "mrqa_squad-validation-987", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-3051", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-469", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5568", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-6909", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-776"], "OKR": 0.74609375, "KG": 0.296875, "before_eval_results": {"predictions": ["A progressive tax", "Jacksonville", "monophyletic", "Orthogonal components", "Fox Network", "the Anhaltisches Theater in Dessau", "Anna Clyne", "Terence Winter, based on the memoir of the same name by Jordan Belfort", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Formula E", "Eastern College Athletic Conference", "Kim Jong-hyun", "Peter Chelsom", "The Ninth Gate", "heavy metal", "The Slipper and the Rose", "Los Angeles", "Sharyn McCrumb", "acid house", "in Austria, south Germany, German Switzerland, and Slovenia at the end of the 18th century", "Capture of the Five Boroughs", "Miranda Leigh Lambert", "Shenandoah National Park", "Briton Allan McNish", "10 Years", "Haleiwa Ali'i Beach Park", "Armin Meiwes", "1886", "Rockhill Furnace, Pennsylvania", "northeastern part", "non-alcoholic", "Entrepreneur Media Inc.,", "the lead roles of Timmy Sanders and Jack", "PBS stations nationwide,", "second largest", "Citric acid", "in 1911", "michael andrew atherton", "Modest Petrovich Mussorgsky", "Walt Disney Feature Animation", "The 2017\u201318 Premier League", "torpedo boats", "1972", "Geographical Indication tag", "Ringo Starr", "the Celtics", "World Championship Wrestling", "1994", "TD Garden", "the Chechen Republic", "Chrysler", "Harvard University", "the EPs Sounds of the Season", "Tenochtitlan", "The Tax Reform Act of 1986", "michael andrew atherton", "the Mexican government", "The Cartoon Network", "he fears a desperate country with a potential power vacuum that could lash out.", "the Catholic League", "his 19-year-old son Krishna Rajaram", "michael andrew atherton", "to earn the nickname Super Eli", "purple"], "metric_results": {"EM": 0.4375, "QA-F1": 0.525123278478058}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.47058823529411764, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4444444444444445, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4165", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-2056", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4737", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-4298", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9487", "mrqa_triviaqa-validation-7575", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-453", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-364", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4240", "mrqa_searchqa-validation-1374"], "SR": 0.4375, "CSR": 0.5084134615384616, "EFR": 0.9722222222222222, "Overall": 0.6242521367521368}, {"timecode": 26, "before_eval_results": {"predictions": ["bacteriophage T4", "1698", "Xingu", "Ruhr", "Nairobi, Kenya", "Heinkel Flugzeugwerke", "Victor Garber", "Pope John X", "Stanmore, New South Wales", "aged between 11 or 13 and 18", "\"Histoires ou contes du temps pass\u00e9\"", "Orchard Central", "Anthony", "late eighteenth century", "\"The Snowman\"", "1979", "Premier League club Liverpool and the England national team", "port city of Aden", "British", "second cousin", "2008", "Archie", "after", "17 December 177026 March 1827", "Crystal Dynamics", "Cleveland Cavaliers", "goalkeeper", "Debbie Harry", "\"media for the 65.8 million,\"", "John Travolta", "Hall & Oates", "port of Mazatl\u00e1n", "striker", "Las Vegas, Nevada", "1919", "Kevin Spacey", "\"Love Streams\"", "Michael Edwards", "\"The Rite of Spring\"", "Lake Wallace", "England", "1993", "Boston Celtics", "New Executive Office Building", "6,396", "Australian coast", "The Saturdays", "Attack the Block", "Leonarda Cianciulli", "stadium", "German-dominated", "Pantone Matching System (PMS)", "1600 BC", "Ewan McGregor", "1996", "a peplos", "Zeebrugge", "mexico", "more than 15,000", "10 to 15 percent", "\"It has never been the policy of this president or this administration to torture.\"", "Tarzan", "mexico", "mexico"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6191477793040293}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-1352", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-1677", "mrqa_searchqa-validation-3515", "mrqa_searchqa-validation-13669"], "SR": 0.515625, "CSR": 0.5086805555555556, "EFR": 1.0, "Overall": 0.6298611111111111}, {"timecode": 27, "before_eval_results": {"predictions": ["immediately north of Canaveral at Merritt Island", "pedagogic diversity", "Catholic", "Extension", "Cinderella", "Dan Tyminski", "Guthred", "October 17, 2017", "Indian state of West Bengal", "Dennison", "Boeing EA-18G Growler", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Paper", "Sir Matthew Alistair Grant", "Whitney Houston", "Dunlop Tyres", "Bonkyll Castle", "Cheick Isma\u00ebl Tiot\u00e9", "Algernod Lanier Washington", "\"40 Days and 40 Nights\"", "due to a leg injury", "Antonio Salieri", "American", "Europe", "What You Will", "Brooklyn, New York", "Thriller (genre)", "Jesper Myrfors", "The Supremes", "Cersei Westerister", "Kalokuokamaile", "Antonesische Onafhankelijkheidsoorlog", "Don Bluth", "1970", "Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Hong Kong Disneyland", "London", "I Should Have Known Better", "September 8, 2017", "FBI", "Christine MacIntyre", "1995", "Wildhorn, Bricusse and Cuden", "M1914 machine gun", "David J. O'Connell", "Prussian statesman", "January 2004", "co-founder and lead guitarist", "\"Go Fund Yourself\"", "7", "October 25, 1881", "J. Robert Oppenheimer", "Pradyumna", "Mark Jackson", "R / T", "corsets", "Equatorial Guinea", "c3H8O3", "Victor Mejia Munera", "Chris Robinson and girlfriend Allison Bridges", "off east", "bromide", "septum", "zhail them on"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5728983918128655}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.4210526315789474, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3840", "mrqa_squad-validation-1916", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3916", "mrqa_hotpotqa-validation-4924", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3346", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-3400", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-2957", "mrqa_naturalquestions-validation-4039", "mrqa_triviaqa-validation-2051", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-6398"], "SR": 0.484375, "CSR": 0.5078125, "EFR": 1.0, "Overall": 0.6296875}, {"timecode": 28, "before_eval_results": {"predictions": ["public (government) funding", "boarding schools", "coordinated, evolving projects", "hanover", "Plas Johnson", "rhodes", "Gordon Ramsay", "Gorbachev", "stripes", "private, privately, in confidence, confidentially, behind closed doors, surreptitiously,", "Rameses II", "Anna (Julia Roberts)", "colossus of rhodtt", "colossus colossus", "Paddy Dooley", "rhodvirus", "colossus of rhodes", "the Central African Republic", "colossus rhodes", "colossus", "Khomeini", "Babley, Richard", "wry, compassionate, and brimm[ing] with... open-minded intelligence", "colomte de Winter", "April", "Eric Morley", "colopril", "The Garrick Club", "Belle", "Jimmy Greaves", "Manhattan", "Marc Norman", "The Greatest", "colossus", "off the coast of Northumberland", "pepper", "jazz pianist", "Seattle", "colossus", "Cardiff", "Louisiana", "colossus", "Tahrir Square", "Romanian", "bathtub curve", "colossus michael andrew atherton", "Snooty", "Borodin", "colossus of rhodes", "colossus", "Greek", "purple granadilla", "Thomas Lennon", "Haikou on the Hainan Island", "Leviathan", "Denmark and Norway", "1966", "alpine species", "Florida's Everglades", "Garth Brooks", "glamorous, sexy and international.", "How I Learned to Drive", "Glengarry Glen Ross", "shark"], "metric_results": {"EM": 0.3125, "QA-F1": 0.40451388888888884}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.33333333333333337, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6891", "mrqa_squad-validation-6918", "mrqa_squad-validation-4846", "mrqa_triviaqa-validation-256", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-6165", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-4619", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-6096", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-1045", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-3586", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-7182", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-9024", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-2506", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1004", "mrqa_searchqa-validation-8934"], "SR": 0.3125, "CSR": 0.5010775862068966, "EFR": 1.0, "Overall": 0.6283405172413794}, {"timecode": 29, "before_eval_results": {"predictions": ["Chicago Theological Seminary", "CBS", "$100,000", "Super Bowl LII", "starch", "Taylor Michel Momsen", "the Kennedy Space Center ( KSC ) in Florida", "Bob Pettit and Kobe Bryant", "James W. Marshall", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "Randy VanWarmer", "Commander in Chief of the United States Armed Forces", "Emma Watson and Dan Stevens", "between 8.7 % and 9.1 %", "2018", "if the occurrence of one does not affect the probability of occurrence of the other", "Jason Flemyng", "Chesapeake Bay, south of Annapolis in Maryland", "northern China", "T.J. Miller", "in Pyeongchang County, Gangwon Province, South Korea", "the status line", "in the eye", "maurice mary", "Triple Alliance of Germany", "Andrew Lloyd Webber", "1955", "Nick Sager", "Buffalo Lookout", "Humpty Dumpty", "Charlene Holt", "1 US dollar", "Leonard Nimoy", "1960", "Bumblebee", "10.5 %", "beneath the liver", "Andy Serkis", "West Norse sailors", "Kristy Swanson", "Christy Plunkett", "early to mid-2000s", "Fleetwood Mac", "technological advances in printing", "Cairo, Illinois", "in the 1970s and'80s", "in the Hebrew Bible", "Chandler ( George Strait )", "Psychomachia", "January 2, 1971", "The Miracles", "taxonomy", "muscles", "babe", "charlian heston", "Love Child", "Nineteen Eighty-Four", "Bardot", "27,", "Long Island convenience store", "Mitt Romney", "rock", "stomach", "bronchitis"], "metric_results": {"EM": 0.5, "QA-F1": 0.5761236707371576}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5764", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-6865", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-3968", "mrqa_hotpotqa-validation-4870", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-1979", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7426", "mrqa_searchqa-validation-647"], "SR": 0.5, "CSR": 0.5010416666666666, "EFR": 1.0, "Overall": 0.6283333333333333}, {"timecode": 30, "before_eval_results": {"predictions": ["San Jose", "number of quality rental units", "sports tourism", "ellipse", "corey", "charlie sheen", "Spanish Republic", "taximeter", "jaguar", "charlie sheen", "Harry Reid", "charlie sheen", "axis", "forge", "The Edison Kinetoscope", "In No Country for Old Men", "flowers", "Blackbird", "Footprints", "charlie sheen", "The Royal Report", "corey", "(I don't kill my wife", "charlie sheen", "The Memorykeeper's daughter", "(1876", "hubris", "Yahtzee", "charlie sheen", "HTML", "hives", "74.3", "William S. Hart", "charlie sheen", "Pride and Prejudice", "charlie sheen", "corey", "Munich", "Michael Jordan", "Groundhog Day", "without \"grudge or grumblings\"", "corey takei, Mr. Sulu of Star Trek, Comes Out and Speaks Out", "tropical rainforest", "pastry", "corey", "corey", "Boston", "King of the Railway Set", "Arctic Ocean", "pizze Napoletane", "butternut squash", "Spain", "Thomas Chisholm", "May 2002", "1936", "Newfoundland", "corey piscator", "charlie sheen", "1911", "Bryan Charles Kocis", "\"Principia\"", "hezbollah", "Adidas", "anti-trust laws."], "metric_results": {"EM": 0.359375, "QA-F1": 0.40921188186813184}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.7499999999999999, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-315", "mrqa_squad-validation-7576", "mrqa_squad-validation-2965", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-2374", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-11167", "mrqa_searchqa-validation-6792", "mrqa_searchqa-validation-5093", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-16307", "mrqa_searchqa-validation-12408", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-15027", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-5162", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-12415", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-13549", "mrqa_searchqa-validation-9773", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-9379", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-10080", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-6412", "mrqa_naturalquestions-validation-10656", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3030", "mrqa_hotpotqa-validation-4718", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-1406"], "SR": 0.359375, "CSR": 0.4964717741935484, "EFR": 1.0, "Overall": 0.6274193548387097}, {"timecode": 31, "before_eval_results": {"predictions": ["Rev. Paul T. Stallsworth", "blue", "Bill Cosby", "satirical erotic romantic comedy", "Andrew Ryan", "the Social Democratic Party of Austria (SP\u00d6)", "January 21, 2016", "The Worcester Cold Storage and Warehouse Co.", "elizabeth Stefanik", "Fleetwood Mac", "Odense Boldklub", "The Kahan Commission", "Bangkok", "The Oklahoma Sooners", "Merrimack", "Charlie Wilson's War", "\"The Late Late Show\"", "Mark Anthony \"Baz\" Luhrmann", "two", "The Indianapolis Motor Speedway", "Romagnol", "Anita Dobson", "a family member", "Januaryis Joplin", "\"The Worm\"", "Eliot Cutler", "Mercury Records", "1970s and 1980s", "C. J. Cherryh", "Pablo Escobar", "16,116", "Rockland", "\"Slaughterhouse-Five\"", "The Adventures of Huckleberry Finn", "Mittagong", "Frank Sinatra", "Robert L. Stone", "goalkeeper", "Philadelphia", "New York", "Town of Oyster Bay", "Sinngedichte", "The Highwayman", "Madrid", "Kevin Spacey", "Arizona State University", "Blue Grass Airport", "Kenneth Hood", "1952", "the Nebula Award, the Philip K. Dick Award, and the Hugo Award", "\"I'm Shipping Up to Boston\"", "the Royal Navy", "Lenny Jacobson", "Mowgli", "1935", "a lion", "\"foreigner\"", "kentown", "Winter Park", "the European Commission", "Friday", "Blu Cantrell", "Ulysses S. Grant", "Ukraine"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6084935897435897}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 0.4, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-2607", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-5034", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-6414", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-6055"], "SR": 0.515625, "CSR": 0.4970703125, "EFR": 1.0, "Overall": 0.6275390625}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh L. Dryden", "2004", "Kenya", "The Rocky Horror Picture Show", "Scarface", "Argentina", "Apollo 11 mission", "jellyfish", "March", "Wooden clogs", "Mickey Mouse", "the World Bank", "Who was Dan Dares greatest enemy", "Kofi Annan", "chlorophyll", "the Star Chamber", "Taggart", "i second that emotion", "the Atlantic Ocean", "i second that emotion", "Didier Drogba", "Stobart", "Route 66", "Brussels", "the French frigate L'Heureux", "John Poulson", "Charles de Gaulle", "the Maastricht Treaty", "Jack\" Frost", "The Precambrian Shield", "Ambroz Bajec-Lapajne", "the Solent", "vomiting", "The Red Lion", "Bristol Aeroplane Company", "Spinach", "Tony Meo", "\u201cArgo\u201d", "Libra (\u264e) is the seventh astrological sign", "Surrey", "1971", "chippenham", "Budapest", "the Aconcagua Valley", "Don Quixote", "borax", "the Copenhagen S-Bahn", "Jamaica", "Peter Nichols", "Diana Dors", "Kent", "Vickers-Armstrong's", "Ray Charles", "very important", "customary units", "Miller Brewing Company", "northwestern Italian coast", "Sydney Opera House", "devastation", "her decades-long portrayal of Alice Horton", "\"The train ride up there is spectacular.", "Peter Bogdanovich", "Mourning Dove", "Cyprus, a country that was united and then... single sovereign country"], "metric_results": {"EM": 0.390625, "QA-F1": 0.47195148601398607}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-2998", "mrqa_triviaqa-validation-7050", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-5360", "mrqa_triviaqa-validation-3435", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5129", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-7262", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-5817", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-3368", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-5611"], "SR": 0.390625, "CSR": 0.493844696969697, "EFR": 1.0, "Overall": 0.6268939393939394}, {"timecode": 33, "before_eval_results": {"predictions": ["New York and Virginia", "1887", "Lana Del Rey", "1,228 km / h ( 763 mph )", "New England Patriots", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "Antarctica", "Mitch Murray", "blue", "Yamauchi", "`` God Save the Queen ''", "775", "chthonic gods", "diner", "fruit juice", "Longline fishing", "Jesus", "a habitat", "Simone Vangsness", "Central Germany", "Andrew Johnson", "Etienne de Mestre", "Agamemnon", "electors", "Julia Ormond", "Sauron's assistance", "1961", "ste\u026and / STAYND", "2013", "March 1", "published on November 12, 1976", "redox reaction", "Spain", "MacKenzie Mauzy", "Paul Lynde", "an elevator ride, or approximately thirty seconds to two minutes", "Jocelyn Flores", "fled to exile in the Netherlands", "builted from 1887 -- 89 as the entrance to the 1889 World's Fair", "erosion", "March 2, 2016", "cranberry sauce", "1996", "Ray Charles", "18", "the Ramones", "1800", "the Anglo - Norman French waleis", "Frank Theodore", "New Jersey", "May 2010", "France", "bbc", "dennis taylor", "a centaur", "getaway driver", "cricket fighting", "dennis taylor", "drama of the action in-and-around the golf course has enraptured fans of the game", "dennis taylor", "California's Stanford University,", "Tunisia", "RAND Corporation", "bios & Profiles"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4965166429475016}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.8, 0.06451612903225806, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.24000000000000005, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7058823529411764, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3127", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7802", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4561", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-5607", "mrqa_hotpotqa-validation-1997", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2243", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-9333"], "SR": 0.4375, "CSR": 0.4921875, "EFR": 1.0, "Overall": 0.6265625}, {"timecode": 34, "before_eval_results": {"predictions": ["Venus", "Beyonc\u00e9 and Bruno Mars", "Zeebo", "first time in Brahmagupta's Brahmasputha Siddhanta ( 7th century )", "2018", "abusive husband", "premiered on CBS on September 29, 2017", "`` stellar nurseries '' or `` star - forming regions '', collapse and form stars", "gears that can be changed to allow a wide range of vehicle speeds, and also in the differential, which contains the final drive to provide further speed reduction at the wheels", "Universal Pictures", "Mahalaxmi Iyer", "1942", "Nick Sager", "the Parliament of the United Kingdom", "prophets and beloved religious leaders", "the state legislators of Assam", "Gastric acid, gastric juice or stomach acid", "Renishaw Hall, Derbyshire, England", "accomplish the objectives of the organization", "sport utility vehicles", "Isabella Palmieri", "temperature at which the phase transition occurs", "Mind your Ps and Qs is an English expression meaning `` mind your manners '', '' mind your language '', `` be on your best behaviour '' or similar", "Germany", "1989", "the Spanish Dominican Tom\u00e1s de Torquemada", "Bob Gaudio", "1975", "Mel Gibson", "Procol Harum", "Erica Rivera", "zinc", "a proprietary library classification system first published in the United States by Melvil Dewey in 1876", "2003", "Lucas Black", "Wednesday, 5 September 1666", "California State Route 1, and entrances in Carmel and Pacific Grove", "the management team", "clinical tract, thyroid, breast, lung, salivary glands, eye, and skin", "Flex SDK", "Steveston Outdoor pool", "Robin Cousins", "a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Ukraine", "Lula", "1840", "when the car comes to a halt", "egg that are usually used as gifts on the occasion of Easter", "Derrick Henry", "lowest air temperature record", "`` Mirror Image ''", "mounted inside the pedestal's lower level", "Kellog", "wilson", "Brian Close", "\"I have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "Galleria Vittorio Emanuele II", "every aspect of public and private life", "reached an agreement late Thursday to form a government of national reconciliation.", "Olympic medal", "Henry Ford", "Toyota", "Cabinet and Vice Presidents", "cancer"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5649032619427357}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.3333333333333333, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.7428571428571429, 0.0, 0.5, 0.7499999999999999, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.5454545454545454, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.20000000000000004, 0.13333333333333333, 0.0, 0.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-10576", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-1562", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-15641"], "SR": 0.40625, "CSR": 0.48973214285714284, "EFR": 0.9210526315789473, "Overall": 0.610281954887218}, {"timecode": 35, "before_eval_results": {"predictions": ["domestic Islamists who attacked it (bin Laden being a prime example),", "a supposed mild euphoric", "december", "Venezuela", "Milwaukee", "boxing", "Peter Pan", "december", "the Arctic Ocean", "a bottle", "fish", "Lafayette", "malcom", "a tropical depression, tropical storm, or hurricane", "the Royal Marines Band", "Alexander Pushkin", "Australia", "Munich Crisis", "Newspapers", "a shift", "the papacy", "Arkansas", "subclue 2", "Renoir", "de savoir", "libretti", "Innsbruck", "Lance ito", "Microsoft", "a fern", "Sony", "Norse seafarers", "Atlantic City", "Blackwater USA", "elephants", "American Airlines", "ibex", "Odysseus", "Geronimo", "London", "hawson", "Netherlands", "Pocahontas", "the Lion, the Witch, and the Wardrobe", "John Galt", "a chalkboard", "the Chicago Mercantile Exchange", "Las Vegas", "danskin", "wheat", "Madrid Symphony Orchestra", "a ostrich", "1943", "European settlements", "in vertebrates", "James I", "penrhyn", "psychological horror", "John Morgan", "Hungarian Rhapsody No. 2", "1176", "Senate Republican aides", "63", "\"perezagruzka'"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5182291666666666}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false], "QA-F1": [0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9699", "mrqa_searchqa-validation-11665", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-265", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-4473", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-11675", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-5646", "mrqa_searchqa-validation-3975", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-16094", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-2225", "mrqa_triviaqa-validation-3348", "mrqa_hotpotqa-validation-3745", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352"], "SR": 0.484375, "CSR": 0.48958333333333337, "EFR": 1.0, "Overall": 0.6260416666666667}, {"timecode": 36, "before_eval_results": {"predictions": ["electric lighting", "James W. Marshall", "Terrell Suggs", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "Lucknow in September 1906", "2013", "National Industrial Recovery Act", "The User State Migration Tool", "the Battle of Antietam", "William DeVaughn", "in the World Trade Center complex", "Southend", "Los Angeles", "sovereign states", "Victor Dhar", "31 January 1934", "Filipino", "first significant migration of Scottish settlers to Nova Scotia in 1773", "modern random - access memory ( RAM )", "May 31, 2012", "April 1917", "Bart Cummings", "October 27, 1904", "Harishchandra", "Olivia Olson", "1990", "the Canadian rock band Nickelback", "Bill Pullman", "BC Jean", "in IMAX 3D", "Frankie Muniz", "stratum lucidum", "60", "Hasmukh Adhia", "four", "one retina", "1980s", "in soils", "card verification data", "commands Titus to ordain presbyters / bishops", "bohrium", "Britain", "Escherichia coli", "Archduke Franz Ferdinand", "June 1991", "2010", "he lost the support of the army, abdicated in November 1918, and fled to exile in the Netherlands", "in the basic curriculum -- the enkuklios paideia or `` education in a circle ''", "Mike Czerwien", "103", "Vienna", "English", "Mexico", "\u201cThe Pope?", "$10.5 million", "Al Horford", "John Wilkes Booth", "$22 million", "leftist Workers' Party", "his three children", "cotton", "Russell Crochez", "the Mighty Quinn", "Towcester"], "metric_results": {"EM": 0.515625, "QA-F1": 0.628751240079365}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.37499999999999994, 0.6666666666666666, 0.0, 0.08333333333333334, 0.0, 0.5, 0.6666666666666666, 0.16666666666666669, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1, 0.5714285714285715, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-2833", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-4195", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-538", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1953", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-1862"], "SR": 0.515625, "CSR": 0.49028716216216217, "EFR": 0.9354838709677419, "Overall": 0.6132792066259809}, {"timecode": 37, "before_eval_results": {"predictions": ["Joseph Swan", "the United States", "South Africa", "Turkey", "stahl", "a cappella", "sunburn", "Peterloo", "Aglet", "Saturday Night Live", "FC Bayern M\u00fcnchen", "solstices", "Bonnie and Clyde", "English", "copper", "Dawn French", "Blackstar", "brazil", "Doris Lessing", "Scooby Doo", "Swaziland", "brazil", "Kent", "mudflats", "bets", "stuttgart", "Kent", "Rodgers & Hammerstein", "Cowboy", "Galileo Galilei", "Mata Hari", "stalin", "Sweet dreams", "medellin", "Ariel", "spark plugs", "Celts", "Boulder", "painkillers", "Iran", "Belle de Jour", "brazil", "Abba", "rain", "blue", "Phobos", "France", "Frederick", "Kunsky", "death and dying", "Lady Penelope", "the forces of Andrew Moray and William Wallace", "142,907", "mid November", "fan interviews, previews and reviews of Arsenal matches", "Theo Walcott", "Ben Ainslie", "two Metro transit trains that crashed the day before,", "thunderstorms", "women", "the hat", "sunflower", "Madonna", "March 24,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5214514652014652}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.8, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-5175", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-541", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-7730", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6503", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-2982", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-330", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-8884", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-869", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-6291"], "SR": 0.46875, "CSR": 0.48972039473684215, "EFR": 1.0, "Overall": 0.6260690789473684}, {"timecode": 38, "before_eval_results": {"predictions": ["the Rip\")", "tyne", "liver", "40 days", "table salt", "Bears", "cuba", "chancery", "Colin Cant", "Stevie Wonder", "the head", "hound", "hanover", "a star", "Charles I of England", "workless households", "scales", "Dirty Dancing", "goddess of Revenge", "Diana Ross", "Montezuma", "a warm sunny day in July 2008", "Paul Anka", "Carthage", "albion", "a Dagger", "Blade Runner", "Jay-Z", "leopard", "cymbals", "cuba", "libretto", "Andrew Mitchell", "flidelio", "South Africa", "Christian Dior", "albion", "Killer whales", "cuba", "France", "cherry", "fasting", "Cyprus", "speed camera", "Duke", "lizard", "fandango Groovers", "frauds", "a sea horse", "1, 3, 5, 7, or 9", "Tony Blair", "quartz", "54 Mbit / s, plus error correction code,", "Manley", "Stacey Kent", "Eyes Wide Shut", "Anthony Ray Lynn", "piano", "act of confusion and then shock among festival goers.", "Kim Jong Il", "French Guiana", "albion", "heraldry", "Tiger Woods"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4584155701754386}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.8, 1.0, 0.10526315789473685, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-2584", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3351", "mrqa_naturalquestions-validation-6603", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-5730", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-2594", "mrqa_searchqa-validation-9940", "mrqa_searchqa-validation-4817", "mrqa_newsqa-validation-3899"], "SR": 0.40625, "CSR": 0.4875801282051282, "EFR": 1.0, "Overall": 0.6256410256410256}, {"timecode": 39, "before_eval_results": {"predictions": ["\"No, that\\'s no good\"", "laldi", "Midnight Cowboy", "alfa", "dandruff", "Amanda Barrie", "a hoy", "Niger", "central Stockholm", "Tangled", "dog", "James Douglas", "Bulls Eye", "georgia", "Laurent Planchon", "Timothy Carroll", "Jane Austen", "pembrokeshire Coast National Park", "Kevin macdonald", "peppers", "geologic era", "Jimmy Boyd", "Isambard Kingdom Brunel", "georgia", "1957", "bristol", "albion", "butter", "albion", "Ralph Vaughan Williams", "musical scale", "domestic cat", "pink flannel", "georgia tchaikovsky", "Shanghai", "Spain", "grow in St. James Parish", "Thursday", "Guru Nanak", "bleak house", "inigo Montoya", "aliquot", "Thomas Horner", "georgia", "lolita", "cuckoo bird", "Dame Margaret Rutherford", "Ford", "Alice Cooper", "Majorca (Mallorca)", "Mixing blood from two individuals", "Royal Bengal Tiger", "1.5 times the Schwarzschild radius", "Santa Claus", "syndicated columnist", "1999", "Sela Ward", "Body Works", "forgery and flying without a valid license", "137", "a log cabin", "St. Patrick's Day", "quarterback", "Marshall"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4584573412698413}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6497", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-7408", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-6153", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5688", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-1036", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2738", "mrqa_triviaqa-validation-4297", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-2711", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8554", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2100", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-807", "mrqa_naturalquestions-validation-9755"], "SR": 0.390625, "CSR": 0.48515624999999996, "EFR": 1.0, "Overall": 0.6251562500000001}, {"timecode": 40, "before_eval_results": {"predictions": ["19th Century", "Famous Players", "georgia", "taylor de Quincey", "yersinia pestis", "horse", "buffalo", "rome\u2019s", "a pigeon", "Sarajevo", "the Bill of Rights", "the end of the song", "Neighbours", "bligh", "trumpet", "Westminster Abbey", "origami", "electric resistance", "Arabian Gulf", "redmond darwin", "devon", "devon", "jack", "smith darwin", "bach", "Tomorrow Never Dies", "indian", "a Great Dane", "norway", "brazil", "New Hampshire", "king james i", "charlie fenton", "jane", "purple rain", "cliffs of insanity", "buckoos", "cliffs of insanity", "rome", "tuesday", "Southwest Airlines", "bach", "jane deaver", "The Comedy of Errors", "chicago", "devon", "first lady Betty Ford", "devon", "devon", "Radicalization", "rhodin", "Humpty Dumpty and Kitty Softpaws", "1998", "Mahalaxmi Iyer", "the ENnies", "the 100th anniversary of the first \"Tour de France\"", "Mach number", "Janet and la Toya", "2.5 million", "researchers", "tuesday darwin", "Curb Your Enthusiasm", "nibelung", "the highest inequality of opportunity"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4158110119047619}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.5, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-2275", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-4716", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4538", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-863", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-4337", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7516", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-2372", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-11519", "mrqa_naturalquestions-validation-3969"], "SR": 0.3125, "CSR": 0.4809451219512195, "EFR": 1.0, "Overall": 0.6243140243902439}, {"timecode": 41, "before_eval_results": {"predictions": ["1220", "philippine islands", "nixon", "nixon", "asiatic sarmatia", "secker", "abacus", "Robin Hood", "sisyphus", "Velazquez", "South Africa", "caracas", "oregon", "nymphomaniac", "death", "angola", "not Private Eye", "thomas", "Neil Armstrong", "Jean-Paul Sartre", "Popowo", "thomas turner", "rust", "thomas nixon", "Wiltshire", "tbilisi", "thomas thomas", "othello", "nine", "thomas thomas", "Lacock Abbey", "alex b'Stard", "domestic cat", "anita Brookner", "james", "Golda Meir", "Black Sea", "bagram", "Susie Dent", "a power outage", "london", "archers", "shylock", "james sousa", "roodee", "james Boyd", "thomas", "thomas", "tyne", "thomas", "Dry Ice", "Pat McCormick", "19 June 2018", "18 - season", "1993 to 1996", "james aprile", "September 23, 2016", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26", "June 6, 1944", "sniff out cell phones.", "a bassoon", "o.K. Corral", "butternut", "thomas"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5304067460317461}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.5, 0.3333333333333333, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5582", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-3306", "mrqa_triviaqa-validation-5625", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-2641", "mrqa_triviaqa-validation-7225", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-3866", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-9161", "mrqa_searchqa-validation-233"], "SR": 0.453125, "CSR": 0.48028273809523814, "EFR": 0.9714285714285714, "Overall": 0.618467261904762}, {"timecode": 42, "before_eval_results": {"predictions": ["lack of reliable statistics from this period", "buses, subways and trolleys", "Borussia Monchengladbach", "persian coastline", "revolution of values", "Jeddah, Saudi Arabia", "40", "getting his chest waxed", "\"I'm just getting started.\"", "Manny Pacquiao", "$250,000", "27,", "Michelle Obama", "Alberto Fujimori", "air traffic delays", "The Book", "the \" Michoacan Family,\"", "64", "life in prison.", "fastest", "Department of Homeland Security Secretary Janet Napolitano", "Iran", "ended his playing career", "on the set", "South Florida", "the United States Holocaust Memorial Museum", "ice jam", "breast cancer", "Benazir Bhutto", "July", "U.S. senators", "us", "Larry Ellison", "our mutual friend", "her fianc\u00e9,", "Cal Ripken Jr.", "Johannesburg", "cancer", "acid attack", "Vernon Forrest", "urged NATO to take a more active role in countering the spread of the", "one", "comfort those in mourning,", "byproducts emitted during the process of burning and melting raw materials", "about 5:20 p.m.", "Herman Thomas", "we say to the people of Gaza, give more resistance and we will be with you in the field,", "a man's lifeless, naked body", "\"release\" civilians", "Dodi Fayed", "wind", "when a population temporarily exceeds the long term carrying capacity of its environment", "Barcelona", "emperor Cuauhtemoc", "danzig", "our mutual friend", "purdy", "Antonio Lippi", "Thorgan Ganael Francis Hazard", "Braehead", "brazil", "jimmy jim", "Cy Young", "the Statler Brothers"], "metric_results": {"EM": 0.359375, "QA-F1": 0.41833546703973534}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.8, 0.14634146341463414, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-49", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3979", "mrqa_naturalquestions-validation-5597", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-1641", "mrqa_triviaqa-validation-4313", "mrqa_hotpotqa-validation-3610", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-10019"], "SR": 0.359375, "CSR": 0.47747093023255816, "EFR": 1.0, "Overall": 0.6236191860465116}, {"timecode": 43, "before_eval_results": {"predictions": ["25,033", "the House of Borromeo", "Washington, D.C.,", "1943", "a facelifted 850 saloon", "the Mountain West Conference", "Kevin Willis", "Western Europe", "political thriller", "Continental AG", "English football league system", "from 1989 until 1994", "the Distinguished Service Cross,", "\"50 best cities to live in.\"", "Bridgetown", "Pakistani cinema", "Emmanuel Ofosu Yeboah", "Ant-Man", "Bhushan Patel", "1986", "1916", "Reginald Engelbach", "Vince Staples", "Archbishop of Canterbury", "Galway", "\"My Backyard\"", "15 October 1988", "coaxial cable", "\"Northern Lights\"", "three different covers", "Malayalam fantasy comedy", "1815\u20131867", "August 11, 1946", "Charlie Kaufman", "May 26, 2010", "\"Estadio de L\u00f3pez Cort\u00e1zar\"", "Hanna-Barbera", "Christian Duguay", "1985", "Gal Gadot", "Meghan Markle", "a Boeing B-17 Flying Fortress", "Erika Girardi", "Joe Scarborough", "Crimean", "76,416", "Bonkyll Castle", "second cousin once removed", "the 2012 Summer Olympics", "Sony Studio Liverpool", "Brig Gen Augustine Warner Robins", "United Nations", "a fictional character", "two occasions", "(Reese) BASS", "blue", "elbow", "Citizens of the lower house of parliament,", "the Employee Free Choice act", "the release of the four men", "a rake", "the East End of London", "a carriage", "a (Reese) Teak"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6623263888888888}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.888888888888889, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7278", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-1464", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7240", "mrqa_triviaqa-validation-2264", "mrqa_newsqa-validation-2070", "mrqa_searchqa-validation-602", "mrqa_searchqa-validation-10888"], "SR": 0.5625, "CSR": 0.47940340909090906, "EFR": 1.0, "Overall": 0.6240056818181818}, {"timecode": 44, "before_eval_results": {"predictions": ["British Prime Minister Edward Heath", "Sean Yseult", "Washington, D.C.", "over 12 million", "(Guerra de Titanes)", "Conservatorio Verdi", "President of the United States", "\"the backside.\"", "Angelo Bruno", "Miranda July", "Free State of Jones", "Andy Cohen", "Denmark", "2015 Orange Bowl", "Margarine Unie", "death", "Fort Valley, Georgia", "Bill Paxton", "Vladimir Menshova", "Kramer Guitars", "in the Dominican Republic in the first half of the 20th century", "Humberside Airport", "2017", "Douglas Jackson", "wooden roller", "Blackpool Football Club", "William Lyon Mackenzie King", "\" Family Guy\"", "\"Loiter Squad\"", "Chrysler", "Bruce David Grobbelaar", "Honda Ballade", "ascona", "Boston Celtics", "Austrian", "Australian Electoral Division of Fawkner", "Socrates", "Caesars Entertainment Corporation", "Hindi", "Richard Masur", "Irish Chekhov", "311", "\"Dr. Gr\u00e4sler, Badearzt\" by Arthur Schnitzler", "Alexandre Dimitri Song Billong", "Arizona Health Care Cost Containment System", "Mineola", "Gian Carlo Menotti", "Olympic bobsledder", "Mazda Capella", "102,984", "Roscoe Lee Browne", "1972", "James P. `` Sulley '' Sullivan", "over 38 million", "The Spectator", "Easter Parade", "Elgar himself recalled how the work came to be conceived on the evening of 21 October 1898", "last summer.", "almost 100", "into the Southeast,", "the pie", "Great Balls of Fire", "trade series", "One Direction"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6080255681818182}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 0.4, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-2979", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-4710", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-15766", "mrqa_searchqa-validation-12050"], "SR": 0.453125, "CSR": 0.47881944444444446, "EFR": 1.0, "Overall": 0.6238888888888889}, {"timecode": 45, "before_eval_results": {"predictions": ["American Revolution", "a \"piece of demonstrated\"", "Queen Elizabeth II", "gaius Julius Caesar", "Dr. Joel Fleischman Departs", "cocoa butter", "Kokomo", "Esther", "Stanley D. Winck", "Lorne Greene", "minigolf", "Campbell Brown", "Punxsutawney, Pennsylvania", "Bratislava", "yellow fever", "sea otters", "M&M's", "submarine", "a \"piece of death\"", "Watergate break-in", "dressage", "astronomy", "Mickey Mouse", "a \"piece of a bud\"", "a \"piece of professor\"", "stick", "Medusa", "a \"piece\" spiral", "a \"piece of tabby\"", "the staff", "Voyager 1", "Farsi (Persian)", "ketones", "a \"piece\"", "China", "Helen of Troy", "Vegetarianism", "a \"piece of peace sign\"", "Morrie: An Old Man, a Young Man", "English Monarchs", "Rajasthan", "Ben Kingsley", "The New York Times", "Oklahoma Sooners Football", "a \"road\"", "a \"piece\" knife", "a \"piece of office of, say, one-third of its members\"", "Wordsworth", "brushes", "a \"dwarf planet\"", "a \"piece of insanity\"", "Swedien and Jones", "Rugrats", "Middle Eastern alchemy", "London", "the Isle of Wight", "Peppercorn Pioneer", "\"Queen In-hyun's Man\"", "Oneida Limited", "Daniel Richard \" Danny\" Green, Jr.", "Libreville, Gabon.", "two tickets to Italy on Expedia.", "Christopher Barbour, 36,", "Cahaba"], "metric_results": {"EM": 0.375, "QA-F1": 0.4861979166666667}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.8, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2593", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-9878", "mrqa_searchqa-validation-3796", "mrqa_searchqa-validation-7347", "mrqa_searchqa-validation-13022", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-4671", "mrqa_searchqa-validation-13469", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-8467", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-975", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5006", "mrqa_searchqa-validation-11964", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3322", "mrqa_naturalquestions-validation-9867", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6557", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-3949", "mrqa_triviaqa-validation-888"], "SR": 0.375, "CSR": 0.4765625, "EFR": 0.975, "Overall": 0.6184375}, {"timecode": 46, "before_eval_results": {"predictions": ["the position of people within the four-class system", "Jorge Lorenzo", "Malachy McCourt", "James Cameron", "fungi", "Venus flytrap", "Abraham", "george orwell", "the faggot", "anser", "California Chrome", "Pluto", "Route 66", "the Taklamakan Desert", "the Sindh\u016b River", "Astro-Santa Morph", "The Great Victoria Desert", "German", "the British pop band Go West", "December 18, 1958", "Benjamin Franklin", "Portugal", "Operation Overlord", "Birmingham", "snakes", "Sedgefield", "the Coral Sea", "Saddam Hussein", "Nadia Comaneci", "tank", "North Korea", "a pig", "\"The Proposal\"", "Carmen", "Kenya", "Stephen Potter", "Verona", "Anwar Sadat", "1", "Potomac River", "the Republic of Costa Rica", "darth vader", "Frankfurt", "a chipmunk", "Goldie Hawn", "a pulsar", "Belgium", "the most moving horse stories of all time", "honey", "Bar\u00e7a", "Sun Lust Pictures", "the player", "she is not in love with Chino", "somatic cell nuclear transfer", "early 7th century", "1 January 1788", "Radcliffe College", "11", "\"Twilight\"", "the Louvre.", "Speed Racer", "H. G. Wells", "Queen Elizabeth", "Sir Walter Scott"], "metric_results": {"EM": 0.53125, "QA-F1": 0.618687996031746}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true], "QA-F1": [0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8093", "mrqa_triviaqa-validation-736", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-4088", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5636", "mrqa_triviaqa-validation-7773", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5241", "mrqa_hotpotqa-validation-3234", "mrqa_newsqa-validation-2953", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5788"], "SR": 0.53125, "CSR": 0.4777260638297872, "EFR": 1.0, "Overall": 0.6236702127659575}, {"timecode": 47, "before_eval_results": {"predictions": ["Arabah", "gondola", "Sinclair Lewis", "andy griffith", "The World is not enough", "pigments", "Jonathan Demme", "Vaclav Havel", "Dick Van Dyke", "isabella", "Tina Turner", "July 14th 1789", "Coleraine", "toadstool", "perfume", "Duke Orsino", "iron", "Copenhagen", "The Apprentice", "toadstool", "Cubism", "sahara", "Commonwealth Institute of Science and Industry", "bread and wine", "Charlotte's Web", "indiana", "toadstool", "Charles Foster", "Lorne Greene", "rowing", "Corin Redgrave", "Call My Bluff", "a genie", "Argentina", "frank mccourt", "toadstool", "Caroline Aherne", "germany", "carbon dioxide", "Pears soap", "Donna Summer", "balustrade", "Nottingham", "gdansk", "Welcome Stranger", "taggart", "April", "Chechnya", "a police janitor", "the A- Team", "football", "801,200", "Sir Ronald Ross", "Sun Tzu", "bioelectromagnetics", "Foxborough, Massachusetts", "South Australian Championships", "a city of romance, of incredible architecture and history.", "36", "Michelle Obama", "kbenhavn", "the Proletariat", "sahara", "Floxin"], "metric_results": {"EM": 0.5, "QA-F1": 0.5340909090909091}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-3740", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-1730", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5726", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-334", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-7020", "mrqa_searchqa-validation-15651"], "SR": 0.5, "CSR": 0.47819010416666663, "EFR": 1.0, "Overall": 0.6237630208333333}, {"timecode": 48, "before_eval_results": {"predictions": ["Firth of Forth", "Caesars Entertainment Corporation", "Supergirl", "\u00c6thelred the Unready", "randolph park", "Stephen Mangan", "William McKinley", "1905", "Vanilla air", "Mineola, New York", "roger dovzhenko", "Clark Gable", "Julia Compton Moore", "\"Grease\"", "argentina", "early Romantic period", "The Gettysburg Address", "Harold Edward Holt", "Washington Street", "Lou Diamond Phillips", "Babylon", "second generation", "New York State Route 908M", "The Company", "1827", "Kim Bauer", "seizures", "Lieutenant Martin", "Suffolk", "Prussian", "o", "Cuban-American Major League Clubs Series", "86 ft", "American", "January 2004", "sulfur", "the 45th Infantry Division", "2009", "5", "Anita Dobson", "London", "Boyd Gaming", "February 14, 1859", "Texas Tech Red Raiders", "John McClane", "Larry Gatlin", "924", "held the American record for the most time in space (381.6 days)", "North Carolina", "millheim, Pennsylvania", "his daughter,", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "some product's classification as a cake or biscuit was part of a VAT tribunal in 1991, with the court finding in McVitie's favour that the Jaffa cake should be considered a cake for tax purposes", "Salman Khan", "argentina", "basil", "clio Awards", "The Rosie Show", "North Korea", "over 1,000 pounds", "Julius Caesar", "lava", "the Library", "the stroma"], "metric_results": {"EM": 0.546875, "QA-F1": 0.636751552342585}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, false], "QA-F1": [0.6, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.8695652173913043, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-4008", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-1984", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-659", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-115", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5714", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-6806", "mrqa_triviaqa-validation-6785", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-5430", "mrqa_naturalquestions-validation-4685"], "SR": 0.546875, "CSR": 0.47959183673469385, "EFR": 0.9655172413793104, "Overall": 0.6171468156228008}, {"timecode": 49, "UKR": 0.61328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-2508", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4095", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4589", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5192", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-564", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5755", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-1887", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2722", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3569", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-938", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-13836", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-15433", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-15641", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-16122", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-165", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1954", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4937", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6398", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-8410", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-975", "mrqa_squad-validation-10069", "mrqa_squad-validation-10086", "mrqa_squad-validation-1019", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-10397", "mrqa_squad-validation-10444", "mrqa_squad-validation-10449", "mrqa_squad-validation-1052", "mrqa_squad-validation-1129", "mrqa_squad-validation-1211", "mrqa_squad-validation-1265", "mrqa_squad-validation-1311", "mrqa_squad-validation-139", "mrqa_squad-validation-164", "mrqa_squad-validation-1672", "mrqa_squad-validation-1712", "mrqa_squad-validation-1916", "mrqa_squad-validation-2132", "mrqa_squad-validation-2155", "mrqa_squad-validation-2176", "mrqa_squad-validation-2326", "mrqa_squad-validation-2436", "mrqa_squad-validation-2467", "mrqa_squad-validation-264", "mrqa_squad-validation-2798", "mrqa_squad-validation-2824", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-2906", "mrqa_squad-validation-2914", "mrqa_squad-validation-294", "mrqa_squad-validation-2999", "mrqa_squad-validation-305", "mrqa_squad-validation-3337", "mrqa_squad-validation-3650", "mrqa_squad-validation-3742", "mrqa_squad-validation-3948", "mrqa_squad-validation-4025", "mrqa_squad-validation-4066", "mrqa_squad-validation-4135", "mrqa_squad-validation-4258", "mrqa_squad-validation-4338", "mrqa_squad-validation-4349", "mrqa_squad-validation-44", "mrqa_squad-validation-4472", "mrqa_squad-validation-4480", "mrqa_squad-validation-4605", "mrqa_squad-validation-4607", "mrqa_squad-validation-4686", "mrqa_squad-validation-4835", "mrqa_squad-validation-487", "mrqa_squad-validation-4897", "mrqa_squad-validation-4947", "mrqa_squad-validation-5088", "mrqa_squad-validation-5136", "mrqa_squad-validation-5238", "mrqa_squad-validation-5330", "mrqa_squad-validation-5672", "mrqa_squad-validation-594", "mrqa_squad-validation-60", "mrqa_squad-validation-6362", "mrqa_squad-validation-6562", "mrqa_squad-validation-6737", "mrqa_squad-validation-6737", "mrqa_squad-validation-6811", "mrqa_squad-validation-6918", "mrqa_squad-validation-696", "mrqa_squad-validation-703", "mrqa_squad-validation-7173", "mrqa_squad-validation-7435", "mrqa_squad-validation-754", "mrqa_squad-validation-7576", "mrqa_squad-validation-7598", "mrqa_squad-validation-7814", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-8285", "mrqa_squad-validation-8402", "mrqa_squad-validation-8406", "mrqa_squad-validation-8483", "mrqa_squad-validation-8607", "mrqa_squad-validation-8636", "mrqa_squad-validation-8715", "mrqa_squad-validation-8747", "mrqa_squad-validation-8760", "mrqa_squad-validation-879", "mrqa_squad-validation-8846", "mrqa_squad-validation-9015", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9691", "mrqa_squad-validation-9757", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1297", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2068", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2572", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2970", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3886", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-4904", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-5118", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-5505", "mrqa_triviaqa-validation-5607", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6785", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-92"], "OKR": 0.82421875, "KG": 0.46796875, "before_eval_results": {"predictions": ["a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Jena Malone", "Washington, D.C.", "joined the utopian Ascona community", "John William Henry II", "James Woods", "James Mitchum", "April 1963", "1995", "Tom Shadyac", "Wendell Berry", "Love Hina", "eastern India", "novelty songs, comedy, and strange or unusual recordings", "OutKast", "five aerial victories", "Alain Robbe-Grillet", "the Tangerines", "musical research", "Dragon TV", "Appalachian Mountains", "Bay Ridge, Brooklyn", "Jean- Marc Vall\u00e9e", "over 1.6 million", "1928", "November 20, 1942", "September 26, 2010", "North Greenwich Arena", "1614", "Lucy Maud Montgomery", "Bad Meets Evil", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Saint Michael, Barbados", "Sleepy Hollow", "more than 26,000", "EN World web site", "Charles Russell", "Kj\u00f8benhavns Boldklub", "Patrick John Mercer", "three Golden Globe Awards", "southwest Denver, Colorado near Bear Creek", "Port Clinton", "Art of Dying", "Dallas", "Harvard", "fennec", "Dutch", "Terry Malloy", "Golden Calf", "Kal Ho Naa Ho", "Thorgan Ganael Francis Hazard", "the closing scene of the final episode of the first season", "Everywhere", "The Jawaharlal Nehru Centre for Advanced Scientific Research ( JNCASR )", "Honda", "J. M. W. Turner", "the Republic of Upper Volta", "56,", "Nkepile M abuse", "Eintracht Frankfurt", "U.S. Marines", "Hephaestus", "Amherst College", "two courses"], "metric_results": {"EM": 0.59375, "QA-F1": 0.708407738095238}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [0.26666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-1344", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3430", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-616", "mrqa_searchqa-validation-9636", "mrqa_searchqa-validation-14102", "mrqa_newsqa-validation-492"], "SR": 0.59375, "CSR": 0.48187500000000005, "EFR": 1.0, "Overall": 0.67746875}]}