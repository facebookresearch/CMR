{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=10.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 3980, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "1970s", "Sunni Arabs from Iraq and Syria", "P", "Daniel Burke", "the highest terrace", "major national and international patient information projects", "three", "net force", "12 January", "1976\u201377", "E. W. Scripps Company", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "Book of Discipline", "variable", "coordinating lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "main hall", "the Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation", "2014", "late 1970s", "30% less", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Studs Terkel, American writer, essayist, filmmaker, teacher, and political activist Susan Sontag", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts", "The Prisoners ( Temporary Discharge for Ill Health ) Act", "Carol Ann Susi", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.75, "QA-F1": 0.8083926184110007}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-1692", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-6282", "mrqa_squad-validation-3352", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.75, "CSR": 0.7578125, "EFR": 0.9375, "Overall": 0.84765625}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "His wife Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger NFL", "1950s", "1980s", "the Swiss canton of Graub\u00fcnden", "second use of the law", "free", "1973", "September 1969", "Mansfeld", "Warsaw Stock Exchange", "390 billion", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "13 May 1899", "Lunar Module Pilot", "citizenship", "Merritt Island", "physicians, lawyers, engineers", "return home", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "sequential proteolytic activation of complement molecules", "the head of Lituya Bay in Alaska", "120 m ( 390 ft )", "eighth season will have only six episodes", "100 members", "photoelectric", "Welch, West Virginia", "Declaration of Indian Independence ( Purna Swaraj ) was proclaimed by the Indian National Congress", "twelve Wimpy Kid books", "Hal David and Burt Bacharach", "five points", "The Monitor and Merrimac", "Spain"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7420318153645136}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.8, 0.888888888888889, 0.4444444444444445, 1.0, 0.4, 1.0, 0.5853658536585366, 0.38095238095238093, 0.0, 0.5, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-80", "mrqa_squad-validation-9173", "mrqa_squad-validation-5788", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1454", "mrqa_squad-validation-1841", "mrqa_squad-validation-1220", "mrqa_squad-validation-6645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-2016", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-3996"], "SR": 0.640625, "CSR": 0.71875, "EFR": 0.9130434782608695, "Overall": 0.8158967391304348}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive", "the principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "Abercrombie saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac,", "Egypt", "algae", "4,404.5", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "Yam route systems", "Stairs", "genetically modified plants", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Mark Ronson", "the Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw)", "cloud storage service", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum", "John Elway", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "Golden Gate Bridge", "Michael Schumacher", "10.5 %", "The Man", "President Gerald Ford", "Bud '' Bergstein", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "Forney Hull ( James Frain )", "In December 1971, the `` Smithsonian Agreement '' was reached. In this agreement, the dollar was devalued from $35 per troy ounce of gold to $38", "the land itself, while blessed, did not cause mortals to live forever", "the middle of the 15th century", "6 March 1983", "Viola Larsen", "horror fiction", "26,000"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7667478354978354}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-10293", "mrqa_squad-validation-8763", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-670", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433", "mrqa_hotpotqa-validation-454"], "SR": 0.703125, "CSR": 0.71484375, "EFR": 1.0, "Overall": 0.857421875}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Fort Presque Isle", "wireless", "Beyonc\u00e9 and Bruno Mars", "the Yuan dynasty", "same-gender marriages with resolutions", "red algae red", "after their second year", "1960s", "narcotic drugs were controlled in all member states, and so this differed from other cases where prostitution or other quasi-legal activity was subject to restriction", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot (3.7 million cubic meter)", "50 fund", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "sponges, both ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "dissension and unrest", "the Establishment Clause of the First Amendment or individual state Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "NDS, a Cisco Systems company", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "a thylakoid", "University College London", "to protect their tribal lands from commercial interests", "religious beliefs", "evading it", "the kettle and the Cricket", "Gandhiji", "Vlad the Impaler", "\"Take us the foxes, the little... Alexandra - first cousins - as a means of getting Horace's money, but Horace and Alexandra are repulsed by the suggestion", "Betamax", "Vincent van Gogh, in which Nimoy played Van Gogh's brother Theo.", "8/4 x 365 = 730 days", "The Story of Tiger Woods' 1996 U.S. Amateur Win", "It was Arizona's territorial capital from 1867 to 1877, and a university was founded there in 1885", "Marshall Dillon", "\"Wannabe\" and \"Say You'll Be There\"", "The Best Hotels on Bali: Legian Resort on Seminyak Beach", "The new nightspot Teddy's made this presidential Hollywood hotel a happening # Quiz # Question. 0:29", "Pulsed Laser", "Incomprehensible", "Juno", "Hundreds of species of peat mosses are found in bogs throughout Canada", "why", "Daya", "the fear of meat", "American", "The Mexican military"], "metric_results": {"EM": 0.5, "QA-F1": 0.6135593560525625}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14814814814814814, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1818181818181818, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 0.28571428571428575, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.17391304347826084, 1.0, 0.3076923076923077, 0.0, 0.4, 0.0, 0.0, 0.0, 0.19999999999999998, 0.14285714285714285, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-110", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-3703", "mrqa_squad-validation-10186", "mrqa_squad-validation-4424", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-7088", "mrqa_squad-validation-2804", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073"], "SR": 0.5, "CSR": 0.671875, "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "a Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "hunting", "the member state cannot enforce conflicting laws", "Graham Twigg", "canals", "inversely", "Europe", "he was illiterate in Czech", "colonies", "$37.6 billion", "Kalenjin", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center", "Variable lymphocytes receptors (VLRs)", "Edict of Fontainebleau", "Levi's Stadium", "ten million", "the Lippe", "Video On Demand content", "time and storage", "semester", "the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "Lawrence", "League of the Three Emperors", "science", "143,007", "Bill Clinton", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "drawn the name out of a hat", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Br'er Rabbit", "corruption", "24", "Dover Beach"], "metric_results": {"EM": 0.734375, "QA-F1": 0.8100077771493213}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-4919", "mrqa_squad-validation-4517", "mrqa_squad-validation-1187", "mrqa_squad-validation-8544", "mrqa_squad-validation-6676", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_squad-validation-3943", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616"], "SR": 0.734375, "CSR": 0.6822916666666667, "EFR": 0.9411764705882353, "Overall": 0.811734068627451}, {"timecode": 6, "before_eval_results": {"predictions": ["1540s", "the courts of member states", "its circle logo", "three", "negative", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "distorting the grana and thylakoids", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "overthrow a government", "entertainment", "vote clerk", "high growth rates", "a vicious and destructive civil war", "Sony", "Stagecoach", "Silk Road", "San Diego", "a German Nazi colonial administration", "four public charter schools on the South Side of Chicago", "the means to invest in new sources of creating wealth", "Spanish", "Structural geologists", "president and CEO", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Ben Johnston", "a large green dinosaur", "Taylor Swift", "Eric Edward Whitacre", "the Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "between 11 or 13 and 18", "Heather Langenkamp", "Henry Gwyn Jeffreys Moseley", "racing", "Vilnius Airport (IATA: VNO, ICAO: EYVI)", "It is based in Bury St Edmunds, Suffolk, England", "Charmed", "Lily Hampton", "Liverpool and England", "Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "Obuasi", "79", "Algeria", "romantic", "Biafra", "Polar Cub", "Atlantic City, New Jersey"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7980392156862746}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-9665", "mrqa_squad-validation-7983", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-226", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-13072"], "SR": 0.6875, "CSR": 0.6830357142857143, "EFR": 1.0, "Overall": 0.8415178571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "the working fluid", "a suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "solution", "those who already hold wealth", "the center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "the papacy", "through homologous recombination", "a modern canalized section", "protest against the occupation of Prussia by Napoleon", "improved markedly", "entire length", "computer programs", "General Conference of the United Methodist Church", "1996", "dreams", "The Judiciary", "a deterministic Turing machine", "Bart Starr", "allotrope", "Karluk Kara-Khanid", "Perth, Western Australia", "Ian Rush", "Gerry Adams", "New Orleans Saints", "2016", "four operas", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "Hanoi", "Sevens", "fennec", "Bart Conner", "fantasy", "Martin \"Marty\" McCann", "Black Mountain College", "a historic house museum", "Bothtec", "Cody Miller", "140 to 219", "the \"Father of Liberalism\"", "Christophe Lourdelet", "Pablo Escobar", "African", "Teotihuacan", "Sleeping Beauty", "PeopleMover", "8 December 1985", "Noddy", "Ali Bongo", "Honey Nut Chex", "Ray Harroun", "Drew Barrymore", "David Tennant"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7520833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-3091", "mrqa_squad-validation-1819", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-3885", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869", "mrqa_naturalquestions-validation-1618"], "SR": 0.6875, "CSR": 0.68359375, "EFR": 1.0, "Overall": 0.841796875}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Non-revolutionary", "lower heat addition temperature", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "5th Avenue laboratory fire", "arms", "two", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "TeacherspayTeachers.com", "stratigraphic", "commensal flora", "a + bi", "Dallas, Texas", "Han Chinese and Khitans", "from home viewers", "1330 Avenue of the Americas in Manhattan", "Alberta and British Columbia", "\"Pimp My Ride\"", "Don Johnson", "Money trees", "25 million", "8,515", "13 October 1958", "tailless", "Environmental Protection Agency", "between 1932 and 1934", "an English professional footballer", "Los Angeles", "England", "Armin Meiwes", "Ai-Ling Lee", "Miss Universe 2010", "Adam Amin", "boxer", "Boston University", "Fulham", "A55", "Hugh de Kevelioc", "\u00c6thelstan", "Madras Export Processing Zone", "44", "I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "diastema", "Shirley Horn", "Iran", "Bigfoot", "Papua New Guinea", "Renoir", "Manchester"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7055871212121212}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.3636363636363636, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6789", "mrqa_squad-validation-3391", "mrqa_squad-validation-1501", "mrqa_squad-validation-9859", "mrqa_squad-validation-8356", "mrqa_squad-validation-7643", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-3553", "mrqa_triviaqa-validation-3170"], "SR": 0.640625, "CSR": 0.6788194444444444, "EFR": 1.0, "Overall": 0.8394097222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["$32 billion", "centrifugal governor", "Orange County", "chloroplast peripheral reticulum", "1962", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "most organic molecules", "French", "Museum of the Moving Image in London", "sent missionaries, backed by a fund to financially reward converts to Catholicism", "pyrenoid and thylakoids", "Woodward Park", "Black's Law Dictionary", "25 May 1521", "essentially holy people", "diplomacy or military force", "increase in the land available for cultivation", "the value of the spin", "a pivotal event", "transgender teenage girl", "John Alexander", "David Michael Bautista Jr.", "Black Friday", "actor, singer and a DJ", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia", "Yasir Hussain", "USC Marshall School of Business", "Stephen James Ireland", "Marko Tapani \" Marco\" Hietala", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "4145 ft above mean sea level", "Central Park", "Robert John Day", "Tugurt", "James Tinling", "Italy", "2015 Masters Tournament", "Ulver and the Troms\u00f8 Chamber Orchestra", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "kuskusi", "more than 22 million", "morphine sulfate oral solution 20 mg/ml", "The Ferguson Library", "a species of freshwater airbreathing catfish"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6486222938588926}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4147", "mrqa_squad-validation-3440", "mrqa_squad-validation-2943", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_triviaqa-validation-2522", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.5625, "CSR": 0.6671875, "EFR": 0.9642857142857143, "Overall": 0.8157366071428571}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "the Mocama", "suburban", "vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "Panthers", "Sanders", "even greater inequality and potential economic instability", "Gamal Abdul Nasser", "Immunodeficiencies", "counterflow", "John B. Goodenough", "not covered in any newspapers", "arrows, swords, and leather shields", "Cybermen", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "Standard Model", "Tolui", "Rhine-Ruhr region", "course of study", "Prevenient grace", "Kansas State", "Captain Cook's Landing Place", "Chris Pine", "Yoo Seung-ho", "Battle of the Philippines", "NCAA Division I", "The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Uzumaki", "Tom Jones", "the RATE project", "Barbara Niven", "13\u20133", "Eliot Spitzer", "5,042", "European", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "Repertoire of Pl\u00e1cido Domingo", "Germany", "New Jersey", "Bath, Maine", "Ector County", "Jim Davis", "Buck Owens and the Buckaroos", "World Health Organization", "Emmanuel Ofosu Yeboah", "California", "Panic! at the Disco", "the Royal Firework Music", "Sir Giles Gilbert Scott", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills", "Comoros Islands", "Onomastic Sobriquets In The Food And Beverage Industry", "Dustbin"], "metric_results": {"EM": 0.625, "QA-F1": 0.7200217599745216}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.23255813953488372, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-797", "mrqa_squad-validation-7502", "mrqa_squad-validation-6927", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-5743", "mrqa_naturalquestions-validation-3837", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644"], "SR": 0.625, "CSR": 0.6633522727272727, "EFR": 0.9166666666666666, "Overall": 0.7900094696969697}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate", "Battle of Olustee", "French", "Only 100\u2013150", "Philo of Byzantium", "cooler", "in marine waters worldwide", "$60,000", "his mother", "shock", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "new element", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "It's helping consumers move beyond these hard times and has reignited a whole industry", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "on the Ohio River near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "McCartney", "the deal, which promises cabinet positions to the splinter group of the Movement for Democratic Change, does not involve MDC head Morgan Tsvangirai", "a homicide", "200", "The drug is legal for medical use, but it is trafficked into Hong Kong from other parts of Asia", "opposition party members", "Missouri", "President Obama's race in 2008", "executive director of the Americas Division of Human Rights Watch", "Casa de Campo International Airport", "90", "The station", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "her home", "the Employee Free Choice act", "Obama", "more than 200", "This is not a project for commercial gain", "best-of-three series", "Kaka", "Japanese", "Dan Parris, 25, and Rob Lehr", "Fayetteville", "two", "$2 billion", "Jacob", "Molotov cocktails, rocks and glass", "as many as 250,000", "Andrew Morris", "the Ark of the Covenant", "Jean F Kernel", "Thomas Hardy", "Richmond", "1994", "The Conjuring", "The Gallipoli Campaign", "Lake Michigan", "Nowhere Boy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6382810115231989}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.4, 1.0, 0.0, 0.0625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3087", "mrqa_squad-validation-4611", "mrqa_squad-validation-4524", "mrqa_squad-validation-1313", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-373", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.53125, "CSR": 0.65234375, "EFR": 1.0, "Overall": 0.826171875}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British.", "wealth", "God's grace (which cannot be earned) alone can make them just.", "Napoleon", "new technology and machinery", "Arley D. Cathey", "private actors", "Bell Northern Research", "a body of treaties and legislation,", "1227", "lower lake", "three", "Elders", "587,000", "Private Bill Committees", "Mark Ronson", "the Catechism", "beneath the university's Stagg Field", "Ian Botham", "Pyotr Ilich Tchaikovsky", "Vincent Motorcycle Company", "Simon \"Sam\" Marx", "Salvador Allende", "Marie Antoinette", "Hawaii", "Erik Thorvaldson", "Apollon", "Pal Joey", "Mary Seacole", "green", "Indonesia", "supreme religious leader of the Israelites", "Antonio", "European Economic Community", "Christine Keeler", "Jesus", "Nicholson", "four", "Netherlands", "\"Sugar Baby Love\"", "Coretta Scott", "Sean", "John Denver", "Stage 1", "Travis", "Blue Peter", "Robert Kennedy", "Q", "Paris merchant Jean Marius", "a French author and philosopher", "barber", "Fred Stolle", "Murrah Federal Office Building", "Evita", "a litter of pipes on the mantelpiece", "fortified complex", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "\"It is very easy for comments to be taken out of context and create unnecessary drama -- especially between us women,\"", "a delegation of American Muslim and Christian leaders", "Anne of Cleves", "USC Columbia", "Juan Martin Del Potro."], "metric_results": {"EM": 0.53125, "QA-F1": 0.5837735615079365}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.05555555555555556, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-5431", "mrqa_squad-validation-7974", "mrqa_squad-validation-9418", "mrqa_squad-validation-670", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6974", "mrqa_triviaqa-validation-712", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-5929"], "SR": 0.53125, "CSR": 0.6430288461538461, "EFR": 0.9, "Overall": 0.7715144230769231}, {"timecode": 13, "before_eval_results": {"predictions": ["Polignac's conjecture", "Chilaun", "Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "in Medieval Latin, 9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "to counteract the constant flooding and strong sedimentation in the western Rhine Delta", "Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees", "Dublin, Cork, Youghal and Waterford", "Tangled", "dernell", "moles", "Democritus", "Kwajalein Atoll", "Catherine of Aragon", "julius k Roosevelt", "Steve McQueen", "Portugal", "jazz piano", "two", "kopassus", "in the northwest of England", "dry Ice", "zanesville and Zanesville", "Lucas McCain", "Antarctica", "mercury gilding", "aniridia", "t.S. Eliot", "River Forth", "woe", "NOW Magazine", "Burmese", "Italy", "Canada", "typhoid fever", "juliano Pavarotti", "action figure", "Walt Kowalski-Gran Torino", "2010", "volume of a given mass of a gas", "Venezuela", "al Laurel and Hardy", "temperature", "40", "phrenology", "San Francisco", "Fall 1998", "Marcus Atilius Regulus", "Christopher James \" Chris\" Weidman", "Drillers Stadium", "one", "Virgin America", "Nate O'Reily", "al Jazeera English", "Iran's parliament speaker", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory."], "metric_results": {"EM": 0.5, "QA-F1": 0.5567031926406926}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909]}}, "before_error_ids": ["mrqa_squad-validation-1002", "mrqa_squad-validation-9233", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.5, "CSR": 0.6328125, "EFR": 0.96875, "Overall": 0.80078125}, {"timecode": 14, "before_eval_results": {"predictions": ["in an adult plant's apical meristems", "tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary", "Beijing", "three years", "27 July 2008", "chemically", "Aristotle", "St. George's Church", "Missy", "the University of Aberdeen", "public official", "the most cost efficient bidder", "acorn tree", "gaius", "thighbone", "alpine", "Ukrainian Soviet republic", "kungo", "Clark Kent", "fire", "amber", "high school football", "The executioner's Song", "right angle", "an A-Z of Central Asia", "anamosa", "grouchy", "Ephesus", "asylum", "film", "knife", "glare", "Cologne", "gaius caesar augusta", "crucible", "Kosovo", "James Jeffords", "Prague", "tennis", "silk", "cowboys", "a natural-born citizen", "white-tailed deer", "scholastic.com", "burt Reynolds", "kung fu", "boys", "windjammer", "stanley joe", "germanicus", "Augusta", "counter clockwise", "2013", "Nick Hornby", "Coldplay", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama", "memories of his mother", "Israel"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4427083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-405", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-15235", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-6518", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-325", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3084"], "SR": 0.390625, "CSR": 0.6166666666666667, "EFR": 0.9487179487179487, "Overall": 0.7826923076923077}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes)", "river Deabolis", "April 20", "Rhenus", "1996", "wine", "German", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "crossword", "Cuba Gooding Jr.", "Strongsville, Ohio", "Flemish", "Mastercard", "Robert C. Stempel", "Nashville", "olfactory nerve", "Ivan the Terrible", "Nancy Astor", "liver spot", "Kurt Russell", "Toronto Maple Leaf", "Zsa Zsa Gabor", "performance process", "Delaware", "sugarcane", "(Rabbit) Angstrom", "Johann Strauss II", "joey", "pro bono", "Bologna", "The Fun Factory", "a brown", "Manfred von Richthofen", "Nacho Libre", "copper", "black magic or of dealings with the devil", "hemlock", "Dr. Wigand", "National Poetry Month", "a sesame seed bun", "meager", "Casablanca", "blimps", "( Gustav Kirchhoff)", "a geisha", "Bigfoot", "altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "dark", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6227430555555555}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6463", "mrqa_squad-validation-9248", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-7531", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-14698", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.53125, "CSR": 0.611328125, "EFR": 0.9666666666666667, "Overall": 0.7889973958333334}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite", "respiration", "1997", "the late 1920s", "\u00a31.3bn", "27 July 2008", "unequal", "October 1973", "dragonnades", "Isiah Bowman", "an assembly center", "Ominde Commission", "the Weser", "an open casket on the dining room table", "(Ho) Minh", "a penis", "the Inuit's dwelling", "Detroit Rock City", "the Toronto Blue Jays", "President Lincoln", "Ray Bradbury", "crimes committed out of hatred for someone's race", "Madagascar", "Nicolas Sarkozy", "Rubicon", "(Conello)", "17", "(Louisa) May Alcott", "Play-Doh", "Aphrodite", "the nails", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Clinton", "King Philip", "Bellerophon", "Balaam", "the Wharton School", "the Caine Mutiny", "the Rolling Stone", "(founded 1932)", "(John) Coltrane", "the peace sign", "oxygen", "Sphinx", "Jan Hus", "USA Network", "the Mavericks", "Onegin", "J.L. Hudson's Department Store", "a spinning jenny", "(Santa) Claus", "(Benicio) Snchez", "a doctor", "judges", "attached to another chromosome", "the City of Preston", "Australia", "The Jefferson Memorial", "aged between 11 or 13 and 18", "Michoacan Family", "( Brad Blauser)", "salary", "punishment"], "metric_results": {"EM": 0.46875, "QA-F1": 0.606908195970696}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.8, 0.5, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-12183", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-6610", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-794", "mrqa_triviaqa-validation-4973", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-1759"], "SR": 0.46875, "CSR": 0.6029411764705883, "EFR": 0.9117647058823529, "Overall": 0.7573529411764706}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Tesla", "ring", "Party of National Unity", "22", "Dauphin", "Phillip Marlowe", "piracy", "Roger Clemens", "The Crystal Method", "Puerto Rico", "The Mausoleum", "The World Through More Than One lens", "Switzerland", "Sabena", "The Old Man", "French", "(Bob Fitzsimmons)", "the Nemean lion", "d'Artagnan", "the Bayeux Tapestry", "a mirror", "China", "Shia", "notes placed at the bottom of a page", "Stephen Hawking", "Cicero", "Memphis", "Mountain Dew", "A Streetcar Named Desire", "Quilt", "FRAM", "the House of Representatives", "a Belgian-owned Canadian beer company", "Michael Moore", "Oman", "Chevy", "Ingenue", "Pennsylvania", "El burlador de Sevilla", "Ian Fleming", "Headless Horseman", "London", "Yellowstone", "Ronald Reagan", "Fiddler on the Roof", "Ethiopian", "six 50 minute ( one - hour with advertisements ) episodes", "1992", "a positive +1 ion", "Bromley", "the Ruul", "Cartoon Network", "Caylee Anthony", "know what is important in life", "Rabbani, a former Afghan president who had been leading the Afghan peace council, was killed in an attack at his home.", "Iran's development of a nuclear weapon", "The drama of the action in-and-around the golf course has enraptured fans of the game through the generations and around the world."], "metric_results": {"EM": 0.65625, "QA-F1": 0.6821875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16, 0.33333333333333337, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1659", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-11215", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-1920", "mrqa_searchqa-validation-2355", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-2709", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7140", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-4110"], "SR": 0.65625, "CSR": 0.6059027777777778, "EFR": 0.9545454545454546, "Overall": 0.7802241161616161}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXI", "1993", "June 1979", "friend", "tentacles", "Robert R. Gilruth", "computational", "same-gender marriages", "During the 2006 Israel-Lebanon conflict", "the mid-18th century", "orange", "A Raisin in the Sun", "Sistine Chapel", "bielaruski", "(one tight end and one", "a trowel", "Big Bang", "Sex Pistols", "endodontist", "Saturn", "White Cliffs of Dover", "Genoa", "\"Who is John Galt?\"", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Indiana", "Seattle", "roses", "The Hampton Inn", "a gold palm", "(John) Janetzko", "Copeina arnoldi", "Paul McCartney", "omega-3", "Raphael", "Bachman Turner Overdrive", "horror", "Caddy Shack", "Tokyo", "Panama", "Eppson Center", "Narnia", "Finnegans Wake", "(William) Wordsworth", "Norway", "bears", "nuclear bombs", "(Judas) Iscariot", "elephant", "mazurka", "Sweden", "covert", "Our Country", "May 2010", "in 2014 Blue Bell was the best - selling ice cream brand in the United States", "Guanabara bay", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "the estate", "McFerrin, Robin Williams, and Bill Irwin", "ase", "Michigan and surrounding states and provinces"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5852557684973303}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3157894736842105, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.1739130434782609]}}, "before_error_ids": ["mrqa_squad-validation-499", "mrqa_squad-validation-1696", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-4853", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-2870"], "SR": 0.46875, "CSR": 0.5986842105263157, "EFR": 0.9705882352941176, "Overall": 0.7846362229102166}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "genetically modified", "the Earth", "more than 53,000", "one", "Israeli poet", "two points", "20,000", "the kip", "skeletal muscle and the brain", "2014", "ambiguous", "Montreal", "Sunday evenings", "sperm and ova", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "Arousal regulation", "Charlene Holt", "Ted '' Levine", "1991", "electron shells", "Cornett family", "Acid rain", "October 22, 2017", "inefficient", "he cheated on Miley", "2001", "flawed democracy", "735 feet", "1871", "Ric Flair", "Toledo, Bowling Green, and Mount Union", "a form of business network", "a cylinder of glass or plastic", "James Hutton", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Necator americanus and Ancy Lostoma duodenale", "February 29", "The Lykan", "issues of the American Civil War", "A proton motive force drives protons down the gradient ( across the membrane ) through the proton channel of ATP synthase", "Cecil Lockhart", "Mara Jade", "British and French Canadian fur traders", "semi-autonomous organisational units within the National Health Service in England", "Lou Rawls", "Hermia", "Jupiter", "Latin", "15", "John Robert Cocker", "Silvan Shalom", "a simple puzzle video game", "a palace", "the olfactory nerve", "eucalyptus", "a horse", "oxygen"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6841399485930736}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.625, 1.0, 0.2, 1.0, 0.5, 1.0, 0.6666666666666666, 0.7499999999999999, 0.09090909090909091, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8896", "mrqa_squad-validation-880", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_triviaqa-validation-2227"], "SR": 0.578125, "CSR": 0.59765625, "EFR": 0.9629629629629629, "Overall": 0.7803096064814814}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "the Cloth of St Gereon", "Thomas Sowell", "more than 70", "death of a heretic", "choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape of a 15-year-old girl", "illegal crossings into U.S. waters", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer in five vignettes", "well over 1,000 pounds", "political dead-end", "Mutassim", "from Texas and Oklahoma to points east", "Polo", "his mother", "Amstetten", "computer problems", "Silvan Shalom", "Climatecare", "Steve Jobs", "12-hour-plus", "Brad Blauser", "childbirth", "consumer confidence", "5:20 p.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "India", "1964", "Davidson", "Pakistan's combustible Swat Valley", "Friday", "1979", "the United States", "GospelToday", "chief executive officer", "no chance", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Giovani dos Santos is set to take up the vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast midfielder Yaya Toure in the non-EU berths permitted under Spanish Football Federation (RFEF) rules", "Michael Schumacher", "Gustav", "gun", "Henrik Stenson", "orphanage in Abeche", "40", "Derek Mears", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter festivals", "Whitsunday", "Dee", "\"Dumb and Dumber\"", "the Sugar Bowl", "Earl Warren", "converging", "autu", "season five", "The Force uprising ( 2015 )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6029254302897394}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.8750000000000001, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.052631578947368425, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7266", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-3226", "mrqa_hotpotqa-validation-1094", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.515625, "CSR": 0.59375, "EFR": 0.967741935483871, "Overall": 0.7807459677419355}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress among teachers.", "Combined Statistical Area", "chief electrician", "Newton", "static friction", "the assassination of US President John F. Kennedy", "responsibility for the abductions", "Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Kenneth Cole", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "\"no more than an official of the most tyrannical dictatorial state in the world.", "\"Golden Girls\"", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Wednesday.", "Cash for Clunkers", "\"I'm certainly not nearly as good of a speaker as he is.\"", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad, Iran.", "Amanda Knox's aunt", "jazz", "$530 million in debt", "\"Doogie Howser, M.D.\"", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two contestants.", "Bill", "J.G. Ballard", "a nurse who tried to treat Jackson's insomnia", "Sarah", "\"They left without me,'", "1981", "\"17 Again,\"", "Nigeria", "$80,000 a year", "Republicans", "EU naval force", "Kate Hudson", "Omar Bongo", "the Delta Queen steamboat", "Hyundai Steel", "skeletal dysplasia,", "London Heathrow's Terminal 5.", "canceled the swimming privileges of a nearby day care center", "February 12", "more than 30 Latin American and Caribbean nations", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "military strike", "White House Executive Chef", "Russell Huxtable", "Willy Russell", "Budapest", "\"Mortal Kombat X\"", "The Kingdom of Northumbria", "\"Get Thee To A Nunnery\"", "Romanian Communist Party", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Otto Eduard Leopold"], "metric_results": {"EM": 0.53125, "QA-F1": 0.625686435704818}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.5714285714285715, 1.0, 1.0, 0.11764705882352941, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-251", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107"], "SR": 0.53125, "CSR": 0.5909090909090908, "EFR": 1.0, "Overall": 0.7954545454545454}, {"timecode": 22, "before_eval_results": {"predictions": ["several experimental setup to produce X-rays.", "WMO Executive Council and UNEP Governing Council", "Saxon chancellery", "New York and Virginia,", "two", "glowed even when turned off.", "five female pastors", "resources that could sustain future exploration of the moon and beyond.", "sovereignty over them.", "April 6, 1994", "Prague", "\"project work\"", "a federal judge in Mississippi", "the department has been severely affected by the earthquake,", "$22 million", "a few family treasures.", "a music video on his land.", "\"wildcat\" strikes", "\"Watchmen\"", "The Real Housewives of Atlanta", "18", "88", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "a president who understands the world today, the future we seek and the change we need.", "military trials for some Guant Bay detainees.", "Vrishti Bhowmik.", "Larry King", "Steven Chu", "racially motivated.", "Michael Partain", "male veterans struggling with homelessness and addiction.", "it will be the longest domestic relay in Olympic history,", "Zimbabwe's dire economic situation.", "No. 1", "nine", "Four bodies", "Friday", "Kingdom City", "Rima Fakih", "two contestants.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Alwin Landry's supply vessel Damon Bankston", "researchers", "involvement during World War II in killings at a Nazi German death camp in Poland.", "opium", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "84-year-old", "Robert Park", "Rima Fakih", "Isthmus of Corinth", "Nalini Negi", "( 2017 - 12 - 10 )", "Runcorn", "collarbone", "geographic horizon", "UFC 50: The War of '04", "June 11, 1973", "San Diego County Fair", "Toy Story", "Viva Zapata", "A Fairy Tale of Home"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5565307878307398}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.8235294117647058, 1.0, 0.5, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.9565217391304348, 0.10256410256410256, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-2356", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-1475", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.421875, "CSR": 0.5835597826086957, "EFR": 0.972972972972973, "Overall": 0.7782663777908343}, {"timecode": 23, "before_eval_results": {"predictions": ["phycobilin phycoerytherin", "was lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition", "Behind the Sofa", "Tulsa, Oklahoma.", "56", "Yemen", "2005", "Karen Floyd", "six Iraqis and wounded 10 others", "the two bodies", "Haiti", "Susan Boyle", "Saturday", "Spain", "Jared Polis", "Janet and La Toya", "Dangjin", "30", "Michael Krane,", "lightning strikes", "Evans", "Italian government", "flooding was so fast that the thing flipped over", "threatening messages", "Noriko Savoie", "drafting a new constitution", "he was bleeding profusely", "Abrahamson", "martial arts", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "then-Sen. Obama", "Congress", "curfew", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "once on New Year's Day and once in June, to mark the queen's \"official\" birthday.", "the government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.", "Zuma", "haute, bandeau-style little numbers", "nine", "Iraq", "September 11, 2001", "about 50", "a group of teenagers.", "in body bags on the roadway near the bus,", "al Fayed", "Desmond Tutu", "$17,000", "Jobs", "$81,880", "to provide school districts with federal funds", "a spiritual conversion", "Jason Lee", "REM sleep", "substantive", "Kent", "beer and soft drinks", "five aerial victories", "Cherokee River", "The U.S. Founders and Cyrus the Great of Persia", "an retired captain in the United States Navy", "Florida"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6447709050140342}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.15789473684210525, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-2957", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-10787"], "SR": 0.59375, "CSR": 0.583984375, "EFR": 0.9230769230769231, "Overall": 0.7535306490384616}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "Kathrin Hoelzl of Germany", "one of the shocks of the year", "him to step down as majority leader.", "United Nations World Food Program vessels carrying food and relief supplies to war-torn Somalia,", "gang rape of a 15-year-old girl on the campus of Richmond High School in Northern California", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "The Louvre", "his club", "continuously trying to best your own fuel economy achievements,\"", "1979", "one of its diplomats in northwest Pakistan", "jazz", "an antihistamine and an epinephrine auto-injector for emergencies,", "Bangladesh", "technology experts Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "12 million", "President Sheikh Sharif Sheikh Ahmed", "a government-run health facility that provides her with free drug treatment.", "Britain's Got Talent", "military personnel", "behind the counter.", "11", "one Iraqi soldier,", "A man who was raised at Camp Lejeune", "her fianc\u00e9", "racial intolerance.", "animal products.", "El Viceroy", "Symbionese Liberation Army", "8.8 million", "to work together to stabilize Somalia and cooperate in security and military operations.", "would compromise the public broadcaster's appearance of impartiality.", "black is beautiful", "$104,327,006 paid for \"L'Homme Qui Marche I, bronze\" (Walking Man 1), 1960, by Alberto Giacometti.", "Picasso's muse and mistress, Marie-Therese Walter.", "to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of disaster assistance for parts of the Midwest that have been hit by record floods.", "off the coast of Dubai", "fallen comrades lost in the heat of battle.", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "27 Awa", "Mark Obama Ndesandjo", "\"Dance\" rated highly for Oxygen,", "famous faces like NHL hockey star Alexander Ovechkin, Russian Vogue editor in chief Aliona Vodianova, acclaimed conductor Valery Gergiev, the \"Russian Madonna\" singer Valeriya, and London-", "\"Steamboat Bill, Jr.", "fatally shooting a limo driver on February 14, 2002.", "nucleus", "Treaty of Paris between France and the Sixth Coalition, and the Treaty of Kiel", "Sebastian Lund ( Rob Kerkovich )", "President Obama", "Tom Watson", "Sandi Toksvig", "Hispania Racing F1 Team", "prime minister", "Walt Disney World Resort in Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.390625, "QA-F1": 0.506870318552062}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.18181818181818182, 1.0, 0.4, 0.10526315789473685, 1.0, 0.0, 1.0, 0.8, 1.0, 0.25, 0.5714285714285715, 0.33333333333333337, 0.0, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.125, 1.0, 0.08, 0.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-4927", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.390625, "CSR": 0.5762499999999999, "EFR": 0.9743589743589743, "Overall": 0.7753044871794872}, {"timecode": 25, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-3952", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4548", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-4924", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-729", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-3637", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8638", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3254", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-49", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12150", "mrqa_searchqa-validation-12205", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-13756", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14282", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14700", "mrqa_searchqa-validation-14743", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-2355", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-4973", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6870", "mrqa_searchqa-validation-7227", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7584", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-8335", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-87", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10026", "mrqa_squad-validation-10026", "mrqa_squad-validation-10100", "mrqa_squad-validation-10254", "mrqa_squad-validation-10406", "mrqa_squad-validation-10418", "mrqa_squad-validation-1146", "mrqa_squad-validation-1166", "mrqa_squad-validation-1187", "mrqa_squad-validation-1218", "mrqa_squad-validation-126", "mrqa_squad-validation-1295", "mrqa_squad-validation-1313", "mrqa_squad-validation-1341", "mrqa_squad-validation-1407", "mrqa_squad-validation-1501", "mrqa_squad-validation-1549", "mrqa_squad-validation-159", "mrqa_squad-validation-1640", "mrqa_squad-validation-1662", "mrqa_squad-validation-1692", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1775", "mrqa_squad-validation-1877", "mrqa_squad-validation-1906", "mrqa_squad-validation-1960", "mrqa_squad-validation-2049", "mrqa_squad-validation-2059", "mrqa_squad-validation-2105", "mrqa_squad-validation-2113", "mrqa_squad-validation-2136", "mrqa_squad-validation-2207", "mrqa_squad-validation-2435", "mrqa_squad-validation-2466", "mrqa_squad-validation-2518", "mrqa_squad-validation-2530", "mrqa_squad-validation-281", "mrqa_squad-validation-2833", "mrqa_squad-validation-2858", "mrqa_squad-validation-2941", "mrqa_squad-validation-298", "mrqa_squad-validation-3091", "mrqa_squad-validation-3100", "mrqa_squad-validation-3127", "mrqa_squad-validation-3132", "mrqa_squad-validation-3149", "mrqa_squad-validation-3259", "mrqa_squad-validation-3260", "mrqa_squad-validation-3312", "mrqa_squad-validation-3319", "mrqa_squad-validation-3440", "mrqa_squad-validation-3454", "mrqa_squad-validation-3524", "mrqa_squad-validation-3632", "mrqa_squad-validation-3716", "mrqa_squad-validation-3813", "mrqa_squad-validation-3862", "mrqa_squad-validation-3865", "mrqa_squad-validation-3918", "mrqa_squad-validation-3943", "mrqa_squad-validation-4010", "mrqa_squad-validation-4047", "mrqa_squad-validation-4075", "mrqa_squad-validation-4078", "mrqa_squad-validation-4083", "mrqa_squad-validation-4102", "mrqa_squad-validation-4175", "mrqa_squad-validation-4315", "mrqa_squad-validation-4429", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-457", "mrqa_squad-validation-4673", "mrqa_squad-validation-4706", "mrqa_squad-validation-4770", "mrqa_squad-validation-4775", "mrqa_squad-validation-4844", "mrqa_squad-validation-4973", "mrqa_squad-validation-498", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5023", "mrqa_squad-validation-5037", "mrqa_squad-validation-5102", "mrqa_squad-validation-5135", "mrqa_squad-validation-5178", "mrqa_squad-validation-5194", "mrqa_squad-validation-5213", "mrqa_squad-validation-5226", "mrqa_squad-validation-526", "mrqa_squad-validation-5486", "mrqa_squad-validation-549", "mrqa_squad-validation-5513", "mrqa_squad-validation-5581", "mrqa_squad-validation-5741", "mrqa_squad-validation-5784", "mrqa_squad-validation-5812", "mrqa_squad-validation-5863", "mrqa_squad-validation-5871", "mrqa_squad-validation-5876", "mrqa_squad-validation-5972", "mrqa_squad-validation-6029", "mrqa_squad-validation-6059", "mrqa_squad-validation-6080", "mrqa_squad-validation-6121", "mrqa_squad-validation-6154", "mrqa_squad-validation-6166", "mrqa_squad-validation-6177", "mrqa_squad-validation-6242", "mrqa_squad-validation-6430", "mrqa_squad-validation-6588", "mrqa_squad-validation-6598", "mrqa_squad-validation-6614", "mrqa_squad-validation-6676", "mrqa_squad-validation-6685", "mrqa_squad-validation-6694", "mrqa_squad-validation-6721", "mrqa_squad-validation-6741", "mrqa_squad-validation-6789", "mrqa_squad-validation-6789", "mrqa_squad-validation-6801", "mrqa_squad-validation-6875", "mrqa_squad-validation-6921", "mrqa_squad-validation-7135", "mrqa_squad-validation-7159", "mrqa_squad-validation-716", "mrqa_squad-validation-7173", "mrqa_squad-validation-7229", "mrqa_squad-validation-7273", "mrqa_squad-validation-7434", "mrqa_squad-validation-7458", "mrqa_squad-validation-7576", "mrqa_squad-validation-7596", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-7967", "mrqa_squad-validation-7981", "mrqa_squad-validation-80", "mrqa_squad-validation-8035", "mrqa_squad-validation-8151", "mrqa_squad-validation-8176", "mrqa_squad-validation-8343", "mrqa_squad-validation-8356", "mrqa_squad-validation-8397", "mrqa_squad-validation-8420", "mrqa_squad-validation-8439", "mrqa_squad-validation-8485", "mrqa_squad-validation-8503", "mrqa_squad-validation-855", "mrqa_squad-validation-855", "mrqa_squad-validation-8608", "mrqa_squad-validation-8616", "mrqa_squad-validation-8719", "mrqa_squad-validation-8733", "mrqa_squad-validation-880", "mrqa_squad-validation-880", "mrqa_squad-validation-8833", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-890", "mrqa_squad-validation-8914", "mrqa_squad-validation-8924", "mrqa_squad-validation-9020", "mrqa_squad-validation-9066", "mrqa_squad-validation-913", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9220", "mrqa_squad-validation-9237", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9299", "mrqa_squad-validation-9333", "mrqa_squad-validation-940", "mrqa_squad-validation-9406", "mrqa_squad-validation-9436", "mrqa_squad-validation-9470", "mrqa_squad-validation-9559", "mrqa_squad-validation-962", "mrqa_squad-validation-9665", "mrqa_squad-validation-9686", "mrqa_squad-validation-9752", "mrqa_squad-validation-9753", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_squad-validation-9931", "mrqa_squad-validation-9960", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2794", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-339", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4945", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6753", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7367", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-980"], "OKR": 0.791015625, "KG": 0.45234375, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "large scale", "Anthony Hopkins", "New Zealand", "Tamar", "rhododendron", "35", "a market where members join together as syndicates to insure risks.", "beetle", "arthropods", "Ub Iwerks", "Westminster Abbey", "holography,", "Pelias", "Sarah Collins", "Northumbria", "Harvard", "cricketer", "Seymour Hersh,", "Noun", "copper and zinc", "Tigris", "Cordelia", "there were many kinds of publications that told both news and rumours.", "seborrheic dermatitis", "33", "a varietal", "Joseph Smith,", "Chicago", "palladium", "the moon", "13.", "a petticoat", "The Virgin Spring,", "Canada", "Clement Attlee", "Stockholm", "Peter Parker", "Golda Meir", "Lesa Ukman", "bullfight", "Sparks", "Ginger Rogers", "the Rock of Gibraltar", "Comedy Playhouse", "citric", "Charles Darwin", "John Denver,", "Mr. Boddy", "Marie Van Brittan Brown", "southern California", "1995", "Bourbon", "Taylor Swift.", "Rihanna", "had his personal.40-caliber pistol,", "The Detroit, Michigan, radio station promotion held three years ago was like a class to help women \" learn how to dance and feel sexy,\"", "Amy Bishop,", "cactus", "the Louvre", "Seaver College"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5705639367816092}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.16666666666666666, 0.4827586206896552, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-8618", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-7521", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.515625, "CSR": 0.5739182692307692, "EFR": 0.9354838709677419, "Overall": 0.7028960530397022}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Day of the Doctor\"", "third", "affordable housing", "Mao Zedong", "Verona", "Pontiac Silverdome", "tusks", "a charcoal powered grill, stove or teppanyaki", "Frank McCourt", "jules Verne", "j Judy Cassab", "sturgess", "Schengen Area", "\u201cA\u201d", "city of the United Kingdom", "Famous Players Corporation", "the Monkees", "Gerald Durrell", "jzebel", "County Cork", "jason", "Arabian", "Halifax", "Noises Off", "jimmy osmond", "Frank Wilson", "Carlos the Jackal", "Edwina Currie", "julia Lipnitskaya", "stanley", "1768", "For Gallantry", "Friday's", "Abkhazia and South Ossetia", "Cahaba", "Ever Decreasing Circles", "Tahrir Square", "plutonium", "Count de La F\u00e8re", "27", "Jack Ruby", "Jacopo tintoretto", "Eric Coates", "Saudi Arabia", "\"Reckless Burning\"", "Thailand", "Sydney", "a dove", "Tunisia", "Prince Philip", "Apsley House", "Tokyo", "Edgar Lungu", "49 cents", "a resting heart rate over 100 beats per minute", "672", "\"Linda McCartney's Life in Photography\"", "Robert Frost's former home in Franconia, New Hampshire,", "an @", "Juan Martin Del Potro.", "27", "Edgar Allan Poe", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.515625, "QA-F1": 0.562689393939394}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7774", "mrqa_squad-validation-8026", "mrqa_triviaqa-validation-5980", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-4910", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-12829"], "SR": 0.515625, "CSR": 0.5717592592592593, "EFR": 1.0, "Overall": 0.7153674768518519}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "70", "forced Tesla out leaving him penniless.", "benazir butto", "Iran's nuclear program.", "at least 27", "former Beatles", "FBI Special Agent Daniel Cain,", "acid", "Wally", "2008", "after Wood went missing off Catalina Island,", "Rima Fakih", "Afghanistan", "Florida's Everglades.", "dilshan scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai on Wednesday.", "1950s", "64", "Iran's parliament speaker", "27-year-old", "Alexandros Grigoropoulos", "about $163 million (180 million Swiss francs)", "baggage from the 80s and has grown beyond a resort town into something more substantial.", "carbon neutrality", "oaxaca,", "Orbiting Carbon Observatory", "Switzerland", "jonathan armstrong", "Janet and La Toya", "more than 22 million", "hours", "combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "he's burned badly on the backs of his knees and every time he moves his knee, it pulls, and it starts to bleed.", "al-Shabaab", "booked on an outstanding arrest warrant relating to a domestic violence case,", "sustain future exploration of the moon and beyond.", "his business dealings", "opryland", "numbering", "Ferry vessels have been crowded, \"but they're not going anyplace we wouldn't expect them to,\"", "he was diagnosed with skin cancer.", "al Qaeda", "president Barack Obama", "\"gotten the balance right\"", "The oceans", "slashed open his left eyebrows. They burned his back as well", "doctors", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "1932", "between 1923 and 1925", "gilda", "jeremy dryden", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire", "Benjamin Disraeli", "sun"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5798609430383961}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.125, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.20689655172413793, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-14496"], "SR": 0.46875, "CSR": 0.5680803571428572, "EFR": 1.0, "Overall": 0.7146316964285714}, {"timecode": 28, "before_eval_results": {"predictions": ["Bermuda 419 turf", "25-foot", "manipulates symbols", "Hyundai Steel", "Monday night", "Bailey, Colorado", "most of the children also probably come from Chadian villages along Chad's border with Sudan.", "40", "the Illuminati", "in a public housing project,", "toxic smoke", "Lucky Dube,", "two Israeli soldiers,", "space shuttle Discovery", "Gavin de Becker", "a nuclear weapon", "in Japan", "Arizona", "Southeast Asia", "Tetris", "outside influences", "compromise the public broadcaster's appearance of unbiasedity.", "flipped and landed on its right side,", "suppress the memories and to live as normal a life as possible;", "Tuesday in Los Angeles.", "immediately appeal the ruling and seek a stay of the order with the U.S. Court of Appeals for the District of Columbia.", "the area was sealed off,", "knocking the World Cup off the front pages for the first time in days.", "21 million barrels of oil every day in the United States", "\"project work\"", "\"Oprah: A Biography,\"", "80 percent", "London's", "to try to make life a little easier for these families by organizing the distribution of wheelchair,", "Ozzy Osbourne", "$50", "Australian officials", "Capitol Records,", "Bill Klein,", "gun", "At least 38", "Argentina", "underprivileged.", "Somalia's piracy problem was fueled by environmental and political events.", "\"State of Play\"", "Kabul", "22", "Steven Gerrard", "12.3 million", "the area was sealed off,", "Rima Fakih", "Old Trafford", "to help bring creative projects to life ''", "season two", "Mary Elizabeth Patterson", "Mozart's", "The Fifth Amendment", "Nepal", "Merck Sharp & Dohme", "Fort Albany", "Knoxville, Tennessee", "Jawaharlal Nehru", "Transpiration", "hypomanic"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5393836152882205}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.31578947368421056, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1789", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-79", "mrqa_hotpotqa-validation-4763", "mrqa_searchqa-validation-10091", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.40625, "CSR": 0.5625, "EFR": 0.9210526315789473, "Overall": 0.6977261513157894}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "ancient cult activity", "US - grown fruit", "sex organs", "August von Mackensen", "galaxy", "August 6", "Doug Diemoz", "Colony of Virginia", "Monk's Caf\u00e9", "central plains", "Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "maintenance utility", "July 4, 1776", "pick yourself up and dust yourself off and keep going '", "John Garfield", "captains of sailing ships to cross the world's oceans for centuries", "October 12, 1979", "Lorazepam", "the 2013 non-fiction book of the same name by David Finkel", "Cadillac", "Ethel `` Edy '' Proctor", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "Husrev Pasha", "Stephanie Judith Tanner", "palmar aspect of these fingers", "McFerrin", "Watson and Crick", "Gorakhpur", "Patris", "the Rashidun Caliphs", "Lake Powell", "ornament", "September 6, 2019", "its population", "substitute good", "Marries Veronica", "over 74", "1987", "cunnilingus", "October 2000", "New York City", "Mamata Banerjee", "a major worldwide economic downturn", "closing of the atrioventricular valves and semilunar valves", "Hermann Ebbinghaus", "The Miracles", "used their knowledge of Native American languages as a basis to transmit coded messages", "Donny Osmond", "mercenaries,", "George Herbert Walker Bush", "gmbH", "7.63\u00d725mm Mauser", "seven", "Muslim", "two remaining crew members from the helicopter,", "Saturday's Hungarian Grand Prix.", "Rickey Henderson", "Lake Baikal", "Manny Ramirez"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5342678115307425}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.7142857142857143, 1.0, 0.14814814814814817, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333336, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 0.9333333333333333, 1.0, 1.0, 0.20689655172413793, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-2194", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.40625, "CSR": 0.5572916666666667, "EFR": 0.9210526315789473, "Overall": 0.6966844846491227}, {"timecode": 30, "before_eval_results": {"predictions": ["address information", "Pleurobrachia", "1953", "AT&T", "western mainland Europe", "Chingachgook", "shoes.", "nine", "r Rashid Akmaev,", "acetylene", "a deeply rooted part of", "fiber", "red deer", "What's in a name?", "Winston Rodney", "sand", "Nanjing", "Custer", "Concorde", "the Sun King", "GILBERT & SullIVAN", "Barnes & Noble", "the Belgae", "Ned Lamont", "the Boston Marathon", "fibreboard", "tin", "song of Norway", "Frida Kahlo", "refused to attend the victor's inauguration", "\"Y\" 2 \"K\":", "Fat Man", "Hair", "William Randolph Hearst", "basalt", "ale", "primate", "telephone operator", "\"Year 3000\"", "Luther Jones", "The New Colossus", "yelp", "Richard Wagner", "Sarah, Duchess of York", "what those four years of college", "middleweight champion", "bronchodilators", "Forty", "Argon Glow Lamps", "Red", "a Chenard", "Earl Long", "Neil Patrick Harris", "Wyatt and Dylan Walters", "1999", "vitamin D", "three", "Alberto juantorena", "R&B", "Awake", "Doctor of Philosophy", "Pakistan", "in Atlanta in 1996.", "an African-American woman for the job."], "metric_results": {"EM": 0.390625, "QA-F1": 0.4213541666666667}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false], "QA-F1": [0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14740", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-3715", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-14012", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-282", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-723"], "SR": 0.390625, "CSR": 0.5519153225806452, "EFR": 1.0, "Overall": 0.711398689516129}, {"timecode": 31, "before_eval_results": {"predictions": ["non-Mongol physicians", "Prospect Park", "the laser", "volume", "a squint", "Breakfast at Tiffany's", "Diners' Club", "Christian Dior", "Pittsburgh", "Juliet", "Notre Dame", "Table Mountain", "Jamie", "His Majesty's Ship Bounty", "Cecil Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "Pennsylvania", "Mike Huckabee", "Queen", "monosodium glutamate", "Chance", "gampi", "Tenzing Norgay", "Samuel Beckett", "Rachel Carson", "Vietnam", "sports", "William Morris", "Franklin D. Roosevelt", "Prince William", "Betty Suarez", "R", "Zechariah", "New Jersey", "Lake Ontario", "Matt LeBlanc", "Baltimore", "Grant", "kismet", "Willy Wonka", "artillery", "aluminum", "Abraham Lincoln", "Ned Kelly", "uranium", "gravitational", "Isis", "quiveir", "Heroes", "on the two tablets", "organ transplant of a kidney into a patient with end - stage renal disease", "seven", "Geheimrat Dr. Max", "Rocky Marciano", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "February 20, 1978", "two years,", "Lee Probert", "as time goes on, it kind of becomes more and more of a phenomenon.\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5609375}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-8269", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-1379", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-10344", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-10042", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_triviaqa-validation-2878", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-2123"], "SR": 0.5, "CSR": 0.55029296875, "EFR": 1.0, "Overall": 0.71107421875}, {"timecode": 32, "before_eval_results": {"predictions": ["the same as the weight of the air that rushed back in", "Fresno Street and Thorne Ave", "the Black Death", "Kenneth", "John Stuart Mill", "Herman Melville", "CIA", "mezzo piano", "Rickey Henderson", "Benazir Bhutto", "D. carota ssp. maritimus", "John Grunsfeld", "Llados", "1976", "Galileo Descartes", "atoms", "Dust jacket", "Rudy Giuliani", "the Espionage Act", "Maryland", "Sif", "New Jersey", "The Omega Man", "a walk-in pantry", "a barrel", "the Summer Olympics", "Hugo Chvez", "Shamir", "Hinduism", "tin", "Diana", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Tiger Woods", "Los Angeles", "the East Wind", "Richard III", "cut waiting, improved care, transform", "the pen", "Croatia", "Douglas Adams", "Krogstad", "Hawaii", "Stephen Crane", "Prussia", "Sophocles", "Mark Cuban", "the Thought Police", "a bust", "Central Park", "Lewis Carroll", "Deathly Hallows", "Florida and into the town of Coconut Cove", "aeoline", "trumpet", "Mel Gibson", "2.1 million", "Edward James Olmos", "\"My Backyard\"", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6356592908902692}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913042, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-5449", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-13862", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-14835", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-7160"], "SR": 0.5625, "CSR": 0.5506628787878788, "EFR": 1.0, "Overall": 0.7111482007575758}, {"timecode": 33, "before_eval_results": {"predictions": ["the BBC", "pathogens, an allograft", "a large concrete block is next to his shoulder, with shattered pieces of it around him.", "hours", "28", "back at work", "Oxbow,", "201-262-2800", "opium", "\"I think she's wacko.\"", "the annual White House Correspondents' Association dinner", "Hussein's Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar", "the station", "women protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "Arkansas", "Cash for Clunkers", "environmental efforts", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers,", "different women coping with breast cancer in five vignettes.", "a missile", "Police", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium", "Roger Federer", "Brooklyn, New York, for Miami Beach, Florida,", "over 1000 square meters in forward deck space,", "CNN", "no chance", "St. Louis, Missouri", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "two years ago", "two", "a bald Bard with a small beard, and bags under his eyes", "the self-styled revolutionary Symbionese Liberation Army", "an \"explosion of violence.\"", "two tickets to Italy on Expedia.", "Colombia", "in-cabin lighting system", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka, seeking a win to level the series at 1-1,", "Appathurai", "$40 and a bread", "African National Congress Deputy President Kgalema Motlanthe", "the Ming dynasty", "George II", "2014 -- 15", "1919", "Javier Bardem", "Scotland", "Erika Girardi", "\" Terry the Tomboy\"", "Harriet Tubman", "Mrs. Potts", "\"Spokescandy.\"", "\"The Land of the Free and the Home of theBrave\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.7100425207961972}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7058823529411765, 0.5, 1.0, 0.11764705882352941, 1.0, 0.6, 0.0, 1.0, 1.0, 0.3333333333333333, 0.888888888888889, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6]}}, "before_error_ids": ["mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1981", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-1622", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.59375, "CSR": 0.5519301470588236, "EFR": 0.9615384615384616, "Overall": 0.703709346719457}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority", "Washington State's decommissioned Hanford nuclear site,", "in Yemen.", "bankruptcy", "nearly $2 billion in stimulus funds", "a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "South African", "children of street cleaners and firefighters.", "Joan Rivers", "$3 billion,", "hardship for terminally ill patients and their caregivers,", "Honduran", "Brazil", "three different videos that we like and want to know which ones you think are the best.", "strife in Somalia,", "Roy", "WBO welterweight title", "relatives of the five suspects,", "Meredith Kercher.", "that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman.", "Alicia Keys", "to work together to stabilize Somalia and cooperate in security and military operations.", "Friday,", "cancerous tumor.", "20", "Gary Brooker", "$1.5 million", "Matt Kuchar", "40", "model of sustainability.", "glamour and hedonism", "J. Crew.", "returning combat veterans could be recruited by right-wing extremist groups.", "543", "The patient, who prefers to be anonymous,", "Robert Gates", "Israel", "on 112 acres about 30 miles southwest of Nashville,", "hospitalized briefly three months ago after suffering a seizure while being interviewed on a TV show in Los Angeles, California.", "in Seoul,", "Nicole", "that she was going to be on the Olympic medals podium.", "next week.", "Adam Lambert", "regulators in the agency's Colorado office", "early detection and helping other women cope with the disease.", "Her husband and attorney, James Whitehouse,", "Hundreds of women protest", "Indian monks", "Lionel Hardcastle", "Stephen Lang", "Dick Van Dyke", "(Bokm\u00e5l)", "Beer", "Revengers Tragedy", "1972", "Ben Black Elk", "The Hogan Family", "the hippopotamus", "St Paul"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5604405097826151}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.125, 1.0, 0.6666666666666666, 0.0, 0.5, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 0.21428571428571425, 0.5, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.375, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-2761", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-4378", "mrqa_searchqa-validation-7879"], "SR": 0.453125, "CSR": 0.5491071428571428, "EFR": 1.0, "Overall": 0.7108370535714286}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts which they returned to Earth.", "Border Reiver", "July 4, 1826", "rum", "Nantucket", "an Islamic leadership position.", "maple sap", "Malibu", "Sisyphus", "sound", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "crabs", "Caliban", "Purple", "the Black Sea", "the Battle of the Little Bighorn", "the Shakers", "a bellwether", "Hermeneutics", "potato chips", "Boxer", "The Spiderwick Chronicles", "Mabel Harding", "Las Vegas", "choosing actors", "the Rose Bowl", "Norman Rockwell", "the beehive", "light tunais", "Napa", "Eurail France", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "Saturday Night Fever", "12 men", "Nancy Pelosi", "journal", "Jupiter", "Sadat", "a sundae", "Grace Evans,", "50 million cells per litre", "Volitan Lionfish", "HIV", "Edwin", "Bonnie Aarons", "Wednesday, 5 September 1666", "pop ballad", "Seth", "Lou Gehrig", "a small hole or tear in a piece of material,", "1949", "Aamir Khan", "My Gorgeous Life", "Argentine and Buenos Aires", "Raymond Thomas", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6299938725490196}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.35294117647058826, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-193", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-2343", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_searchqa-validation-12977", "mrqa_searchqa-validation-12788", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-1965"], "SR": 0.53125, "CSR": 0.5486111111111112, "EFR": 1.0, "Overall": 0.7107378472222222}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "Virginia", "the Mojave Desert", "bullion", "Supernanny", "the Atlantic", "Cincinnati", "mosque", "(Henry) Hudson", "the Peashooter", "Dry ice", "Roosevelt", "Entourage", "eels", "Philadelphia", "The Museum of Modern Art", "the Unicorn", "John C. Frmont", "Russia", "(BRA STREISAND)", "Hermann Hesse", "the Taj Mittal", "Johnson", "The Toreador Song", "Margaret Mitchell", "(La) Esmerelda", "Sultans of Swing", "Pandarus", "meaning", "Burt Reynolds", "the Sphinx", "Satchmo Armstrong", "Saudi Arabia", "American new wave", "Arby\\'s", "coffee", "Fikkle Fame", "(Robert) Burns", "The Incredible Hulk", "Winnipeg", "the Memphis Belle", "Burkina Faso", "the Central Pacific", "the office of solicitor general", "Icelandic", "a wolf", "(Alan) Lewan", "Edith Piaf", "Ivan IV", "a prologue", "clay", "master carpenter Anthony Mayfield", "Jack Gleeson", "Phil Hurtt", "animals Phobia", "Massachusetts", "the City of Starachowice", "W. P. Lipscomb", "2009", "Democratic", "meteorologist", "$104,327,006", "\"17 Again,\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6752", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-14520", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-11632", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-15286", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-9131", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-5571", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_triviaqa-validation-1125", "mrqa_hotpotqa-validation-2000", "mrqa_newsqa-validation-3951"], "SR": 0.5625, "CSR": 0.5489864864864865, "EFR": 0.9642857142857143, "Overall": 0.7036700651544401}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "KFC", "oats", "Mitt Romney", "Ivan the Terrible", "Sally Field", "1927", "Egypt", "pi", "tin", "Lake Pontchartrain", "Clark Griswold\\'s", "W", "Marriott", "the Principality of Monaco", "Canada", "The Secret", "the gold rush", "collagen", "China", "a compound", "the cranes", "a claw", "Alzheimer", "the Gulf of Mexico", "Stephen F. Austin", "Euclid", "Eva Peron", "Cain", "Yitzhak Edward \"Ed\" Asner", "X-Men", "the Louvre", "the chinook", "Prison Break", "Mercury", "Maine", "cheese made from sheep or goat's milk", "Meg", "The Sonnet", "the umpire", "Hans", "Peter Bogdanovich", "the Ramone", "Pilate", "BOAT PROPULSION", "the Quaternary Period", "nolo contendere", "Junior Walker", "the Czech Republic", "the Tuna", "cut - throat competition", "John Ernest Crawford", "beta decay", "France", "Priam", "Mariette", "Charles Quinton Murphy", "\"Sausage Party\" (2016)", "Australian", "the sins of the members of the church,", "$22 million", "\"17 Again,\"", "Nelson County"], "metric_results": {"EM": 0.59375, "QA-F1": 0.646875}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-8068", "mrqa_naturalquestions-validation-2918", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900"], "SR": 0.59375, "CSR": 0.5501644736842105, "EFR": 1.0, "Overall": 0.7110485197368421}, {"timecode": 38, "before_eval_results": {"predictions": ["tuition fees", "Holden Caulfield", "Bill Hickok", "Yellow fever", "a recession", "mermaid", "Jay Silverheels", "Singapore", "M1 Abrams", "a drumstick", "a horse", "Spider-Man", "Witness", "Jack the Ripper", "3800", "Alan Shore", "taxonomic", "Spain", "cerebrospinal fluid", "Francesco Schettino", "Macbeth William Shakespeare", "comedy", "Mary Poppins", "Casowasco Camp", "The Fresh Prince of Bel-Air", "Nod", "watermelon", "bathwater", "Second Marriage Ceremony", "Livin' On A Prayer", "Sherlock Holmes", "a lollipop", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "Reuleaux", "German", "Katamari Damacy", "Mark Twain", "Margaret Thatcher", "The Queen of Spades", "tungsten", "forests", "Olympia", "Waylon Jennings", "T. E. Lawrence", "Brazil", "British Columbia", "Sydney Pollack", "Scrapple", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "the different levels of importance of human psychological and physical needs", "Kelly's Eye", "Fraser Island", "Wright brothers", "jurisdiction", "Canada Goose", "Sandro Bondi refused to attend Cannes,", "voluntary manslaughter", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola,", "Pygmalion"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5843807234432234}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.09523809523809522, 1.0, 0.8717948717948718, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-3282", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-5541", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-6803", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-4043", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-2005", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-600"], "SR": 0.515625, "CSR": 0.5492788461538461, "EFR": 1.0, "Overall": 0.7108713942307692}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "Boogie Woogie Bugle Boy", "European", "Charlie Chaplin", "Glory", "Sweeney Todd", "The Bridge on the Kwai", "the Byzantine Empire", "Independence", "Jefferson", "Ezra Pound", "the Amazon", "a ready-to-use cotton swab", "California", "Dixie", "Objective Research Services", "Warren Harding", "engrave", "Shue", "Francis Crick", "Jay and Silent Bob", "Heath", "Abkhazia", "William Shakespeare", "Hawaii", "a key", "Tito", "conformation", "Ratatouille", "neurons", "Calvin Coolidge", "Mark Cuban", "Rudy Giuliani", "eyes", "Tony Dungy", "the Danube", "a knife Damascus Steel", "the marathon", "life", "a crop", "the endgame", "GIGO", "Johannes Brahms", "Charleston", "Italian", "The Grapes of Wrath", "a bicentennial", "Byzantines", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Tara", "March 15, 1945", "Charles Darwin", "Old Trafford", "Spider-Man", "Honey Irani", "global peace", "Kalahari Desert", "a Christian farmer", "Bob Dole", "Bollywood superstar", "managing his time"], "metric_results": {"EM": 0.5, "QA-F1": 0.5710565476190477}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-459", "mrqa_searchqa-validation-11089", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-11929", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-15687", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4073"], "SR": 0.5, "CSR": 0.548046875, "EFR": 0.96875, "Overall": 0.704375}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "horror", "Oakdale", "Missouri", "the FAI Junior Cup", "Flaw", "alt-right", "the Drudge Report", "15,000 people", "Yellow fever", "Pitch Perfect 2", "1934", "a record of 13\u20133", "We Need a Little Christmas", "Tsavo East National Park", "the New York Islanders", "Algirdas", "nearly 80 years", "Jean Acker", "the Championship", "the Gettysburg Address", "Whitney Houston", "Manchester United", "The Rite of Spring", "Dave Allen", "26,000", "Kristin Scott Thomas", "Ed Lee", "1958", "1993", "burlesque", "Afro-Russian", "Loretta Lynn", "Lancashire, England", "the B-17 Flying Fortress", "1 December 1948", "11", "the XXIV Summer Universiade", "2012", "1994", "Kansas City", "1999", "Pinellas County", "beer", "London", "Ployer Peter Hill", "Mindy Kaling", "1988", "Leonard Cohen", "Erika Mitchell Leonard", "Mason Alan Dinehart", "Tevye", "Sir Tom Finney", "Cameroon", "collecting samples of blood and other fluids from patients", "by military personnel to hazardous materials", "two", "Iggy Pop invented punk rock.", "the Merchant of Venice", "the Mayor of Casterbridge", "Leonardo DiCaprio", "an unhealthy ex-lover"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6656472816399287}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1749", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_naturalquestions-validation-6326"], "SR": 0.59375, "CSR": 0.5491615853658536, "EFR": 0.9615384615384616, "Overall": 0.7031556343808629}, {"timecode": 41, "before_eval_results": {"predictions": ["a fumble", "10", "did not identify any of the dead", "Les Bleus", "2005", "more than 4,000", "The Valley Swim Club", "an angry mob.", "normal maritime", "Sri Lanka", "killed Lauterbach", "an average of 25 percent", "fatally shooting a limo driver", "Al Nisr Al Saudi", "as many as 50,000", "piano", "$250,000", "a \"prostitute\"", "the mammoth's skull", "tax", "Brazil", "some three months before the crimes", "Russia", "Facebook and Google", "Salt Lake City, Utah", "Manmohan Singh's Congress party", "Haiti", "Tuesday afternoon", "Pakistan", "23 years", "a head injury.", "Uzbekistan", "an open window", "Leo Frank", "Paul McCartney and Ringo Starr", "Haiti's capital, Port-au-Prince", "President Robert Mugabe", "free", "one count of attempted murder in the second degree", "Diversity", "on-loan David Beckham claimed his first goal in Italian football.", "\"He is more American than German.\"", "\"Twilight\"", "forgery and flying without a valid license", "11", "A third beluga whale belonging to the world's largest aquarium has died", "Fayetteville, North Carolina,", "An Indonesian military transport plane crashed into a residential area in East Java early Wednesday, killing 98 people,", "al Qaeda", "Secretary of State Hillary Clinton", "Rihanna", "angular rotation", "from the right side of the heart to the lungs", "54 Mbit / s", "County of Gloucestershire", "the B-17 Flying Fortress", "Quaker Puffed Rice", "Oakdale", "Victoria", "Guillermo del Toro", "stocks", "Monty Python and the Holy Grail", "Sweden", "Freight"], "metric_results": {"EM": 0.546875, "QA-F1": 0.643748308982684}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 0.125, 0.0, 0.5714285714285715, 1.0, 1.0, 0.7272727272727273, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-1659", "mrqa_naturalquestions-validation-5552", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376", "mrqa_hotpotqa-validation-403", "mrqa_searchqa-validation-10945"], "SR": 0.546875, "CSR": 0.5491071428571428, "EFR": 0.9655172413793104, "Overall": 0.7039405018472906}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Arizona", "Zimbabwe", "Italian Serie A", "Darrel Mohler", "her dancing against a stripper's pole.", "the \" Michoacan Family,\"", "WTA Tour titles", "MDC head Morgan Tsvangirai", "42", "recite her poetry", "glaciers in the European Alps may melt as soon as 2050,", "80 percent", "1979", "\"Follow the Sun,\"", "Elena Kagan", "CBS, CNN, Fox and The Associated Press.", "an auxiliary lock", "1-1", "Umar Farouk AbdulMutallab", "Myanmar", "Collier County Sheriff Kevin Rambosk", "Marcus Schrenker,", "Abu Sayyaf", "poems telling of the pain and suffering of children", "the program was made with the parents' full consent.", "(the Democratic VP candidate", "The Red Cross, UNHCR and UNICEF", "Russian", "debris", "not guilty of affray", "capital murder and three counts of attempted murder", "Basel", "17", "a Daytime Emmy Lifetime Achievement Award", "state senators", "31 meters (102 feet)", "nude beaches.", "women coping with breast cancer", "Misty Croslin", "shark River Park in Monmouth County", "three out of four", "Islamabad", "partying", "Capitol Hill.", "would not do it", "1940's", "March 22", "think about saving the rainforests", "at a depth of about 1,300 meters in the Mediterranean Sea.", "\"Antichrist\"", "a major fall in stock prices", "Thomas Jefferson", "Alexander Salkind", "Orion", "brown", "Selfie", "2002", "England", "Los Alamos National Laboratory", "the Rat", "rain", "Crawford", "the Pyrenees"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6402963789682539}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 0.13333333333333333, 0.5714285714285715, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3073", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5989", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.515625, "CSR": 0.548328488372093, "EFR": 0.967741935483871, "Overall": 0.7042297097711928}, {"timecode": 43, "before_eval_results": {"predictions": ["the north,", "legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees", "33-year-old", "that these \"fusion teams,\" as they're being called, have come into effect.", "hardship for terminally ill patients and their caregivers,", "Jaime Andrade", "Zac Efron", "finance", "nearly $2 billion", "The National Infrastructure Program,", "In 1937,", "The station", "Krishna Rajaram,", "a man's lifeless, naked body", "Robert Mugabe", "the wife of Gov. Mark Sanford,", "Afghanistan's restive provinces", "Saturday.", "$1.5 million", "a violent government crackdown seeped out.", "Iran could be secretly working on a nuclear weapon", "the fact that the teens were charged as adults.", "death squad killings", "Elena Kagan", "Hyundai Steel", "100 percent", "Saturday", "Afghanistan,", "prisoners at the South Dakota State Penitentiary", "seven", "200", "Pakistan", "Seminole Tribe", "Rima Fakih", "in a tide of migrants washing up in South Africa.", "Barack Obama", "helicopters and unmanned aerial vehicles", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah", "1,500", "A receptionist", "$50 less", "$60 billion on America's infrastructure.", "ALS6", "Malayalam", "Mad - Eye Moody and Hedwig", "1960", "Aston Lower Grounds", "peasants, small and medium-size farmers, migrants and agricultural workers", "pool", "1822", "The Dressmaker", "Trilochanapala", "crote", "a buffalo", "The Wizard of Oz", "the occipital lobe"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6325419372294372}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_naturalquestions-validation-922", "mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.546875, "CSR": 0.5482954545454546, "EFR": 0.9655172413793104, "Overall": 0.703778164184953}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf.", "Los Angeles", "Chris Eubank Jr.", "Duval County, Florida", "Benj Pasek and Justin Paul", "Andes", "1952", "1200000", "19th", "January 28, 2016", "Araminta Ross", "Roger Staubach", "1944", "Highlands", "Franconia, New Hampshire", "Operation Watchtower", "Dan Crow", "\"War & Peace\"", "Amberley Village", "What Are Little Boys Made Of", "Berea College", "Omaha Nighthawks", "Luca Guadagnino", "Liesl", "Germany and other parts of Central Europe,", "New York Islanders", "Todd Phillips", "26,788", "the Troubles", "1967", "Marktown", "jus sanguinis", "Radcliffe College", "Charles Guiteau", "Ford Motor Company", "If the citizen's heart was heavier than a feather", "India", "Lutheranism", "Charmed", "25 million", "The Snowman", "Ella Fitzgerald", "X-Men United", "Rain Man", "Interscope Records", "British East India Company", "4,000", "\"the most influential private citizen in the America of his day\"", "I'm Shipping Up to Boston", "American", "Scottish singer and \"Britain's Got Talent\" winner Jai McDowall", "China", "sixth - largest country by total area", "beginning of the American colonies", "Nicola Adams", "gansbaai", "Russia", "dependable Camry", "Steven Green", "in a hotel, North Carolina,", "Destiny's Halloween", "rattlesnake", "Riddles in the Dark", "healthy, wealthy, and wise"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6444207702020202}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.8, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-2355", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-13986"], "SR": 0.578125, "CSR": 0.5489583333333333, "EFR": 1.0, "Overall": 0.7108072916666666}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "Indian Ocean waters", "30", "crocodile eggs", "Colorado prosecutor", "Jared Polis", "on Saturday.", "the devastation from Tuesday's earthquake", "in July for A Country Christmas,", "sniff out cell phones.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Herman Cain", "\"17 Again,\"", "North Korea intends to launch a long-range missile in the near future,", "Wigan Athletic", "Romney", "two years ago.", "businessman", "Picasso's muse and mistress, Marie-Therese Walter.", "low-calorie meals", "Heshmatollah Attarzadeh", "the ireport form", "government", "Nine out of 10 children", "Jaime Andrade", "Sen. Joe Lieberman,", "a crocodile", "a bronze medal", "more than 200.", "Congress", "Susan Boyle", "military ID cards, known as \"Common Access Cards,\"", "Phillip A. Myers.", "Obama's", "Gyanendra, 60,", "homicide.", "Casey Anthony, 22,", "officers at a Texas  airport", "10 municipal police officers", "UNICEF", "the couple's surrogate", "228", "Kerstin and two of her brothers, ages 18 and 5,", "2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Donald Trump and Joan Rivers", "supermodel and philanthropist", "Jacob Zuma,", "in the Oaxacan countryside of southern Mexico", "Wenger", "slavery", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "enkuklios paideia", "Enid Blyton", "Johnny Mathis", "Beverly Hills Cop,", "Champion Jockey", "Luca Guadagnino", "Ms. Jackson", "unknown", "how timing shapes and supports brain function", "a jigger", "unarmed"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6323479471916971}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.47619047619047616, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.7499999999999999, 0.5714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5640", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.53125, "CSR": 0.5485733695652174, "EFR": 0.9666666666666667, "Overall": 0.7040636322463768}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "bipartisan", "Nirvana", "\"Dancing With the Stars\"", "the Orange County District Attorney's Office.", "12.3 million", "Mexico", "Real Madrid", "Michael Arrington,", "Brett Cummins,", "Indian Army", "Saturday", "Nicole", "legitimacy of that race.", "Adidas,", "Dennis Davern,", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "promise to improve health and beauty.", "Chinese", "Newcastle", "\"Nothing But Love\"", "allegedly involved in forged credit cards and identity theft", "June 6, 1944,", "Iran,", "2-1", "October 19,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Seoul,", "promotes fuel economy and safety while boosted the economy.", "ALS6", "eight", "Siri.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback Forestry", "the children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "they did not know how many people were onboard.", "attempting illegal crossings", "American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "38", "Her husband and attorney, James Whitehouse,", "blacks and Hispanics", "two", "the most gigantic pumpkins in the world,", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel", "Discworld", "Japan", "fox hunting", "New York", "travel diary", "16,116", "smoke", "maple juice", "a bumblebee", "Rowan Blanchard (born October 14, 2001)"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7246717258436008}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.8, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.625, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-11573", "mrqa_hotpotqa-validation-664"], "SR": 0.671875, "CSR": 0.5511968085106382, "EFR": 1.0, "Overall": 0.7112549867021276}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Airlines", "A Rush of Blood to the Head", "5", "Chicago", "The Ones Who Walk Away from Omelas", "teenage", "Dennis Kux", "drawing the name out of a hat", "Luke Bryan", "I-League", "two or three", "Badfinger", "Lady Frederick Windsor", "point-coloration pattern", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1946 and 1947", "5,112", "1992", "residential", "143,372", "6'5\"", "Mickey Gilley's", "Swiss Confederation", "German Shepherd", "Mexican", "December 24, 1973", "1933", "Tremont, Maine", "Kristoffer Rygg", "1730", "London Luton Airport.", "the Salzburg Festival", "Mississippi", "India", "1959", "Imelda Marcos", "Randall Boggs", "Part II", "Bunker Hill", "lion", "Royal", "World War II", "Knoxville", "Three's Company", "Doomtree", "Conservatives", "Linda McCartney's Life in Photography", "English", "September 14, 2008", "73", "Buffalo Bill", "Romania", "Zephyr", "mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "hall", "Lehman Bros International"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6331845238095237}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-4435", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5435", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-10434", "mrqa_triviaqa-validation-2701"], "SR": 0.546875, "CSR": 0.5511067708333333, "EFR": 1.0, "Overall": 0.7112369791666666}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "St Petersburg", "sushi", "offensive", "Vulcan", "the Pilgrims", "Fawn Hall", "waive", "Shakespearean Heroines", "Barnum", "Johnny Weissmuller", "cathode", "a torque screw", "the first", "Marlon Brando", "Middle Dutch", "Impressionists", "Kentucky", "the ruddy", "Brussels", "Macbeth", "General Lee", "piracy", "Fyodor Dostoevsky", "Martin Luther", "Clue", "Edgar Allan Poe", "German", "Stephen A. Douglas", "every seventh year", "Mike Connors", "Juno Jim", "Jim Inhofe", "sancire", "Corpus Christi", "South Africa", "the ostrich", "a constitution", "slow-starting night", "mug", "Desperate Housewives", "Galileo Galilei", "Canada", "Anne Hathaway", "a strike", "the Grail", "West Virginia", "Thomas Jefferson", "home theater", "the family chain", "kritiks", "Khrushchev", "1904", "a young girl", "Bobby Tambling", "ambidevous", "chariots", "Humberside Airport", "more than 265 million", "100 million", "to help rebuild the nation's highways, bridges and other public-use facilities.", "a head injury.", "Pope Benedict XVI", "Charles II"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5680555555555555}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-3406", "mrqa_searchqa-validation-11080", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-4061", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_searchqa-validation-15062", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.484375, "CSR": 0.5497448979591837, "EFR": 0.9393939393939394, "Overall": 0.6988433924706247}, {"timecode": 49, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1056", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-211", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-785", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8428", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.765625, "KG": 0.48671875, "before_eval_results": {"predictions": ["National Security Agency", "the Heisman", "Brandi Chastain", "the Colorado River", "Pamela Anderson", "carioca", "Treasure Island", "Pocahontas", "improv", "Robert Kennedy", "an", "a spray nozzle", "Great American Novel", "Matthew Broderick", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "draft", "Ernest Lawrence", "rodeo", "fresco", "Nevil Shute", "Grant", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "a skintight", "Bulworth", "the small intestine", "the mouthpiece", "Key West", "The Fellowship of the Ring", "MCA", "bug spray", "Manhattan", "February 2, 2012", "Leontyne Price", "a composting material", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "Sarah Jessica Parker", "a spring", "Trinidad", "a burnoose", "Philadelphia", "peanut butter", "Edgar Allan Poe", "leather", "Alexei Luthor", "food and clothing", "Schwarzenegger", "Master Christopher Jones", "Aleph", "terrybank thistle", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "The incident Sunday evening", "three out of four", "poems", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5574652777777778}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6040", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-10510", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-7250", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-14216", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-1897", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12067", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301"], "SR": 0.46875, "CSR": 0.548125, "EFR": 1.0, "Overall": 0.715171875}]}