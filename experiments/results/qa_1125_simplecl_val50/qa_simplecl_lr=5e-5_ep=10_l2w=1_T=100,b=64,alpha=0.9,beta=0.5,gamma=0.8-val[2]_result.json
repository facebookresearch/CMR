{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2140, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Ed Asner", "arrows", "1st century BC", "Marburg Colloquy", "Brookhaven", "ca. 2 million", "the Hungarians", "Mercury", "19th Century", "Art Deco style in painting and art", "The ability to make probabilistic decisions", "impact process effects", "1999", "phagosome", "the mass of the attracting body", "the Association of American Universities", "three", "allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "freight services", "up to four minutes", "the Little Horn", "Muslim and Chinese", "intracellular pathogenesis", "Santa Clara, California", "1784", "George Low", "Annual Conference Cabinet", "three", "Students", "Atlantic", "2001", "1887", "Chicago Bears", "John Harvard", "increase its bulk and decrease its density", "literacy and numeracy", "Christmas Eve", "the state", "Paris", "gender roles and customs", "outdated or only approproriate", "soy farmers", "United States", "Albert Einstein", "the number of social services that people can access wherever they move", "Tesco", "ABC Inc.", "1776", "wireless", "an electric current", "Warszowa", "the courts of member states", "supervisory church body", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church", "Manakin Episcopal Church", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Westminster", "Von Miller", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "Khwarezmia", "Queen Elizabeth II", "CBS", "Pittsburgh Steelers", "The chloroplast peripheral reticulum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8875363542546205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212122, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1826", "mrqa_squad-validation-4874", "mrqa_squad-validation-4283", "mrqa_squad-validation-1802", "mrqa_squad-validation-6210", "mrqa_squad-validation-3650", "mrqa_squad-validation-7430", "mrqa_squad-validation-6136"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["the Inland Empire", "New Zealand", "Jacksonville", "Newton's First Law", "the ability to pursue valued goals", "May 1888", "lecture theatre", "more than 28 days", "elliptical", "Boston", "Wednesdays", "Orange", "three", "Lampea", "San Jose State", "March 29, 1883", "between AD 0\u20131250", "Pleurobrachia", "eleven", "punts", "Solim\u00f5es Basin", "1474", "Arizona Cardinals", "Julia Butterfly Hill", "Orange", "Doctor in Bible", "left Graz", "waldzither", "over $40 million", "14th century", "6.7+", "end of the 19th century", "peace", "$40,000", "Cloth of St Gereon", "time and space", "7,000", "elementary particles", "indigenous", "3.5 billion", "New York City O&O WABC-TV and Philadelphia O&o WPVI-TV", "John Fox", "architectural", "Prime ideals", "Normant", "Leonardo da Vinci", "2003", "modern buildings", "Charles River", "KOA", "a disaster", "no contest", "Latin", "Manakin Town", "40,000", "After liberation", "\"winds up\" the debate", "10%", "The Daily Mail is mentioned in The Beatles\u2019 hit single Paperback Writer  The Yorkshire Post was the first British newspaper to report on The Abdication Crisis", "Uncle Tom's Cabin", "The liver", "No man", "Martina Hingis", "Ukraine borders with seven countries"], "metric_results": {"EM": 0.875, "QA-F1": 0.8942708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4458", "mrqa_squad-validation-1775", "mrqa_squad-validation-1001", "mrqa_squad-validation-696", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-7750", "mrqa_naturalquestions-validation-646"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 2, "before_eval_results": {"predictions": ["$155 million", "CBS", "San Jose State", "Half", "the evolution of the German language and literature", "Qur'an", "Brotherhood", "high wages", "Tolui", "legon, the current King of Thebes, who is trying to stop her from giving her brother Polynices a proper burial", "the object's weight", "over half", "1960s", "two months", "his friends Johannes Bugenhagen and Philipp Melanchthon", "1805", "Elders", "30\u201375%", "45,000 pounds", "self molecules", "Taishi", "1960", "Captain America: Civil War", "political divisions", "D loop mechanism", "Monterey", "The Book of Common Prayer", "14", "Charleston", "fear of their lives", "hot winds blowing from nearby semi-deserts", "intracellular pathogenesis", "Safari Rally", "10,006,721", "Philip Segal", "decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases", "1965", "quantum gravity", "German Te Deum", "Stanford Stadium", "Jin", "Trevathan", "Doctor Who", "1206", "clinical services", "CRISPR sequences", "Queen Elizabeth II", "zero", "1992", "food security", "plasmas", "their low ratio of organic matter to salt and water", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "time goes", "guardian", "guardian", "black", "one mile above sea level"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6923261634199134}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9714285714285714, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-6099", "mrqa_squad-validation-6641", "mrqa_squad-validation-2547", "mrqa_squad-validation-8360", "mrqa_squad-validation-2577", "mrqa_squad-validation-8747", "mrqa_squad-validation-5893", "mrqa_squad-validation-2906", "mrqa_squad-validation-1860", "mrqa_squad-validation-10427", "mrqa_squad-validation-6178", "mrqa_squad-validation-6405", "mrqa_squad-validation-1435", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11930", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-12889", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3857"], "SR": 0.609375, "CSR": 0.7864583333333334, "EFR": 1.0, "Overall": 0.8932291666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["fewer than 10 employees", "1624", "Hangzhou", "committee", "the 19th century", "1962", "dealing with patients' prescriptions and patient safety issues", "a group that included priests, religious leaders, and case workers as well as teachers", "Vistula River", "1290", "21 October 1512", "140,079", "a double membrane", "August 1967", "German", "27-30%", "four", "the 50 fund", "Arizona Cardinals", "Peanuts", "comb rows", "calcitriol", "Warsaw", "time", "since at least the mid-14th century", "the mitochondrial double membrane", "Mike Figgis", "in an adult plant's apical meristems", "isopentenyl pyrophosphate synthesis", "Associating forces with vectors", "prime ideals", "The Three Doctors", "Von Miller", "four", "the Koori", "1910\u20131940", "pressure swing adsorption", "Luther", "English", "a lack of remorse", "the fundamental means by which forces are emitted and absorbed", "the A1 (Gateshead Newcastle Western Bypass)", "Sun Life Stadium", "the Duchy of Prussia, the Channel Islands, and Ireland", "John Houghton", "February 2015", "draftsman", "the Mollusca", "Orestes", "the Galapagos Islands", "the term \"act of terror\"", "denver", "the Forty-merchandising", "denver", "denver", "the Mycenaean civilization", "a biological process that displays an endogenous, entrainable", "the Belasco Theatre", "the Normandy Landings", "the Forty", "the fibula", "Il Trovatore", "the South Pole", "the Royal Border Bridge"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6195211038961039}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6345", "mrqa_squad-validation-2192", "mrqa_squad-validation-4724", "mrqa_squad-validation-3347", "mrqa_squad-validation-4730", "mrqa_squad-validation-1036", "mrqa_squad-validation-776", "mrqa_squad-validation-3673", "mrqa_squad-validation-2132", "mrqa_squad-validation-6737", "mrqa_squad-validation-10310", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-8509", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-10694", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-7003"], "SR": 0.59375, "CSR": 0.73828125, "EFR": 0.9615384615384616, "Overall": 0.8499098557692308}, {"timecode": 4, "before_eval_results": {"predictions": ["Germany and Austria", "Centrum", "blue police box", "to spearhead the regeneration of the North-East", "Rhenus", "the Gramme dynamo", "under the wing of the secular powers", "in 1281 as the official calendar of the Yuan dynasty", "Zhongtong", "11.1%", "1538", "Deacons", "the New Testament", "experience, ideology, and weapons", "25", "May 2013", "K-9 and Company", "in capturing prey", "a four-carbon compound", "livestock pasture", "Ford", "1,300,000", "it has trouble distinguishing between carbon dioxide and oxygen", "two tumen (20,000 soldiers)", "eight", "algorithm", "WzzM and WOTV", "Orange", "tentilla", "gender roles and customs", "social unrest and violence", "Woodward Park", "1745", "the Battle of Olustee", "observer status", "the 50-yard line", "3D printing technology", "The Malkin Athletic Center", "24\u201310", "6.7+", "empires", "the domestic legislation of the Scottish Parliament", "the most effective of the three therapies", "Bloomberg", "the oceans are growing crowded, and governments are increasingly trying to plan their use", "a new generation of innovative, exciting skyscrapers", "a lump in Henry's nether regions", "the war years", "the computer processing unit (CPU) market", "Matt Kuchar and Bubba Watson", "the fastest circumnavigation of the globe in a powerboat", "a large concrete block", "the BBC's central London offices", "Buddhism", "Manchester City", "Noriko Savoie", "three", "change course", "Tsvangirai", "A Lion Among Men", "1971", "honey", "Chelsea Lately", "Luxembourg"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7150793650793651}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.09523809523809525, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.6666666666666666, 0.0, 0.2, 0.0, 0.8333333333333333, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9250", "mrqa_squad-validation-1320", "mrqa_squad-validation-2288", "mrqa_squad-validation-8231", "mrqa_squad-validation-4947", "mrqa_squad-validation-4258", "mrqa_squad-validation-8832", "mrqa_squad-validation-6046", "mrqa_squad-validation-1643", "mrqa_squad-validation-4572", "mrqa_squad-validation-7094", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2807", "mrqa_naturalquestions-validation-47", "mrqa_hotpotqa-validation-547"], "SR": 0.609375, "CSR": 0.7125, "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["with money from foreign Islamist banking systems, especially those linked with Saudi Arabia", "Anheuser-Busch InBev", "4000 years of glass making", "$37.6 billion", "Anglo-Saxons", "seven", "Golden Gate Bridge", "Southwest Fresno", "divergent boundaries", "chloroplasts are surrounded by a double membrane", "QuickBooks", "surface condensers", "clinical pharmacists", "seal", "Philip Howard", "Duke Richard II of Normandy", "capturing three traders", "three", "Spanish", "Golden Super Bowl", "the constitutional traditions common to the member states", "pharmacological effect", "Huguenots", "10\u20137", "Polish Academy of Sciences", "spherical and highly refractive bodies", "Nurses", "New England Patriots", "Time magazine", "Class II MHC molecules", "two plastid-dividing rings", "George Westinghouse", "by disrupting their plasma membrane", "internal combustion engines", "indirectly", "Religious and spiritual teachers", "B cells", "property damage", "human rights abuses against ethnic Somalis", "Goa", "How I Met Your Mother", "France's famous Louvre museum", "Leo Frank", "Greece", "Graziano Transmissioni", "opposition parties", "204,000", "United's", "the release of the four men", "how health care can affect families", "Ed McMahon", "at least $20 million to $30 million", "Evan Bayh", "Friday", "Ali Larijani", "paper ballots", "Sodra nongovernmental organization", "sodium dichromate", "promotes fuel economy and safety", "heart rate that exceeds the normal resting rate", "heavy breeds", "Denmark", "Cincinnati", "Donald Sutherland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6536947070494865}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.7000000000000001, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4705882352941177, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9691", "mrqa_squad-validation-754", "mrqa_squad-validation-5750", "mrqa_squad-validation-8715", "mrqa_squad-validation-1090", "mrqa_squad-validation-10244", "mrqa_squad-validation-3610", "mrqa_squad-validation-3075", "mrqa_squad-validation-827", "mrqa_squad-validation-8826", "mrqa_squad-validation-8867", "mrqa_squad-validation-6644", "mrqa_squad-validation-10444", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-4043", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-4171", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-2465"], "SR": 0.53125, "CSR": 0.6822916666666667, "EFR": 1.0, "Overall": 0.8411458333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["18 February 1546", "11", "neither conscientious nor of social benefit", "University of Chicago Press", "$2 million", "2015", "1762", "biased against Genghis Khan", "Warsaw Stock Exchange", "they are often branched and entangled with the endoplasmic reticulum", "computational resource", "to denote unknown or unexplored territory", "Nicholas Stone", "early Lutheran hymnals", "a straight line", "1991", "William Smith", "William Pitt", "geochemical component", "Theory of the Earth to the Royal Society of Edinburgh", "Japan", "Super Bowl Opening Night", "the Working Group chairs", "laws of physics", "Denver's Executive Vice President of Football Operations and General Manager", "noisiest", "independent of each other", "issues under their jurisdiction", "unsuccessful", "human", "if they are homebound", "eliminate all multiples of 1 (that is, all other numbers) and produce as output only the single number 1", "nerves", "1671", "the Reichstag", "aldehydes and ketones", "Howard Dean III", "heart, blood, and blood vessels", "Troggs", "six", "Slovakia", "Diana the Princess", "slave trade", "vena cava", "Virginia", "5 feet 8 Inches from the floor", "Tartarus", "a kind of backdrop that often represents the sky is known", "Nancy Reagan", "Persian Achaemenid Empire", "a piece of luxury", "Zeppelin", "San Francisco", "64", "Datson, H., Birch,... plus assorted small iron and slag particles.", "Dies at 65", "Judas", "a group of characters who have safely gotten off the Island", "a comic book series by Robert Kirkman, Tony Moore, and Charlie Adlard", "Love Is All Around", "Los Angeles Dance Theater", "Somalia", "nine companies to stop selling unapproved pain-relief drugs.", "18"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5973090277777777}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6230", "mrqa_squad-validation-8765", "mrqa_squad-validation-5588", "mrqa_squad-validation-4005", "mrqa_squad-validation-5054", "mrqa_squad-validation-383", "mrqa_squad-validation-10398", "mrqa_squad-validation-6337", "mrqa_squad-validation-3113", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4830", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8993", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-1637", "mrqa_searchqa-validation-12770", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-10057", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1064"], "SR": 0.546875, "CSR": 0.6629464285714286, "EFR": 1.0, "Overall": 0.8314732142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["extinction", "oxygen", "reduce growth", "Torchwood", "9.1 million", "little", "individual countries", "cattle", "Mongol", "a maze of semantical problems and grammatical niceties", "five", "Wahhabism", "British", "Finsteraarhorn", "Abilene", "white", "Yosemite Freeway", "Thanksgiving", "874.3 square miles", "Two thirds", "the Privy Council", "well into the nineteenth century", "capability deprivation", "Daily Mail", "San Mateo", "Spanish", "around 300,000", "cryptomonads", "Swahili", "hymn-writer", "a site of starch accumulation in plants that contain them", "Bryant", "Jupiter", "a tornado", "Rodeo", "hog", "Barack Obama", "Kenny G", "a cup", "a peacock unitard", "Annapolis", "Spring", "klammeraffe", "female", "Allah", "bones", "Python", "Cecil B. DeMille", "Cold Mountain", "Faith Hill", "Ben Affleck", "the European", "V", "time", "a jazz saxophonist", "Sweden", "Indonesia", "hydrogen", "Alexandria", "Perfume", "the New Jersey Economic Development Authority's 20% tax credit", "Georgetown", "Essex Eagles", "Alzheimer's disease"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6444602272727273}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4198", "mrqa_squad-validation-7626", "mrqa_squad-validation-1938", "mrqa_squad-validation-6220", "mrqa_squad-validation-9588", "mrqa_squad-validation-4562", "mrqa_squad-validation-8189", "mrqa_squad-validation-8828", "mrqa_searchqa-validation-47", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-943", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-8990", "mrqa_searchqa-validation-4050", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10916", "mrqa_searchqa-validation-5178", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3468"], "SR": 0.59375, "CSR": 0.654296875, "EFR": 1.0, "Overall": 0.8271484375}, {"timecode": 8, "before_eval_results": {"predictions": ["shocked", "lymphocytes", "producers of the show", "BSkyB", "Kawann Short", "Daidu in the north", "silent film", "22", "the park", "1965", "tidal currents", "Concentrated O2", "Ma Jianlong", "Demaryius Thomas", "Lake Constance", "the Orange Democratic Movement", "Bannow Bay", "the Red Army", "20th century", "ITT", "1966", "masses", "Linebacker", "high art and folk music", "four", "with six series of theses", "the midseason forensic investigation drama Body of Proof", "seven-eighths", "the cardinal de Richelieu", "the Atlas Mountains", "Madrid", "the Danube", "an exclamation to Yahweh", "leather", "Lewis Pullman", "the plums", "the Messiah's birth", "Sappho", "Possession", "the tonka bean", "the divisor", "Hypnos", "Texas", "the IHOP family restaurant chain", "a black breed and a white breed mingled to produce this type of cow named for a German region", "the Bill of Rights", "the ACT", "Rio", "Henry David Thoreau", "Santa Ana", "Dick Cheney", "the Earth", "Gustave Eiffel", "Edward Hopper", "the CIA", "D'Artagnan", "the Venus figurines", "1985", "apple", "its air-cushioned sole", "13", "Fort Worth", "Agent 99", "private"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6390190972222223}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3478", "mrqa_squad-validation-1169", "mrqa_squad-validation-2474", "mrqa_squad-validation-5926", "mrqa_searchqa-validation-15994", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-15182", "mrqa_searchqa-validation-523", "mrqa_searchqa-validation-15584", "mrqa_searchqa-validation-9386", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-10315", "mrqa_searchqa-validation-14478", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-9179", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4461"], "SR": 0.578125, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.8229166666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["\"Provisional Registration\"", "August 15, 1971", "Levi's Stadium", "the United Nations Framework Convention on Climate Change", "Inflammation", "Brown v. Board of Education", "15 May 1525", "The Walt Disney Company", "Dundee", "61", "During the Second World War", "the integer factorization problem", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence", "Exploration is still continuing to determine if there are more reserves", "prep schools", "Cultural imperialism", "a strong Islamist outlook", "lengthening rubbing surfaces of the valve", "$32 billion", "keyed Northumbrian smallpipes", "the Dutch Republic", "Alex Haley", "three", "the helmeted honeyeater", "4:51", "Khrushchev", "Hera", "cherries", "Elton John", "Cuba", "the Battle of Thermopylae", "Kievan Rus", "Kroc", "cricket", "white", "Washington State", "Preamillo", "Genoa", "3", "tarn", "972", "buffalo", "Ann Widdecombe", "Isosceles", "the Old Kent Road", "Tuesday", "stone", "Ab Fab", "Massachusetts", "Barrow", "California", "the Susquehanna River", "80\u2019s", "the maqui berry", "Singapore", "Wigan Warriors", "Davos", "eight", "playing a wide range of starring or supporting roles", "the Home Rule Party", "Janet Napolitano", "J. Crew", "Deval Patrick", "Grover"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6309027777777778}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7060", "mrqa_squad-validation-9552", "mrqa_squad-validation-8273", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2533", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-1203", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2672", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1553", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-7509"], "SR": 0.5625, "CSR": 0.6375, "EFR": 0.9642857142857143, "Overall": 0.8008928571428571}, {"timecode": 10, "before_eval_results": {"predictions": ["-s", "environmental determinism", "4 August 2010", "King George III", "radio", "Edsen Khoroo", "League of Augsburg", "Duarte Barbosa", "the People's Republic of China", "the Roman Catholic Church", "Amazonia: Man and Culture in a Counterfeit Paradise", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "Sydney", "five", "January 18, 1974", "Spanish", "NFL", "extremely difficult", "student populations", "Catholic", "Parliament of the United Kingdom", "296", "the Ghent-Terneuzen Canal", "mulberry", "a bacteria", "a roadside tree", "Ken Russell", "Dan Dare", "crimean", "Smiths", "Mike Tyson", "Turkey", "Pesach", "Brian Deane", "kaleidoscopes", "Uranus", "Apollon", "George Carlin", "Soviet Union", "Sydney", "Los Angeles", "Underground", "puck", "barbarian", "passion fruit", "Portugal", "cricket", "Serena Williams", "63 to 144 inches", "The Titanic", "William Tell", "Christian Dior", "a jack-in-the-box-like creature", "Mendip Hills", "Wichita", "eukharisti\u0101", "New Croton Reservoir in Westchester and Putnam counties", "andr\u00e9 3000", "Leucippus", "Stephen King", "Venus Williams", "firefighter", "Python and the Holy Grail", "barbarian"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6972222222222222}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-6263", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-7222", "mrqa_triviaqa-validation-2265", "mrqa_naturalquestions-validation-7138", "mrqa_newsqa-validation-2710", "mrqa_searchqa-validation-3397", "mrqa_searchqa-validation-8589"], "SR": 0.65625, "CSR": 0.6392045454545454, "EFR": 0.9545454545454546, "Overall": 0.796875}, {"timecode": 11, "before_eval_results": {"predictions": ["the method by which the medications are requested and received", "salvation", "jugs and candlesticks", "they produce secretions (ink) that luminesce at much the same wavelengths as their bodies", "zaju", "administration", "Chivas USA", "Edinburgh", "The Pink Triangle", "the dot", "Magdalen Tower", "an international data communications network", "public service", "Guy de Lusignan", "tiger team", "a sub-group", "The European Commission", "completed (or local) fields", "a fundamental error", "Mongol and Turkic tribes", "hizbullah", "five", "Whist", "morocco", "morocco", "achromatopsia", "rod", "Pluto", "iron, morolybdenum, and titanium", "copper", "The Hague", "Vancouver Island", "Ironside", "morocco", "morocco", "brown trout", "Beyonce", "Wordsworth", "man V Food", "morocco", "Samuel Johnson", "Conrad Murray", "Mary Poppins", "Bennett Cerf", "morocco", "morocco", "morocco", "morocco", "Oslo", "horses", "Rhododendron", "morocco", "morocco", "Shanghai", "morocco", "underground railways with two connected railway cars on inclined tracks", "Snake River Valley", "17 October 2006", "beer", "Hoover Dam", "morocco", "morocco", "Edgar Allan Poe", "morocco"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5701121794871795}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5460", "mrqa_squad-validation-6530", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-3550", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-3846", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-6189", "mrqa_triviaqa-validation-3023", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-890", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-8473"], "SR": 0.546875, "CSR": 0.6315104166666667, "EFR": 1.0, "Overall": 0.8157552083333334}, {"timecode": 12, "before_eval_results": {"predictions": ["a gift from God", "Greenland", "1724 to 1725", "placing them on prophetic faith", "1.25 million", "720p high definition", "five", "Maria Goeppert-Mayer", "the International Association of Methodist-related Schools, Colleges, and Universities", "one", "an majority in Parliament, a minority in the Council, and a majority in the Commission", "President Mahmoud Ahmadinejad", "Newcastle Eagles", "cholera", "other senior pharmacy technicians", "relative units of force and mass", "AD 14", "orogenic wedges", "his exploration and settlement of what is now Kentucky, which was then part of Virginia but on the other side of the mountains from the settled areas.", "The Handmaid's Tale", "chimpanzee", "The Fault in Our Stars", "The Ballade", "puzzle video", "1898", "400 MW", "Total Nonstop Action Wrestling", "galt\u00fcr avalanche", "Archbishop of Canterbury", "1861", "Disneyland, California", "David Villa", "Red and Assiniboine Rivers", "Bergen County, New Jersey", "Continental Army", "Jack Kilby", "darryl", "andrew kadyrov", "July 16, 1971", "1933", "The Heirs", "a combination of colours and blocks, which is a representation of the Baudot code.", "1959", "1887", "Mark Dayton", "Marvel Comics", "Rihanna", "Nick Cassavetes", "Lamar Hunt", "Sarah Winnemucca", "Jean Baptiste Point du Sable", "England", "Paul W. S. Anderson", "a basilica", "1963", "Ricky Nelson", "Wakanda and the Savage Land", "mercury", "phobias", "drug trafficking is a transnational threat, and therefore national initiatives have their limitations, and it is in everyone's interest to stop this proliferation.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "elizabeth pays", "dapple-grey", "pre-Columbian times"], "metric_results": {"EM": 0.609375, "QA-F1": 0.695987345987346}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.12121212121212123, 0.04761904761904762, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4958", "mrqa_squad-validation-5889", "mrqa_squad-validation-4079", "mrqa_squad-validation-10428", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5658", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-validation-6015", "mrqa_triviaqa-validation-2685", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-7025", "mrqa_naturalquestions-validation-8227"], "SR": 0.609375, "CSR": 0.6298076923076923, "EFR": 1.0, "Overall": 0.8149038461538461}, {"timecode": 13, "before_eval_results": {"predictions": ["\u00a341,004", "The Rocket", "Tolui", "lower lake", "Gospi\u0107, Austrian Empire", "since 2001", "impossible", "Southwest Fresno", "5,000", "Huguenot", "ABC News Now", "sold Wardenclyffe for $20,000 ($472,500 in today's dollars)", "\u00c9mile Girardeau", "Brownlee", "The Five Doctors", "The average fee is around \u20ac5,000 annually for most schools", "NCAA Division II", "Adrian Lyne", "his most brilliant student", "Las Vegas", "Hugh de Kevelioc", "2017", "Dallas", "Rudolf Schenker", "Shrek", "Lucille Ball", "\"Grimjack\" (from First Comics)", "16\u201321", "Vince Guaraldi", "Anthony Stephen Burke", "Michael Redgrave", "Minnesota's 8th congressional district", "Highlands Course", "Hawaii", "Liquidambar styraciflua", "Marquis de Lafayette", "Gujarat", "three", "Winter Haven", "four", "The Process", "Mindy Kaling", "Surrey", "Claudio Javier L\u00f3pez", "Kanye West", "FCI Danbury", "two", "US Naval Submarine Base New London submarine school", "Las Vegas", "Pope John X", "2013", "Arlo Looking Cloud", "Rwandan genocide", "Larnelle Harris", "Secretary of State", "in 2015, 2017", "1982", "rod", "Chris Robinson", "Gossip Girl", "fluid dynamics", "out", "the Egyptian Goddess of Creation", "Richie Unterberger"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6086004273504273}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.5, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3176", "mrqa_squad-validation-3287", "mrqa_squad-validation-1488", "mrqa_squad-validation-7792", "mrqa_squad-validation-7036", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-3556", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-4422", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-16181", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-469"], "SR": 0.53125, "CSR": 0.6227678571428572, "EFR": 1.0, "Overall": 0.8113839285714286}, {"timecode": 14, "before_eval_results": {"predictions": ["10,000", "perpendicular to the velocity vector", "Inherited wealth", "December 1963", "Only the series from 2009 onwards", "religious freedom in the Polish\u2013Lithuanian Commonwealth", "Spreading", "kilogram-force", "ten times their own weight", "Quaternary", "1887", "jellyfish and turtles", "a symbiotic relationship", "introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them", "Vistula River", "a handful of British military personnel operating in Iraq will be withdrawn to Kuwait after Iraq's parliament adjourned without passing a deal that would let them stay,\" the spokesman said.", "Apple CEO Tim Cook", "palesa and Alebohang", "March 8", "Mike Meehan", "the Catholic League", "1,000 pounds", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "Friday", "Movahedi", "different women coping with breast cancer in five vignettes", "mud coming out of the top of the derrick", "a smile on her face", "$40 and a quarter of bread", "Lance Cpl. Maria Lauterbach", "Hyundai Steel", "London", "more than 4,000", "Val d'Isere, France", "the test results by the medical examiner's office, Garavaglia said.", "Four Americans", "$14.1 million", "1616", "Saturn", "Chile", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "Buddhism", "J.Crew", "U.N. officials", "\"I always kind of admired him, oddly.\"", "boyhood experience in a World War II internment camp", "suppress the memories and to live as normal a life as possible", "4 meters (13 feet) high", "the Irish capital", "Republican", "in the county jail in Spanishfork,", "Mandi Hamlin", "Islamabad", "9 a.m.", "March 26, 1973", "Indian Ocean", "argument form", "the toe-line", "Sevens", "England", "Yemen", "Domenikos Theotokopoulos", "peter", "mercury"], "metric_results": {"EM": 0.359375, "QA-F1": 0.48141966540404046}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.3, 1.0, 0.6666666666666666, 0.8750000000000001, 0.4, 0.0, 0.8000000000000002, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10400", "mrqa_squad-validation-7770", "mrqa_squad-validation-4856", "mrqa_squad-validation-10458", "mrqa_squad-validation-4607", "mrqa_squad-validation-1672", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-3798", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-391", "mrqa_naturalquestions-validation-6733", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-12191", "mrqa_triviaqa-validation-3839"], "SR": 0.359375, "CSR": 0.6052083333333333, "EFR": 0.975609756097561, "Overall": 0.7904090447154472}, {"timecode": 15, "before_eval_results": {"predictions": ["Prospect Park", "Khanbaliq", "Quaternary", "1870", "water", "a prime number", "The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments.", "the Camisards", "$40 million", "GTE", "1,100", "caveat", "Oligocene", "Melodie Rydalch", "Charles Darwin", "Conway, Arkansas,", "March 24,", "the Beatles", "Robert Park", "Adriano", "Eleven people died and 36 were wounded in the Monday terror attack,", "2007", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "\"She had a smile on her face, like she always does when she comes in here,\"", "56,", "Ben Roethlisberger", "The Lost Symbol", "Heshmatullah Attarzadeh", "In fashionable neighborhoods of Tokyo customers are lining up for vitamin injections that promise to improve health and beauty.", "18 federal agents and two soldiers", "Seoul.", "resources", "66, served as vice-chairman of Hussein's Revolutionary Command Council.", "he won two Emmys for work on the 'Columbo' series starring Peter Falk.", "\"I have a very, very good family that I love back home in America, and are 100 percent behind you, and we thank God every day that you have our back.\"", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Rwanda", "75.", "to kill members of the Zetas, a ruthless cartel whose area of influence includes the eastern state of Veracruz.", "closing these racial gaps.", "\"He was held in federal custody over the weekend.", "President Bush", "a hospital in Amstetten,", "African National Congress Deputy President Kgalema Motlanthe", "\"There is nothing for me there in our country any more. I had no job and I could not afford anything. Even when I was working life was tough,\"", "\"The Kirchners have been weakened by this latest economic crisis,\"", "President Kennedy came down to Hoboken in '61 and I was only 10 years old.", "a strict interpretation of the law,", "telling CNN his comments had been taken out of context.", "Iran", "20% tax credit", "July 23.", "70,000 or so are estimated to be there now.", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "Robert Remak", "Tim McGraw", "Prussian 2nd Army", "cabbage", "a homebrew campaign setting", "Beno\u00eet Jacquot", "white, pale green, blue, gold, pink (rare), reddish-yellow or", "the Capitol", "The Left Book Club", "holography"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5714843897351519}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6428571428571429, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0975609756097561, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.4444444444444445, 0.0, 0.09523809523809525, 0.0, 0.29629629629629634, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6923076923076924, 0.2962962962962963, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9016", "mrqa_squad-validation-397", "mrqa_squad-validation-10449", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2727", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-11133", "mrqa_triviaqa-validation-6296"], "SR": 0.4375, "CSR": 0.5947265625, "EFR": 0.9722222222222222, "Overall": 0.7834743923611112}, {"timecode": 16, "before_eval_results": {"predictions": ["two thousand", "address information", "high risk of a conflict of interest and/or the avoidance of absolute powers.", "to look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system.", "Thames River", "Lt Col Paul von Lettow-Vorbeck", "several hundred thousand, some 30% of the city", "the Tower District", "Ted Ginn Jr.", "Catch Me Who Can", "John Fox", "the housing bubble", "137", "Adam Lambert and Kris Allen,", "Brian Smith", "\"Hillbilly Handfishin'\"", "Charles Lock", "voluntary negligence", "his enjoyment of sex and how he lost his virginity at age 14.", "his injuries,", "in 1979", "murder", "next year", "U.S. special envoy to Sudan,", "Christopher Savoie", "Anil Kapoor.", "Afghanistan and India", "Dr. Albert Reiter,", "\"The IAEA has inspected the known nuclear sites where they may be enriching uranium", "brutal choice", "Matthew Fisher", "cancer,", "Courtney Love,", "us to step up.\"", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "your own environmental videos", "two women", "Queen Elizabeth's birthday", "Monday,", "First Stop Resource Center and Housing Program", "Yusuf Saad Kamel", "hand-painted Swedish wooden clogs", "11 healthy eggs", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "to fritter his cash away on fast cars, drink and celebrity parties.", "Fullerton, California,", "around the United States", "in the 1950s,", "U.S. troops", "Sonia, a single mother with HIV in Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "More than 120 groups across six continents are holding vegan bake sales from April 24 through May 2.", "\"The Rosie Show,\"", "Trevor Rees,", "Oxbow,", "gastrocnemius", "Ed Sheeran", "20", "National Museum of Australia", "first of three films that comprise the \"Three Colours\"", "2001", "vingtaines", "U.S.", "George Blake", "Bogota"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5789520375457875}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.9743589743589743, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.27272727272727276, 0.22222222222222224, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.08333333333333333, 0.0, 1.0, 0.0, 0.6666666666666666, 0.14285714285714285, 0.05555555555555555, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-8570", "mrqa_squad-validation-8339", "mrqa_squad-validation-914", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-2957", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-1251", "mrqa_hotpotqa-validation-4647", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-10239", "mrqa_triviaqa-validation-6739"], "SR": 0.46875, "CSR": 0.5873161764705883, "EFR": 1.0, "Overall": 0.7936580882352942}, {"timecode": 17, "before_eval_results": {"predictions": ["lower levels of inequality", "a pharmacy practice residency", "questions and answers", "wakes (sed vigilat) and experiences visions", "Captain Francis Fowke,", "12 January", "2 million", "Zagreus", "CBS", "17", "temperate", "Rod Blagojevich,", "\"Sesame Street's\"", "Windsor, Ontario,", "$50 less,", "Afghanistan's restive provinces", "400 scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a place to sleep.", "collaborating with the Colombian government,", "Iran", "Russian concerns about the missile defense system.", "Sharon Bialek", "Matthew Fisher,", "unclear,", "in the north and west of the country,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation", "introduce legislation Thursday, to improve the military's suicide-prevention programs.\"", "$250,000", "Sunday afternoon.", "Derek Mears", "a motor scooter", "Gary Player,", "nieb\u00fcll", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "kite boards", "Virgin America", "International Polo Club Palm Beach in Florida.", "Daniel Wozniak,", "22-year-old", "Omar bin Laden,", "\"What she's doing is putting a personal and human face on the issue... there's nothing more crucial,\"", "400 Afghan farmers", "U.S. Food and Drug Administration", "Casa de Campo International Airport", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "2002", "at checkposts and military camps in the Mohmand agency,", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "Friday,", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "crocodile eggs", "from Geraldine Ferraro to Bill Clinton.", "\"aceli Valencia, was mopping the kitchen in their family home on a typical warm spring morning in Phoenix, Arizona,", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "senators", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "Messenger orbiter", "Arlene Phillips", "23 July 1989", "Ry\u016bkyuan people", "surrealism", "C. S. Lewis", "7", "rice wine", "Halifax"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5468643935765499}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true], "QA-F1": [0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.125, 1.0, 0.23529411764705885, 1.0, 1.0, 0.0, 0.3529411764705882, 0.5217391304347826, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7692307692307693, 0.15384615384615388, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.1, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7341", "mrqa_squad-validation-2408", "mrqa_squad-validation-10114", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-157", "mrqa_naturalquestions-validation-132", "mrqa_triviaqa-validation-1659", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-6296"], "SR": 0.46875, "CSR": 0.5807291666666667, "EFR": 1.0, "Overall": 0.7903645833333334}, {"timecode": 18, "before_eval_results": {"predictions": ["melatonin", "constant factors and smaller terms", "Shi Bingzhi", "Baron Dieskau", "linear", "Advanced Steam", "Defensive ends", "the dot", "chastity", "European Court of Justice", "bronze medal in the women's figure skating final,", "\"trying to steal the election\" and \"intimidating the population and election officials as well.\"", "UK", "\"Gandhi,\"", "Argentina", "Congress", "28", "Frank Ricci,", "the project, which is designed to promote private sector investment in a variety of gas-related industries,", "\"terrifying.\"", "Bill & Melinda Gates Foundation", "$106,482,500", "people give the United States abysmal approval ratings.", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "\"political and religious\"", "$163 million (180 million Swiss francs)", "Afghan lawmakers", "North Korea", "a precaution and did not change the threat level, an administration official said.", "the mammoth's fossil was found along with 16 other deposits at the site that paleontologists \"tree-boxed\" along with the surrounding dirt,", "\"global security, prosperity and freedom.\"", "because the federal government is both dishonest and reckless.", "Molotov cocktails, rocks and glass.", "\"wildcat\" strikes, unsanctioned by national unions, at other sites across the country.", "Ben Roethlisberger", "Dr. Christina Romete,", "Ewan McGregor", "Brazil", "Meira Kumar", "next week.", "other parts of Asia, such as India and mainland China,", "Lindsey Vonn", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "race", "Los Ticos", "the Formagruppen ( Engelbrektsgatan 8)", "trying to detonate an explosive device in his underwear aboard a Christmas 2009 flight to Detroit,", "two people", "40-year-old", "Pakistan's North West Frontier Province,", "Casey Anthony,", "the iconic Hollywood headquarters of Capitol Records,", "Emma Watson and Dan Stevens", "2002", "his finger.", "\"Sunny Afternoon\"", "Che Guevara", "Miller Brewing", "Elizabeth I", "John Fogerty", "Garonne", "giraffe", "cheese"], "metric_results": {"EM": 0.515625, "QA-F1": 0.600410969004719}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.2857142857142857, 0.8571428571428571, 1.0, 0.0, 0.9714285714285714, 0.0, 0.5, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.625, 0.2666666666666667, 0.0, 0.0, 0.0, 0.16, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2702702702702703, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1603", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9104", "mrqa_triviaqa-validation-7611"], "SR": 0.515625, "CSR": 0.5773026315789473, "EFR": 0.967741935483871, "Overall": 0.7725222835314092}, {"timecode": 19, "before_eval_results": {"predictions": ["1876", "1507", "Danny Trevathan", "11", "that unless he were removed from the school, Tesla would be killed through overwork.", "Japanese", "Muqali, a trusted lieutenant,", "2011 and 2012,", "Pittsburgh Steelers", "apartment building", "Aung San Suu Kyi", "(The Frisky)", "3rd District of Utah.", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Stephen Tyrone Johns", "three", "procedures", "acid attack by a spurned suitor.", "most of those who managed to survive the incident hid in a boiler room and storage closets", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to the coming days.\"", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Courtney Love,", "33-year-old", "cell phones", "a book.", "he was diagnosed with skin cancer.", "to leave once there is a solid political solution to the conflict.", "Ashley \"A.J. Jewell,", "at least 17", "from her father's home in Satsuma, Florida,", "to the southern city of Naples", "Hugo Chavez", "London's", "home in rural California,", "knocking the World Cup off the front pages for the first time in days.", "Old Trafford", "the area of the 11th century Preah Vihear temple", "steam-driven, paddlewheeled overnight passenger boat.", "individual pieces.", "Alaska or Hawaii.", "homicide", "The Ski Train", "Aniston, Demi Moore and Alicia Keys", "Lillo Brancato Jr.", "intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "Blagojevich", "$60 billion on America's infrastructure.", "protective shoes", "public-sector labor", "U.S. President-elect Barack Obama", "Burhanuddin Rabbani, a former Afghan president who had been leading the Afghan peace council,", "was depressed over a recent breakup, grabbed the gun and  took her own life.", "often discard beer bottles on pebbled walkways.", "Sedimentary rock", "3.45 billion years ago ( 2.45 Ga ), during the Siderian period, at the beginning of the Proterozoic eon", "London", "Colorado", "Bangor International", "his 2005 collaboration with GZA, \"Grandmasters\", and his 2007 collaboration with Sick Jacken, \" Legend of the Mask and the Assassin\"", "Suffragist", "Canterbury", "Tunisia", "Silver", "Bonnie and Clyde"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6435170575171709}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.10810810810810811, 0.07692307692307693, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.8333333333333333, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5, 0.5714285714285715, 1.0, 0.0, 1.0, 0.2857142857142857, 0.4, 0.0, 1.0, 0.896551724137931, 0.0, 1.0, 0.28571428571428575, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1326", "mrqa_squad-validation-6143", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-413", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-6758", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2805"], "SR": 0.484375, "CSR": 0.57265625, "EFR": 0.9696969696969697, "Overall": 0.7711766098484849}, {"timecode": 20, "before_eval_results": {"predictions": ["the Hostmen", "Greg Brady", "Jacksonville,", "Hungarians", "De Materia Medica", "John D. Rockefeller", "four", "lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Beijing, China", "Virgil Tibbs", "Thaddeus Rowe Luckinbill", "up to 100,000", "the United States, its NATO allies and others", "Virginia Dare", "JackScanlon", "Kylie Minogue", "94 by 50", "Lalo Schifrin", "MGM Resorts International", "16 August 1975", "seawater", "1962", "Buddhist", "1978", "1927, 1934, 1938, 1956", "1969", "New England Patriots", "Joseph Heller", "the north pole", "1,350", "Leonard Bernstein", "25 September 2007", "Howard Caine", "Branford College", "62", "the team", "November 2014", "Archduke Franz Ferdinand of Austria", "Koine Greek : apokalypsis", "October 1941", "to refer to either peace between two entities ( especially between man and God or between two countries )", "bageecha or bagicha", "Cee - Lo", "before they kill him", "lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Labour Party", "three times", "November 25, 2002,", "December 27, 2015", "Peter Greene", "31 March 1909", "Ed Sheeran", "two", "Alberto Salazar", "live animals", "American", "Hoosick", "CEO of an engineering and construction company with a vast personal fortune.", "1.2 million", "The Three Little Pigs", "Robert Louis Stevenson", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Russia and China", "Hanford Nuclear Site,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5961061507936508}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3086", "mrqa_squad-validation-6314", "mrqa_squad-validation-8027", "mrqa_squad-validation-6737", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-8934", "mrqa_triviaqa-validation-3886", "mrqa_hotpotqa-validation-2298", "mrqa_newsqa-validation-3687", "mrqa_searchqa-validation-13486", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-2446"], "SR": 0.515625, "CSR": 0.5699404761904762, "EFR": 0.9354838709677419, "Overall": 0.752712173579109}, {"timecode": 21, "before_eval_results": {"predictions": ["about 515 million years ago", "Spanish", "an attack on New France's capital, Quebec", "the Fresno Traction Company", "Westminster", "the ancestors of chloroplasts", "24 of the 32 songs", "Sauron", "Washington metropolitan area", "pH 7 ( 25 \u00b0 C )", "the breast or lower chest of beef or veal", "the ruling city of the Northern Kingdom of Israel,", "in either Tagalog or English", "around 1600 BC", "1984", "Michael Phelps", "Rajendra Prasad", "Ren\u00e9 Georges Hermann - Paul", "Donna", "the New York Yankees", "Orangeville, Ontario, Canada", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Janie Crawford,", "the 2nd century", "in the pancreas", "the Elk and Kanawha Rivers", "1961", "the latest version", "in rocks and minerals", "Michael Schumacher", "currency option", "1957", "1775", "1995", "Field Marshal Paul von Hindenburg", "2018", "Ireland", "Kit Harington", "her boyfriend Lance", "embryo", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Baker, California, USA", "Raja Dhilu", "October 12, 1979", "Guy Berryman", "Canadian ice dancers", "Sophocles", "Tim Allen as Luther Krank", "thick skin", "a cake", "India", "Brazil, Turkey and Uzbekistan", "on Chesapeake Bay,", "batsman", "the Major General of the Navy", "Marktown", "approximately 14,000", "that Iran could be secretly working on a nuclear weapon", "Honduras", "Pardon of Richard Nixon", "Ellen DeGeneres", "1961", "punk rock", "Westfield Old Orchard"], "metric_results": {"EM": 0.375, "QA-F1": 0.5099226590403061}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.9333333333333333, 0.0, 0.30769230769230765, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.8, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 0.8, 0.0, 0.4, 0.0, 0.5454545454545454, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.4, 0.0, 0.0, 1.0, 1.0, 0.08, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4472", "mrqa_squad-validation-8780", "mrqa_squad-validation-2387", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-4641", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-3883", "mrqa_hotpotqa-validation-3984"], "SR": 0.375, "CSR": 0.5610795454545454, "EFR": 1.0, "Overall": 0.7805397727272727}, {"timecode": 22, "before_eval_results": {"predictions": ["present-day Pittsburgh, Pennsylvania", "Stanford University", "linebacker", "the Mongol and Turkic tribes", "between 1859 and 1865", "Danny Lane", "in the New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13 )", "The Fixx", "President Andrew Johnson", "Hellenism", "Mark Jackson", "Manhattan Island", "The Chainsmoker and British rock band Coldplay", "an annual income of US $11,770", "modestly", "week 4", "L.K. Advani", "the President", "Zachary John Quinto", "A.R. Rahman", "Massachusetts", "a single particle", "Thomas Jefferson", "the head of Lituya Bay in Alaska", "Manhattan, the Bronx, Queens, Brooklyn, and Staten Island", "on a sound stage in front of a live audience in Burbank, California", "Grace Zabriskie", "2014", "Yuzuru Hanyu", "Glenn Close", "Elk", "flawed democracy", "China", "Kirk Douglas", "Jodie Foster", "2007 -- 08", "Neil Patrick Harris", "8ft", "Owen Vaccaro", "bacteria", "on the lateral side", "Lynda Carter", "erosion", "90 \u00b0 N 0 \u00b0 W \ufeff / \ufefc 90 \u00b0N - 0 \u00b0 E", "London to Canterbury", "into the bloodstream or surrounding tissue following surgery, disease, or trauma", "in 2005", "February 29", "1840s", "9.7 m ( 31.82 ft )", "Montgomery", "Juliet", "Hotel California", "Paul Revere", "beetle", "Pearl Jam", "2005", "Dan Tyminski", "The 19-year-old woman", "in the country's largest city of Karachi", "at least nine", "Bashar al-Assad", "the New Revised Standard Version", "biathlon"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6256702473754104}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.25, 1.0, 0.34782608695652173, 0.33333333333333337, 0.8, 1.0, 1.0, 0.5, 0.2857142857142857, 0.5714285714285715, 0.0, 0.5714285714285715, 0.14814814814814814, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.8, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8148148148148148, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.5454545454545454, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10228", "mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2079", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-7486", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-1046", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-9457", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-2101", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1295", "mrqa_searchqa-validation-15510"], "SR": 0.46875, "CSR": 0.5570652173913043, "EFR": 0.9411764705882353, "Overall": 0.7491208439897699}, {"timecode": 23, "before_eval_results": {"predictions": ["internal strife", "a new stage in the architectural history of the regions they subdued", "Fresno", "castles and vineyards", "below 0 \u00b0C (32 \u00b0F)", "Von Miller", "Kansas", "Thaddeus Rowe Luckinbill", "December 25", "2002", "the Mandate of Heaven", "Geoffrey Zakarian", "Christopher Allen Lloyd", "prenatal development", "Holly", "Vijay Prakash", "Article 1, Section 2", "the Constitution of India came into effect on 26 January 1950", "Thomas Marvolo Riddle", "Dick Rutan and Jeana Yeager", "in sequence with each heartbeat", "Ren\u00e9 Descartes", "James P. Flynn", "detritus", "September 27, 2017", "Johnny Logan", "1981", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Tony Orlando and Dawn", "February 10, 2017", "Alex Skuby", "the Colony of Virginia", "March 2016", "1922", "Richard Roxburgh", "Bacon", "an explosion", "Heather Stebbins", "Redenbacher family", "sometimes ambiguous designation of two classes of organic compounds", "`` 0 '' trunk code", "April 1, 2016", "Friedman Billings Ramsey", "New York City", "cutting surfaces", "Massachusetts Compromise", "Justin Timberlake", "Andrew Moray and William Wallace", "Alamodome and city of San Antonio", "asexually", "John Garfield", "In 1871", "eye", "The History Boys", "sturgeon", "the White Knights of the Ku Klux Klan", "five", "Darkthrone", "Kingman Regional Medical Center,", "Phillip A. Myers", "CAIRO, Egypt", "Antarctica", "axon", "shiitake"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5901289682539683}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 0.3333333333333333, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333333, 0.2857142857142857, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1129", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1445", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-1207", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-3651", "mrqa_hotpotqa-validation-395", "mrqa_newsqa-validation-505", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-12624"], "SR": 0.46875, "CSR": 0.5533854166666667, "EFR": 0.9705882352941176, "Overall": 0.7619868259803921}, {"timecode": 24, "before_eval_results": {"predictions": ["research, exhibitions and other shows", "no damage", "Stadtholder William III of Orange", "1945", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Jim Thorpe", "the first hole of a sudden-death playoff", "the onset and progression of Alzheimer's disease", "Disco", "the \"Cisleithanian\" half of Austria-Hungary", "the Indian School of Business", "New York City", "Charles Whitman", "C. H. Greenblatt", "\"The Curious Case of Benjamin Button\"", "A55", "Corendon Dutch Airlines", "86", "Minneapolis, Minnesota", "the sea loch", "Fatih Ozmen", "U.S.", "Pacific Place", "writing for \"The New York Times\" and \"Popular Mechanics\", and is a regular contributor to various CNBC shows such as \"On the Money\"", "the Donny & Marie Showroom", "City of Westminster, London", "2016", "Steve Cuden", "New York University School of Law", "Crips", "Harper's Bazaar", "dementia", "the Province of Ferrara", "Guadalcanal Campaign", "Bishop's Stortford", "Starvation Is Motivation", "Barbara Niven", "Black Friday", "Archbishop of Canterbury", "TD Garden", "Vic Chesnutt", "the Teutonic Knights", "Australian", "Julie Taymor", "Easy", "World War I", "79 AD", "musicologist", "Portland, OR", "Yoruba", "\"Lucky\"", "Charles Otto Puth Jr.", "2007 and 2008", "2001", "1966", "Jeremy Clarkson", "Jehan Mubarak", "Medellin", "Joe Jackson", "alternative-energy vehicles", "2004", "genes", "olive", "Tjejmilen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6154141865079366}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.0, 0.8571428571428571, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2153", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-5348", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-431", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-2659", "mrqa_triviaqa-validation-3361", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2930", "mrqa_searchqa-validation-5511"], "SR": 0.53125, "CSR": 0.5525, "EFR": 0.9333333333333333, "Overall": 0.7429166666666667}, {"timecode": 25, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-1123", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-4562", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3206", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-3654", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-11385", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11900", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12624", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-1335", "mrqa_searchqa-validation-13486", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14883", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-16181", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3783", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-4857", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8589", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-9756", "mrqa_squad-validation-10045", "mrqa_squad-validation-10069", "mrqa_squad-validation-10074", "mrqa_squad-validation-10086", "mrqa_squad-validation-10216", "mrqa_squad-validation-10228", "mrqa_squad-validation-10254", "mrqa_squad-validation-10310", "mrqa_squad-validation-10324", "mrqa_squad-validation-10338", "mrqa_squad-validation-10353", "mrqa_squad-validation-1036", "mrqa_squad-validation-10378", "mrqa_squad-validation-10477", "mrqa_squad-validation-1090", "mrqa_squad-validation-1320", "mrqa_squad-validation-1450", "mrqa_squad-validation-1603", "mrqa_squad-validation-1636", "mrqa_squad-validation-1672", "mrqa_squad-validation-1694", "mrqa_squad-validation-178", "mrqa_squad-validation-1802", "mrqa_squad-validation-1852", "mrqa_squad-validation-1855", "mrqa_squad-validation-1857", "mrqa_squad-validation-1938", "mrqa_squad-validation-1967", "mrqa_squad-validation-2040", "mrqa_squad-validation-2126", "mrqa_squad-validation-2153", "mrqa_squad-validation-2216", "mrqa_squad-validation-2289", "mrqa_squad-validation-2384", "mrqa_squad-validation-2400", "mrqa_squad-validation-2436", "mrqa_squad-validation-2460", "mrqa_squad-validation-2477", "mrqa_squad-validation-255", "mrqa_squad-validation-2577", "mrqa_squad-validation-2602", "mrqa_squad-validation-2619", "mrqa_squad-validation-268", "mrqa_squad-validation-2693", "mrqa_squad-validation-2773", "mrqa_squad-validation-2782", "mrqa_squad-validation-2798", "mrqa_squad-validation-282", "mrqa_squad-validation-2824", "mrqa_squad-validation-285", "mrqa_squad-validation-2929", "mrqa_squad-validation-3019", "mrqa_squad-validation-3041", "mrqa_squad-validation-3135", "mrqa_squad-validation-3185", "mrqa_squad-validation-320", "mrqa_squad-validation-3337", "mrqa_squad-validation-3476", "mrqa_squad-validation-353", "mrqa_squad-validation-3589", "mrqa_squad-validation-3709", "mrqa_squad-validation-383", "mrqa_squad-validation-3931", "mrqa_squad-validation-3948", "mrqa_squad-validation-3955", "mrqa_squad-validation-397", "mrqa_squad-validation-3993", "mrqa_squad-validation-4005", "mrqa_squad-validation-4079", "mrqa_squad-validation-4140", "mrqa_squad-validation-415", "mrqa_squad-validation-4181", "mrqa_squad-validation-427", "mrqa_squad-validation-4291", "mrqa_squad-validation-4305", "mrqa_squad-validation-4333", "mrqa_squad-validation-4338", "mrqa_squad-validation-4472", "mrqa_squad-validation-462", "mrqa_squad-validation-4686", "mrqa_squad-validation-4704", "mrqa_squad-validation-4835", "mrqa_squad-validation-4856", "mrqa_squad-validation-4870", "mrqa_squad-validation-5054", "mrqa_squad-validation-5088", "mrqa_squad-validation-5096", "mrqa_squad-validation-5154", "mrqa_squad-validation-5176", "mrqa_squad-validation-5238", "mrqa_squad-validation-5302", "mrqa_squad-validation-5326", "mrqa_squad-validation-5376", "mrqa_squad-validation-550", "mrqa_squad-validation-5537", "mrqa_squad-validation-5541", "mrqa_squad-validation-5588", "mrqa_squad-validation-5616", "mrqa_squad-validation-5672", "mrqa_squad-validation-5703", "mrqa_squad-validation-5767", "mrqa_squad-validation-5777", "mrqa_squad-validation-5913", "mrqa_squad-validation-60", "mrqa_squad-validation-60", "mrqa_squad-validation-607", "mrqa_squad-validation-6099", "mrqa_squad-validation-6126", "mrqa_squad-validation-6143", "mrqa_squad-validation-6178", "mrqa_squad-validation-6220", "mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6362", "mrqa_squad-validation-6395", "mrqa_squad-validation-6414", "mrqa_squad-validation-6564", "mrqa_squad-validation-660", "mrqa_squad-validation-6641", "mrqa_squad-validation-6737", "mrqa_squad-validation-6754", "mrqa_squad-validation-6782", "mrqa_squad-validation-68", "mrqa_squad-validation-6817", "mrqa_squad-validation-6915", "mrqa_squad-validation-696", "mrqa_squad-validation-7018", "mrqa_squad-validation-703", "mrqa_squad-validation-7069", "mrqa_squad-validation-707", "mrqa_squad-validation-7150", "mrqa_squad-validation-7161", "mrqa_squad-validation-7180", "mrqa_squad-validation-7198", "mrqa_squad-validation-7260", "mrqa_squad-validation-7399", "mrqa_squad-validation-754", "mrqa_squad-validation-7552", "mrqa_squad-validation-7597", "mrqa_squad-validation-7640", "mrqa_squad-validation-765", "mrqa_squad-validation-7678", "mrqa_squad-validation-7770", "mrqa_squad-validation-7782", "mrqa_squad-validation-7814", "mrqa_squad-validation-7856", "mrqa_squad-validation-7882", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-804", "mrqa_squad-validation-8056", "mrqa_squad-validation-8104", "mrqa_squad-validation-8115", "mrqa_squad-validation-8189", "mrqa_squad-validation-8226", "mrqa_squad-validation-8226", "mrqa_squad-validation-8285", "mrqa_squad-validation-8406", "mrqa_squad-validation-8480", "mrqa_squad-validation-8527", "mrqa_squad-validation-8629", "mrqa_squad-validation-8735", "mrqa_squad-validation-8760", "mrqa_squad-validation-8765", "mrqa_squad-validation-8832", "mrqa_squad-validation-884", "mrqa_squad-validation-8867", "mrqa_squad-validation-890", "mrqa_squad-validation-8957", "mrqa_squad-validation-898", "mrqa_squad-validation-9031", "mrqa_squad-validation-9066", "mrqa_squad-validation-9135", "mrqa_squad-validation-9186", "mrqa_squad-validation-9227", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9391", "mrqa_squad-validation-9392", "mrqa_squad-validation-9465", "mrqa_squad-validation-9504", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9652", "mrqa_squad-validation-9658", "mrqa_squad-validation-9771", "mrqa_squad-validation-979", "mrqa_squad-validation-9818", "mrqa_squad-validation-987", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-3051", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-469", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5568", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-6909", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-776"], "OKR": 0.861328125, "KG": 0.4390625, "before_eval_results": {"predictions": ["progressive tax", "Jacksonville", "monophyletic", "Orthogonal components", "Fox Network", "La clemenza di Tito", "Anna Clyne", "Terence Winter, based on the memoir of the same name by Jordan Belfort", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Formula E", "Eastern College Athletic Conference", "Kim Jong-hyun", "Peter Chelsom", "The Ninth Gate", "heavy metal", "Cinderella", "Los Angeles", "Sharyn McCrumb", "Acid house", "Austria, south Germany, German Switzerland, and Slovenia at the end of the 18th century", "\"Fimm Borginn\"", "Miranda Leigh Lambert", "Shenandoah National Park", "BBC Formula One coverage", "10 Years", "Haleiwa Ali'i Beach Park", "Armin Meiwes", "1886", "Rockhill Furnace, Pennsylvania", "northeastern", "coca wine", "Entrepreneur", "the lead roles", "PBS", "second largest", "acid", "in 1911", "Johnnie Ray", "The Five", "Walt Disney Feature Animation", "in 2018", "torpedo boats", "1972", "Geographical Indication tag", "Ringo Starr", "Celtics", "World Championship Wrestling", "1994", "TD Garden", "the Chechen Republic", "Chrysler", "Princeton University", "2005", "the emperor Cuauhtemoc and Tenochtitlan, the capital of the Aztec Empire", "Tax Reform Act of 1986", "Henry Louis", "Mexico", "ThunderCats", "\"he was preparing the country for war and death, and to hand power to Kim Jong Un,\"", "the Catholic League", "Krishna Rajaram,", "gorgicus", "to earn the nickname Super Eli", "green"], "metric_results": {"EM": 0.5625, "QA-F1": 0.642686212998713}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.1081081081081081, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-2056", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-4298", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9487", "mrqa_triviaqa-validation-7575", "mrqa_newsqa-validation-2772", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4240"], "SR": 0.5625, "CSR": 0.5528846153846154, "EFR": 0.9642857142857143, "Overall": 0.7135121909340659}, {"timecode": 26, "before_eval_results": {"predictions": ["bacteriophage T4", "1698", "Xingu", "The Ruhr", "Dar es Salaam", "Heinkel Flugzeugwerke", "Jesus", "Pope John X", "Harold Holt", "aged between 11 or 13 and 18", "Sleeping Beauty", "Orchard Central", "Tom Holland", "late eighteenth century", "The Snowman", "1979", "Premier League club Liverpool and the England national team", "port city of Aden,", "British", "Prince Louis of Battenberg", "2008", "Archie Andrews", "2 May 2015", "17 December 177026 March 1827", "Crystal Dynamics", "Cleveland Cavaliers", "goalkeeper", "Debbie Harry", "\"media for the 65.8 million,\"", "John Joseph Travolta", "Hall & Oates", "the port of Mazatl\u00e1n", "Michael Owen", "Las Vegas, Nevada", "1919", "Kevin Spacey", "Love Streams", "Michael Edwards", "The Rite of Spring", "Lake Wallace", "England", "1993", "Boston Celtics", "The Eisenhower Executive Office Building", "6,396", "Australian coast", "The Saturdays", "Attack the Block", "Leonarda Cianciulli", "Morse Field", "Tudor music and English folk-song", "CMYKOG process", "1600 BC ( possibly a fragmentary copy of a text from 2500 BC )", "Obi - Wan McGregor", "between 1939 and 1948", "a palla", "RoRo", "Lyrical Ballads", "15,000", "10 to 15 percent", "\"It has never been the policy of this president or this administration to torture.\"", "Tarzan of the Apes", "Japan", "potp0urri"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6688873626373626}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.923076923076923, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.28571428571428575, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-1352", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5619", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-1677", "mrqa_newsqa-validation-4143", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-13669"], "SR": 0.5625, "CSR": 0.5532407407407407, "EFR": 1.0, "Overall": 0.7207262731481481}, {"timecode": 27, "before_eval_results": {"predictions": ["immediately north of Canaveral at Merritt Island", "pedagogic diversity of his/her students", "Catholic", "Extension", "Cinderella", "Dan Tyminski", "Guthred", "WWE 2K18", "Kolkata", "Dumb and Dumber", "Boeing EA-18G Growler", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Paper", "Alistair Grant", "Whitney Elizabeth Houston", "Dunlop Tyres", "Bonkyll Castle", "Cheick Isma\u00ebl Tiot\u00e9", "Algernod Lanier Washington", "erotic romantic comedy", "due to a leg injury", "Antonio Salieri", "American", "Europe", "Cesario", "Brooklyn, New York", "Thriller", "Jesper Myrfors", "The Supremes", "\"A Song of Ice and Fire\"", "Elizabeth Keka\u02bbaniau La\u02bbanui Pratt", "Indonesian National Revolution", "Charlie B. Barkin", "1979", "Chief of the Operations Staff of the Armed Forces High Command", "Hong Kong Disneyland", "London", "\"I Should Have Known Better\"", "September 8, 2017", "FBI", "Christine MacIntyre", "April 25, 1995", "Wildhorn, Bricusse and Cuden", "Hotch kiss M1914 machine gun", "James Brolin", "Prussian army general", "January 2004", "Peter Lawrence Buck", "ten", "seven", "October 25, 1881", "Julius Robert Oppenheimer", "Pradyumna", "Mark Jackson", "R / T badging", "The Colossus of Rhodes", "Equatorial Guinea", "c3H8O3", "identity documents", "Chris Robinson and girlfriend Allison Bridges", "off east  Africa", "corsets", "septum", "cQR - CQ Press Library"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5670204156223893}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.4000000000000001, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3840", "mrqa_squad-validation-1916", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-873", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3346", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-3400", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-2957", "mrqa_naturalquestions-validation-4039", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-6398"], "SR": 0.484375, "CSR": 0.55078125, "EFR": 1.0, "Overall": 0.720234375}, {"timecode": 28, "before_eval_results": {"predictions": ["public (government) funding", "boarding schools and day schools", "a program of coordinated, evolving projects sponsored by the National Science Foundation", "Hanover", "Henry Mancini", "b. A person of the Japanese", "Gordon Ramsay", "Gorbachev", "\"M*A*S*H\" or the Bill Murray character in \"Stripes.\"", "secret", "Charlotteton Heston", "Anna (Julia Roberts)", "binky", "orchid", "Paddy Doherty", "smallpox", "the Hanging Gardens of Babylon", "the Central African Republic", "Yeehaw", "F\u00fcr Elise", "Iran", "Who\u2019s Who", "wry, compassionate, and brimm[ing] with... open-minded intelligence", "Milady de Winter", "April", "Eric Morley", "ADHD and hypertension", "the Garrick Club", "a beast", "Fabio Capello", "New York", "Stoppard", "The Greatest", "the manager", "the British charts", "a Scotsman\u2019s bonnet", "jazz pianist", "Seattle", "the Cross Foxes Inn", "Cardiff", "Baton Rouge", "stromberg carburetors", "Tahrir Square", "Romanian", "the \"useful life period\"", "Michael Caine", "Lord Snooty", "Robert Wright", "Jesse James", "Meerkat", "Greek", "passion fruit", "Thomas Lennon", "Haikou on the Hainan Island", "Thomas Hobbes in his Leviathan", "Denmark and Norway", "1966", "North America", "Florida's Everglades", "Garth Brooks", "glamorous, sexy and international.", "drive", "a set of steak knives", "Sebastian Stark"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5276537698412699}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.28571428571428575, 0.4285714285714285, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6891", "mrqa_squad-validation-6918", "mrqa_squad-validation-4846", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-6165", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-2313", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-7182", "mrqa_naturalquestions-validation-4427", "mrqa_hotpotqa-validation-2910", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1004", "mrqa_searchqa-validation-12186", "mrqa_searchqa-validation-15919"], "SR": 0.46875, "CSR": 0.5479525862068966, "EFR": 1.0, "Overall": 0.7196686422413794}, {"timecode": 29, "before_eval_results": {"predictions": ["Chicago Theological Seminary", "CBS", "$100,000", "Super Bowl LII", "starch", "Taylor Michel Momsen", "Kennedy Space Center ( KSC ) in Florida", "Kyrie Irving", "James W. Marshall", "in florida", "Randy VanWarmer", "negotiates treaties with foreign nations", "Emma Thompson", "between 8.7 % and 9.1 %", "2018", "if the occurrence of one does not affect the probability of occurrence of the other", "Jason Flemyng", "Chesapeake Bay, south of Annapolis in Maryland", "northern China", "T.J. Miller", "Pyeongchang County, Gangwon Province, South Korea", "the status line", "retina", "jimmy heston", "Triple Alliance of Germany, Austria - Hungary, and Italy", "Andrew Lloyd Webber", "1955", "Nick Sager", "Buffalo Lookout", "Humpty Dumpty", "Charlene Holt", "close to 5,770", "the original Star Trek television series", "1960", "Sam", "10.5 %", "beneath the liver", "Andy Serkis", "West Norse sailors", "Kristy Swanson", "Christy", "early to mid-2000s", "Fleetwood Mac", "technological advances in printing", "Illinois", "in the 1970s and'80s", "in the books of Exodus and Deuteronomy", "Wyatt", "Psychomachia", "January 2, 1971", "The Miracles", "taxonomy", "arm", "jimmy heston", "Charlie Sheen", "\"Twice in a Lifetime\"", "Nineteen Eighty-Four", "Bardot", "teenager", "Long Island convenience store", "Romney", "Peter", "heart disease", "dancing with the stars"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5844122023809524}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.45454545454545453, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5764", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-6865", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-6508", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-1958", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-4017", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7426"], "SR": 0.515625, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.719453125}, {"timecode": 30, "before_eval_results": {"predictions": ["San Jose", "quality rental units", "cultural tourism and sports tourism", "a circle", "redwood", "Tara Lyn Charendoff", "Spanish Republic", "taximeter", "coyote", "The Sun Also Rises", "Harry Reid", "Ray", "Axis", "forge", "Kinetoscope", "J! Archive", "flowers", "Blackbird", "Footprints", "Caliban", "corey prapavessis", "U.S. Census Bureau", "The Fugitive", "Jesus", "The Memory Keeper's daughter", "George Eliot", "hubris", "Yahtzee", "(TV Series 19841992)", "markup language", "hives", "74.3 years", "William S. Hart", "( Bakrih al-Bukhari, Volume 1, Book 8, Number 367)", "Pride and Prejudice", "adjectives", "Kosher Wines", "Munich", "Michael Jordan", "(St.) Andrew's Day", "Ariel", "Hikaru Sulu", "parrots", "dough", "Kyushu", "honey", "Boston", "Fisher- Price", "Arctic Ocean", "the Italian flag", "Pumpkin soup", "Spain", "Thomas Chisholm", "May 2002", "1936", "Newfoundland and Labrador", "The Gangster Story", "Monty Python's Spamalot", "1911", "Harlow Cuadra and Joseph Kerekes", "Voltaire in Love", "Israel", "Adidas", "Intel has systematically given PC makers and stores rebates to keep computers with AMD chips off the shelves."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6067708333333334}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-315", "mrqa_squad-validation-2965", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-2374", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-11167", "mrqa_searchqa-validation-5093", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-16307", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-12415", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-9773", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-9379", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-6412", "mrqa_naturalquestions-validation-10656", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3030", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-3915"], "SR": 0.546875, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.719453125}, {"timecode": 31, "before_eval_results": {"predictions": ["Rev. Paul T. Stallsworth", "white", "Bill Cosby", "satirical erotic romantic comedy", "the Ferengi bartender Quark", "the Social Democratic Party of Austria", "1970", "Bloomingdale Firehouse", "elizabeth Stefanik", "Fleetwood Mac", "Odense Boldklub", "Supreme Court", "Bangkok", "The Oklahoma Sooners", "Weare", "Charlie Wilson", "The Late Late Show", "Mark Anthony", "two", "Indianapolis Motor Speedway", "the Byzantine Exarchate of Ravenna", "Anita Dobson", "a family member", "October 4, 1970", "The Worm", "Eliot Cutler", "Blackheart Records", "1970s and 1980s", "C. J. Cherryh", "Pablo Escobar", "16,116", "Nyack", "\"Slaughterhouse-Five\"", "Adventures of Huckleberry Finn", "wine and cellar door", "Frank Sinatra", "Robert L. Stone", "goalkeeper", "Philadelphia", "New York", "Town of Oyster Bay", "Sinngedichte", "Highwayman", "Madrid", "Kevin Spacey", "Arizona State University", "Blue Grass Airport", "Lawton Chiles", "1952", "the Nebula Award, the Philip K. Dick Award, and the Hugo Award", "I'm Shipping Up to Boston", "the Royal Navy", "Lenny Jacobson", "Hathi Jr", "1935", "a serpent", "\"Araf\"", "kenty", "tunnels", "anti-trust laws.", "Friday", "the High Plains", "Franklin D. Roosevelt", "Ukraine"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6356770833333334}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-2607", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-844", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-5034", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-6414", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-6055"], "SR": 0.578125, "CSR": 0.5478515625, "EFR": 1.0, "Overall": 0.7196484375}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh L. Dryden", "2004", "Kenya", "The Rocky Horror Picture Show", "Trainspotting", "Argentina", "Apollo 11 Lunar Module (LM) \"Eagle\"", "jellyfish", "March", "a clogs", "Fauntleroy", "the World Health Organization", "Eat porridge", "Kofi Annan", "oxygen", "the first daily newspaper in London", "Taggart", "Han Solo", "the Atlantic Ocean", "i second that emotion", "Sven Goran Eriksson", "the BBC motto", "Route 66", "Brussels", "Flora MacDonald", "John Poulson", "Charles de Gaulle", "the euro", "Jack Frost", "Saskatchewan (Province)", "Ambroz Bajec-Lapajne", "the Solent", "vomiting", "The Red Lion", "Bristol Aeroplane Company", "lettuces", "Steve Davis", "i second that emotion", "Gemini", "Surrey", "1969", "chippenham", "London", "Chile", "William Shakespeare", "sodium tetraborate decahydrate", "a \"metropolitano\" (in Turin) or \"surburbano\" ( in Milano", "Jamaica", "Peter Nichols", "Jason David", "Kent", "Vickers-Armstrong's", "Ray Charles", "Rigor mortis is very important in meat technology", "USCS or USC", "Miller Brewing", "northwestern", "Sydney", "the earthquake's aftermath", "her decades-long portrayal of Alice Horton", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Peter Bogdanovich", "a mourning dove", "Cyprus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6048350087412586}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.12121212121212123, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-5360", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-4986", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5129", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-6989", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-5817", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-3368", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-16539"], "SR": 0.53125, "CSR": 0.5473484848484849, "EFR": 1.0, "Overall": 0.7195478219696969}, {"timecode": 33, "before_eval_results": {"predictions": ["New York and Virginia", "1887", "Lana Del Rey", "1,228 km / h ( 763 mph )", "New England Patriots", "Doc '' Brown, Ph. D.", "Antarctica", "Mitch Murray", "blue", "Gunpei Yokoi", "John Bull", "775 rooms", "eusebeia", "waiting tables at the Moondance Diner", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "a long line", "Jesus'birth", "a habitat", "Kirsten Simone Vangsness", "Central Germany", "Andrew Johnson", "Etienne de Mestre", "Aegisthus", "electors", "Julia Ormond", "Sauron's assistance", "1961", "ste\u026and / STAYND", "2013", "`` leaper ''", "a book", "a usually red oxide formed by the redox reaction of iron and oxygen in the presence of water or air moisture", "Spain", "Taylor Hayes", "Paul Lynde", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Jocelyn Flores", "fled to exile in the Netherlands", "most - visited paid monument in the world", "erosion", "March 2, 2016", "a large roasted turkey", "1996", "Ray Charles", "16", "the Ramones", "1800", "Anglo - Norman French waleis", "Frank Theodore `` Ted '' Levine", "Los Angeles", "May 2010", "France", "bbc", "dennis taylor", "a centaur", "singer and a DJ", "cricket fighting", "Luis Edgardo Resto", "drama that pulls in the crowds", "a Nazi German death camp", "Islamabad", "Tunisia", "a research organization that develops solutions to public policy challenges to help make communities throughout the world safer and more secure", "a bach"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5266832756354191}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.5, 0.06451612903225806, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.7368421052631579, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8205128205128205, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3127", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4561", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-5607", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-1997", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2118", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-9333"], "SR": 0.453125, "CSR": 0.5445772058823529, "EFR": 1.0, "Overall": 0.7189935661764706}, {"timecode": 34, "before_eval_results": {"predictions": ["Venus", "Beyonc\u00e9 and Bruno Mars", "Cal", "7th century", "2018", "her abusive husband", "September 29, 2017", "interstellar medium", "transmission and final drive", "Universal Pictures", "Tanvi Shah", "March 14, 1942", "Nick Sager", "London boroughs, Metropolitan boroughs, unitary authorities, and district councils", "prophets", "Assam", "digestion of proteins", "Renishaw Hall, Derbyshire, England", "accomplish the objectives of the organization", "pickup trucks", "Isabella Palmieri", "101.325 kPa", "Mind your Ps and Qs", "Germany", "20 November 1989", "Spanish Dominican Tom\u00e1s de Torquemada", "Four Seasons", "1975", "Mel Gibson", "Procol Harum", "Erica Rivera", "zinc", "a proprietary library classification system", "2003", "Sebastian Lund", "5 September 1666", "California State Route 1", "The management team", "in various submucosal membrane sites", "Flex SDK, a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "Steveston Outdoor pool", "Phillip Schofield and Christine Bleakley", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Ukraine", "Lula", "1850", "braking", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "Derrick Henry", "\u2212 89.2 \u00b0 C", "a cliffhanger showing the first few moments of Sam's next leap", "engraved on a bronze plaque and mounted inside the pedestal's lower level", "Cheerios", "dennis taylor", "Brian Close", "AC/DC are an Australian hard rock band, formed in Sydney in 1973 by brothers Malcolm and Angus Young", "Galleria Vittorio Emanuele II", "every aspect of public and private life", "reached an agreement late Thursday to form a government of national reconciliation.", "Olympic medal", "Henry Ford", "Toyota", "Abraham Lincoln", "a tumour"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5399759992163009}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 1.0, 0.0, 0.19999999999999998, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.3571428571428571, 0.06896551724137931, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.2758620689655173, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-1562", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-15641"], "SR": 0.40625, "CSR": 0.540625, "EFR": 0.9473684210526315, "Overall": 0.7076768092105262}, {"timecode": 35, "before_eval_results": {"predictions": ["domestic Islamists who attacked it", "euphoric", "\"Bistro", "Venezuela", "Mexico", "boxing News", "\"All children, except one, grow up\"", "i second that name found in the title of Robert Browning's poem about him", "the Arctic Ocean", "air pressure", "weir", "Gilbert du Motier", "Elijah Muhammad", "a fistro.com", "rockers", "Alexander Pushkin", "Australia", "Munich", "Puebla", "a night shift", "bbcian Captivity of the Papacy", "Arkansas River Valley", "Robots", "Pierre-August Renoir", "hawd-mwah-zell", "\"Opera Synopses A Guide to The Plots And Characters\"", "Innsbruck", "Lance Ito", "Microsoft", "a ferns", "America", "vikings", "Atlantic City", "Blackwater USA", "elephants", "American Airlines", "a fexes", "Odysseus", "bbclan", "Kensington Palace", "Bill Carver", "we today call.... who came accompanied by an army of 10,000 men in 1567", "hawhatan", "c.S. Lewis", "John Galt", "the amygdala, the part of the brain that", "Chicago Mercantile Exchange", "Las Vegas", "\"Tights: not just for dancing\"", "wheat", "Juan Carlos I", "a ostrich", "1943", "Payaya Indians", "beneath the liver", "James I", "penrhyn", "psychological horror", "John Morgan", "Hungarian Rhapsody No. 2 in C-sharp minor, S.244/2", "Henry II", "Sen. Debbie Stabenow", "63", "\"perezagruzka\" (the Russian word for reset,)"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4776785714285714}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false], "QA-F1": [0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9699", "mrqa_searchqa-validation-11665", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-265", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-4473", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-11675", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-15317", "mrqa_searchqa-validation-5646", "mrqa_searchqa-validation-3975", "mrqa_searchqa-validation-16363", "mrqa_searchqa-validation-8694", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-15775", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-16094", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-3348", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3745", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352"], "SR": 0.40625, "CSR": 0.5368923611111112, "EFR": 0.9473684210526315, "Overall": 0.7069302814327485}, {"timecode": 36, "before_eval_results": {"predictions": ["electric lighting", "James W. Marshall", "Terrell Suggs", "the Earth's axial tilt, which fluctuates within a margin of 2 \u00b0 over a 40,000 - year period", "Lucknow", "the eighth episode of Arrow's second season", "National Industrial Recovery Act ( NIRA )", "The User State Migration Tool", "the Battle of Antietam", "William DeVaughn", "the National September 11 Memorial plaza", "Southend Pier", "Santa Monica", "sovereign states", "Brian Steele", "31 January 1934", "Filipino", "1773", "modern random - access memory ( RAM )", "September 15, 2012", "April 1917", "Bart Cummings", "October 27, 1904", "Harishchandra", "Olivia Olson", "1990", "Billy Gibbons", "Bill Pullman", "BC Jean", "2016", "Frank Muniz", "stratum lucidum", "60", "Hasmukh Adhia", "four", "a retina", "the 1980s", "inorganic forms", "signature panel code ( SPC )", "exercise general oversight", "bohrium", "Britain", "Escherichia coli", "Archduke Franz Ferdinand", "June 1991", "2010", "he lost the support of the army, abdicated in November 1918, and fled to exile in the Netherlands", "in De Inventione by Marcus Tullius Cicero", "Mike Czerwien", "103", "Vienna", "(Barbados)", "(Mississippi)", "Stalin", "$10.5 million", "Alfred Joel Horford", "Andrew Johnson", "$30.6 million", "Workers' Party", "his mother, Katherine Jackson", "cotton", "Dennis Haysbert", "Quinn the Eskimo", "Towcester"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6693898115773116}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.7692307692307692, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.08333333333333334, 1.0, 0.5, 0.6666666666666666, 1.0, 0.888888888888889, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.6666666666666666, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-4351", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1953", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-1862"], "SR": 0.515625, "CSR": 0.5363175675675675, "EFR": 0.9354838709677419, "Overall": 0.7044384127070619}, {"timecode": 37, "before_eval_results": {"predictions": ["Joseph Swan", "the United States", "South Africa", "first among equals", "shine", "a cappella", "albinism", "Peterloo", "an aglet", "Saturday Night Live", "FC Bayern M\u00fcnchen", "winter", "Bonnie and Clyde", "english", "copper", "Dawn French", "Blackstar, Becomes His First No. 1", "brazil", "Doris Lessing", "Scooby-Doo", "Swaziland", "the elephant House at London Zoo", "Kent", "a mudflats", "a points based scoring system", "a saddler", "Kent", "Rodgers and Hammerstein", "Boy George", "Galileo Galilei", "Zelle", "Midsomer Murders", "Marilyn Manson", "Medellin, Colombia's second largest city,", "The Tempest", "spark plugs", "brazil", "Boulder Dam", "long-term effects", "Saudi Arabia", "Belle de Jour", "Morecambe", "a bba", "heavy", "blue", "Asaph Hall", "France", "geena Davis", "Kunsky", "psychology", "Lady Penelope", "the forces of Andrew Moray and William Wallace", "142,907", "mid November", "YouTube", "Theo James Walcott", "Ben Ainslie", "the District of Columbia near Takoma Park, Maryland.", "heavy turbulence", "women coping with breast cancer in", "a balloon", "a sunflower", "Madonna", "March 24,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5974702380952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7074", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-687", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-8884", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-6291"], "SR": 0.5625, "CSR": 0.5370065789473684, "EFR": 1.0, "Overall": 0.7174794407894736}, {"timecode": 38, "before_eval_results": {"predictions": ["\"the Rip\"", "folk band", "the liver", "40 days", "sodium", "Bears", "cuba", "'In Chancery'", "Phil Redmond", "Stevie Wonder", "a head", "a hound", "hanover", "the moon", "king Charles I", "work", "scales", "Dirty Dancing", "henry", "Diana Ross", "albion", "a 1934 Austin seven box saloon", "Richie Unterberger", "republic", "cuba", "the king", "Blade Runner", "Jay-Z", "leopons", "cymbals", "\u201cBuddies\u201d", "flancesco Maria piave", "Tory MP Andrew Mitchell", "flidelio", "South Africa", "Christian Dior", "sweden", "a killer whale", "cuba", "cuba", "raspberries", "pilgrimage", "Cyprus", "speed camera", "second rank", "a lizard", "Fandango Groovers", "frauds", "a dolphin", "30", "Tony Blair", "quartz", "54 Mbit / s", "Manley", "Stacey Kent", "Eyes Wide Shut", "Anthony Lynn", "piano", "tribute to pop legend Michael Jackson, who died Thursday afternoon in Los Angeles.", "U.S. President Bill Clinton", "French Guiana", "albion", "a coat", "stadium leader Jean Van de Velde"], "metric_results": {"EM": 0.421875, "QA-F1": 0.449032738095238}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2224", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-6161", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3351", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-2594", "mrqa_searchqa-validation-9940", "mrqa_searchqa-validation-4817", "mrqa_newsqa-validation-3899"], "SR": 0.421875, "CSR": 0.5340544871794872, "EFR": 0.972972972972973, "Overall": 0.711483617030492}, {"timecode": 39, "before_eval_results": {"predictions": ["\"No, that's no good\"", "albion", "Midnight Cowboy", "norman", "seborrheic dermatitis", "Amanda Barrie", "a hoy of Dordrecht", "Niger", "central Stockholm", "Tangled", "dogs", "georgia", "Bulls Eye", "pomeon which the sun scarce designs to shed its light", "normiluz and Von Egmond", "Martin Clunes", "Jane Austen", "pembrokeshire Coast National Park", "Kevin macdonald", "peppers", "cenozoic", "jimmy Boyd", "Isambard Kingdom Brunel", "france", "1957", "Devon", "la Marseille", "moctre d\u2019Hotel", "acid-mediated coagulation", "Ralph Vaughan Williams", "musical scale", "animal sanctuaries", "flannel", "tyotr Ilich Tchaikovsky", "Shanghai", "Spain", "farm", "Tuesday", "Guru Nanak", "bleak house", "Inigo Montoya", "phosphorus", "Little Jack Horner", "Doha, Qatar", "Dolores Haze", "cuckoo", "georgia Christie", "Ford", "Alice Cooper", "Majorca", "blood transfusions", "Royal Bengal Tiger", "inward spiral", "Max", "New York City", "1999", "Sela Ann Ward", "\" Body Works\"", "forgery and flying without a valid license,", "183", "a log cabin", "March", "linebacker", "Angel Recording Studios"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5018601190476191}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6497", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-7408", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6153", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5688", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2738", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-2711", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-777", "mrqa_searchqa-validation-807", "mrqa_naturalquestions-validation-9755"], "SR": 0.453125, "CSR": 0.53203125, "EFR": 0.9714285714285714, "Overall": 0.7107700892857143}, {"timecode": 40, "before_eval_results": {"predictions": ["in the 19th Century", "Famous Players", "indianapolis", "smith", "black death", "horse", "buffalo", "caligula", "a raven", "Sarajevo", "the Bill of Rights", "fine", "Neighbours", "smith", "trumpet", "Westminster Abbey", "origami", "the value of unknown electrical resistance", "the Arabian Gulf", "smith", "charles darwin", "avunculicide", "jack Nicholson", "\u201cGone With the Wind\u201d", "diesel", "Tomorrow Never Dies", "indian", "dogs", "smith", "indianapolis", "New Hampshire", "jimmy I", "charlie fenton", "japan", "Purple Rain", "smith", "winds", "smith", "Venice", "3", "Southwest Airlines", "dogs", "smith", "The Comedy of Errors", "charlie", "gelyn Jones", "first lady Rosalynn Carter", "charlie", "norway", "Basque separatist", "robinsons", "humpty Dumpty", "August 18, 1998", "Vijay Prakash", "the ENnie Award", "the 100th anniversary of the first \"Tour de France\" bicycle race", "Mach number (M or Ma)", "Janet and La Toya", "more than 2.5 million", "researchers", "smith", "charles darlin", "irena", "inequality of opportunity related to gender was low"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3678977272727273}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454]}}, "before_error_ids": ["mrqa_squad-validation-9257", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-4716", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4538", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-863", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-4337", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-4928", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-864", "mrqa_newsqa-validation-2372", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-15441", "mrqa_searchqa-validation-11519", "mrqa_naturalquestions-validation-3969"], "SR": 0.3125, "CSR": 0.5266768292682926, "EFR": 0.9772727272727273, "Overall": 0.7108680363082039}, {"timecode": 41, "before_eval_results": {"predictions": ["1220", "Spain", "hula hoops", "true car", "mountain torrents", "roddy darwin", "a counting table", "Robin Hood", "aeolus", "vii vii", "South African", "caracas", "stockholm", "james chicago", "james vii", "angola", "william vii", "david Bowie", "Buzz Aldrin", "james paul sartre", "japan", "james vii Hawkins", "ferric oxide", "james aniston", "pembroke", "tbilisi", "james philippine", "jealousy", "nine", "glenn close", "lacock Abbey", "alex bbc", "domestic cat", "anita Brookner", "james vii", "Golda Meyerson", "Black Sea", "bagram Theater Internment Facility", "Susie Dent", "a power outage", "vii Josef", "The Archers", "Launcelot Gobbo", "james ochinton", "Chester racecourse", "james boyd", "james spencer", "The Four Marx Brothers", "river tyne", "henry monarchy", "Dry Ice", "Pat McCormick", "19 June 2018", "2001", "from 1993 to 1996", "james Gandolfini", "September 29, 2017", "he and the other attackers were from Pakistan and asked for a meeting with Pakistan's High Commission.", "June 6, 1944", "sniff out cell phones.", "a double bass", "o.K. Corral", "butternut squash", "phoenicia"], "metric_results": {"EM": 0.40625, "QA-F1": 0.47303240740740743}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.07407407407407407, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5582", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-2886", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-4885", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-3306", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-5625", "mrqa_triviaqa-validation-6097", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-2641", "mrqa_triviaqa-validation-7225", "mrqa_hotpotqa-validation-3866", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-10746", "mrqa_searchqa-validation-9161", "mrqa_searchqa-validation-233"], "SR": 0.40625, "CSR": 0.5238095238095238, "EFR": 1.0, "Overall": 0.7148400297619047}, {"timecode": 42, "before_eval_results": {"predictions": ["the lack of reliable statistics from this period", "work rule issues", "Eintracht Frankfurt", "Comoros Islands", "our mutual friend", "Jeddah, Saudi Arabia", "40", "chest", "\"I'm just getting started.\"", "Manny Pacquiao", "$250,000", "27,", "British Prime Minister Gordon Brown", "executive director of the Americas Division of Human Rights Watch", "Salt Lake City, Utah", "dancy-Power Automotive", "the federal chamber of deputies", "64", "New Delhi, India", "fastest", "Department of Homeland Security Secretary Janet Napolitano", "iran's parliament speaker", "ended his playing career", "\"E! News\"", "Florida", "Madeleine k. Albright", "ice jam", "exposure by military personnel to contaminated water", "Benazir Bhutto", "July", "U.S. senators", "angola", "Larry Ellison", "a Christian farmer", "her fianc\u00e9,", "cal Ripken Jr.", "Johannesburg", "cancer", "acid attack", "Vernon Forrest", "urged NATO to take a more active role in countering the spread of the", "one", "comfort those in mourning", "byproducts emitted during the process of burning and melting raw materials", "about 5:20 p.m.", "Mobile County Circuit Judge Herman Thomas", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "a man's lifeless, naked body", "\"release\" civilians,", "Trevor Rees", "we can use solar and renewable energy", "when a population temporarily exceeds the long term carrying capacity of its environment", "Real Madrid", "emperor Cuauhtemoc", "city of wilson", "misery chastain", "purdy", "Antonio Lippi", "Thorgan Ganael Francis Hazard", "River Clyde", "aston villa", "tarzan", "Cy Young", "Reese Witherspoon"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5710386835386836}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8000000000000002, 1.0, 0.7272727272727273, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 0.14545454545454548, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3979", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-1641", "mrqa_triviaqa-validation-4313", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-5649"], "SR": 0.46875, "CSR": 0.5225290697674418, "EFR": 1.0, "Overall": 0.7145839389534884}, {"timecode": 43, "before_eval_results": {"predictions": ["tenth-largest", "House of Borromeo", "Washington, D.C.,", "1943", "a facelifted 850 saloon", "the Mountain West Conference", "the National Basketball Association (NBA)", "Western Europe", "movie scripts", "Schaeffler AG,", "English football", "1989 until 1994", "the Distinguished Service Cross,", "\"50 best cities to live in.\"", "Saint Michael, Barbados", "Lollywood", "Emmanuel Ofosu Yeboah", "Ant-Man", "Bhushan Patel", "1986", "1964", "Reginald Engelbach", "Vince Staples", "Archbishop of Canterbury", "Galway", "Lynyrd Skynyrd", "2008\u201309", "coaxial", "\"Northern Lights\"", "three different covers", "Malayalam fantasy comedy", "Regno di Dalmazia", "August 11, 1946", "John Malkovich", "born September 6, 1967", "\"Estadio de L\u00f3pez Cort\u00e1zar\"", "Cartoon Network Studios", "Nicolas Vanier", "1985", "Wonder Woman", "Meghan Markle", "a Boeing B-17 Flying Fortress", "a Portuguese dancer and choreographer", "Charles Joseph Scarborough", "English", "76,416", "Bonkyll Castle", "second cousin once removed", "2012 Summer Olympics", "PlayStation 2", "Brig Gen Augustine Warner Robins", "United Nations", "Lewis Carroll", "two", "the UK\u2019s Trade Mark Registration Act 1875", "blue", "elbow", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "the Employee Free Choice Act", "the release of the four men", "a rake", "Jack the Ripper", "a carriage", "teak"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6696337076771859}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.888888888888889, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08695652173913042, 1.0, 0.5, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7278", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-1776", "mrqa_naturalquestions-validation-4007", "mrqa_triviaqa-validation-2264", "mrqa_newsqa-validation-2070"], "SR": 0.578125, "CSR": 0.5237926136363636, "EFR": 1.0, "Overall": 0.7148366477272727}, {"timecode": 44, "before_eval_results": {"predictions": ["British Prime Minister Edward Heath", "former White Zombie bassist Sean Yseult", "Washington, D.C.", "over 12 million", "Dulce Maria Garc\u00eda Rivas", "Conservatorio Verdi", "President of the United States", "the backside", "\"the Gentle Don\"", "The Future", "Newton Knight", "Adam Karpel", "Danish national men's ice hockey team", "2015 Orange Bowl", "Margarine Unie", "death", "Fort Valley, Georgia", "Bill Paxton", "Vladimir Valentinovich Menshov", "Kramer", "the Dominican Republic", "Humberside Airport", "2017", "Douglas Jackson", "wooden roller", "Blackpool Football Club", "William Lyon Mackenzie King", "Ted", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Fiat Chrysler Automobile N.V.", "Bruce Grobbelaar", "Honda Ballade", "Ascona", "Boston Celtics", "Austrian", "Australian Electoral Division", "Sun Tzu", "American singer Toni Braxton", "Hindi", "Richard Masur", "Brian Patrick Friel", "Bad Religion", "\"Dr. Gr\u00e4sler, Badearzt\"", "Alexandre Dimitri Song Billong", "Arizona Health Care Cost Containment System", "Mineola", "Gian Carlo Menotti", "bobsledder", "Mazda", "102,984", "Roscoe Lee Browne", "1972", "Mike Gibbs", "216", "The Spectator", "Easter Parade", "Elgar", "last summer.", "almost 100", "into the Southeast,", "the pie", "Myra Lewis", "heresy", "One Direction"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6658651244588745}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 0.7272727272727272, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.4, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3087", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-15766", "mrqa_searchqa-validation-3263"], "SR": 0.515625, "CSR": 0.5236111111111111, "EFR": 1.0, "Overall": 0.7148003472222222}, {"timecode": 45, "before_eval_results": {"predictions": ["the American Revolution", "quod erat demonstrandum", "a \"piece of gold roses", "the Belgae", "Northern Exposure", "cocoa butter", "Kokomo", "Esther", "Warren Harding", "Monty Hall", "miniature golf", "American Morning", "Punxsutawney, Pennsylvania", "Pannonia", "yellow fever", "sea otter", "MMs", "franchise", "a rod", "Watergate break-in", "dressage", "astronomers", "Mickey Mouse", "the stigma", "Full Professor", "a \"piece of real fruit\"", "Medusa", "a staircase", "a tabby", "the Staff", "Voyager 1", "Farsi (Persian)", "fat", "objects", "Partland China", "Helen of Troy", "meat", "peace sign", "Morrie: An Old Man, a Young Man", "John, Harry Three", "Rajasthan", "sexy Beast", "a \"piece of interest\"", "NHL", "a \"road above\"", "White bread and butter", "a \"piece of office of, say, one-third of its members expires", "Wordsworth", "brushes", "a \"dwarf planet\"", "Haroun", "Vincent Price", "Rugrats in Paris : The Movie", "Middle Eastern alchemy", "London", "Isle of Wight", "Peppercorn Pioneer", "\"Queen In-hyun's Man\"", "Oneida Limited", "Michael Jordan", "Libreville, Gabon.", "tickets", "his client, Brett Cummins,", "tuscaloosa"], "metric_results": {"EM": 0.484375, "QA-F1": 0.55703125}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.5, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2593", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-13022", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-14414", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-8467", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-975", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-70", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-5006", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3322", "mrqa_triviaqa-validation-6557", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-3949", "mrqa_triviaqa-validation-888"], "SR": 0.484375, "CSR": 0.5227581521739131, "EFR": 1.0, "Overall": 0.7146297554347826}, {"timecode": 46, "before_eval_results": {"predictions": ["the position of people within the four-class system was not an indication of their actual social power and wealth,", "Jorge Lorenzo", "Malachy McCourt", "Indiana Jones", "fungi", "Venus flytrap", "Abraham", "room the store", "Faggot", "a skein", "California Chrome", "Pluto", "Route 66", "Taklamakan Desert", "South Asia", "Astronaut", "Great Victoria Desert", "German state of North Rhine-Westphalia", "Go West", "December 18, 1958", "year 1752", "Portugal", "Operation Overlord", "Birmingham", "snakes", "Sedgefield in North East England", "Coral Sea", "Saddam Hussein", "Nadia Comaneci", "tank", "South Korea", "pigs", "One Guys and a Girl", "Carmen", "Kenya", "Stephen Potter", "Casa di Giulietta", "Anwar Sadat", "three", "Potomac", "Argentina", "Luke", "Frankfurt", "Mouse", "Goldie Hawn", "pulsar", "Belgium", "shows the feelings and hardships of not just horses from long ago, but even horses now", "a little extra juice and zest", "Benfica", "Sun Lust Pictures", "Games", "she is not in love with Chino", "somatic cell nuclear transfer", "early 7th century", "1 January 1788", "Radcliffe College", "11", "\"Twilight\"", "the museum itself.", "Speed Racer", "H.G. Wells", "Queen Elizabeth", "Sir Walter Scott"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6420465225563909}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true], "QA-F1": [0.10526315789473684, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8093", "mrqa_triviaqa-validation-736", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3840", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-7773", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5241", "mrqa_hotpotqa-validation-3234", "mrqa_newsqa-validation-2953", "mrqa_searchqa-validation-4652"], "SR": 0.578125, "CSR": 0.523936170212766, "EFR": 1.0, "Overall": 0.7148653590425532}, {"timecode": 47, "before_eval_results": {"predictions": ["Arabah", "Venice", "Sinclair Lewis", "the bear", "definitely not enough", "pigments", "Jonathan Demme", "Vaclav Havel", "Dick Van Dyke", "millais", "Tina Turner", "1789", "toadstewart", "glasses", "perfume", "if", "iron", "Copenhagen", "The Apprentice", "sedge", "Analytical Cubism", "sahara desert", "CSIRO", "eucharist", "Charlotte's Web", "sedge", "silks", "William Randolph Hearst", "Lorne Greene", "rowing", "andie MacDowell", "Call My Bluff", "a\u201d on his forehead", "Argentina", "frank mccourt", "oats", "Caroline Aherne", "Home Guard", "carbon dioxide", "soap", "Donna Summer", "a balustrade", "nottingham", "gdansk Poland", "Welcome Stranger", "strathclyde Police", "April", "chechnya", "a police janitor", "the A- Team", "football", "801,200", "Sir Ronald Ross", "Sun Tzu", "bioelectromagnetics", "Foxborough, Massachusetts", "Speedway World Championship", "her most important work is her charity, the Happy Hearts Fund.", "11", "Michelle Obama", "kbenhavn", "the Communist Manifesto", "sah", "fluoroquinolones"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6393736471861471}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-7450", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-1730", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5726", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-334", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-15651", "mrqa_newsqa-validation-1804"], "SR": 0.59375, "CSR": 0.525390625, "EFR": 1.0, "Overall": 0.7151562499999999}, {"timecode": 48, "before_eval_results": {"predictions": ["East Lothian", "Caesars Entertainment Corporation", "Supergirl", "king \u00c6thelred the Unready", "shaun the sheep", "Stephen Mangan", "William McKinley", "1905", "Vanilla Air", "Mineola, New York", "dovzhenko", "Strange Interlude", "Julia Compton Moore", "Olivia Newton-John", "1988", "early Romantic period", "Gettysburg address", "Harold Edward Holt", "Washington Street", "Mathew Sacks", "Babylon", "Ford Falcon", "Southern State Parkway", "The Company", "1827", "Kim Bauer", "United States Food and Drug Administration (FDA)", "Edward James Olmos", "Bury St Edmunds, Suffolk, England", "prussian", "O", "1909", "86 ft", "American", "January 2004", "sulfur mustard", "The 45th Infantry Division", "2009", "5 Grammy Award nominations", "angie Watts", "London", "Boyd Gaming", "February 14, 1859", "Texas Tech University", "John McClane", "Larry Wayne Gatlin", "924", "held the American record for the most time in space (381.6 days)", "North Carolina", "Harrisburg", "daughter", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "cake", "Salman Khan", "space shuttle", "basil", "Clio Awards", "The Rosie Show,\"", "North Korea", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "Julius Caesar", "a volcano's lava flow", "the Library of Congress", "thylakoid membrane"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6660547144582472}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false], "QA-F1": [0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.6666666666666666, 0.8695652173913043, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-4008", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1115", "mrqa_hotpotqa-validation-2614", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5714", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-6806", "mrqa_triviaqa-validation-6785", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-1028", "mrqa_naturalquestions-validation-4685"], "SR": 0.578125, "CSR": 0.5264668367346939, "EFR": 0.9629629629629629, "Overall": 0.7079640849395313}, {"timecode": 49, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-2508", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4095", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4589", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5192", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-564", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5755", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-1887", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2722", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3569", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-938", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-13836", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-15433", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-15641", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-16122", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-165", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1954", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4937", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6398", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-8410", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-975", "mrqa_squad-validation-10069", "mrqa_squad-validation-10086", "mrqa_squad-validation-1019", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-10397", "mrqa_squad-validation-10444", "mrqa_squad-validation-10449", "mrqa_squad-validation-1052", "mrqa_squad-validation-1129", "mrqa_squad-validation-1211", "mrqa_squad-validation-1265", "mrqa_squad-validation-1311", "mrqa_squad-validation-139", "mrqa_squad-validation-164", "mrqa_squad-validation-1672", "mrqa_squad-validation-1712", "mrqa_squad-validation-1916", "mrqa_squad-validation-2132", "mrqa_squad-validation-2155", "mrqa_squad-validation-2176", "mrqa_squad-validation-2326", "mrqa_squad-validation-2436", "mrqa_squad-validation-2467", "mrqa_squad-validation-264", "mrqa_squad-validation-2798", "mrqa_squad-validation-2824", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-2906", "mrqa_squad-validation-2914", "mrqa_squad-validation-294", "mrqa_squad-validation-2999", "mrqa_squad-validation-305", "mrqa_squad-validation-3337", "mrqa_squad-validation-3650", "mrqa_squad-validation-3742", "mrqa_squad-validation-3948", "mrqa_squad-validation-4025", "mrqa_squad-validation-4066", "mrqa_squad-validation-4135", "mrqa_squad-validation-4258", "mrqa_squad-validation-4338", "mrqa_squad-validation-4349", "mrqa_squad-validation-44", "mrqa_squad-validation-4472", "mrqa_squad-validation-4480", "mrqa_squad-validation-4605", "mrqa_squad-validation-4607", "mrqa_squad-validation-4686", "mrqa_squad-validation-4835", "mrqa_squad-validation-487", "mrqa_squad-validation-4897", "mrqa_squad-validation-4947", "mrqa_squad-validation-5088", "mrqa_squad-validation-5136", "mrqa_squad-validation-5238", "mrqa_squad-validation-5330", "mrqa_squad-validation-5672", "mrqa_squad-validation-594", "mrqa_squad-validation-60", "mrqa_squad-validation-6362", "mrqa_squad-validation-6562", "mrqa_squad-validation-6737", "mrqa_squad-validation-6737", "mrqa_squad-validation-6811", "mrqa_squad-validation-6918", "mrqa_squad-validation-696", "mrqa_squad-validation-703", "mrqa_squad-validation-7173", "mrqa_squad-validation-7435", "mrqa_squad-validation-754", "mrqa_squad-validation-7576", "mrqa_squad-validation-7598", "mrqa_squad-validation-7814", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-8285", "mrqa_squad-validation-8402", "mrqa_squad-validation-8406", "mrqa_squad-validation-8483", "mrqa_squad-validation-8607", "mrqa_squad-validation-8636", "mrqa_squad-validation-8715", "mrqa_squad-validation-8747", "mrqa_squad-validation-8760", "mrqa_squad-validation-879", "mrqa_squad-validation-8846", "mrqa_squad-validation-9015", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9691", "mrqa_squad-validation-9757", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1297", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2068", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2572", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2970", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3886", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-4904", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-5118", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-5505", "mrqa_triviaqa-validation-5607", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6785", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-92"], "OKR": 0.830078125, "KG": 0.48046875, "before_eval_results": {"predictions": ["a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Jena Malone", "Washington, D.C.", "joined the utopian Ascona community", "John W. Henry", "Mos Def", "James Mitchum", "4 April 1963", "1995", "Tom Shadyac", "Wendell Berry", "Keitar\u014d Arima", "eastern", "novelty songs, comedy, and strange or unusual recordings", "OutKast", "five", "Alain Robbe-Grillet", "the Seasiders", "Musicology", "Dragon TV", "the New York-New Jersey Highlands region of the Appalachian Mountains", "Bay Ridge, Brooklyn", "Jean- Marc Vall\u00e9e", "over 1.6 million", "1928", "November 20, 1942", "September 26, 2010", "North Greenwich Arena", "1614", "Lucy Maud Montgomery", "Bad Meets Evil", "nausea, vomiting, diarrhea, jaundice, and abdominal pain", "Saint Michael, Barbados", "Sleepy Hollow", "more than 26,000", "EN World web site", "Charles Russell", "KB", "Robert Jenrick", "Golden Globe Award", "southwest Denver, Colorado", "Port Clinton", "Art of Dying", "Dallas", "Harvard", "fennec", "Dutch", "Terry Malloy", "Golden Calf for Best Actor in 2013", "Kal Ho Naa Ho", "Thorgan Ganael Francis", "the closing scene of the final episode of the first season", "Everywhere", "Bangalore", "honda CBR1000RR", "J. M. W. Turner", "the Republic of Upper Volta", "56", "Nkepile Mabuse", "Eintracht Frankfurt", "the Derna harbor", "Hephaestus", "Amherst College", "two courses"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7487892316017316}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [0.26666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.7272727272727273, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1498", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1476", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-616", "mrqa_searchqa-validation-9636", "mrqa_searchqa-validation-14102", "mrqa_newsqa-validation-492"], "SR": 0.578125, "CSR": 0.5275000000000001, "EFR": 1.0, "Overall": 0.7109687499999999}]}