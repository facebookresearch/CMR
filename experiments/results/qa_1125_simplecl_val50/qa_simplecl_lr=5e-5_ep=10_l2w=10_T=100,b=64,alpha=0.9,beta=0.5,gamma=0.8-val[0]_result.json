{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=10_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=10.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=10_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2120, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "Atlantic", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "the infected corpses", "the United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "an algorithm for multiplying two integers can be used to square an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "Swedishex GmbH & Co KG", "More than 1 million", "2011", "by the market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Buddhism", "Matthew 16:18", "U.S.-flagged Maersk Alabama", "Rwanda", "revelry", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7736301892551893}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-4289", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-2520", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.71875, "CSR": 0.7734375, "EFR": 0.9444444444444444, "Overall": 0.8589409722222222}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "a multi-party system", "allows those tainted by sin to nevertheless make a truly free choice to accept or reject God's salvation in Christ", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "Arizona Cardinals", "1991", "Chaffee", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "more integral", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ronnie Wood and Brandon Block", "Documents", "the company's factory in Waterford City, Ireland", "nitrogen", "Christopher Nolan", "The Florida current feeds the Gulf Stream that flows east of Cape Hatteras;  (3) the looping motion in Buoy ID 30688", "six", "It always begins with the music", "conductor", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.765625, "QA-F1": 0.8017423876798877}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16216216216216214, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-803", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-8316", "mrqa_squad-validation-9805", "mrqa_squad-validation-235", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "stout man with a \"double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck.\"", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "charleston", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President (currently an ex-Luxembourg Prime Minister, Jean-Claude Juncker)", "2000", "oxygen", "increase", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "multiple revisions", "philanthropic initiative", "integer factorization problem", "inequality", "Isel", "adapted quickly and often married outside their immediate French communities", "John Hancock, John Adams, John Quincy Adams, Rutherford B. Hayes, Theodore Roosevelt, Franklin D. Roosevelt", "Charles-Fer Ferdinand University", "that he had drowned in the Mur River", "yellow fever outbreaks", "Jim Nantz and Phil Simms", "lysozyme and phospholipase A2", "Brazil", "energy stored in an H+ or hydrogen ion gradient", "the late 19th century", "Channel Islands", "charleston", "charleston", "charleston", "charleston", "charleston", "charleston", "charleston", "charleston", "gulfina", "study insects and their relationship to humans, other organisms, and the environment", "charleston", "travis", "charleston", "ireland", "charleston", "24 hours a day and 7 days a week", "Sponsorgate", "charleston"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5206801470588236}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.8235294117647058, 1.0, 0.20000000000000004, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-9286", "mrqa_squad-validation-4293", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-1150", "mrqa_squad-validation-597", "mrqa_squad-validation-8906", "mrqa_squad-validation-2463", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-1581", "mrqa_hotpotqa-validation-437", "mrqa_hotpotqa-validation-3821"], "SR": 0.484375, "CSR": 0.69921875, "EFR": 0.8787878787878788, "Overall": 0.7890033143939394}, {"timecode": 4, "before_eval_results": {"predictions": ["in plants that contain them", "Parliament of Victoria", "Zaha Hadid", "the Marquis de Vaudreuil", "Science and Discovery", "Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "a number of stages", "The Skirmish of the Brick Church", "port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "Sabina Guzzanti", "22", "terror groups that they say were planning numerous suicide attacks, including in the country's largest city of Karachi.", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "a way of getting into that Lexus, Lincoln, Infiniti ororsche you always wanted, without laying out $70,000 or $80,000 for something you're not actually going to live in.", "Muslim", "this will be the first time any version of the Magna Carta has ever gone up for auction", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "CNN's Campbell Brown", "Julissa Brisman", "one", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "colombia"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6923655939648586}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3, 0.15999999999999998, 1.0, 0.11428571428571428, 0.0, 0.32, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.671875, "CSR": 0.69375, "EFR": 0.9047619047619048, "Overall": 0.7992559523809524}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a high-level marketing manager, was given the job of turning the business around.", "500,000", "Ofcom", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence.", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "Philippines", "the Broncos", "1950s to 2011", "the spoils of the war", "German Te Deum.", "1795", "Bermuda 419", "evaporated to cool oxygen gas", "Infinity Broadcasting Corporation", "\"semi-legal\" and was the only opposition group in Egypt able to field candidates during elections.", "1972", "rudimentary", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "the Secretary of State and ambassadors are appointed by the President, with the advice and consent of the Senate", "It is mainly for the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings.", "from an Ohio newspaper on 8 February 1925", "Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress", "angular rotation", "Panning", "Justin Timberlake", "the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "police, troops, and military experts. European nations contribute nearly 6,000 units to this total", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "Lowe's has 62 locations in Canada", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman reprising his role as detective Alex Cross.", "David Gahan ( / \u0261\u0251\u02d0n / ; born David Callcott ; 9 May 1962 ) is an English singer - songwriter", "it includes a restaurant, spa, and bed - and - breakfast and provides guided tours which feature the history and alleged paranormal activity of the site", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "people prepare for the long Lenten fast", "Jaipur", "Jonas Olsson,", "a relatively small and fast naval ship designed to carry torpedoes into battle.", "Newport"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6622156005544163}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 0.2666666666666667, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.25, 1.0, 0.7200000000000001, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 0.2857142857142857, 0.3157894736842105, 1.0, 0.0, 0.0, 0.3636363636363636, 0.631578947368421, 0.0, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-9552", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-7488", "mrqa_squad-validation-9908", "mrqa_squad-validation-3473", "mrqa_squad-validation-9635", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.546875, "CSR": 0.6692708333333333, "EFR": 0.9655172413793104, "Overall": 0.8173940373563218}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "\"social and political action,\" declared that \"To perform its mission in the society, a university must sustain an extraordinary environment of freedom of inquiry and maintain an independence from political fashions, passions, and pressures.\"", "1936", "the New Birth", "gold", "a deficit", "Costiff collection of 178 Vivienne Westwood costumes", "reciprocating Diesel engines, and gas turbines,", "disease", "\"TFIF\"", "Confucian propriety and ancestor veneration", "\"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "a chain or screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples.", "end of the season", "10", "Michael, Alexander, William, Joshua, Daniel, Jayden, Noah and Anthony.", "African-Americans", "will not support the Stop Online Piracy Act", "David Duchovny, playing what the tabloidoids would have you believe is an autobiographical role, has managed to hang onto his Bukowski-phase well into his forties.", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer, can put users in a dazed stupor for about two hours,", "New York apartment in 1980", "Stuttgart on Sunday.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "North Korea's reclusive leader Kim Jong- Il", "first five Potter films", "know what's important in life", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "a \"stressed and tired force\" made vulnerable by multiple deployments", "James Whitehouse, has been quoted as saying she has terminal brain cancer, according to a blog called Manson Family Today.", "we want to ensure we have all the capacity that may be needed over the course of the coming days.", "a series of monthly meals for people with food allergies.", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "Mike Gatting", "Colgate University", "Church of Christ, Scientist", "a fat or fatty acid in which there is at least one double bond within the fatty acid chain", "New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13 ) indicate that all the apostles were men"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6519581194948842}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.28571428571428575, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.16, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.45454545454545453, 0.2857142857142857]}}, "before_error_ids": ["mrqa_squad-validation-7950", "mrqa_squad-validation-5441", "mrqa_squad-validation-3370", "mrqa_squad-validation-6001", "mrqa_squad-validation-486", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.578125, "CSR": 0.65625, "EFR": 0.9629629629629629, "Overall": 0.8096064814814814}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "increased", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "P = PSPACE", "July 1969", "German citizens when Hitler's secret police demanded to know if they were hiding a Jew in their house.", "yellow chlorophyll precursor", "spontaneous", "the courts of member states", "gold", "TARDIS", "Buckland Valley near Bright", "Scottish rivers", "\"Bricks for Warsaw\"", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "their \"Freshman Year\" experience", "India", "Zulfikar Ali Bhutto, former president and prime minister of Pakistan", "at the country's third-largest oil refinery", "April 24 through May 2", "Krishna Rajaram,", "early detection", "250,000", "Timothy Masters", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jason Chaffetz", "William S. Cohen", "\"Dance Your Ass Off\"", "interrogations that have been obtained from detainees through interrogation and cruel treatment, such as waterboarding, will no longer be admitted as evidence before the commissions", "Gary Brooker", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Japanese officials", "patrolling the pavement", "\"Empire of the Sun,\"", "Norman given name Robert", "Olympics", "Matt Winer", "Henry Fonda", "opposite R\u00fcgen island", "Mustique, St. Vincent & the Grenadines, West Indies", "green"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6935687576312577}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 0.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-3938", "mrqa_squad-validation-1556", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_hotpotqa-validation-4367", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-4305"], "SR": 0.609375, "CSR": 0.650390625, "EFR": 0.84, "Overall": 0.7451953124999999}, {"timecode": 8, "before_eval_results": {"predictions": ["2009", "The British provided medical treatment for the sick and wounded French soldiers and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.", "Roman Catholic", "a renegade Time Lord who desires to rule the universe", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday", "\"Journey's End\"", "immediate", "Levi's Stadium", "Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "Chinggis Khaan", "Galileo Galilei and Sir Isaac Newton", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "Islamic", "$24.1 million", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie", "finance", "terminal brain cancer.", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice act", "separated", "Animal Planet", "crashing his private plane into a Florida swamp.", "there were no radar outages and said it had not lost contact with any planes during the computer glitches.", "54 bodies", "early detection and helping other women cope with the disease.", "Diversity", "$250,000", "make sure water continues flow through the river channel and not spread out over land.", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president was declared the winner of the country's presidential elections on Thursday,", "2050", "Alfredo Astiz, a former Navy captain whose boyish looks and deceitful ways", "Abdullah Gul,", "Carl Froch", "The Everglades, known as the River of Grass,", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans Pelicans", "large sums were still demanded for the bulbs of a very few rare and... a Haarlem connoisseur named Aert Huybertsz. paid 850 guilders for a single bulb", "MIBs", "They", "Hockey has to be the most difficult sport to be a play-by-play announcer."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6751858278174068}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.13333333333333333, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407408, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.3076923076923077, 1.0, 0.0, 0.2857142857142857, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-10341", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.59375, "CSR": 0.6440972222222222, "EFR": 0.8461538461538461, "Overall": 0.7451255341880342}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "His wife Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "hoos", "30%\u201350% O2", "very badly disposed towards the French, and are entirely devoted to the English.", "the top 15 most populous", "CRISPR sequences", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "the substance of the statement", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "engaging with the Taliban in Pakistan and Afghanistan.", "25", "a treadmill", "couple's surrogate lost the pregnancy.", "environmental and political events", "he fears a desperate country with a potential power vacuum that could lash out.", "at least two and a half hours", "Elin Nordegren", "American and European consumers", "6,000", "cortisone", "President Clinton", "delivered three machine guns and two silencers to the hip-hop star,", "Morgan Tsvangirai.", "policing the world and Africa", "future relations between the Middle East and Washington", "canyon in the path of the blaze", "Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school", "strife in Somalia", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "they did not know how many people were onboard.", "London", "after Shawn's kidnapping", "immediate physical and social setting in which people live or in which something happens or develops.", "William Tell", "Andr\u00e9 3000", "Groundhog Day", "Cleopatra,", "fairground"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6653195154873544}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.17391304347826086, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.8333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-2429", "mrqa_squad-validation-9194", "mrqa_squad-validation-9484", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-11812"], "SR": 0.59375, "CSR": 0.6390625, "EFR": 0.8076923076923077, "Overall": 0.7233774038461538}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley, New York", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever", "DC traction", "The Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "July 18, 1994", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "light snow or flurries", "Willem Dafoe", "\"Maude\"", "Phillip A. Myers", "Korea", "The University of California San Diego has suspended a student who admitted to hanging a noose in a campus library,", "58", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Lance Cpl. Maria Lauterbach", "Dangjin", "\"novel that you would embarrassed to buy,\"", "Chinese President Hu Jintao", "magazine, GospelToday,", "Mikey", "October 3,", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next.", "President Bush", "Jeffrey Jamaleldine", "2,800", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "bankruptcy", "16 August 1975", "Bonnie Aarons", "one", "kabinett -", "Lionsgate", "James David Lofton", "Sanskrit", "hair-like structures that help paramecium move around."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6658617424242425}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-7230", "mrqa_squad-validation-1299", "mrqa_squad-validation-8655", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.578125, "CSR": 0.6335227272727273, "EFR": 0.9259259259259259, "Overall": 0.7797243265993266}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company under the Office of Alien Property (OAP) seal.", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.", "Elway", "Philo of Byzantium", "36 acres", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "Euler's totient function", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "pink mouse type creatures", "antelope", "nipples", "Precambrian period", "cooperative", "Anastasia Dobromyslova", "Lady Gaga", "9", "The Live Read of Space Jam 2", "European radishes", "Robert Ludlum", "magical", "(.mov)", "The Donington Grand Prix Collection is, quite simply, the largest showcase of Grand Prix racing cars in the world", "It\u2019s Only Rock and Roll", "Hebrew alephbet", "London Underground Piccadilly Line", "Wisconsin", "orangutan", "Manet", "The Magic Finger", "Oberlin College in Ohio,", "2005", "1969", "DodgeDodge", "dolt", "Venice", "a peplos.", "Enrico Caruso", "Elizabeth Arden", "lightweight baby buggy with a collapsible support assembly", "Sir Hardy Amies", "Antigua and Barbuda", "patronymic surname", "Can't Get You Out of My Head ''", "Cody Miller", "Bloomingdale Firehouse", "Israel's vice prime minister compared Iran to Nazi Germany", "Golden Gate Yacht Club of San Francisco", "Antonio Canova", "\"No woman, no cry\"", "Buddhism"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6182562625728676}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.2608695652173913, 1.0, 0.8, 1.0, 0.7234042553191489, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-2331", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983", "mrqa_searchqa-validation-13120"], "SR": 0.515625, "CSR": 0.6236979166666667, "EFR": 0.967741935483871, "Overall": 0.7957199260752689}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "70-50's", "Panini", "new laws or amendments to existing laws as a bill", "anti-colonial movements", "Rhine Valley", "Streptococcus", "experience", "Zhongshu Sheng", "legitimate medical purpose", "cases of an express wish of the people to withdraw from the EU", "1788", "2006", "Roman Catholic", "Henry of Navarre", "John Wesley", "because the nationalisation law was from 1962, and the treaty was in force from 1958,", "Eternal Heaven", "Ness Point", "lachey", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Nick Boles", "he was not a British soldier and did not have amnesia.", "Vladivostok", "Sheryl Crow", "tESLAR", "Camellia sinensis", "AFC Wimbledon", "Bob Monkhouse and Kenneth Connor", "Malaysia", "astronomy", "gin", "George Clooney", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "champagne", "a pluvial was an extended period of abundant rainfall lasting many thousands of years.", "the U.S.", "Brigit Forsyth", "salt", "state of Japan", "\"problem play\"", "Thomas Edward Lawrence", "Kent", "Renoir\u00b4s", "Vanguard", "white", "Switzerland", "gin", "people of the United States", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "bremen", "The Goat Amalthea", "Buster Keaton"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6068222402597403}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9452", "mrqa_squad-validation-6655", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-4590", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_searchqa-validation-9647", "mrqa_newsqa-validation-3860"], "SR": 0.578125, "CSR": 0.6201923076923077, "EFR": 0.9629629629629629, "Overall": 0.7915776353276354}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "the Black Death", "their own militia", "days, weeks and months", "Over 61", "quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron", "HYMENAEUS", "gods", "albinism", "The Straits of Tiran", "Brigit Forsyth", "Call My Bluff", "March 10, 1997", "crazed Holiday", "The Battle of the Three Emperors", "Velazquez", "Ashe", "lizards", "strong, cold south-westerly wind", "table tennis", "medical literature", "penhaligon", "Gandalf", "c. Auguste Dupin", "Jinnah International", "Monday", "capital of Venezuela", "beads", "soap", "liquor", "Avro", "Genesis", "Charlie Brooker", "linden", "Harrods", "2007", "cher", "Scarface", "pale yellow", "dior glazing", "bubba", "June 12, 2018", "Filipino American", "London", "Lambic", "Nook", "Steven Green", "c", "fortune", "a sovereign principality located along the Mediterranean Sea in the...", "Synchronicity"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6166666666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_squad-validation-6933", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-6628"], "SR": 0.546875, "CSR": 0.6149553571428572, "EFR": 0.9310344827586207, "Overall": 0.7729949199507389}, {"timecode": 14, "before_eval_results": {"predictions": ["seven months", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "a declining state of mind", "1898", "The Deadly Assassin and Mawdryn", "radioisotope thermoelectric generator", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1997", "Authority", "senior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "World Trade Center Transportation Hub", "Kevin Spacey", "All Hallows'Day", "78", "main type of cell found in lymph", "Bangladesh -- India border", "President", "minor key", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shetty", "metamorphic rock", "January 12, 2017", "the United States", "claims adjuster", "The neck", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column ( spine)", "three", "perennial", "boudin", "kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "NYPD Shield", "BBC building in Glasgow, Scotland", "a thick stack of paper"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7071746056630568}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false], "QA-F1": [0.8, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.37499999999999994, 1.0, 0.28571428571428575, 0.6666666666666666, 0.25, 0.3333333333333333, 1.0, 0.5, 0.2857142857142857, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_squad-validation-7670", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.5625, "CSR": 0.6114583333333333, "EFR": 0.9285714285714286, "Overall": 0.770014880952381}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "The Late Show with Stephen Colbert", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "CBS", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "60", "Seattle, Washington", "Battle of Antietam", "Dimitar Berbatov and Carlos Tevez", "In Time", "early 3rd century", "Glenn Close", "three", "Agostino Bassi", "five", "Malibu, California", "the church at Philippi", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Jane Fonda", "Bhupendranath Dutt", "Grey Wardens", "Dr. Lexie Grey ( Chyler Leigh )", "Majandra Delfino", "September 1972", "Uruguay", "Alex Skuby", "Thomas Middleditch", "The National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "domesticated sheep goes back to between 11000 and 9000 BC", "1970s", "Director of National Intelligence", "D.A.D. a.", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Thespis", "Portugal. The Man", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "conanous wings", "Code 02PrettyPretty", "musician", "the parliament within 15 days.", "abduction of minors", "new Mexico, Colorado, Wyoming and Montana,", "Pablo Neruda", "Stage Stores, Inc.", "1881"], "metric_results": {"EM": 0.484375, "QA-F1": 0.591587449009324}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5454545454545454, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-551", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674", "mrqa_searchqa-validation-13473", "mrqa_searchqa-validation-5103", "mrqa_hotpotqa-validation-1852"], "SR": 0.484375, "CSR": 0.603515625, "EFR": 0.8787878787878788, "Overall": 0.7411517518939394}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "the Arizona Cardinals", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "Khitan", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Vancouver", "Microsoft Office", "SAVE", "SAS Fr\u00f6sundavik Office Building in Solna Municipality, Sweden", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award Best Actor", "Jello Biafra drew on Nardwuar's face with a marker pen", "the 1745 rebellion of Charles Edward Stuart", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements", "Sir William McMahon", "North Sea", "7.63\u00d725mm Mauser", "Academy Award for Best Animated Feature", "CAC FC-1 \"Xiaolong\"", "Delacorte Press.", "Neighbourhood is generally defined spatially as a specific geographic area and functionally as a set of social networks.", "Secretariat", "Wake Island", "Hydrogen vehicle", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "\"Southern Living\"", "Thomas Harold Amer", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Alticor", "Parlophone Records", "South Africa", "Surrey", "\"My Sassy Girl\"", "Charles Russell", "Boyd Gaming", "Anthony Davis of the New Orleans Pelicans", "1968", "Glenn Close", "Florence Glenda Ballard", "Neighbours", "Ewan McGregor", "2011", "Alice Lloyd College", "an enslaved African American", "power-sharing talks", "Brown-Waite"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6596947017443342}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.5, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655"], "SR": 0.578125, "CSR": 0.6020220588235294, "EFR": 0.8888888888888888, "Overall": 0.7454554738562091}, {"timecode": 17, "before_eval_results": {"predictions": ["forces", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter status", "1830", "nonfunctional pseudogenes", "inner chloroplast membrane", "Charlie Harper", "Stevie Wonder", "beaver", "La Boh\u00e8me", "formic acid", "Toledo", "Zimbabwe", "Mr. Boddy", "Edward \"Ted\" Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "hound", "Xenophon", "London Pride", "Fresh Water Load Line", "Nick Hornby", "\"The Two Gentlemen of Verona\"", "Charles V", "King of Mann", "Lagertha", "weight plates", "\"big house\"", "Hadrian", "California", "murine typhus", "Moonee Ponds, a suburb in Melbourne, Victoria", "Essen", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "Manchester United", "Prokofiev", "Jessica Simpson", "Boy George", "Finland", "3000m", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "It was a Confederate victory", "New Jewel Movement", "sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "Oshkosh", "\"Papa's Got a Brand New Bag\"", "Jesse James", "\"The Sunday Thing\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.6505580357142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.28571428571428575, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7089", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.5625, "CSR": 0.5998263888888888, "EFR": 0.9642857142857143, "Overall": 0.7820560515873016}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "high", "Manakin Town", "northwest", "fewer than 10 employees", "Middle Miocene", "magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "1994", "Coppolas and, technically, the Farrow / Previn / Allens", "Allison Janney", "Isthmus of Corinth", "Aphasia is an inability to comprehend and formulate language because of damage to specific brain regions", "Splodgenessabounds", "Tyrion", "electron donors", "Alison", "1985", "19 state rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Alice", "2 September 1990", "Coton in the Elms", "pass grades 1 ( threshold 85 %, a distinction )", "Zoe Badwi", "in 1995", "Definition of the problems and / or goals", "16 August 1975", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "`` Killer Within ''", "Western Australia", "arterioles", "July 21, 1861", "Dr. Addison Montgomery", "capital and financial markets", "An optional message body", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "universal significance", "September 2017", "moral", "Rising Sun Blues", "Part 2", "dumbo", "the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "jegna", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6498854366307927}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4210526315789474, 1.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735"], "SR": 0.5625, "CSR": 0.5978618421052632, "EFR": 0.9642857142857143, "Overall": 0.7810737781954887}, {"timecode": 19, "before_eval_results": {"predictions": ["Luther reviews and reaffirms, on the one hand, what has been called the \"second use of the law,\"", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "Long troop deployments", "Joe Pantoliano", "a Florida girl who disappeared in February", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "32 percent", "Falkland Islands to carry a government permit.", "Tuesday", "forgery and flying without a valid license", "Anil Kapoor", "55", "President Obama", "unwanted baggage from the 80s", "The Louvre", "snowstorm", "exotic sports cars", "11 healthy eggs", "Mutassim", "two Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18,", "\"GoldenEye\"", "NATO fighters followed the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic,", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "\u00a341.1 million", "Kingman Regional Medical Center", "CNN", "Manmohan Singh", "Michael Jackson", "be silent", "40 militants and six Pakistan soldiers dead", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Louisiana", "the Southeast", "jenkins", "Carol Browner", "\"A Mother For All Seasons.\"", "a tracheotomy, a surgical procedure in which an opening is made into the airway through an incision in the neck to allow for suction of fluid out of the lungs.", "back at work", "Georgia Aquarium", "27", "Corbin Bleu and Karina Smirnoff", "John Adams", "ciorb", "Zager and Evans", "Bob Hurley", "fourth term", "obscenity", "cavalry", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5996247964997965}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true], "QA-F1": [0.14814814814814814, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.13333333333333336, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.923076923076923, 0.6666666666666666, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.484375, "CSR": 0.5921875, "EFR": 0.8181818181818182, "Overall": 0.7051846590909091}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death wish Coffee", "L", "Cameroon", "1994", "ballots", "clothes that are consistent and accessible", "three empty vodka bottles,", "training Afghan police and troops, before trading his uniform for a diplomat's business suit.", "Bobby Darin,", "Nico Rosberg", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "composer of \"Phantom of the Opera\" and \"Cats\"", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony, 2,", "Amanda Knox's aunt", "half Moon Bay (some weighing well over 1,000 pounds)", "Iran's development of a nuclear weapon", "12 shades of violet, including a welcoming, bright blue-purple during the day, a softer violet hue after dusk, and a deep, relaxing near-black on red-eyes when it's time to sleep.", "allegedly faking a doctor's note and was restricted from leaving his house in Tokyo,", "ceo Herbert Hainer", "Brett Cummins,", "a nearby day care center whose children are predominantly African-American.", "inmates", "Col. Elspeth Cameron-Ritchie,", "E! News", "Six members of Zoe's Ark", "jobs", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "Mexico's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "\"They left without me,'", "Alexandros Grigoropoulos,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "a 57-year old male deep in a mid-life crisis is proven.", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Matthew Fisher,", "\"outlaws\"", "boogeyman Jason Voorhees", "This was not any one particular act; this was just a combination of things that had a medical impact on him, that hurt his health.", "Sea World in San Antonio", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "the WSSRC had received about 50 formal applications for speed attempts during 2008.", "the Ku Klux Klan", "MGM prohibited the release until The Wizard of Oz ( 1939 ) had opened and audiences heard Judy Garland perform it", "Yale University", "Bury, Oldham, Rochdale, Stockport, Tameside, Trafford, Wigan, and the cities of Manchester and Salford.", "stamens", "Malayalam movies", "August 17, 2017", "a jacket, gloves or a briefcase", "mice followed, in the '80s | clone. right: Dave.", "Hodel", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "Coldplay"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5020393866610973}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 0.0, 0.0, 1.0, 0.9523809523809523, 0.0, 0.25, 0.23076923076923078, 0.8, 1.0, 0.6153846153846153, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.14814814814814814, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.19047619047619047, 0.15384615384615385, 1.0, 0.1111111111111111, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5263157894736842, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-10284", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.40625, "CSR": 0.5833333333333333, "EFR": 0.9473684210526315, "Overall": 0.7653508771929824}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "\u201cUnder The Sea\u2019 From \u201cThe Little Mermaid\u2019 Is The Best Way To Start Your Weekend\u201d", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "undersirts", "Madonna's", "Glasgow", "road map", "Australia", "Gastronomy", "Pearson PLC.", "Irish Setter", "American Civil War", "Loch Awe", "rochner", "Tasmania", "medium-sized", "Taiwan (or Republic of China)", "Harrisburg", "mink mink,", "glockenspiel", "Dr John Sentamu", "rochoon", "Cruella de Vil", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "Charlemagne", "the A's west to Oakland,", "Russell Crowe", "Theodore Roosevelt", "roch", "Robin Goodfellow", "Samuel Butler", "chamomile tea", "Ireland", "tarn", "Atlantic", "Albert Square", "Newbury", "the Old Testament", "estimated 70 million people", "Target Corporation", "Sister, Sister (1982 film)", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "International NGO", "John Jackson Dickison", "better conditions for inmates, like Amnesty International.", "talk show queen Oprah Winfrey.", "his mother."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6111111111111112}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.546875, "CSR": 0.5816761363636364, "EFR": 0.896551724137931, "Overall": 0.7391139302507836}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "hydrocodone", "ireland", "Robert Peary", "pearls", "Utah", "Carrie Underwood", "whisky", "he made his horse a consul, his palace a brothel, and his...", "Google", "Langston Hughes", "Pain tolerance", "blacking", "Tito Puente", "riata", "unFINISHED OPERAS", "USS LST 325", "black and Tan Coonhound", "David Beckham", "Arturo Toscanini", "economics", "Andes flight disaster", "the triumphal arch", "Montenegro", "discus", "kouign", "basidiomycota", "James Gandolfini", "Georgia Thomas", "president of the Democratic Republic of the Congo", "blacksmith", "a tangible and visible entity", "black", "Plutarch", "Rudy Giuliani", "masa", "40 seconds", "the Vikings.", "fairfield Street", "champs Elysees", "typhoid fever", "fjord", "baviere-quebec.org", "Williamsburg", "Jul 13, 2010", "tualatin", "hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off in this type of 10-letter chemical reaction", "John Knox", "the internal reproductive anatomy", "$612.4 million", "risk factors for disease and targets for preventive healthcare", "jape", "Tesco", "A4", "Graham Hill", "Battelle Energy Alliance.", "IT", "debris", "$10 billion", "Bailey, Colorado,"], "metric_results": {"EM": 0.34375, "QA-F1": 0.40585847975553857}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.09090909090909091, 1.0, 0.5, 0.0, 0.14285714285714285, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_squad-validation-6429", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1118", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-1997"], "SR": 0.34375, "CSR": 0.5713315217391304, "EFR": 0.9523809523809523, "Overall": 0.7618562370600414}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "the Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "the Gulf of Mexico", "domestic cat in America", "sister", "Basel, Switzerland", "The Argonauts", "prometheus", "Altamont Speedway Free Festival", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "a multi-user real-time virtual world described entirely in text.", "Italy", "Khaki", "magma", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "President George W. Bush", "the Earth", "Nafea Faa Ipoipo?", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "Earthquake", "Fife", "Money Saving", "Adidas", "The Hunting of the Snark", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "the unoccupied square or one occupied by an opponent's piece, which is captured and removed from play.", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "The Wall Street Journal Europe", "fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7294730392156863}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848"], "SR": 0.671875, "CSR": 0.5755208333333333, "EFR": 1.0, "Overall": 0.7877604166666666}, {"timecode": 24, "before_eval_results": {"predictions": ["illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "the chosen machine model", "20th Century Fox, Lionsgate, Paramount Pictures, Universal Studios and Walt Disney Studios", "1997", "a suite of network protocols created by Digital Equipment Corporation", "his son, Isaac, and daughter, Rebecca.", "15", "the first home series defeat on Australia in almost 16 years", "between Pyongyang and Seoul", "fatally shooting a limo driver on February 14, 2002.", "11", "change course", "Alwin Landry's supply vessel Damon Bankston", "Chaffetz is a conservative Republican married father of three who is sleeping on a cot in his congressional office to save money.", "money or other discreet aid for the effort", "Sarah", "illegal crossings into U.S. waters.", "environmental", "Costa Rica", "Afghan security", "Saturday", "38", "70,000 or so", "the single European Sky initiative", "E! News", "national coach", "Steve Williams", "McDonald's", "crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned,", "pastor Paula White", "at the end of the year it's been very hectic.", "Diego Maradona", "Dog patch Labs", "club-themed", "two", "Itawamba County School District", "the former Massachusetts governor", "EU naval force", "Plymouth Rock", "Liza Murphy", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "police", "former U.S. secretary of state", "At least 33", "Samir Kuntar", "get better skin, burn fat and boost her energy.", "the black market of prison life", "the most powerful form of prevention is believing that students can help stop crime from happening.", "Alwin Landry's", "karthik Rajaram", "Sunday,", "killing", "an Irish feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "Northern Ireland", "radio waves", "art", "the 16th season for the Minnesota Timberwolves in the National Basketball Association.", "23", "South America", "freestyle", "the Nightingale", "the Kingdom of the Crystal Skull"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5270772316647943}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 0.09090909090909091, 0.8333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7741935483870968, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 0.5, 0.0, 0.15384615384615383, 0.0, 0.19354838709677422, 1.0, 0.5, 0.25, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7272727272727273]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-610", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-1450", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_searchqa-validation-1545", "mrqa_searchqa-validation-3826"], "SR": 0.359375, "CSR": 0.566875, "EFR": 0.926829268292683, "Overall": 0.7468521341463414}, {"timecode": 25, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.802734375, "KG": 0.475, "before_eval_results": {"predictions": ["its 50th anniversary special", "Thomas Savery", "Vicodin, generically known as hydrocodone", "Eastern crops such as carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago", "a violent separatist campaign", "Eleven people died and 36 were wounded", "269,000", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "38 feet", "Eintracht Frankfurt", "150", "a man had been stoned to death by an angry mob.", "Russian bombers", "41,", "Los Alamitos Joint Forces Training Base", "the company has not yet managed to sell the concept to a buyer.", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "Quebradillas.", "the Russian air force,", "34", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "carbon neutral airline.", "Amanda Knox's aunt", "\"They don't go that you could cut off all the commands out to the control surfaces,\"", "ensuring that all prescription drugs on the market are FDA approved", "customers", "Tom Baer.", "Pakistan", "The oceans are kind of the last frontier for use and development,\"", "a two-piece bathing suit", "Brian Mabry", "iTunes,", "Sunday.", "60 euros -- $89 --", "American Civil Liberties Union", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "The tower will be built in the Saudi town of Jeddah and will be part of a larger project that will cost $26.7 billion, (100 billion Saudi riyals)", "his wife's name", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "2006", "San Diego", "five", "Bergdahl, 23, was captured June 30 from Paktika province in southeastern Afghanistan,", "@", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "National Police", "heart", "Hyderabad", "between the Mediterranean Sea to the north and the Red Sea", "to stay, abide", "Las Vegas", "Jackson Pollock,", "Wye", "McComb, Mississippi", "October 4, 1970", "King Duncan", "Brasstown Bald", "Monopoly", "taking a walk"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5292503571397412}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.8750000000000001, 1.0, 0.8, 0.25, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.29411764705882354, 0.0, 1.0, 0.0, 0.07407407407407408, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.1111111111111111, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.2608695652173913, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-1982", "mrqa_searchqa-validation-9476"], "SR": 0.4375, "CSR": 0.5618990384615384, "EFR": 0.9444444444444444, "Overall": 0.7079874465811966}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "people were throwing Molotov cocktails, rocks and glass.", "Krishna Rajaram,", "at least 25", "Booches Billiard Hall,", "finance", "Ross Perot", "Hong Kong's Victoria Harbor", "2002", "six prostitutes and a runaway involved in the drug trade.", "the legitimacy of that race.", "think are the best.", "three", "Monday", "Scarlett Keeling", "two years,", "nearly 28 years", "inspectors in the agency's Colorado office", "military trials for some Guantanamo Bay detainees.", "in July", "The poster boy of Indian action films", "the local political representative", "Marxist guerrillas admit they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "\"disagreements\" with the Port Authority of New York and New Jersey,", "September,", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation and had two broken bones in his neck,", "Phil Spector", "Kim Il Sung", "1994", "numerous suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "cars, people have chosen their rides based on what their cars say about them.", "sexual assault", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "in his 60s,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.", "same-sex civil unions,", "military veterans", "barter -- trading goods and services without exchanging money", "semi-autonomous organisational units", "6 - 6 with one win against a team from the lower Football Championship Subdivision ( FCS )", "Bongos", "William Edward \"Jack\" Frost,", "the innermost digit of the forelimb", "1968", "25 million", "Peoria, Illinois", "Hawaii", "KID-FRIendly 4- LETTER", "King", "Ottoman Empire"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6223873725911769}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 0.06666666666666667, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.10256410256410256, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.782608695652174, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1875", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-9563", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.546875, "CSR": 0.5613425925925926, "EFR": 1.0, "Overall": 0.7189872685185186}, {"timecode": 27, "before_eval_results": {"predictions": ["in the early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "bank robber John Dillinger,", "what caused the collapse of the building", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "Brazil's response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "Lashkar-e-Tayyiba (LeT)", "$1.5 million", "2006", "Rev. Alberto Cutie", "Oakland A's", "eight Indian army troopers, including one officer, and 17 militants,", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "in a Starbucks", "BADBUL", "might rise,", "2008", "near the Somali coast", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "between government soldiers and Taliban militants", "Iraq", "Iran", "last month's Mumbai terror attacks", "people have chosen their rides based on what their cars say about them.", "July", "International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "Four Americans", "her father, Josef Fritzl,", "Glasgow, Scotland", "At least 38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "in the city center,", "a broken pelvis", "Wednesday", "abduction of minors", "gun", "Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "Missouri", "largest", "beta blockers"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7288781441259891}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.3636363636363636, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.09523809523809525, 0.0, 0.6666666666666666, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4094", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-436", "mrqa_naturalquestions-validation-6383", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-14535"], "SR": 0.5625, "CSR": 0.5613839285714286, "EFR": 0.9285714285714286, "Overall": 0.7047098214285714}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author", "changing land officials into large debts that cannot be repaid, ownership of private industries thus expanding the controlled area, or having countries agree to uneven trade agreements forcefully.", "1981", "forgery and flying without a valid license,", "a racially-tinged remark made by his former caddy,", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force.", "shoot down the satellite that North Korean officials said they plan to launch.", "semiconductors", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "seven", "the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations --", "Anil Kapoor.", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "The Rosie Show", "Form Design Center.", "killing at least 27 Awa Indians in southwest Colombia in the past two weeks.", "Christianity and Judaism", "the Dalai Lama's", "Russia", "around 8 p.m. local time Thursday", "Passers-by", "one day,", "executive director of the Americas Division of Human Rights Watch,", "750", "300", "Matthew Fisher", "The Ski Train", "Big Brother.", "Ozzy Osbourne", "AbdulMutallab,", "some U.S. senators", "inconclusive", "about 5:20 p.m. at Terminal C", "environmental and political events.", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600", "a million", "Sen. Arlen Specter", "Deutschneudorf,", "legislation that would let prisons jam cell-phone signals within their walls.", "a deceased organ donor,", "bragging about his sex life", "divided by body plan", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the Sidgwick Avenue arts faculty", "Douglas Hofstadter", "\"The Dark Tower\"", "American", "Marmee's", "Castle Rock", "tomatoes, mozzarella, or ham."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6679213626193816}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.6, 0.25806451612903225, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.08, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.578125, "CSR": 0.5619612068965517, "EFR": 0.8888888888888888, "Overall": 0.6968887691570881}, {"timecode": 29, "before_eval_results": {"predictions": ["downward pressure on wages.", "poison", "438,000 species", "Marty Ingels", "a type of Russian made RF connector for coaxial cables.", "Pakistan A", "Everbank Field.", "7 members appointed by the chief executive.", "Battle of Dresden", "James FitzJames, 1st Duke of Berwick", "1965", "Paris at Charles de Gaulle Airport.", "fifth", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven", "Province of Syracuse", "1963", "coca wine", "puzzle", "Knoxville, Tennessee", "Sam Roberts Band, The Trews, SomeKinda Wonderful, and Saintseneca.", "Oryzomyini", "Tom Kartsotis,", "2017", "Wayman Tisdale", "Mexico", "Kolkata", "Northern Ireland.", "late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell,", "23 March 1991", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Films.", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey.", "Floridians", "the same decree as the one issued in \"Tinsley v. Treat\"", "1996 NBA Slam Dunk Contest.", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "a field in Somerset County, Pennsylvania,", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes,", "IB Primary Years Program for children aged 3 to 12", "Richard Parker", "off the southernmost tip of the South American mainland", "Charlotteton Heston", "allergic reaction", "Peter Townsend.", "3,000 kilometers (1,900 miles)", "remains committed to British sovereignty", "recent theft in Switzerland of two paintings by Pablo Picasso,", "Russia", "shrimp", "Australia"], "metric_results": {"EM": 0.59375, "QA-F1": 0.676219495051732}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.3333333333333333, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.5, 1.0, 0.6666666666666666, 1.0, 0.21052631578947364, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-4033", "mrqa_searchqa-validation-2585"], "SR": 0.59375, "CSR": 0.5630208333333333, "EFR": 0.9615384615384616, "Overall": 0.711630608974359}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Germany's position in a Europe", "London", "Dave Thomas", "a farmers' co-op", "Danish", "1903", "attack on Pearl Harbor.", "other individuals, teams, or entire organizations.", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Dirk Werner Nowitzki", "Cecil B. DeMille Award", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E.", "New Jersey", "Hl\u00edn", "86,112", "Celtic", "Ouse and Foss", "the United States", "British comedian", "Apatosaurus", "1910", "American", "Frank Thomas' Big Hurt", "\"Gliding Dance of the Maidens\"", "Margarine Unie", "Winecoff Hotel fire", "New York City.", "The Seduction of Hillary Rodham", "2005", "Lambic", "Ubisoft", "Argentina", "Larry Alphonso Johnson Jr.", "Mike Mills", "nuclear weapons", "Joseph E. Grosberg", "Chelsea Lately", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "New York", "discus thrower", "Villa Park", "2005", "228", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "a rapid pulse, anxiety, and trouble breathing.", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6846685606060605}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6399999999999999, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-13695", "mrqa_searchqa-validation-6975"], "SR": 0.5625, "CSR": 0.5630040322580645, "EFR": 0.9285714285714286, "Overall": 0.7050338421658986}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "a huge terrestrial globe", "Timber Mountain", "Iowa Demonstration Construction Grant Program", "A People's History of the United States", "Nassau", "a gemstone formed by the nacreous inner shell", "HIV", "Martin Van Buren", "Shinkansen", "Rigoletto", "aardwolf", "Beijing", "a mile in under four minutes.", "Inuk", "San Jose, California, United States", "Yves Saint Laurent", "reindeer", "King Claudius", "the War of 1812", "Anna Mary Robertson", "Sailor Moon", "jalapeo peppers", "Dan Brown", "a bear", "a flurry", "Rock and Roll Hall of Famers", "Monty Python and the Holy Grail", "cathode", "(Milton) Berle", "george herbert walker bush", "Belgium", "lunar module", "Almagro", "Dan Marino", "Mars", "clownfish", "E = mc2", "The Love Guru", "Las Vegas", "soy", "a butterfly", "heavy drinking", "orangutan", "Sonora", "death of Caesar", "Prime Minister of Israel Yitzhak Rabin.", "Saul", "Gettysburg", "Jack Gleeson", "a board of wood", "Buddhism", "Carl Johan", "Portugal", "Graham Bond", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "knocking the World Cup off the front pages", "12.3 million people worldwide"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5591517857142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-4905", "mrqa_naturalquestions-validation-5896", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.53125, "CSR": 0.56201171875, "EFR": 0.9666666666666667, "Overall": 0.7124544270833334}, {"timecode": 32, "before_eval_results": {"predictions": ["Second World War", "62", "Herbert Henry Asquith", "40", "Libya", "Shania Twain", "Hillsborough", "glucagon", "The New York Yankees", "rapid eye movement", "green, red, white", "Ann Dunham", "Ba'ath Party", "French", "Jim Branning", "Ohio", "Francis Matthews", "photographic", "magnetite", "Noah", "Royal Albert Hall, London", "New", "Prince Andrew and Sarah Ferguson", "Mercury", "a watt", "Peter Butterworth", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "Rio de Janeiro", "His name has become synonymous with someone who lives in hopeful expectation.", "aged 75 or older", "Jennifer Lopez", "1664", "Morgan Choir", "Fred Perry", "Downton Abbey", "Martina Hingis", "O\u2019Ahern", "cyclops", "Toonhound", "Michael Miles", "Sheryl Crow", "a cheery optimist", "Pomona", "Milan", "Mike Skinner", "E North America", "a black Ferrari", "a branch of mathematics", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "Campbell Soup Company", "Kirkcudbright", "Indiana National Guard soldiers", "an insect sting", "recognizes the importance of Turkey and wants to engage with it from the start.", "Jennifer Aronofsky", "a hermit", "lungs"], "metric_results": {"EM": 0.453125, "QA-F1": 0.530109126984127}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-5258", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-6656", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-2413", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.453125, "CSR": 0.5587121212121212, "EFR": 0.9428571428571428, "Overall": 0.7070326028138528}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "chicken", "Hudson Bay", "florida", "allergens like molds, pollen and animals.", "stanley", "Ast\u00e9rix", "london", "Belfast", "wind", "fire", "Robin Hood's A Holy Grail", "West Point", "Andy Warhol", "La Mancha", "John Cable", "london", "the solar system.", "potatoes", "Moldova", "Mitsubishi A6M Zero Fighter", "Dartford Warblers", "Franz Liszt", "Estimate", "baroudeur", "iron", "Pet Sounds", "Madness", "Buxton", "Discretion", "Christian Dior", "Rudyard Kipling", "london", "the Philippines", "beaver", "Mel Blanc", "leopard", "riggs", "Ellen Morgan", "phil Woolas", "5000 meters", "horse", "rennet", "Newfoundland and Labrador", "crow", "Yellowstone", "St. Thomas the Apostle", "Cebu", "Hugh Laurie", "Buddhism", "Guy Berryman", "Ohio", "Port Melbourne", "\u00c6thelthryth", "Scarface", "forgery and flying without a valid license,", "It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory over FK Ventspils.", "Liza Murphy, 42,", "Spock", "Kazakhstan", "Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain and Switzerland."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5401041666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_triviaqa-validation-3524", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-525", "mrqa_searchqa-validation-11382"], "SR": 0.484375, "CSR": 0.5565257352941176, "EFR": 1.0, "Overall": 0.7180238970588235}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business districts", "prairies", "Bologna, Italy", "George Santayana", "marsupials", "Alice Cooper", "beta-Blockers", "piano", "peter Kay", "sky", "shildon", "appalachian mountains", "spain", "ballet", "earthquakes", "black Eyed peas", "lizard", "blackburn", "mule train", "The Mystery of Edwin Drood", "pommel horse", "a scarlet tanager", "Dick Van Dyke", "Egremont", "Numb3rs", "monegro", "phrixus", "spainon", "Canada", "ink sac", "Pears soap", "Some Like It Hot", "moll", "Ireland", "Mike Meyers", "a sea horse", "a delorean", "magma", "Passepartout", "Thank you", "spainland", "spainland", "shrek", "26.22", "Cleveland Brown", "fennel", "One Direction", "spain", "Uranus", "st. Mary Mead", "Charles Lindbergh", "September 2001", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "beer mugs", "Pittsburgh", "Pakistan's High Commission in India", "shock, quickly followed by speculation about what was going to happen next,\"", "Hunter S. Thompson", "tchaikovsky", "Howard Carter"], "metric_results": {"EM": 0.46875, "QA-F1": 0.521875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4530", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-78"], "SR": 0.46875, "CSR": 0.5540178571428571, "EFR": 0.9117647058823529, "Overall": 0.699875262605042}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "John Forster", "Matlock", "American Civil War", "armageddon", "ticks and mites", "Arafura Sea", "a labyrinth", "Tigris", "spitze", "furrow", "Spain", "carousel", "bullfighting", "Mike Brady, a widowed architect", "tenor", "cat food", "fidelio", "Guys and Dolls", "Julian Fellowes,", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "Ghana", "armageddon", "G. Ramon", "jane fonda", "rachmaninoff", "Finland", "stars with gravity", "Mille marques", "spelunking", "Bill Haley & His comets", "50p", "Muriel Spark", "good morning to all", "seven", "opossum", "pickwick", "a machine that cuts the bread finely while efficiently and securely wrapping the entire bread", "crime drama", "raven", "Jordan", "bPA", "nelsons Column", "Etruscan", "Ken Burns", "carriageway", "heather Stanning and Helen Glover", "Pyotr Ilich Tchaikovsky", "Sheikh Mujib", "Sagittarius", "Donna", "season four", "sinoatrial node", "Yubin, Yeeun", "tomato", "2002", "\"wildcat\" strikes, unsanctioned by national unions,", "L'Aquila earthquake,", "March 24,", "prince of Edinburgh", "equinox", "Pocahontas"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5545758928571428}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.4, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.14285714285714285, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-400", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.484375, "CSR": 0.5520833333333333, "EFR": 0.9090909090909091, "Overall": 0.6989535984848485}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "Kim", "city of Acacias", "Branson,", "Gordon Ramsay", "Denis Law", "Robert Kennedy", "nitrogen oxides", "Margot Betti", "ringway", "Portuguese", "travelocity", "Avengers", "Richmondshire", "black mask", "Paul Simon", "a ghost", "canola", "Tina Turner", "Sweeney Todd", "Strasbourg, Alsace, France", "Bolivia", "John Donne", "Uranus", "Rio Grande", "Percheron", "middle-West Side New Yorker", "black", "joan foyle", "James I", "One Foot in the Grave", "Bronx", "comets", "George Santayana", "archery gloves", "borowdale", "Krankies", "Tomas De Torquemada", "Daniel Barenboim", "Canada", "rum", "seattlepi.com", "ghee", "George III", "Justin Bieber", "Hyperbole", "oldpatricktoe-end", "June", "David Graham", "Ceylon", "screwdrivers", "the Kansas City Chiefs", "G minor", "A Christmas Story", "2004", "The Outsiders", "Amberley Village", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "President Obama", "Josef Fritzl,", "cixi", "Brigham Young", "pearl", "chalk quarry"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6036458333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-2834", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-4108", "mrqa_hotpotqa-validation-2330", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120"], "SR": 0.546875, "CSR": 0.5519425675675675, "EFR": 0.9655172413793104, "Overall": 0.7102107117893757}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "neutral", "piped masonry", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist James Hutton", "N 17.44667 \u00b0", "joy of living", "420", "John Prine and Roger Cook", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "January", "Los Angeles", "February 10, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "hieroglyphs for the digits", "Niveditha, Diwakar, Shruti", "freehold", "John C. Reilly", "DNA nucleus", "Mace Windu", "Travis Tritt and Marty Stuart", "1976", "Bee Gees", "Matt Czuchry", "Pradyumna", "1853", "Lands End", "Psychomachia", "New Jersey Devils", "two", "7.6 mm", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria", "Canadian Rockies continental divide east to central Saskatchewan", "Maginot Line", "France", "Dumbo", "purple rain", "Charles Guiteau", "Gettysburg Address", "iTunes, iTunes Radio, and iTunes Music", "$273 million", "India", "Jeddah, Saudi Arabia", "Desperate Housewives", "cannonball run", "chilpancingo", "Tuesday"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6080690934970826}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.6363636363636364, 1.0, 1.0, 0.8, 0.2608695652173913, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07272727272727272, 0.25, 0.5454545454545454, 1.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-5119", "mrqa_searchqa-validation-2335"], "SR": 0.515625, "CSR": 0.5509868421052632, "EFR": 0.9354838709677419, "Overall": 0.704012892614601}, {"timecode": 38, "before_eval_results": {"predictions": ["Isaac Newton", "roughly five hundred experts across the world", "the United States", "Kim Basinger", "fall of 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in positions Arg15 - Ile16", "Charles Crozat Converse", "Lady Gaga", "Chicago metropolitan area", "the president", "Domhnall Gleeson", "eusebeia", "Pastoral farming", "Notts County", "f\u0254n", "Stephen A. Douglas", "1984", "related to the Common Germanic word guma", "Pakistan", "21 February", "El Filibusterismo", "Bryan Cranston", "thylakoid membranes", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "Franklin", "January 1923", "18 Divisional Round", "602", "maximum energy of 687 keV", "between $10,000 and $30,000", "September 1980", "1931", "University of Oxford", "Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "The statesmen", "Randy", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "a divergent tectonic plate boundary", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "mrs Sir John Major", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "starting a dialogue while maintaining sanctions,", "10 below", "General Motors'", "David McCullough", "Rendezvous with Rama", "CERN", "Coimbra"], "metric_results": {"EM": 0.578125, "QA-F1": 0.626109068627451}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.32, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.578125, "CSR": 0.5516826923076923, "EFR": 0.9259259259259259, "Overall": 0.7022404736467236}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "Diary of a Wimpy Kid", "Jenny Slate", "Active absorption", "Philippe Petit", "1980", "January 2004", "provinces along the Yangtze River and in provinces in the south", "Toby Keith", "electronic computers", "17 - year - old", "punk rock", "Set six months after Kratos killed his wife and child, he has been imprisoned by the three Furies for breaking his blood oath to Ares", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "XXXX", "Gestalt", "83 volumes", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "John DiMaggio", "Richard Crispin Armitage", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "late 2018 or early 2019", "diffuse nebulae", "effectively overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "McKim Marriott", "John F. Kelly", "Charles Sherrington", "1886", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "1960s", "biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "1940", "Tadheus `` Tad '' Stone", "Mark Jackson", "Michael Buffer", "God and Father of all, who is over all and through all and in all", "on location", "the United States of America", "New England", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "4.5", "Juan Manuel de Ayala", "Joseph Smith", "funny Folks", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew", "me and Bobby McGee", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6219598733445441}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4347826086956522, 0.3076923076923077, 0.17142857142857143, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.47058823529411764, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.20000000000000004, 1.0, 0.4, 0.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-10341"], "SR": 0.46875, "CSR": 0.549609375, "EFR": 1.0, "Overall": 0.716640625}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "two soldiers, a policeman and four militants", "Joan Rivers", "\"You're The One That I Want\"", "glamour and hedonism", "2-0", "more than 15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "Zimbabwe President Robert Mugabe", "two weeks ago", "NATO", "Switzerland", "Monday", "second time since the 1990s", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "\"it is impossible to turn back the tide of globalization.\"", "Clifford Harris,", "Oaxaca, Arizona", "Robert Barnett,", "a class A traffic violation that can command a fine of $627, Hastings said.", "41", "Nick Adenhart", "a strict interpretation of the law,", "Derek Mears", "Nieb\u00fcll", "rural Tennessee", "Tuesday afternoon.", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "Section 60.", "Ali Bongo", "The Transportation Security Administration", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Derek Mears", "Operation Pipeline Express.", "help rebuild the nation's highways, bridges and other public-use facilities. The National Infrastructure Program, as he called it, will spend 570 billion pesos ($42 billion).", "East Java", "children's hospital in St. Louis, Missouri.", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2007", "P.V. Sindhu", "Mexico", "Snickers candy bars", "Monodon monoceros", "(one)", "Anaheim, California", "uncle Juan Nepomuceno Guerra", "Bergen", "embalming", "Cartagena", "Graphical user interface", "UK Kennel Clubs"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6560980159140442}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.30769230769230765, 1.0, 0.0, 0.375, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.25, 0.3870967741935484, 0.07142857142857142, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.29629629629629634, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.9523809523809523, 0.3076923076923077, 0.4, 1.0, 0.3333333333333333, 0.5161290322580645, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.484375, "CSR": 0.5480182926829269, "EFR": 0.9090909090909091, "Overall": 0.6981405903547672}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1997", "displacement", "the modern state system", "Megan Park", "the currency used by the institutions of the European Union", "Kate Walsh", "September 14, 2008", "American country music artist Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point of a liquid", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "international aid", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "estimated in 2009 to be less than $10,000 per year", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "Kris Kringle", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "collect menstrual flow", "pigs", "General George Washington", "Spanish", "Virgil Tibbs", "an integral membrane protein that builds up a proton gradient across a biological membrane", "usually by the sinoatrial node", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six 50 minute ( one - hour with advertisements ) episodes", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups.", "the Intertropical Convergence Zone ( ITCZ )", "Missouri River", "the right to vote", "brain", "10 June 1940", "Tandi, in Lahaul", "Alberich", "ear", "brazil", "The Dressmaker", "$10.5 million (USD 8 million)", "Tim Whelan", "on the project, which is designed to promote private sector investment in a variety of gas-related industries, on September 21.", "Denver, Colorado.", "the Sadr City and Adhamiya districts of Baghdad City,\"", "the United States", "Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6768786143489653}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.8, 0.6666666666666666, 0.7000000000000001, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6451612903225806, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_hotpotqa-validation-157", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-1237"], "SR": 0.546875, "CSR": 0.5479910714285714, "EFR": 0.896551724137931, "Overall": 0.6956273091133005}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "Water extinguishment", "in Middlesex County, Province of Massachusetts Bay", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "Article One of the United States Constitution", "Zeus", "during Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the center", "to bring", "Hermann M\u00fcller", "Ceramic art", "the Soviet Union", "Covington, Kentucky", "New Mexico", "to condense the steam coming out of the cylinders or turbines", "December 15, 2017", "Paradise, Nevada", "L.K. Advani", "differential erosion", "Glenn Close", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "a pair of compasses", "Charlotte Thornton", "the Northeast Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Daya Jethalal Gada", "2,140 kilometres ( 1,330 mi )", "asexually", "1926", "East Asia", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Alfheim", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "won two", "prostate cancer", "wyvern", "Lord Fauntleroy", "a key ring or a decorative key fob", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.658187240999741}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5384615384615384, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.8000000000000002, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.14814814814814814, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.515625, "CSR": 0.5472383720930232, "EFR": 0.9032258064516129, "Overall": 0.6968115857089272}, {"timecode": 43, "before_eval_results": {"predictions": ["2003", "February 27, 2007", "pick yourself up and dust yourself off and keep going", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "as the arms of the king of Ireland", "Miami Heat", "1982", "After World War I", "in the mid - to late 1920s", "Maximilien Robespierre", "Juan Francisco Ochoa", "Augustus Waters", "Camille Pissarro", "Virgil Ogletree", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "rootlets", "Alex Ryan", "habitat", "2018", "Advanced Systems Format ( ASF )", "100", "Toledo", "embryo", "During the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "in Paradise, Nevada", "Alicia Vikander as Lara Croft", "annually in late January or early February", "Ashoka", "the name of a work gang", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Bumblebee", "in the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "AMX - 30", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "2018", "Florida", "American comedy web television series", "Puerto Rico", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "winter", "Pangaea", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\"", "'We want to reset our relationship and so we will do it together.'\"", "America's infrastructure.", "North Atlantic Treaty Organization", "Bob Kerrey", "Jane Goodall", "Forrest"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6499594155844156}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 0.09523809523809525, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_searchqa-validation-13337", "mrqa_hotpotqa-validation-1921"], "SR": 0.53125, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.71609375}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "lightweight aluminum foil", "Laurel, Mississippi", "about the outdoors", "Indianola", "Escorts Limited", "Jean Baptiste Point Du Sable", "1992", "Cher", "Alabama", "Jim Harrison", "Montreal", "Tomorrowland", "fennec fox", "the United States Army", "stop motion animation", "Jean Acker", "4,530", "Leucippus", "Caesars Entertainment Corporation", "Terrence \"Uncle Terry\" Richardson", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "the F-15", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Nidahas Pakshaya", "Bishop's Stortford", "ambassador to Ghana", "Marvin Hamlisch", "1991\u201392", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "OutKast", "Richard Street", "Zaire", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "bringing French cuisine to the American public", "Paraguayan Guarani", "2006", "perjury", "Operation Neptune", "Mary Elizabeth Hartman", "over 9,000 employees", "Rosalind Bailey", "potential of hydrogen", "Alamodome in San Antonio, Texas", "Carrie", "The Finger Tab", "Kent", "almost 9 million", "Kenya", "2008", "terrorism", "Moses", "Chapter 5", "Wilson Pickett"], "metric_results": {"EM": 0.5, "QA-F1": 0.6129407051282051}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-993", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.5, "CSR": 0.5458333333333334, "EFR": 0.9375, "Overall": 0.7033854166666667}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "1858", "Australian", "1903", "navigation by river", "Naomi Wallace", "McLaren-Honda", "Tufts College", "Macau", "Azeroth", "Squam Lake", "The Livingston family", "Tayeb Salih", "King James II of England", "God Save the Queen", "203", "Scotland", "\"rock and roll\"", "GmbH", "Mick Jackson", "Lalit", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "\"Sully\" Sullenberger III", "(FORTE)", "Pacific War", "Romantic", "Hugh Dowding", "AMC Entertainment Holdings, Inc.", "New York Islanders", "fennec", "1978", "John Surtees", "French", "Pacific Place", "The Australian women's national soccer team", "is a song by American singer-songwriter Taylor Swift,", "Rudebox", "about 5320 km", "Gianni Schicchi", "\"Super Hit\"", "Sacramento Kings", "Walldorf, Baden-W\u00fcrttemberg", "Fife", "Fyvie Castle", "Faisal Qureshi", "the British Army", "exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Frances Ethel Gumm", "Switzerland", "Quadricycle", "NATO's International Security Assistance Force", "2,000", "Cyprus", "his pregnant wife", "jeddah", "Joseph Crowley", "two"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5630774456521739}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-3592", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.46875, "CSR": 0.5441576086956521, "EFR": 0.9705882352941176, "Overall": 0.709667918797954}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "\"The Small House of Uncle Thomas\"", "Republican", "1996", "5", "Greenland shark", "The Word", "President Abraham Lincoln", "St Jude Thaddeus", "Van Diemen\u2019s Land", "death penalty statutes in Florida, Georgia, and Texas", "xerophyte", "Jack Roosevelt Robinson", "Manhattan", "Dian Fossey (Sigourney Weaver)", "MI5", "Harrow", "creme anglaise", "pasta", "pork", "curling", "Victoria Coren Mitchell", "Gettysburg", "Chile", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "President Obama", "Canada", "a personalized all-access pass to everything local on events, music, restaurants, news and more.", "the Dominican Republic", "David Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "Florida", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Daily Herald", "(1914\u20131918)", "food that is permissible according to Islamic law.", "2016", "the Supreme Court of Canada", "2017", "Chief of Protocol of the United States", "Diamond White", "1987", "Daniel Nestor, from Canada", "Jeddah, Saudi Arabia", "cause of death", "Beatrix Potter", "Dan Eggen and Elizabeth Williamson", "Reader's Digest Association", "(GNT)"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6045138888888889}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.8, 0.3333333333333333, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3458", "mrqa_triviaqa-validation-3989", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-9246", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-252", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.484375, "CSR": 0.5428856382978724, "EFR": 0.8787878787878788, "Overall": 0.6910534534171503}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "davio capllo", "the last battle on Scottish soil took place on a nearby moor at Culloden", "Runic", "Spain", "Rugby", "Max Planck", "rotherham United", "heat transfer", "Misery", "Styal", "olukord", "blind beggar", "Mr Brainwash", "l Leroy Burrell (United States)", "parlopone", "Wild Atlantic Way", "John Denver", "an", "noddy", "Lackawanna six", "Brazil", "scrabble", "muezzin", "a window", "keel", "madame bovary", "Apollo 11", "cellophane", "Nikola Tesla", "ottman", "evita", "albino sperm whale", "Randy Turpin", "east fife", "st Pancras International Station", "social environment", "presliced bread", "Dilbert", "mayor of casterbridge", "nunc dimittis", "French", "medea", "eastern France", "cribbage", "Westlife", "Johannesburg", "France", "muffin man", "korea", "Prince James, Duke of York and of Albany ( later King James II & VII )", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "from Galveston, Texas, to Veracruz, Mexico", "Kuwait-based Wataniya Airways", "Robert Frost", "King Henry VIII", "pueblo North Medical Offices", "Mitsubishi Lancer OZ Rally"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6173344017094018}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.5625, "CSR": 0.5432942708333333, "EFR": 1.0, "Overall": 0.7153776041666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Route sixty-six", "sesame street", "marcella Hazan,", "cabbage", "australia", "Quincy Magoo", "kolkhis", "Ash", "opossum", "New Zealand", "jug band", "100", "Auric Goldfinger", "1984", "pike", "kharazmian dynasty", "1875", "tax collector", "penny", "maria maria", "maison of anjou", "bagram", "jenkins", "Chrysler", "ushanka", "dandy", "law", "united nation of people from many different backgrounds and beliefs.", "Brazil", "pei Tang", "biathlon", "nebraska", "Charlie Chan", "Vienna, Austria", "white", "jaws", "Paul Rudd", "rabbit", "Scotland", "jerry leadbetter", "Charles Foster Kane", "hindu", "menorah", "Dutch", "Texas", "Super Bowl Sunday", "a rudder", "Little Tommy Stout", "marmole king", "azalea", "Ireland", "Chuck Noland", "the Colony of Virginia", "in Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park, Kenya", "2010", "in Vancouver, British Columbia,", "10 below", "100 to 150", "coins", "the American Kennel Club", "Omaha", "jedoublen"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5801738267770876}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.9565217391304348, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-726", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.484375, "CSR": 0.5420918367346939, "EFR": 0.9696969696969697, "Overall": 0.7090765112863328}, {"timecode": 49, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.771484375, "KG": 0.44921875, "before_eval_results": {"predictions": ["aretha Franklin, Paul Simon, The Rolling Stones", "Iran", "tobacco", "francesco", "Dick Francis", "Daniel Boone", "Thames Street", "jennifer roosevelt", "satyrs", "crab", "La Boh\u00e8me", "IBM", "wishbone", "Garrick club", "Lackawanna 6", "aves", "britten", "the American Civil War", "\"black\"", "jennifer henderson", "Jimmy Robertson", "Florence", "tsar Ivan IV", "Augustus", "Severn", "australia", "south Africa", "perennial rye grass", "Nicaragua", "Churchill", "wars of roses", "Chemnitz", "Baltimore Sun", "trout", "Joseph Dubonnet", "wali Muhammad", "be\u2022lize", "library of Congress", "hair loss", "tandem bicycle", "Charles Edward Springall", "robin hood", "Chris Martin", "bilma Flintstone", "(Lee Ingleby)", "rugby", "honda", "a Steely Dan song \"Deacon Blues\"", "35", "tobacco", "heifer", "free floating", "Harrison Ford", "New Orleans", "a pinball machine", "Texas Tech University", "Loughborough, Leicestershire, in the East Midlands of England", "Herman Cain", "the Afghan rebels: anticommunism.", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "George Babbitt", "Oklahoma", "vodka", "four"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4881620115995116}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.390625, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.70234375}]}