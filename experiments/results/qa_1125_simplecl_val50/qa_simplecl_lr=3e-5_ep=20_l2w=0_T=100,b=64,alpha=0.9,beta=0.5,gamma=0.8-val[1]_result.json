{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4180, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "the 1970s", "Sunni Arabs from Iraq and Syria", "P,NP-complete, orNP-intermediate", "Thomas Murphy", "the highest terrace", "major national and international patient information projects and health system interoperability goals", "three", "net force", "12 January", "1976\u201377", "Cleveland, Phoenix, Detroit and Denver", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "The Book of Discipline", "complicated definitions", "coordinating lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "the main hall", "the Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation of a drug treatment for an individual", "2014", "late 1970s", "30% less steam", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Saul Bellow, political philosopher, literary critic and author of the New York Times bestseller \"The Closing of the American Mind\" Allan Bloom", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts", "The Prisoners ( Temporary Discharge for Ill Health ) Act, commonly referred to as the Cat and Mouse Act, was an Act of Parliament passed in Britain under Herbert Henry Asquith's Liberal government in 1913", "Mrs. Wolowitz", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8028687280399117}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.06451612903225806, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-5952", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.765625, "CSR": 0.765625, "EFR": 0.9333333333333333, "Overall": 0.8494791666666667}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "beat so that the propulsion stroke is away from the mouth, although they can also reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger Goodell", "1950s", "1980s", "Cologne, Germany", "second use of the law", "free", "1973", "September 1969", "Mansfeld", "Warsaw Stock Exchange", "390 billion individual trees divided into 16,000 species", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "1898", "Lunar Module Pilot", "citizenship", "Merritt Island", "accountants", "severed all relations with his family to hide the fact that he dropped out of school", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification", "Lituya Bay in Alaska", "120 m ( 390 ft )", "Game of Throne", "100 members", "photoelectric", "Welch, West Virginia", "Indian National Congress", "twelve Wimpy Kid books", "Hal David and Burt Bacharach", "six points", "Merrimac", "Spain"], "metric_results": {"EM": 0.71875, "QA-F1": 0.787724883623321}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.4, 1.0, 0.1875, 0.38095238095238093, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-9173", "mrqa_squad-validation-5788", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1841", "mrqa_squad-validation-1146", "mrqa_squad-validation-1220", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6125", "mrqa_searchqa-validation-3996"], "SR": 0.71875, "CSR": 0.75, "EFR": 0.9444444444444444, "Overall": 0.8472222222222222}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive; therefore, the defects justifying rebellion must be much more serious than those justifying disobedience", "the principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "recalled and replaced by Jeffery Amherst", "Egypt", "algae", "245,306", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "spy network and Yam route systems", "Stairs", "genetically modified plants", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Bruno Mars", "the Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw), who may sign them into law", "cloud storage service", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "the Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum (\"town of the Ubii\")", "John Elway", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "the Golden Gate Bridge", "Michael Schumacher", "10.5 %", "The Man", "President Gerald Ford", "Jane Fonda", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "in awe of Novalee, and had seen her enter the store at closing time, smashes through the window to help deliver her child", "September 1973", "the land itself, while blessed, did not cause mortals to live forever", "the middle of the 15th century", "6 March 1983", "James G. Kiernan", "horror fiction", "26,000"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7909939132425545}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6798", "mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-10293", "mrqa_squad-validation-4759", "mrqa_squad-validation-8763", "mrqa_squad-validation-6154", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433"], "SR": 0.6875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Fort Le Boeuf", "wireless", "Beyonc\u00e9 and Bruno Mars", "Yuan", "same-gender marriages with resolutions", "red algae red", "after their second year", "1960s", "Yameester van Maastricht", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot (3.7 million cubic meter)", "the 50 fund", "force, stating that British colonists would not be safe as long as the French were present", "ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "power, famine, and bitterness among the populace", "the Establishment Clause of the First Amendment or individual state Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "NDS", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "their parent thylakoid", "motorway underpass without pedestrian access", "to protect their tribal lands from commercial interests", "religious beliefs", "force on a guilty plea", "YamI Notes - Gvsu  All of these older editions have the same pagination, and all are out of print", "Yamchand Gandhi", "Vlad the Impaler", "The Little Foxes", "the 1982 Sony SL-2000 portable", "Leonard Nimoy", "Yam + MP", "Tiger Woods", "1867 to 1877", "Marshall Dillon", "Yam/Who Do You", "Seminyak Beach", "Mary F. Kennedy", "LASER abbreviation", "Hans Christian Andersen", "Yamahuah Reigns", "a genus (scientific group) called Sphagnum", "Yam perch", "Andrew Taggart", "Yamchophobia- Fear of scratches or being scratched", "American", "Enrique Torres"], "metric_results": {"EM": 0.5, "QA-F1": 0.5813244047619048}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-10204", "mrqa_squad-validation-110", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-3703", "mrqa_squad-validation-10186", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-7088", "mrqa_squad-validation-2804", "mrqa_squad-validation-8767", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073", "mrqa_newsqa-validation-496"], "SR": 0.5, "CSR": 0.6875, "EFR": 0.96875, "Overall": 0.828125}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "fell from his horse while hunting", "the member state cannot enforce conflicting laws", "British bacteriologist J. F. D. Shrewsbury", "a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals", "inversely to member state size", "Europe", "he arrived too late", "colonies", "$37.6 billion", "Kenyan athletes", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center in San Jose", "lymphocytes-derived molecule", "the Edict of Fontainebleau", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California", "ten million people", "the Lippe", "Video On Demand content", "time and storage", "semester calendar beginning in early September and ending in mid-May", "the courts of member states and the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "The Jewish Star", "the League of the Three Emperors", "the field of science", "143,007", "the National Intelligence Council (NIC)", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "drawing the name out of a hat", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Br'er Rabbit", "corruption", "24 hours", "Dover Beach"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8270517676767677}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.9523809523809523, 0.8, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-6218", "mrqa_squad-validation-4210", "mrqa_squad-validation-1187", "mrqa_squad-validation-457", "mrqa_squad-validation-6676", "mrqa_squad-validation-12", "mrqa_squad-validation-9753", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.71875, "CSR": 0.6927083333333333, "EFR": 0.9444444444444444, "Overall": 0.8185763888888888}, {"timecode": 6, "before_eval_results": {"predictions": ["the 1540s", "the Court of Justice of the European Union", "its circle logo", "three", "negative long-term impact", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "starch buildup in the chloroplasts, possibly due to less sucrose being exported out of the chloroplast (or more accurately, the plant cell)", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "to overthrow a government (or to change cultural traditions, social customs, religious beliefs, etc...revolution doesn't have to be political", "entertainment", "vote clerk", "high growth rates", "vicious and destructive", "Sony", "Stagecoach", "Silk Road", "San Diego", "Central Poland", "four public charter schools on the South Side of Chicago", "invest in new sources of creating wealth or to otherwise leverage the accumulation of wealth", "Spanish", "Structural geologists", "president and CEO of ABC", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Benjamin Burwell Johnston", "Sinclair Oil Corporation", "Taylor Swift", "Eric Edward Whitacre", "Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "11 or 13 and 18", "Heather Elizabeth Langenkamp", "Henry Gwyn Jeffreys Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI)", "Bury St Edmunds, Suffolk, England", "Charmed", "Jas (Jasmine) Salford", "Liverpool and England international player", "the Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "the Ashanti Region", "25.2 % ( 79 out of 313 )", "Algeria", "a novel", "the Eastern part", "Polar Bear", "The Atlantic City Boardwalk"], "metric_results": {"EM": 0.515625, "QA-F1": 0.7070354278074866}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.2, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9411764705882353, 0.9333333333333333, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 0.4, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.8, 0.6666666666666666, 1.0, 0.8, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3939", "mrqa_squad-validation-5774", "mrqa_squad-validation-5213", "mrqa_squad-validation-8914", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-913", "mrqa_squad-validation-7983", "mrqa_squad-validation-7543", "mrqa_squad-validation-5651", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_naturalquestions-validation-2159", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-5279", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-13072"], "SR": 0.515625, "CSR": 0.6674107142857143, "EFR": 1.0, "Overall": 0.8337053571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "the working fluid", "suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "the solution", "means to invest in new sources of creating wealth or to otherwise leverage the accumulation of wealth", "the center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "papacy", "through homologous recombination", "a modern canalized section", "in protest against the occupation of Prussia by Napoleon in 1806-07", "improved markedly", "nearly visible along the entire length of the lake", "computer programs", "General Conference of the United Methodist Church", "1996", "dreams", "The Judiciary", "a deterministic Turing machine", "Bart Starr", "allotrope", "Karluk Kara-Khanid ruler", "Perth", "Ian Rush", "Gerry Adams", "New Orleans Saints", "1974", "four", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "the capital of the Socialist Republic of Vietnam", "Sevens", "Fennec fox or fennec (\"Vulpes zerda\")", "Bart Conner", "fantasy role-playing game", "Martin McCann", "Black Mountain College", "a historic house museum", "Bothtec", "Cody Miller", "140 to 219", "John Locke", "Christophe Lourdelet", "Pablo Escobar", "African descent", "Teotihuacan", "an orally transmitted version of the originally literary tale published by Charles Perrault in \"Histoires ou contes du temps pass\u00e9\"", "Disneyland", "1985", "Noddy", "an successor to President Omar Bongo", "Wheat Chex", "Ray Harroun", "Emily Blunt", "David Tennant"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6736156204906205}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.20000000000000004, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-7547", "mrqa_squad-validation-3091", "mrqa_squad-validation-9287", "mrqa_squad-validation-1819", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-3413", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-3885", "mrqa_naturalquestions-validation-4388", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869", "mrqa_naturalquestions-validation-1618"], "SR": 0.5625, "CSR": 0.654296875, "EFR": 1.0, "Overall": 0.8271484375}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Revolutionary civil disobedience", "the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "the 5th Avenue laboratory fire of March 1895", "arms", "two independent mechanisms", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "teachers through the web in order to earn supplemental income", "stratigraphic correlation", "commensal flora", "a + bi", "Dallas, Texas", "Central Asian Muslims", "from home viewers who made tape recordings of the show", "1330 Avenue of the Americas in Manhattan", "Alberta and British Columbia", "Pimp My Ride", "Don Johnson", "Section.80", "25 million", "8,515", "13 October 1958", "jet-powered tailless delta wing high-altitude strategic bomber", "Environmental Protection Agency", "between 1932 and 1934", "an English professional footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Dusty Dvoracek", "boxer", "Boston University", "Fulham", "A55", "Hugh de Kevelioc", "\u00c6thelstan", "Madras Export Processing Zone", "44", "Division I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "A diastema ( plural diastemata )", "Alison Krauss", "Iran", "Bigfoot", "Papua New Guinea", "Renoir", "Manchester"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7933666300097751}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6451612903225806, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3391", "mrqa_squad-validation-3405", "mrqa_squad-validation-2238", "mrqa_squad-validation-9859", "mrqa_squad-validation-7643", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305"], "SR": 0.71875, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.8307291666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["$159 million", "centrifugal governor", "Orange County", "chloroplast peripheral reticulum", "1962", "the European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "its unpaired electrons", "Louis XIV", "Museum of the Moving Image in London", "he sent missionaries, backed by a fund to financially reward converts to Catholicism", "pyrenoid and thylakoids", "Woodward Park", "force and violence and refusal to submit to arrest", "25 May 1521", "essentially holy people", "diplomacy or military force", "an increase in the land available for cultivation", "the value of the spin", "pivotal event", "one of the youngest publicly documented people to be identified as transgender", "Trent Alexander-Arnold", "David Michael Bautista Jr.", "Black Friday", "American actor", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia", "Yasir Hussain", "USC Marshall School of Business", "Stephen Ireland", "Marko Tapani \" Marco\" Hietala", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "2500 ft", "Central Park", "Robert John Day", "Afroasiatic", "James Tinling", "Italy", "the PGA Tour", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "couscous", "22 million", "morphine sulfate oral solution 20 mg/ml", "Harvard Law", "Clarias"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6899203707819694}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333326, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7173", "mrqa_squad-validation-4147", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.609375, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "Timucuan Ecological and Historic Preserve", "suburban shopping areas", "early vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "tight end Owen Daniels", "Sanders", "economic instability", "Gamal Abdul Nasser", "Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production", "counterflow", "lithium-ion battery developer John B. Goodenough", "the installation of pagan images in the Temple in Jerusalem", "machine gun", "the Autons with the Nestene Consciousness and Daleks", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "Standard Model", "Tolui", "the Rhine-Ruhr region", "course of study", "Prevenient grace", "Kansas State", "Captain Cook's Landing Place", "Chris Pine", "Yoo Seung-ho", "the Battle of the Philippines", "NCAA Division I", "The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Okinawa Uzumaki", "Tom Jones", "the RATE project", "Barbara Niven", "13\u20133", "John Faso", "5,042", "European culture", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "William Shakespeare", "Germany and other parts of Central Europe", "New Jersey", "Massachusetts", "Ector County", "Jim Davis", "Buck Owens", "World Health Organization", "Emmanuel Ofosu Yeboah", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Heather Stebbins", "the Royal Firework Music", "Sir Giles Gilbert Scott", "the rig that sank the oil rig", "the Comoros Islands", "Onomastic Sobriquets In The Food And Beverage Industry", "London"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6454004329004329}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7269", "mrqa_squad-validation-5010", "mrqa_squad-validation-797", "mrqa_squad-validation-6495", "mrqa_squad-validation-8072", "mrqa_squad-validation-6927", "mrqa_squad-validation-9815", "mrqa_squad-validation-7729", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-2783", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.5625, "CSR": 0.6477272727272727, "EFR": 1.0, "Overall": 0.8238636363636364}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate the cocky", "Battle of Olustee", "French", "Only 100\u2013150", "Philo of Byzantium", "The climate is cooler", "in marine waters worldwide", "$60,000", "his mother's genetics and influence", "shock", "cytotoxic natural killer cells", "a new element to the standard Christian suspicion of Judaism", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "a whole industry", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "Paul McCartney", "the deal, which promises cabinet positions to the splinter group of the Movement for Democratic Change, does not involve MDC head Morgan Tsvangirai", "\"If you don't have a cause of death, isn't it possible that it might have been an accident?\"", "200", "a very small number of young people taking drugs. We are now more worried about the trend,\" Wong said. \"We don't want a runaway trend; that's why we are stepping up action.\"", "opposition party members", "North Carolina", "to step down as majority leader", "executive director of the Americas Division of Human Rights Watch", "Casa de Campo International Airport", "90", "The station", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "in her home", "Employee Free Choice act", "Bush administration", "more than 200", "It is done with the parents' full consent", "their own", "Kaka", "Christopher Savoie", "Dan Parris, 25, and Rob Lehr, 26", "near Fort Bragg", "two", "$2 billion", "Jacob", "Molotov cocktails, rocks and glass", "as many as 250,000", "Andrew Morris", "Ark of the Covenant", "Jean F Kernel ( 1497 -- 1558 ), a French physician", "The dust was, that as she now stood excited, wild, and honest as the day", "Richmondshire Museum", "1994", "The films", "The Gallipoli Campaign", "Lake Michigan", "Nowhere Boy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6332472432081807}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.25, 0.7499999999999999, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.0625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285714, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1919", "mrqa_squad-validation-3087", "mrqa_squad-validation-4611", "mrqa_squad-validation-4524", "mrqa_squad-validation-1313", "mrqa_squad-validation-1257", "mrqa_squad-validation-6588", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-373", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-6176", "mrqa_hotpotqa-validation-3952", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.484375, "CSR": 0.6341145833333333, "EFR": 1.0, "Overall": 0.8170572916666666}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British.", "wealth", "every good work designed to attract God's favor is a sin.", "Napoleon", "new technology and machinery", "Arley D. Cathey", "private actors.", "Bell Northern Research", "body of treaties and legislation,", "1227", "lower lake", "three", "Elders", "587,000", "A further type of committee", "Mark Ronson", "the Catechism", "Stagg Field.", "Ian Botham", "Pyotr Tchaikovsky", "Vincent Motorcycle Company", "Minnie Marx", "Salvador Allende", "Marie Antoinette", "Hawaii", "Erik Thorvaldson", "Apollon", "Pal Joey", "Mary Jane Grant", "green", "Indonesia", "supreme religious leader of the Israelites", "Antonio", "European Economic Community", "Christine Keeler", "Jesus", "Nicholson", "four", "Netherlands", "Sugar Baby Love", "Rosa Parks Bus", "Sean", "John Denver", "Stage 1", "Travis", "The Show", "Robert Kennedy", "Q", "umbrella", "Jean-Paul Sartre", "barber", "Evonne Goolagong Cawley", "Murrah Federal Office Building", "Evita", "tobacco", "fortified complex", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,", "a delegation of American Muslim and Christian leaders", "The marriage to Henry VIII lasted less than a year,", "USC Columbia", "Juan Martin Del Potro."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6151785714285714}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-5431", "mrqa_squad-validation-7974", "mrqa_squad-validation-9418", "mrqa_squad-validation-670", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-5929"], "SR": 0.53125, "CSR": 0.6262019230769231, "EFR": 0.9666666666666667, "Overall": 0.7964342948717948}, {"timecode": 13, "before_eval_results": {"predictions": ["Cram\u00e9r's conjecture", "Chilaun", "the Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "Medieval Latin, 9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "to counteract the constant flooding and strong sedimentation in the western Rhine Delta", "the Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees.", "Dublin, Cork, Youghal and Waterford", "Tangled", "julius", "moles", "leucippus", "fred", "Anne Boleyn", "Calvin", "Steve McQueen", "Portugal", "albert tatum", "three", "komando Pasukan Khusus", "the northwest of England", "carbon dioxide", "the state capital of Ohio", "Lucas McCain", "Antarctica", "mercury gilding", "achromatopsia", "stearns Eliot", "the River Forth", "woe", "the newspaper Daily Bugle", "Burmese", "Italy", "Canada", "typhoid fever", "fred Adams", "an action figure", "al Bundy", "2010", "einasto's law", "Venezuela", "Laurel and Hardy", "the ozone layer", "40", "phrenology", "San Francisco", "Fall 1998", "Marcus Atilius Regulus", "Christopher Weidman", "Drillers Stadium", "one", "Virgin America", "John Grisham", "National Secretaries Week", "Iran's parliament speaker", "fK Ventspils."], "metric_results": {"EM": 0.5, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8994", "mrqa_squad-validation-1002", "mrqa_squad-validation-9233", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.5, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.80859375}, {"timecode": 14, "before_eval_results": {"predictions": ["an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary civil disobedience", "Beijing", "three years", "27 July 2008", "chemically bonded to each other", "Aristotle", "St. George's Church", "Missy", "Strathclyde Regional Council debating chamber in Glasgow, and to the University of Aberdeen", "public official", "the most cost efficient bidder", "gaius caesar augustus germanicus", "dexter bennett", "gaius caesar augustus germanicus", "Olympia", "Ukrainian Soviet Republic", "gaius caesar augustus germanicus", "andrew johnson", "gaius caesar augustus bennett", "amber", "andrew johnson", "a pardon", "gaius caesar augustus germanicus", "bishkek Tajikistan", "anamosa", "andrew johnson", "Ephesus", "Camelot", "film", "knife", "fiery light", "Cologne", "andrew johnson", "gaius caesar augustus germanicus", "Kosovo", "andrew johnson", "Prague", "tennis", "prufon", "dexter bennett", "andrew johnson", "gaius virginianus", "Japan", "gaius caesar augustus bennett", "kung fu grip", "and girls", "accordion", "prufrock and other observations", "gaius caesar augustus germanicus", "Augusta", "in a counter clockwise direction around the Sun", "March 31, 2013", "prufrock and other observations", "gaius caesar augustus germanicus", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama's current \"middle way approach,\"", "gaius caesar augustus ginsberg", "Israel"], "metric_results": {"EM": 0.34375, "QA-F1": 0.39765625000000004}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.7000000000000001, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.25, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-3488", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-405", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-4647", "mrqa_triviaqa-validation-224", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3084"], "SR": 0.34375, "CSR": 0.5989583333333333, "EFR": 1.0, "Overall": 0.7994791666666666}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi", "river Deabolis", "April 20", "Gaulish name R\u0113nos", "1996", "wine", "German-Swiss", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "crossword", "Paula Abdul", "Ohio", "Flanders", "MasterCard", "Robert Stempel", "Nashville", "the olfactory nerve", "Ivan the Terrible", "Nancy Astor", "(solar lentigo)", "Kurt Russell", "Toronto Maple", "Zsa Zsa Gabor", "Vladimir Nemirovich-Danchenko", "Utah", "Rum", "(Rabbit) Angstrom", "Johann Strauss II", "joey", "pro bono", "Bologna", "a candy store", "a brown beer", "Anthony Fokker", "Nacho Libre", "copper", "black magic or of dealings with the devil", "hemlock", "Jeffrey Wigand", "National Poetry Month", "(Lettuce)", "meager", "Casablanca", "squadrons", "Gustav Kirchhoff", "a geisha", "a mermaid", "Altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "Noir", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6375000000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-9270", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-14330", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-12729", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.546875, "CSR": 0.595703125, "EFR": 1.0, "Overall": 0.7978515625}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite tribe", "respiration", "1997", "late 1920s", "\u00a34.2bn", "27 July 2008", "unequal", "October 1973", "military troops", "Isiah Bowman", "assembly center", "Ominde Commission", "Rhus", "Evita", "Ho Chi Minh", "circum", "the Inuit", "Detroit", "the (Montreal) Blue Jays", "Walt Whitman", "(Ray Bradbury)", "hate crimes", "King Julien", "Nicolas Sarkozy", "the Rubicon", "(Montreal)", "17", "(Louisa) May Alcott", "Play-Doh", "Aphrodite", "Jesus", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Clinton", "King Philip", "( Bellerophontes)", "Balaam", "the Wharton School", "The Caine Mutiny", "Rolling Stone", "F. W. Woolworth Company", "(John) Coltrane", "the peace sign", "oxygen", "the Sphinx", "Jan Hus", "the (Nashville Star)", "the Mavericks", "Onegin", "Macy's", "a spinning mule", "Santa Claus", "(Denzel) Washington", "a doctor", "courts", "a person has the usual two copies of chromosome 21, plus extra material from chromosome 21 attached to another chromosome", "Preston", "Australia", "The Jefferson Memorial", "between 11 or 13 and 18", "Michoacan Family", "( Brad) Blauser", "salary", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6347813644688645}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_squad-validation-3132", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-794", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-1759"], "SR": 0.5625, "CSR": 0.59375, "EFR": 0.9642857142857143, "Overall": 0.7790178571428572}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Tesla", "him", "Party of National Unity", "22", "the Dauphin", "Phillip Marlowe", "piracy", "Cliff Lee", "The Crystal Method", "Puerto Rico", "The Mausoleum", "Million Dollar Baby", "Switzerland", "Lufthansa", "The Old Man", "French", "Joe Louis", "the lion", "the Three Musketeers", "the Bayeux Tapestry", "Porch", "China", "Sunni", "notes", "Stephen Hawking", "Cicero", "Memphis", "Mountain Dew", "Blanche DuBois", "Quilt Crazy", "FRAM", "the House of Representatives", "Blue", "Michael Moore", "Oman", "Silverado", "Ingenue", "Pennsylvania", "Don Juan", "Ian Fleming", "Ichabod Crane", "London", "Yellowstone", "Ronald Reagan", "Fiddler", "Ethiopian", "six 50 minute ( one - hour with advertisements ) episodes", "1992", "a salt", "Bromley", "the Ruul", "John R. Dilworth", "Caylee Anthony", "a list of cars", "a Taliban member", "a nuclear weapon", "ten golf movies ever made"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6588541666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1659", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-11215", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-1920", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-14873", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8864", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-12814", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-3449", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-4110"], "SR": 0.59375, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.796875}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "1993", "June 1979", "Tesla's friend", "tentacles", "Robert R. Gilruth", "circuit complexity", "same-gender marriages", "the 2006 Israel-Lebanon conflict", "the mid-18th century", "orange", "A Raisin in the Sun", "Italy", "White Russia", "one", "a trowel", "Big Bang", "The Sex Pistols", "endodontist", "Saturn", "White Cliffs of Dover", "Genoa", "John Galt", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Utah", "Meg Ryan", "a rose", "Chow Main Street", "21", "the Civil", "Copella arnoldi", "Paul McCartney", "omega", "Raphael", "Bachman Turner Overdrive", "ParaNorman", "Caddy Shack", "Tokyo", "Panama", "Ellen", "Narnia", "Finnegans Wake", "Wordsworth", "Norway", "the Berenstain Bears", "a quake", "Judas", "the African elephant", "the Mazur", "Finland", "a clandestine love affair", "Our Country", "May 2010", "Texas", "Guanabara bay", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "the estate", "Bill Irwin", "ase", "Michigan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6210069444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.1111111111111111]}}, "before_error_ids": ["mrqa_squad-validation-1648", "mrqa_squad-validation-1696", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_naturalquestions-validation-2870"], "SR": 0.5, "CSR": 0.5888157894736843, "EFR": 1.0, "Overall": 0.7944078947368421}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "genetically modified crops", "Earth", "53,000", "one", "poet", "two", "20,000", "the kip", "skeletal muscle and the brain", "2014", "a single peptide bond or one amino acid with two peptide bonds", "the Wanderers", "the results show moved to Sunday evenings", "a zygote with n pairs of chromosomes", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "mindfulness", "Charlene Holt", "Captain Leland Stottlemeyer", "1991", "electron shells", "The Cornett family", "Acid rain", "April 15, 2018", "more of one good can be produced only by producing less of the other", "he cheated on Miley", "2001", "democracy", "735", "1871", "Rick Rude", "an assistant at Toledo, Bowling Green, and Mount Union", "a form of business network", "a cylinder of glass or plastic", "Abraham Gottlob Werner", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another", "Ancy Lostoma duodenale", "March 1", "CSR Racing 2", "the nature of Abraham Lincoln's war goals", "oxygen", "Cecil Lockhart", "Mara Jade", "British and French Canadian fur traders", "semi-autonomous", "Lou Rawls", "a man called Lysander", "Jupiter", "Mediterranean", "15", "John Robert Cocker", "Silvan Shalom", "a puzzle video game", "a palace", "the olfactory nerve", "a mottlecah", "a lion", "oxygen"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5656397855616606}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.2222222222222222, 1.0, 0.625, 0.0, 0.2, 0.8, 0.4, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-2379", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-2227"], "SR": 0.46875, "CSR": 0.5828125, "EFR": 0.9411764705882353, "Overall": 0.7619944852941176}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "the Cloth of St Gereon", "Thomas Sowell", "70", "death of a heretic", "choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape", "Florida", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer in five vignettes.", "over 1,000 pounds", "Egyptian State TV", "Mutassim", "Oklahoma", "Ralph Lauren", "\"The Jacksons: A Family Dynasty\"", "Amstetten", "computer problems left travelers across the United States waiting in airports", "Silvan Shalom", "Jonathan Breeze", "Steve Jobs", "12-hour", "prisoners", "June 2004", "consumer confidence", "5:20 p.m.", "North vs. South", "India", "1964", "Davidson", "Swat Valley", "Monday", "1979", "the United States", "The Tom Joyner Morning Show", "Akio Toyoda", "The National September 11 Memorial & Museum Foundation", "\"I have issued an order. I don't want these people interfered with in any way,\"", "Giovani dos Santos", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "children that a French charity attempted to take to France from Chad for adoption", "40", "Derek Mears", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter", "Whit Sunday", "Aberdeen", "\"Dumb and Dumber\"", "The 2003 LSU Tigers football team", "Earl Warren", "a converging lens", "autu", "season six", "The Force Fighters"], "metric_results": {"EM": 0.515625, "QA-F1": 0.595765128968254}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8750000000000001, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7266", "mrqa_hotpotqa-validation-1094", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.515625, "CSR": 0.5796130952380952, "EFR": 1.0, "Overall": 0.7898065476190477}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress among teachers.", "El Centro metropolitan area and San Diego-Carlsbad-San Marcos metropolitan area", "chief electrician", "Newton", "static friction, generated between the object and the table surface", "the assassination of US President John F. Kennedy the previous day;", "\"an affront to Somalia's territorial sovereignty.\"", "Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Awearness Fund", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "\"no more than an official of the most tyrannical dictatorial state in the world.\"", "\"Golden Girls\"", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Wednesday.", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad, Iran.", "Amanda Knox's aunt", "great jazz music and a very cheerful crowd.", "$530 million in debt", "\"Doogie Howser, M.D.\"", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two contestants.", "Bill", "J.G. Ballard", "nurse who tried to treat Jackson's insomnia with natural remedies", "Michelle Obama", "saving and planning for retirement long before his career neared its end.", "1981", "\"17 Again,\"", "Nigeria", "$81,8709", "Republican", "EU naval force", "Chris Robinson", "son of Gabon's former president", "steam-driven, paddlewheeled overnight passenger boat.", "Hyundai Steel", "skeletal dysplasia,", "London Heathrow's Terminal 5.", "\"very diverse\"", "February 12", "more than 30", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "military action because we're getting frustrated seems to me somewhat dangerous.", "White House Executive chef", "Russell Huxtable", "Willy Russell", "London", "\"Mortal Kombat\"", "Old English", "to give birth to children who sin.", "an iron fist for 25 years until he was overthrown and executed on Christmas Day in 1989.", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Ems Telegram"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49883680963644195}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.4, 0.13333333333333333, 0.0, 0.5714285714285715, 1.0, 0.0, 0.11764705882352941, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-10313", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-729", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-251", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-1056"], "SR": 0.40625, "CSR": 0.5717329545454546, "EFR": 1.0, "Overall": 0.7858664772727273}, {"timecode": 22, "before_eval_results": {"predictions": ["high energy single terminal vacuum tube", "WMO Executive Council and UNEP Governing Council", "Germans", "New York and Virginia", "two", "glowed even when turned off.", "a number of celebrities and ministers,", "water in a very dark and very cold place.", "sovereignty over them.", "April 6, 1994", "Prague", "backbreaking labor", "a federal judge in Mississippi on March 22,", "\"We have double work,\"", "$22 million", "severe flooding", "a music video on his land.", "walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "\"Watchmen\"", "The Real Housewives of Atlanta", "18", "88", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide", "change", "military trials", "Sachina Verma", "Larry King", "Steven Chu", "racially motivated.", "Michael Partain", "male veterans struggling with homelessness and addiction.", "longest domestic relay in Olympic history", "Zimbabwe.", "No. 1 slot", "nine", "ash and rubble", "Friday", "Kingdom City", "Rima Fakih", "Tuesday night", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Alwin Landry's supply vessel Damon Bankston", "scientists", "war crimes and crimes against humanity.", "opium", "warning about tendon problems.", "84-year-old", "Robert Park", "Rima Fakih", "the Isthmus of Corinth", "Nalini Negi", "2017", "Runcorn", "collarbone", "paris", "UFC 50: The War of '04", "June 11, 1973", "The Del Mar Fairgrounds", "Toy Story", "Emiliano Zapata", "A Fairy Tale of Home"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5285937255146814}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 1.0, 0.33333333333333337, 0.13333333333333333, 1.0, 0.5, 0.19999999999999998, 0.23529411764705882, 0.7272727272727273, 0.0, 1.0, 1.0, 0.5714285714285715, 0.15384615384615383, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.058823529411764705, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-2356", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4449", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.40625, "CSR": 0.5645380434782609, "EFR": 1.0, "Overall": 0.7822690217391304}, {"timecode": 23, "before_eval_results": {"predictions": ["phycoerytherin", "was lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition", "Behind the Sofa", "Tulsa, Oklahoma.", "56,", "in Yemen", "2005", "Karen Floyd", "Four Americans", "the missing person.", "Haiti", "Susan Boyle", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "Spain", "Jared Polis", "Janet and La Toya,", "Hyundai", "30", "Miriam Brown", "lightning strikes", "Evans", "Italian government", "the flooding was so fast that the thing flipped over,\"", "threatening messages", "stop Noriko Savoie from being able to travel to Japan for summer vacation.", "drafting a new constitution after three decades of Mubarak's rule.", "fake his own death", "Tim Masters,", "martial arts", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "then-Sen. Obama", "Congress", "curfew", "Anne Frank,", "June,", "the government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.", "Zuma", "haute, bandeau-style little numbers", "nine", "Iraq", "2000", "50", "15-year-old", "in body bags on the roadway near the bus,", "Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "Desmond Tutu", "$17,000", "Toy Story", "$81,880", "provide school districts with federal funds", "repudiation, change of mind, repentance, and atonement", "Jason Lee", "sleep", "n\u014dmen", "Kent", "beer and soft drinks", "five aerial victories.", "the Cherokee River", "Snowball", "Apollo 13", "Florida"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5771860206070731}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.962962962962963, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15789473684210525, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2616", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458"], "SR": 0.515625, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.78125}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "Vonn", "Frenchwoman", "him to step down as majority leader.", "United Nations World Food Program", "gang rape", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "The Louvre", "his club", "to best your own fuel economy achievements,\"", "1979", "Heshmat Tehran Attarzadeh", "jazz", "an antihistamine and an epinephrine auto-injector for emergencies,", "Bangladesh,", "Michael Arrington,", "one", "Ahmed,", "Sonia, a single mother with HIV in Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Britain's Got Talent", "military personnel", "placed behind the counter.", "11", "one Iraqi soldier,", "Michael Partain,", "her fianc\u00e9,", "racial intolerance.", "a vegan diet to some of the flavorful foods they can eat.", "Amado Carrillo Fuentes", "the self-styled revolutionary Symbionese Liberation Army", "$8.8 million", "to work together to stabilize Somalia and cooperate in security and military operations.", "would compromise the public broadcaster's appearance of impartiality.", "it -- you know -- black is beautiful,\"", "$104,168,000", "Picasso's muse and mistress, Marie-Therese Walter.", "to stop the Afghan opium trade", "nearly $162 billion in war funding", "off the coast of Dubai", "military veterans", "Springfield, Virginia,", "eight", "Mark Obama Ndesandjo", "\"Dance Your Ass Off.\"", "Russia", "\"Stagecoach\"", "adultery", "nucleus", "Vienna", "Sebastian Lund ( Rob Kerkovich )", "Jimmy Carter", "Tom Watson", "Sandi Toksvig", "The Spyker F1 Team", "3rd Earl of Limerick", "Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5547845511310585}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.4, 0.11764705882352941, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.05555555555555555, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.7499999999999999, 1.0, 0.14285714285714288, 0.0, 0.6666666666666666, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.4375, "CSR": 0.5575, "EFR": 1.0, "Overall": 0.77875}, {"timecode": 25, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-3952", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4548", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-4924", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-729", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-3637", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8638", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3254", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-49", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12150", "mrqa_searchqa-validation-12205", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-13756", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14282", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14700", "mrqa_searchqa-validation-14743", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-2355", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-4973", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6870", "mrqa_searchqa-validation-7227", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7584", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-8335", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-87", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10026", "mrqa_squad-validation-10026", "mrqa_squad-validation-10100", "mrqa_squad-validation-10254", "mrqa_squad-validation-10406", "mrqa_squad-validation-10418", "mrqa_squad-validation-1146", "mrqa_squad-validation-1166", "mrqa_squad-validation-1187", "mrqa_squad-validation-1218", "mrqa_squad-validation-126", "mrqa_squad-validation-1295", "mrqa_squad-validation-1313", "mrqa_squad-validation-1341", "mrqa_squad-validation-1407", "mrqa_squad-validation-1501", "mrqa_squad-validation-1549", "mrqa_squad-validation-159", "mrqa_squad-validation-1640", "mrqa_squad-validation-1662", "mrqa_squad-validation-1692", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1775", "mrqa_squad-validation-1877", "mrqa_squad-validation-1906", "mrqa_squad-validation-1960", "mrqa_squad-validation-2049", "mrqa_squad-validation-2059", "mrqa_squad-validation-2105", "mrqa_squad-validation-2113", "mrqa_squad-validation-2136", "mrqa_squad-validation-2207", "mrqa_squad-validation-2435", "mrqa_squad-validation-2466", "mrqa_squad-validation-2518", "mrqa_squad-validation-2530", "mrqa_squad-validation-281", "mrqa_squad-validation-2833", "mrqa_squad-validation-2858", "mrqa_squad-validation-2941", "mrqa_squad-validation-298", "mrqa_squad-validation-3091", "mrqa_squad-validation-3100", "mrqa_squad-validation-3127", "mrqa_squad-validation-3132", "mrqa_squad-validation-3149", "mrqa_squad-validation-3259", "mrqa_squad-validation-3260", "mrqa_squad-validation-3312", "mrqa_squad-validation-3319", "mrqa_squad-validation-3440", "mrqa_squad-validation-3454", "mrqa_squad-validation-3524", "mrqa_squad-validation-3632", "mrqa_squad-validation-3716", "mrqa_squad-validation-3813", "mrqa_squad-validation-3862", "mrqa_squad-validation-3865", "mrqa_squad-validation-3918", "mrqa_squad-validation-3943", "mrqa_squad-validation-4010", "mrqa_squad-validation-4047", "mrqa_squad-validation-4075", "mrqa_squad-validation-4078", "mrqa_squad-validation-4083", "mrqa_squad-validation-4102", "mrqa_squad-validation-4175", "mrqa_squad-validation-4315", "mrqa_squad-validation-4429", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-457", "mrqa_squad-validation-4673", "mrqa_squad-validation-4706", "mrqa_squad-validation-4770", "mrqa_squad-validation-4775", "mrqa_squad-validation-4844", "mrqa_squad-validation-4973", "mrqa_squad-validation-498", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5023", "mrqa_squad-validation-5037", "mrqa_squad-validation-5102", "mrqa_squad-validation-5135", "mrqa_squad-validation-5178", "mrqa_squad-validation-5194", "mrqa_squad-validation-5213", "mrqa_squad-validation-5226", "mrqa_squad-validation-526", "mrqa_squad-validation-5486", "mrqa_squad-validation-549", "mrqa_squad-validation-5513", "mrqa_squad-validation-5581", "mrqa_squad-validation-5741", "mrqa_squad-validation-5784", "mrqa_squad-validation-5812", "mrqa_squad-validation-5863", "mrqa_squad-validation-5871", "mrqa_squad-validation-5876", "mrqa_squad-validation-5972", "mrqa_squad-validation-6029", "mrqa_squad-validation-6059", "mrqa_squad-validation-6080", "mrqa_squad-validation-6121", "mrqa_squad-validation-6154", "mrqa_squad-validation-6166", "mrqa_squad-validation-6177", "mrqa_squad-validation-6242", "mrqa_squad-validation-6430", "mrqa_squad-validation-6588", "mrqa_squad-validation-6598", "mrqa_squad-validation-6614", "mrqa_squad-validation-6676", "mrqa_squad-validation-6685", "mrqa_squad-validation-6694", "mrqa_squad-validation-6721", "mrqa_squad-validation-6741", "mrqa_squad-validation-6789", "mrqa_squad-validation-6789", "mrqa_squad-validation-6801", "mrqa_squad-validation-6875", "mrqa_squad-validation-6921", "mrqa_squad-validation-7135", "mrqa_squad-validation-7159", "mrqa_squad-validation-716", "mrqa_squad-validation-7173", "mrqa_squad-validation-7229", "mrqa_squad-validation-7273", "mrqa_squad-validation-7434", "mrqa_squad-validation-7458", "mrqa_squad-validation-7576", "mrqa_squad-validation-7596", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-7967", "mrqa_squad-validation-7981", "mrqa_squad-validation-80", "mrqa_squad-validation-8035", "mrqa_squad-validation-8151", "mrqa_squad-validation-8176", "mrqa_squad-validation-8343", "mrqa_squad-validation-8356", "mrqa_squad-validation-8397", "mrqa_squad-validation-8420", "mrqa_squad-validation-8439", "mrqa_squad-validation-8485", "mrqa_squad-validation-8503", "mrqa_squad-validation-855", "mrqa_squad-validation-855", "mrqa_squad-validation-8608", "mrqa_squad-validation-8616", "mrqa_squad-validation-8719", "mrqa_squad-validation-8733", "mrqa_squad-validation-880", "mrqa_squad-validation-880", "mrqa_squad-validation-8833", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-890", "mrqa_squad-validation-8914", "mrqa_squad-validation-8924", "mrqa_squad-validation-9020", "mrqa_squad-validation-9066", "mrqa_squad-validation-913", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9220", "mrqa_squad-validation-9237", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9299", "mrqa_squad-validation-9333", "mrqa_squad-validation-940", "mrqa_squad-validation-9406", "mrqa_squad-validation-9436", "mrqa_squad-validation-9470", "mrqa_squad-validation-9559", "mrqa_squad-validation-962", "mrqa_squad-validation-9665", "mrqa_squad-validation-9686", "mrqa_squad-validation-9752", "mrqa_squad-validation-9753", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_squad-validation-9931", "mrqa_squad-validation-9960", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2794", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-339", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4945", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6753", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7367", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-980"], "OKR": 0.8359375, "KG": 0.45, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "concurring, smaller assessments of special problems", "Anthony Hopkins", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "beetle", "phylum", "Wayne Allwine", "Westminster Abbey", "holography", "Pelias", "Daniel Boaventura", "Northumbria", "Harvard", "cricket", "Seymour Hersh,", "quant", "copper and zinc", "Tigris", "Cordelia", "told both news and rumours.", "seborrheic dermatitis", "four", "a native French grape,", "Joseph Smith,", "Huntington Beach, California", "gold", "shadow", "a number between 10 and 20.", "a palla.", "The Apartment", "Canada", "Clement Attlee", "Stockholm", "Peter Parker", "kibbutznik", "Giorgio Armani,", "corrida de toros,", "The Number One Song in Heaven", "Ginger Rogers", "Plymouth Rock", "Steptoe and Son, Till Death Us do Part, The Liver Birds, All Gas and Gaitors, Are You Being Served?", "citric", "Dr Karen James,", "\"Take Me Home, Country Roads\"", "Mrs. Boddy", "Marie Van Brittan Brown", "southern California", "1995", "Bourbon County", "Taylor Swift", "\"Home\"", "Peterson had his personal.40-caliber Glock when police found him.", "The Detroit, Michigan,", "Amy Bishop Anderson,", "calathus", "the Louvre", "an American private, not-for-profit, coeducational research university"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4961309523809524}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-147", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5832", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6038", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-3796", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-7521", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.453125, "CSR": 0.5534855769230769, "EFR": 0.9714285714285714, "Overall": 0.7016234546703297}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Day of the Doctor\",", "third", "affordable housing", "Mao Zedong", "Verona", "The Silverdome", "elephants", "charcoal", "Frank McCourt", "jules Verne", "jonathan Cassab", "jonathan denver", "Schengen Area", "red", "city of Sheffield, England", "Famous Players-Lasky Corporation", "Gary", "Gerald Durrell", "jonathan", "County Cork", "jason", "Arabian", "Halifax", "Noises Off", "jonathan mccartney", "Frank Wilson", "Carlos the jackal", "Edwina Currie", "Tara Lipinski", "Robert Maxwell", "1768", "\u201cFor Gallantry;\u201d", "son", "Georgia", "Tuscaloosa", "The Good Life", "Tahrir Square", "plutonium", "d'Artagnan", "27", "jack Ruby", "tintoretto", "Eric Coates", "Saudi Arabia", "Lester", "Thailand", "Sydney", "dove", "Tunisia", "Prince Philip", "grosvenor cotes Wyatt", "Tokyo", "Edgar Lungu", "49 cents", "over 100 beats per minute", "672", "\"Linda McCartney's Life in Photography\", \"Some Like It Hot\", \"Kubrick's Napoleon: The Greatest Movie Never Made\",", "Franconia, New Hampshire", "@", "Juan Martin Del Potro", "27", "Edgar Allan Poe", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5411458333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7774", "mrqa_squad-validation-8026", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-2333", "mrqa_searchqa-validation-12829"], "SR": 0.515625, "CSR": 0.5520833333333333, "EFR": 1.0, "Overall": 0.7070572916666666}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "more than 70", "forced Tesla out leaving him penniless.", "Benazir Bhutto", "nuclear", "illis Andrade", "(l-r)", "FBI Special Agent Daniel Cain,", "acid attack by a spurned suitor.", "Wally", "2008", "after Wood went missing off Catalina Island,", "Fakih", "Afghanistan", "Everglades", "made 109 as Sri Lanka, seeking a win to level the series at 1-1,", "1950s", "64,", "Iran's parliament speaker", "27-year-old", "Alexandros Grigoropoulos,", "$4.5 million", "as the new kid on the block in the modern art scene", "global greenhouse emissions.", "oaxaca", "Orbiting Carbon Observatory", "Switzerland", "Kenneth Cole", "Janet and La Toya", "Nine out of 10 children", "about 5:20 p.m.", "combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "his injuries", "al-Shabaab", "booked on an outstanding arrest warrant relating to a domestic violence case,", "sustain future exploration of the moon and beyond.", "his business dealings for possible securities violations", "Opry Mills", "Number Ones", "maritime", "he was diagnosed with skin cancer.", "al Qaeda", "barbara gaffney,", "should have met with the Dalai Lama.", "oceans", "grab him", "doctors", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "December 23, 1942", "between 1923 and 1925", "Gilda", "jeremy Tate", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire State Building", "Benjamin Disraeli", "a rising sun"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6353939463314464}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.22222222222222224, 1.0, 0.18181818181818182, 1.0, 0.6, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-14496", "mrqa_searchqa-validation-15354"], "SR": 0.484375, "CSR": 0.5496651785714286, "EFR": 1.0, "Overall": 0.7065736607142858}, {"timecode": 28, "before_eval_results": {"predictions": ["a hybrid Bermuda 419 turf.", "25-foot", "symbols", "Hyundai", "Monday night", "Florida to Colorado", "kidnapping the children and concealing their identities.", "40", "the Illuminati", "in a public housing project,", "toxic smoke from burn pits", "Lucky Dube,", "two Israeli soldiers,", "space shuttle Discovery", "World-renowned security expert Gavin de Becker", "nuclear weapon", "in Japan", "Arizona", "in the Intertropical Convergence Zone", "Tetris", "outside influences", "aid to Gaza,", "killed two people and injuring more than a dozen,", "suppress the memories and to live as normal a life as possible;", "Tuesday in Los Angeles.", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the helicopter went down in Talbiya,", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "Cash for Clunkers program", "broadband television network.", "talk show queen", "80 percent of the woman's face", "London", "to try to make life a little easier for these families by organizing the distribution of wheelchair,", "Ozzy Osbourne", "$50", "Australian officials", "the iconic Hollywood headquarters of Capitol Records,", "Dr. Jennifer Arnold and husband Bill Klein,", "gun", "38", "Argentine", "the underprivileged.", "Somalia's piracy problem was fueled by environmental and political events.", "\"17 Again\"", "Kabul", "22", "Steven Gerrard", "12.3 million", "a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.M.-backed Iraqi forces and gunmen.", "Rima Fakih", "Old Trafford", "to help bring creative projects to life ''", "season two", "Mary Elizabeth Patterson", "Tonight Ensemble", "The Fifth Amendment", "Nepal", "Merck & Co.", "Fort Albany", "Knoxville, Tennessee", "Jawaharlal Nehru", "stomatal pore", "hypomanic"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6157638322934376}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.3333333333333333, 0.42857142857142855, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.31578947368421056, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1178", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-79", "mrqa_searchqa-validation-10091", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.46875, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.706015625}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "priests and virgins", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Russian army", "near the end of their main sequence lifetime", "August 6", "Doug Diemoz", "Virginia", "Monk's Caf\u00e9", "in the central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "maintenance utility", "July 4", "`` kind of a'pick yourself up and dust yourself off and keep going '", "John Ridgely", "enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "October 12, 1979", "Lorazepam", "Iraq in 2007", "salamislice", "Ethel `` Edy '' Proctor", "a ranking used in combat sports,", "Husrev Pasha", "D.J.", "a nerve that runs near the ulna bone", "McFerrin, Robin Williams, and Bill Irwin", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "`` Rightly Guided Caliphs", "in Lake Powell", "topper or treetopper", "September 6, 2019", "two senators", "substitute good", "Archie Marries Veronica", "over 74", "1987", "cunnilingus", "October 2000", "New York City", "Prafulla Chandra Ghosh", "a declining economy", "in sequence with each heartbeat", "Hermann Ebbinghaus", "The Miracles", "used obscure languages as a means of secret communication during wartime", "Donny Osmond", "Rome and Carthage", "George W. Bush", "gmbH", "7.63\u00d725mm Mauser (.30 Mauser Automatic)", "seven", "Muslim", "two remaining crew members from the helicopter,", "Saturday's", "Rickey Henderson", "Lake Baikal", "\"Harold & Kumar Go to White Castle\""], "metric_results": {"EM": 0.359375, "QA-F1": 0.5382711312809997}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 1.0, 0.1111111111111111, 0.5, 0.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 0.19999999999999998, 0.8, 0.7333333333333334, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.3846153846153846, 1.0, 0.0, 0.25, 0.5, 0.5, 0.5, 1.0, 0.4, 0.8, 0.10526315789473684, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.8, 0.18181818181818182, 1.0, 0.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-3025", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.359375, "CSR": 0.540625, "EFR": 0.9512195121951219, "Overall": 0.6950095274390244}, {"timecode": 30, "before_eval_results": {"predictions": ["a setup phase in each involved node before any packet is transferred to establish the parameters of communication", "cydippid Pleurobrachia", "1953", "AT&T", "North Carolina", "Hutter and Hurry", "shoes", "nine", "Rashid Akmaev,", "acetylene", "an illegal substance", "fiber", "The gray fox's", "What's in a name?", "Winston Rodney", "sand", "Nanjing", "Montana", "John Cleese", "Roi-Soleil", "GILBERT & SullIVAN", "Fox Network", "Gaius Julius Caesar", "Joe Lieberman", "the Boston Marathon", "fibreboard", "tin", "Song of Norway", "Frida Kahlo", "Abigail Adams", "\"Y\" 2 \"K\": An Eskimo", "\"Fat man, you shoot a great game of pool.\"", "Hair", "William Randolph Hearst", "a pumice", "ale", "Homo sapiens", "a telephone operator", "When You Look Me In The Eyes", "Casey Jones", "The New Colossus", "yelping", "Richard Wagner", "Sarah, Duchess of York", "walker", "\"Tom Terrific\"", "bronchodilators", "Four", "Argon Glow Lamps", "Minnesota", "a Chenard", "Earl Long", "Neil Patrick Harris", "Wyatt", "1999", "vitamin D", "three", "Alberto Juantorena", "Boyz II Men", "Awake", "Doctor of Philosophy", "Pakistan", "Atlanta.", "Elena Kagan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.516968201754386}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true], "QA-F1": [0.052631578947368425, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-4455", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-14012", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-282", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297"], "SR": 0.46875, "CSR": 0.5383064516129032, "EFR": 1.0, "Overall": 0.7043019153225807}, {"timecode": 31, "before_eval_results": {"predictions": ["traditional Mongol shamans", "Prospect Park", "the eye", "a volume", "a squint", "Breakfast at Tiffany's", "Diners' Club Card", "Christian Dior", "August Wilson", "Juliet", "Notre Dame", "the Tablecloth", "Tate", "Bligh", "Cecil Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "Pennsylvania", "Mike Huckabee", "Queen", "a monosodium glutamate", "a Chance", "mitsumata", "Edmund Hillary", "Samuel Beckett", "Rachel Carson", "Vietnam", "sports", "Steven Spielberg", "Franklin D. Roosevelt", "Kate", "Ugly Betty", "an R", "Zechariah", "New Jersey", "Lake Ontario", "Matthew Perry", "Marissa Jaret Winokur", "Grant", "a kismet", "the Chocolate Factory", "a battery", "aluminum", "Abraham Lincoln", "Ned Kelly", "a piles of papers", "a gravitational force", "Isis", "a quiver", "Heroes", "on the two tablets", "the source of the donor organ", "seven", "Dr. A.G. Ekstrand", "Rocky Marciano", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "February 20, 1978", "two years,", "Arsene Wenger", "in an interview on CNN's \"Larry King Live.\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.6456597222222222}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224]}}, "before_error_ids": ["mrqa_squad-validation-8204", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-10344", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_triviaqa-validation-2878", "mrqa_newsqa-validation-2123"], "SR": 0.5625, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.704453125}, {"timecode": 32, "before_eval_results": {"predictions": ["the same as the weight of the air that rushed back in", "Fresno Street and Thorne Ave", "the Black Death", "Elton John", "John Stuart Mill", "Herman Melville", "CIA", "pianissimo", "Rickey Henderson", "Jawaharlal Bhutto", "Daucus carotenoids", "John Grunsfeld", "Buhddist", "Canada", "Galileo Descartes", "a quark", "Dust jacket", "Rudy Giuliani", "the First Amendment", "a scallop", "Thor", "New Jersey", "The Omega Man", "a pantry", "a cask, or tun", "the Summer Olympics", "Hugo Chvez", "Who Will Save Your Soul", "Hinduism", "tin", "Jermaine", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Venus Davenport", "Los Angeles", "the east wind", "Richard III", "the Labour Party", "The pen", "Mexico", "Douglas Adams", "Krogstad", "Hawaii", "Hilda", "Prussia", "Sophocles", "Mark Cuban", "Thought Police", "a chest", "Central Park", "Lewis Carroll", "Part 1", "Dana Matherson", "aeoline", "trumpet", "Mel Gibson", "2.1 million", "Edward James Olmos", "Lynyrd Skynyrd", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.53125, "QA-F1": 0.592876552795031}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.08695652173913042, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-5449", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-1755", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-14835", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-11157", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-4767"], "SR": 0.53125, "CSR": 0.5388257575757576, "EFR": 1.0, "Overall": 0.7044057765151516}, {"timecode": 33, "before_eval_results": {"predictions": ["the BBC", "\"self\" constituents (constituents of the body) don't trigger destructive immune responses,", "the man facing up, with his arms out to the side.", "hours", "28", "back at work", "Oxbow,", "Emerson Police Department's tip line", "opium", "\"B Bishop is \"aware of what she's done and she's very sorry for it.\"", "Saturday", "Hussein's Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar", "The station was getting continuing inquiries, and Brett thought it would be best if he resigned,\"", "a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "forgery and flying without a valid license,", "Arkansas", "Cash for Clunkers", "environmental", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers", "different women coping with breast cancer in", "the North Korean regime intends to fire a missile toward Hawaii on July 4.", "police", "a cancer-causing toxic chemical.", "Roger Federer", "Brooklyn, New York,", "over 1000 square meters in forward deck space,", "CNN", "no chance", "St. Louis, Missouri.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "two years ago", "two", "a bald Bard with a small beard and beard, and bags under his eyes.", "Symbionese Liberation Army", "\"You did not start the violence, it was started by the violent elbowing of Marcus McGee in the face by one of your friends, John Doran,\"", "two tickets to Italy on Expedia.", "Colombia", "a welcoming, bright blue-purple", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "South Africa", "Appathurai", "\"I want to be a Muslim just like you,\"", "Kgalema Motlanthe", "the Ming dynasty", "George II ( George Augustus )", "2014 -- 15", "1919", "Javier Bardem", "Scotland", "a family of Portuguese descent", "American Wrestler", "Araminta Ross", "Mrs. Potts", "Peanuts Chocolate Candies", "the national anthem"], "metric_results": {"EM": 0.53125, "QA-F1": 0.611328125}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6585", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1379", "mrqa_naturalquestions-validation-7108", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-1903", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.53125, "CSR": 0.5386029411764706, "EFR": 1.0, "Overall": 0.7043612132352941}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "weren't taking it well.", "Washington State's decommissioned Hanford nuclear site,", "Yemen,", "bankruptcy", "nearly $2 billion", "is a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "Kenya", "children of street cleaners and firefighters.", "\"Apprentice,\"", "$3 billion,", "hardship for terminally ill patients and their caregivers,", "Honduran", "Brazil", "three different videos that we like and want to know which ones you think are the best.", "strife in Somalia,", "Roy", "the WBO welterweight title", "relatives of the five suspects,", "Meredith Kercher.", "trying to save their client from the death penalty", "Demi Moore and Alicia Keys", "military action in self-defense against its largely lawless neighbor.", "Friday,", "a lump in Henry's nether regions was a cancerous tumors.", "20", "Matthew Fisher", "$1.5 million.", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "model of sustainability.", "glamour and hedonism", "J. Crew.", "returning combat veterans", "543", "the patient,", "Robert Gates", "Israel", "on 112 acres about 30 miles southwest of Nashville,", "confirmed that Coleman, 42, was being treated there after being admitted on Wednesday.", "Seoul,", "Nicole", "\"It feels good for me to talk about her,\"", "next week.", "Adam Lambert", "regulators in the agency's Colorado office", "early detection and helping other women cope with the disease.", "James Whitehouse,", "hopes the journalists and the flight crew will be freed,", "Confucianists", "Lionel Hardcastle", "Stephen Lang", "Dick Van Dyke", "Noregi,", "a spirit", "Revengers Tragedy", "1968", "Black Elk", "The Hogan Family", "the hippopotamus", "St Paul"], "metric_results": {"EM": 0.5, "QA-F1": 0.5793257824960127}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.19999999999999998, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.08, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-2761", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-3472", "mrqa_searchqa-validation-7879"], "SR": 0.5, "CSR": 0.5375, "EFR": 1.0, "Overall": 0.7041406250000001}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts which they returned to Earth.", "Border Reiver", "July 4,", "wine", "Nantucket", "Islamic leadership position", "a maple tree", "Malibu", "Sisyphus", "sound", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "a crab", "Prospero", "Purple", "the Black Sea", "the Battle of the Little Bighorn", "The Shakers", "a bellwether", "The Information Philosopher", "potato chips", "Boxer", "The Spiderwick Chronicles", "Mabel Harding", "Las Vegas", "Case", "the Rose Bowl", "Norman Rockwell", "Jackie Kennedy", "light tunais", "Napa", "Italy", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "The Bodyguard", "12 men", "Nancy Pelosi", "a journal", "Jupiter", "Anwar Sadat", "a sundae", "Grace Evans", "50 million", "Volitan Lionfish", "Charlie Sheen", "The Brothers Karamazov", "Bonnie Aarons", "Wednesday, 5 September 1666", "pop ballad", "Ra ther a god of the desert", "Lou Gehrig", "meaning and origin", "1949", "Aamir Khan", "My Gorgeous Life", "London and Buenos Aires", "High Court Judge Justice Davis", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6331188725490197}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-2343", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884"], "SR": 0.578125, "CSR": 0.5386284722222222, "EFR": 1.0, "Overall": 0.7043663194444445}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "Virginia", "Ventured", "bullion", "Supernanny", "the Atlantic", "Cincinnati", "the mosque", "(Henry) Hudson", "the Peashooter", "dry ice", "Theodore Roosevelt", "Entourage", "eel", "Philadelphia", "the Museum of Modern Art", "the unicorns", "John C. Frmont", "Russia", "(BRA) STREISAND", "Hermann Hesse", "the Taj Mahal", "Mnemonic Devices", "Carmen", "Margaret Mitchell", "Claude Frollo", "Sultans of Swing", "Troilus", "cheerful", "Burt Reynolds", "the Sphinx", "Louis Armstrong", "Mecca", "American new wave", "Arby\\'s", "coffee", "(Honneur) Bilodeau", "Robert Burns", "Bruce Banner", "Winnipeg", "the Memphis Belle", "Burkina Faso", "the Central Pacific", "Attorney General", "Icelandic", "a wolf", "(Is) SportsCenter", "Piaf", "Ivan I", "a prologue", "birch", "Anthony Mayfield", "Jack Gleeson", "(Is) Hurtt", "animals", "Massachusetts", "City of Starachowice", "Charles Laughton", "2009", "Democratic National Convention", "meteorologist", "$104,327,006", "\"Hairspray,\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.65625}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6752", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-13205", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-5571", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_hotpotqa-validation-2000", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-1525"], "SR": 0.59375, "CSR": 0.5401182432432432, "EFR": 1.0, "Overall": 0.7046642736486486}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "John Y. Brown", "oats", "Mitt Romney", "Ivan", "Sally Field", "(Charlie) Lindbergh", "Eritrea", "pi", "tin", "the Mississippi River", "a tuxedo", "w", "Marriott", "the Principality of Monaco", "Canada", "the Secret", "gold", "collagen", "China", "a compound", "the cranes", "a claw", "Alzheimer", "the Gulf of Mexico", "Austin", "Euclid", "Eva Peron", "Cain", "Ed Asner", "X-Men", "the Louvre", "achinook", "Michael Scofield", "Mercury", "Maine", "a sheep's milk cheese", "Meg", "Rainer Maria", "Gage", "Hans", "Peter Bogdanovich", "Billy Joel", "Jesus Christ", "boat propulsion", "the Quaternary Period", "no contest", "Jr. Walker", "Czech Republic", "seewhat", "the NIRA", "John Ernest Crawford", "beta decay", "France", "Priam", "Mariette", "Charles Quinton Murphy", "\"Bubbly\"", "Australian", "the sins of the members of the church,", "$22 million", "\"17 Again,\"", "Nelson County,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6177083333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-16789", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-10268", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-9713", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-8387", "mrqa_searchqa-validation-12168", "mrqa_searchqa-validation-8068", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900"], "SR": 0.5625, "CSR": 0.5407072368421053, "EFR": 1.0, "Overall": 0.704782072368421}, {"timecode": 38, "before_eval_results": {"predictions": ["tuition fees", "Holden Caulfield", "Bill Hickok", "Yellow", "a recession", "a mermaid", "Jay Silverheels", "Singapore", "a tank", "a mallet", "a canoe", "Spiderman", "Witness", "Jack the Ripper", "3800", "Shirley Schmidt", "phylum", "Spain", "the brain", "Lord Jim", "Macbeth", "comedy", "Mary Poppins", "catfish", "The Fresh Prince of Bel-Air", "Nod", "watermelon", "a bathwater", "marriage", "A Prayer", "Sherlock Holmes", "a lollipop", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "deagonals", "German", "Katamari Damacy", "Mark Twain", "Margaret Thatcher", "Harry Potter and the Philosopher Stone", "rhodonite", "forests", "Olympia", "Ritchie Valens", "Doctor Zhivago", "Brazil", "British", "Oliver Stone", "a scrapple", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "the different levels of importance of human psychological and physical needs", "one", "Tasmania", "the Wright brothers", "sexual activity", "Canada Goose", "Sandro Bondi refused to attend", "voluntary manslaughter", "\"deep sorrow\"", "Pygmalion"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6307291666666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.1, 1.0, 0.16666666666666669, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-3282", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-6803", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-4043", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-2005", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-600"], "SR": 0.5625, "CSR": 0.5412660256410257, "EFR": 1.0, "Overall": 0.7048938301282052}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "\"Boogie Woogie Bugle Boy\"", "Europe", "Jack Nicholson", "Glory", "Sweeney Todd", "The Bridge on the River Kwai", "the Fall of Constantinople", "Havisham", "Jefferson", "Ford", "the river", "a toothpick", "California", "Dixie", "the National Institutes of Health", "Warren Harding", "a engrave", "the Shue", "Francis Crick", "Jay and Silent Bob", "Heath", "South Ossetia", "Twelfth Night", "Hawaii", "a wrench", "Josip Broz Tito", "wolves", "Ratatouille", "circadian rhythms", "Calvin Coolidge", "Mark Cuban", "Rudolph Giuliani", "eyes", "Tony Dungy", "the Danube", "Andrew Johnson", "26.2", "life", "a herb", "the endgame", "Garbage in", "Johannes Brahms", "Charleston", "Italian", "The Grapes of Wrath", "a bicentennial", "Constantinople", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Anna Murphy", "March 15, 1945", "Charles Darwin", "Old Trafford", "Miles Morales", "Honey Irani", "global peace", "the Kalahari Desert", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "Bob Dole", "Ben Kingsley", "managing his time"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5706845238095237}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-8155", "mrqa_searchqa-validation-9240", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-4452", "mrqa_searchqa-validation-7544", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-4073"], "SR": 0.484375, "CSR": 0.53984375, "EFR": 1.0, "Overall": 0.7046093750000001}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "a 2005 South Korean horror film,", "Oakdale,", "Missouri", "the FAI Junior Cup", "Flaw", "alt-right", "The Drudge Report", "15,000", "Flavivirus", "a cappella singing group", "1934", "a record of 13\u20133,", "The Herald Angels Sing", "Tsavo East National Park", "the New York Islanders", "1345 to 1377", "nearly 80 years", "Jean Acker", "the Premier League", "The Gettysburg Address", "Whitney Houston", "Kylian Mbapp\u00e9", "The Rite of Spring", "1", "26,000", "Kristin Scott Thomas", "Edwin Mah Lee", "1958", "1993", "American burlesque", "Afro-Russian", "Loretta Lynn", "Lancashire, England", "a Boeing B-17 Flying Fortress", "1994\u201395", "11", "the XXIV Summer Universiade", "2012", "1994", "Leawood, Kansas", "1999", "Pinellas County", "beer", "London", "the B-17 Flying Fortress bomber", "Mindy Kaling", "1988", "Leon Uris", "Erika Mitchell Leonard", "Mason Alan Dinehart", "Golde", "Sir Tom Finney", "Cameroon", "collecting samples of blood and other fluids", "by military personnel to hazardous materials", "two", "Iggy Pop invented punk rock.", "a riddle", "the Mayor of Casterbridge", "Leonardo DiCaprio", "a destructive ex-lover"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7043242296918768}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1749", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_naturalquestions-validation-6326"], "SR": 0.59375, "CSR": 0.5411585365853658, "EFR": 1.0, "Overall": 0.7048723323170731}, {"timecode": 41, "before_eval_results": {"predictions": ["a fumble", "10", "did not identify any of the dead.", "Les Bleus", "2005", "more than 4,000", "Specter", "an angry mob.", "normal maritime", "Sri Lanka", "death", "an average of 25 percent", "fatally shooting a limo driver", "The Al Nisr Al Saudi", "as", "piano", "$250,000", "\"prostitute\"", "the mammoth\\'s fossil", "tax", "Los Ticos", "in Iraq", "Russia and China", "Facebook and Google,", "through a facility in Salt Lake City, Utah,", "Manmohan Singh\\'s Congress party", "Haiti", "in his sleep at his Windsor, Ontario, home,", "Pakistan", "23 years.", "head injury", "Tim Cahill", "an open window that fits neatly around him", "Leo Frank", "Paul McCartney", "it has witnessed only normal maritime traffic around Haiti,", "President Robert Mugabe", "don't have to visit laundromats", "one", "Diversity", "on-loan David Beckham claimed his first goal in Italian football.", "his son is fighting an unjust war for an America that went too far when it invaded Iraq", "\"Twilight\"", "forgery and flying without a valid license", "11", "A third beluga whale belonging to the world\\'s largest aquarium has died,", "Fayetteville, North Carolina,", "The crash destroyed four homes and killed two people who lived in at least one of the homes", "the Taliban", "Secretary of State Hillary Clinton", "Rihanna", "angular rotation", "the right side of the heart", "54 Mbit / s", "in the County of Gloucestershire", "the B-24 Liberator", "cereal", "Oakdale,", "Melbourne, Victoria", "Guillermo del Toro", "stocks", "Monty Python and the Holy Grail", "King of Sweden", "FMCSA"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6557522772366522}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727273, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5714285714285715, 0.19999999999999998, 1.0, 0.625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1659", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376", "mrqa_hotpotqa-validation-403", "mrqa_searchqa-validation-1519", "mrqa_searchqa-validation-10945"], "SR": 0.53125, "CSR": 0.5409226190476191, "EFR": 1.0, "Overall": 0.7048251488095238}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Arizona", "Zimbabwe", "Italian Serie A", "Darrel Mohler", "showing her dancing against a stripper's pole.", "the \" Michoacan Family,\"", "WTA Tour titles", "Mugabe's", "42", "takes on the swords of the Taliban.", "either stay home (which might be less depressing and won't add more airline emissions) or get a move on it and see the hot spots", "80 percent", "1979", "\"Follow the Sun,\"", "Sonia Sotomayor", "CBS, CNN, Fox and The Associated Press.", "an auxiliary lock", "1-1", "AbdulMutallab", "Myanmar", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides,", "him,", "Filipino-American", "poems", "the program was made with the parents' full consent.", "Barack Obama", "The International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "Moscow", "debris", "\"Can I just say how pleased I am with today's", "capital murder and three counts of attempted murder", "Basel", "17", "a Daytime Emmy Lifetime Achievement Award.", "state senators", "31 meters (102 feet)", "its nude beaches.", "\"Five\" tells stories of different women coping with breast cancer", "(Misty) Cummings", "shark River Park in Monmouth County", "three", "Islamabad", "partying", "Capitol Hill.", "its nuclear ambitions are for peaceful means,", "1940's", "March 22,", "three", "at a depth of about 1,300 meters in the Mediterranean Sea.", "\"Antichrist\"", "a major fall in stock prices", "Thomas Jefferson", "Alexander Salkind", "Orion", "brown", "Selfie", "23 March 1991", "England", "Los Alamos National Laboratory", "the Rat", "rain", "Crawford", "the Pyrenees"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6095949317226891}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.11764705882352941, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.5000000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.4, 0.0, 0.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-932", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5989", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.484375, "CSR": 0.5396075581395349, "EFR": 1.0, "Overall": 0.704562136627907}, {"timecode": 43, "before_eval_results": {"predictions": ["the north,", "the legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees", "33-year-old", "that the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "taking the product off the market would result in hardship for terminally ill patients and their caregivers,", "Jaime Andrade", "Zac Efron", "finance", "$2 billion", "$42 billion", "1941", "\"The station", "Karthik Rajaram", "Arkansas weatherman", "Robert Mugabe", "the wife of Gov. Mark Sanford,", "Camp Lejeune, North Carolina", "Saturday.", "$1.5 million", "government efforts at control and censorship remain rife across the Middle East and North Africa,", "could be secretly working on a nuclear weapon", "the fact that the teens were charged as adults.", "death squad killings", "Elena Kagan", "Dangjin", "100 percent", "Saturday", "Pakistan's intelligence agency", "prisoners at the South Dakota State Penitentiary", "seven", "200", "Pakistan", "Seminole Indian", "Rima Fakih", "in a Johannesburg church that has become a de facto transit camp,", "Barack Obama", "helicopters and unmanned aerial vehicles", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah", "1,500", "A receptionist", "$50 less,", "$60 billion on America's infrastructure.", "ALS6", "Malayalam", "Harry", "1960", "Villa Park", "landless farmers", "pool", "1822", "The Dressmaker", "Trilochanapala", "frosted crouton", "a buffalo", "ruby red slippers", "the frontal lobe"], "metric_results": {"EM": 0.5, "QA-F1": 0.6143947524662039}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6956521739130436, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.35294117647058826, 0.08695652173913043, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1063", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_naturalquestions-validation-922", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.5, "CSR": 0.5387073863636364, "EFR": 1.0, "Overall": 0.7043821022727272}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf.", "Los Angeles", "Chris Eubank Jr.", "Florida", "Benj Pasek and Justin Paul", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Araminta Ross", "Roger Thomas Staubach", "1944", "Highlands", "Robert Frost's former home in Franconia, New Hampshire,", "Operation Watchtower", "Dan Crow", "War & Peace", "Amberley", "\"What Are Little Boys Made Of?\"", "Berea College", "the Chicago Bears", "Timoth\u00e9e Chalamet", "Charmian Carr", "Germanic", "New York Islanders", "Amy Smart", "26,788", "the Troubles", "1967", "Marktown", "jus sanguinis", "Radcliffe College", "James A. Garfield", "Ford", "the citizen's heart was heavier than a feather", "India", "German", "Charmed", "25 million", "The Snowman", "Ella Jane Fitzgerald", "X-Men: God Loves, Man Kills", "Rain Man", "Interscope Records", "Robert Grosvenor", "4,000", "Henry Luce", "I'm Shipping Up to Boston", "American", "British's Got Talent", "the dynasty", "Australia's capital is Canberra, and its largest urban area is Sydney", "the beginning of the American colonies", "Nicola Adams", "\"bay of geese,\"", "Russia", "shows the world that you love the environment and hate using fuel,\"", "Steven Green", "Fayetteville, North Carolina,", "Chaucer", "rattlesnakes", "suspicion", "healthy, wealthy, and wise"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5608664772727273}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-4454", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-2355", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-12418", "mrqa_searchqa-validation-13986"], "SR": 0.46875, "CSR": 0.5371527777777778, "EFR": 1.0, "Overall": 0.7040711805555556}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "Indian Ocean waters", "three", "crocodile eggs", "Colorado prosecutor", "Polis", "on Saturday.", "Haiti", "in July for A Country Christmas,", "sniff out cell phones.", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas", "Cain", "\"17 Again,\"", "North Korea intends to launch a long-range missile in the near future,", "Everton", "Mitt Romney", "two years ago.", "the Brundell family", "Picasso's muse and mistress, Marie-Therese Walter.", "low-calorie meals", "Heshmatollah Attarzadeh", "the i report form", "government", "Nine out of 10 children", "police", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "a crocodile", "a bronze medal", "wounded more than 200.", "Congress", "Susan Boyle", "ways to speed up screening of service members and, to the extent possible, their families,", "Phillip A. Myers.", "Obama's", "King Birendra,", "she does not expect enough additional evidence to surface for that finding to be revised.", "Casey Anthony, 22,", "\"TSA supports the thoroughness of the officers involved as they were acting to protect the", "Arnoldo Rueda Medina.", "UNICEF", "the pregnancy.", "228", "Kerstin and two of her brothers,", "2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Joan Rivers", "I was born in Nizhny Novgorod", "Jacob Zuma,", "in the Oaxacan countryside of southern Mexico", "Wenger", "slavery", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan ), and grandmother Mo ( Laila Morse )", "the enkuklios paideia", "women", "Johnny Mathis", "The son of a Brooklyn policeman who died when he was eight,", "Champion Jockey", "Luca Guadagnino", "Sleepy Brown", "Maya Angelou", "the brain", "a jigger", "unarmed"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6368364487607908}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5263157894736842, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.515625, "CSR": 0.5366847826086957, "EFR": 1.0, "Overall": 0.7039775815217392}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "\"race for the future... and it won't be won with a president who is stuck in the past,\"", "Nirvana", "\" viewer can vote online, via phone calls or by text messaging,", "without bail", "12.3 million", "Mexico", "United", "Michael Arrington,", "Brett Cummins,", "Indian army", "Saturday", "Nicole", "the legitimacy of that race.", "Adidas", "Dennis Davern,", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "promise to improve health and beauty.", "Chinese", "Newcastle", "\"Nothing But Love\"", "allegedly involved in forged credit cards and identity theft", "June 6, 1944,", "the Middle East and North Africa,", "1-0 draw", "October 19,", "\"It was a wrong thing to say,", "Seoul,", "promotes fuel economy and safety", "amyotrophic Lateral Sclerosis", "eight", "virtual personal assistants", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback Forestry", "the children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "they did not know how many people were onboard.", "attempting illegal crossings", "American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place,", "38", "Her husband and attorney, James Whitehouse,", "blacks, Hispanics and whites", "three", "the most gigantic pumpkins in the world,", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel", "the world of Discworld", "Japan", "fox hunting", "New York", "travel", "16,116", "\"Juno\"", "sap", "a bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6904195122406329}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.8, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.1111111111111111, 0.0, 1.0, 0.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_triviaqa-validation-1729", "mrqa_hotpotqa-validation-2280", "mrqa_searchqa-validation-11573"], "SR": 0.609375, "CSR": 0.5382313829787234, "EFR": 1.0, "Overall": 0.7042869015957447}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Dutch Airlines", "A Rush of Blood to the Head", "5", "Chicago", "The Wind's Twelve Quarters", "child actor", "Dennis Kux", "Midnight Oil", "Brett Ryan Eldredge", "I-League", "two or three", "Nimbus School of Recording Arts", "Lady Frederick Windsor", "point-coloration pattern", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street", "1930 American Pre-Code musical film directed by John Francis Dillon and filmed entirely in Technicolor", "5,112", "1992", "real estate development, leased and management firm,", "14,677", "6'5\"", "Gilley's Club", "Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "German Shepherd", "Mexican", "December 24, 1973", "2003", "the backside", "Ulver and the Troms\u00f8 Chamber Orchestra", "1730", "London Luton Airport", "the Salzburg Festival", "McComb, Mississippi", "Afghanistan", "1959", "F.P. Felix Avenue", "Randall Boggs", "a Passion", "Boston", "a lion", "Royal", "World War II", "Knoxville", "Three's Company", "P.O.S,", "Labour", "\"Linda McCartney's Life in Photography\"", "Erich Maria Remarque", "September 14, 2008", "79", "Buffalo Bill", "Romania", "the James Gang", "Mt Kilimanjaro", "Aung San Suu Kyi", "Afghan National Security Forces", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "a cloakroom", "Bank of England"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5999446733821734}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4615384615384615, 0.1111111111111111, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-3675", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-744", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5531", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_searchqa-validation-6735", "mrqa_triviaqa-validation-2701"], "SR": 0.515625, "CSR": 0.5377604166666667, "EFR": 1.0, "Overall": 0.7041927083333335}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "St Petersburg", "kenya", "a offensive", "Vulcan", "mating", "Fawn Hall", "waived all privileges", "kenya", "Barnum & Bailey Circus", "Johnny Weissmuller", "a negative electrode", "a torque wrench", "gold", "Marlon Brando", "Citation", "impressionist", "University of Kentucky", "ruddy", "Brussels", "Macbeth", "General Lee", "piracy", "Fyodor Dostoevsky", "Martin Luther", "Clue", "Edgar Allan Poe", "German", "Andrew Johnson", "seven years", "Mike Connors", "Citation Jim", "Jim Inhofe", "sancire", "Corpus Christi", "Nigeria", "the ostrich", "a preamble", "a night shift", "kenya", "Desperate Housewives", "Galileo Galilei", "Canada", "Anne Hathaway", "a strike", "a bat", "West Virginia", "James Madison", "movie theater", "Citation", "critic", "Khrushchev", "1904", "a young girl", "Bobby Tambling", "ambidevous", "chariot", "Humberside Airport", "more than 265 million", "100 million", "freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "a head injury.", "Vatican's policy on condom use", "Charles II"], "metric_results": {"EM": 0.5, "QA-F1": 0.5763586956521739}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-15329", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.5, "CSR": 0.5369897959183674, "EFR": 1.0, "Overall": 0.7040385841836735}, {"timecode": 49, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1056", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-211", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-785", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8428", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.84375, "KG": 0.49375, "before_eval_results": {"predictions": ["the NSA", "Heisman Trophy", "Brandi Chastain", "the Colorado River", "Tracy Anderson", "carioca", "a treasure map", "Pocahontas", "improv", "(Whizzer) White", "a ukulele", "a push-button valve", "the Novel", "Ferris B Mueller", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "a draft horse", "Ernest Lawrence", "a rodeo", "a fresco", "Pied Piper", "(Ulysses) Grant", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "a leotard", "Bulworth", "the small intestine", "a mouthpiece", "Cuba", "(Stephen) Frodo", "Olivia Newton-John", "bug spray", "Manhattan", "February 2", "Leontyne Price", "manure", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "(Stephen) Bradshaw", "the Castalian Spring", "Hungary", "a burnus", "Philadelphia", "peanut butter", "Invisible Man", "a grape", "Lex Luthor", "food and clothing", "Jehnna ( Olivia d'Abo )", "Master Christopher Jones", "Hebrew", "Meadowbank", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "three out of four", "poems telling of the pain and suffering of children just like her;", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5689236111111111}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 0.16666666666666669, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9727", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-16920", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-10510", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-1364", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-2904", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3073"], "SR": 0.46875, "CSR": 0.535625, "EFR": 1.0, "Overall": 0.717984375}]}