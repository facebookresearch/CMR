{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4540, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "the Grand Annual Steeplechase at Warrnambool", "Paleoproterozoic", "the end", "1894", "Rhenus", "the Pacific", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "like-minded Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "teaching", "music from the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "from \u00a315\u2013100,000", "the Purus Arch", "the infected corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "in series 1", "the chief electrician position", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "the Masovian Primeval Forest", "in the days, weeks and months after it happened", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "an algorithm for multiplying two integers can be used to square an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "his granddaughter", "Killer T cells", "W\u00fcc\u00fckdeveci", "More than 1 million", "2011", "in the same way as prices for any other good", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Tibetan", "Matthew 16:18", "the U.S. ship that was hijacked off Somalia's coast.", "Wwanda", "the three-day festival has been canceled", "his health", "the Mayflower", "\"A Christmas Memory\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.7183136308136308}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.5, 0.6666666666666666, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-7566", "mrqa_squad-validation-9243", "mrqa_squad-validation-9655", "mrqa_squad-validation-2052", "mrqa_squad-validation-7763", "mrqa_squad-validation-2732", "mrqa_squad-validation-4276", "mrqa_squad-validation-7728", "mrqa_squad-validation-1285", "mrqa_squad-validation-2520", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-7717", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.59375, "CSR": 0.7109375, "EFR": 0.9615384615384616, "Overall": 0.8362379807692308}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "on the coast of Denmark", "quantum mechanics", "On Tesla's 75th birthday in 1931", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel", "comb-like bands of cilia", "each six months", "Japanese imports", "the Electorate of Saxony", "Mark Twain", "the Commission", "1082", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "a presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two-page", "Arizona Cardinals", "1991", "the KC135 weightlessness training aircraft", "Isiah Bowman", "the poor", "100\u2013150", "Executive Vice President of Football Operations", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "more integral within the health care system", "declare martial law", "a customs union", "Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ricky Martin", "NLP Stand For", "Ireland", "nitrogen", "Christopher Nolan", "Agulhas Current Flow Rates", "six Oscars", "Top 10 Dance Crazes", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.734375, "QA-F1": 0.8058779761904762}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-803", "mrqa_squad-validation-1637", "mrqa_squad-validation-4437", "mrqa_squad-validation-3711", "mrqa_squad-validation-9896", "mrqa_squad-validation-7949", "mrqa_squad-validation-235", "mrqa_squad-validation-3967", "mrqa_squad-validation-378", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.734375, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 3, "before_eval_results": {"predictions": ["the 1994 Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "an Eastern Bloc city", "Sakya", "printed", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "best-known legend", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "Jean-Claude Juncker", "2000", "oxygen", "increase local producer prices by 20\u201325%", "prime AS-258 crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "prime matters", "multiple revisions", "the 50 fund", "integer factorization problem", "economic inequality", "Isel", "The Huguenots adapted quickly and often married outside their immediate French communities", "prime Minister Benazir Bhutto", "Charles-Fer Ferdinand University", "he had drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "ATP is synthesized there, in position to be used in the dark reactions", "the late 19th century", "Channel Islands", "in no way", "Alberich", "9", "Emeril Lagasse", "Churchill Downs", "port of Terneuzen", "charleston", "charleston", "Colombia", "study insects and their relationship to humans", "charleston", "travis", "George Fox", "Massachusetts", "charleston", "24 hours a day and 7 days a week", "Sponsorship scandal", "The Clash of Triton"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6998635912698412}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-4293", "mrqa_squad-validation-3957", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-1150", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-1581", "mrqa_hotpotqa-validation-3821"], "SR": 0.640625, "CSR": 0.69921875, "EFR": 0.9565217391304348, "Overall": 0.8278702445652174}, {"timecode": 4, "before_eval_results": {"predictions": ["higher plants", "Parliament of Victoria", "mike atherton", "Fort Edward and Fort William Henry", "Science and Discovery", "Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "The Skirmish of the Brick Church", "port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation", "wage or salary", "Roman Catholic", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "During the 18th and 19th centuries", "apicomplexan", "Academy of the Pavilion of the Star of Literature", "maryland", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "mike atherton", "22", "mike atherton", "mike atherton", "Brian Smith", "mike atherton", "Muslim", "will be the first time any version of the Magna Carta has ever gone up for auction", "a unit of Time Warner", "80th birthday", "fighters", "Capt. Chesley \"Sully\" Sullenberger", "mike atherton", "FBI recordings of his phone calls", "maryland", "one", "mike atherton", "$1,500", "National Industrial Recovery Act", "mike atherton", "Humberside Airport", "mike atherton"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6290809884559885}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-5626", "mrqa_squad-validation-10247", "mrqa_squad-validation-4773", "mrqa_squad-validation-4395", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3195", "mrqa_squad-validation-8759", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.59375, "CSR": 0.678125, "EFR": 1.0, "Overall": 0.8390625}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a data network based on this voice-phone network", "500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "a way of reminding their countrymen of injustice", "June 1978", "Milton Latham", "1914", "Philippines", "Denver's Executive Vice President of Football Operations and General Manager", "the 1970s", "the spoils of the war", "German Te Deum", "1795", "Bermuda 419 turf", "air could be liquefied, and its components isolated, by compressing and cooling it", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "a rudimentary immune system, in the form of enzymes that protect against bacteriophage infections", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "The President is also Commander in Chief of the United States Armed Forces", "a compact layout to combine keys which are usually kept separate", "an Ohio newspaper", "Herbert Hoover", "cannonball", "Panning", "Justin Timberlake", "at least US $2 trillion by GDP in nominal or PPP terms", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "unknown origin", "a yellow background instead of a white one", "19 now open in the province of Ontario", "9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "their babies'birth, or on the family when the Big Three are children ( at least ages 8 -- 10 ) or adolescents", "Morgan Freeman", "David Gahan", "The Stanley Hotel", "a long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the last day before the long fast for the Lent period in many Christian churches", "Jaipur", "Jonas Olsson", "torpedo boats", "grafton Road"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6963009853064033}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.56, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.8421052631578948, 0.058823529411764705, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-376", "mrqa_squad-validation-9908", "mrqa_squad-validation-436", "mrqa_squad-validation-3473", "mrqa_squad-validation-6450", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.5625, "CSR": 0.6588541666666667, "EFR": 0.9642857142857143, "Overall": 0.8115699404761905}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive than their public counterparts", "an antigen from a pathogen in order to stimulate the immune system and develop specific immunity against that particular pathogen", "their disastrous financial situation", "a Serbian Orthodox priest.", "receptions, gatherings or exhibition purposes", "New England Patriots", "the Ralph Nelson-directed Charly", "Henry Cole, the museum's first director, was involved in planning", "steam turbines", "\"social and political action,\"", "1936", "New Birth", "gold", "a deficit", "Vivienne Westwood", "reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications", "disease", "\"TFIF\"", "Confucian propriety and ancestor veneration", "\"Christ and His salvation\"", "five", "both the European Court of Justice and the highest national courts", "1888", "business districts of Downtown San Bernardino", "BBC Radio 5 Live and 5 Live Sports Extra", "1876", "a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox", "IP and AM are defined using Interactive proof systems.", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548 public schools, 489 Catholic schools and 214 independent schools", "Corliss", "teachers in publicly funded schools must be members in good standing with the college", "end of the season", "10 years in prison", "Jonas", "African-Americans", "\"Operation Pull Ryan,\" a grassroots campaign to prevent Ryan's re-election in 2012", "Charles Bukowski-wannabe", "weather is always hot and humid and it rains almost every day of the year", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said.", "10.1", "Sunday.", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "more than 170 were killed and hundreds of others were wounded.", "North Korea's reclusive leader Kim Jong-IlThe missiles can travel about 3,000 kilometers (1,900 miles)", "first five Potter films have been held in a trust fund which he has not been able to touch.", "that you love the environment and hate using fuel", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "a \"stressed and tired force\" made vulnerable by multiple deployments", "James Whitehouse, has been quoted as saying she has terminal brain cancer,", "we want to ensure we have all the capacity that may be needed over the course of the coming days.", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Jody Rosen of Rolling Stone", "Mike Gatting", "Colgate University", "Church of Christ, Scientist", "a fat or fatty acid in which there is at least one double bond within the fatty acid chain", "New Testament"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5316816865909737}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.3, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.5, 0.19999999999999998, 1.0, 0.3571428571428571, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.14285714285714288, 1.0, 0.4, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.3636363636363636, 0.0, 1.0, 0.16666666666666669, 0.0, 0.2222222222222222, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.45454545454545453, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7114", "mrqa_squad-validation-6541", "mrqa_squad-validation-6040", "mrqa_squad-validation-5256", "mrqa_squad-validation-3370", "mrqa_squad-validation-6001", "mrqa_squad-validation-4592", "mrqa_squad-validation-2643", "mrqa_squad-validation-486", "mrqa_squad-validation-3390", "mrqa_squad-validation-1739", "mrqa_squad-validation-2916", "mrqa_squad-validation-3250", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_naturalquestions-validation-7683", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.421875, "CSR": 0.625, "EFR": 1.0, "Overall": 0.8125}, {"timecode": 7, "before_eval_results": {"predictions": ["the 1970s", "his friendship", "increased", "187 feet (57 m)", "pH or available iron", "90\u00b0 to each other", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "unequal", "July 1969", "hiding a Jew in their house", "proplastids", "spontaneous", "the courts of member states and the Court of Justice of the European Union", "gold", "a sentient time-travelling space ship", "New South Wales", "Scottish rivers", "the \"Bricks for Warsaw\" campaign", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "the United States", "\u00a332,583", "21 October 1512", "James O. McKinsey", "\"Dance Your Ass Off.\"", "their \"Freshman Year\" experience", "India", "Benazir Bhutto,", "the Lindsey oil refinery", "April 24 through May 2", "Krishna Rajaram,", "early detection and helping other women cope with the disease", "250,000", "Tim Masters,", "homicide by undetermined means", "Yaya Toure", "12 hours", "about 2,000 people who were traveling from the capital, Dhaka, to their homes in Bhola", "Jason Chaffetz", "Madeleine K. Albright", "the smallest to the largest", "military trials for some Guant Bay detainees", "Matthew Fisher", "Cain", "by 9 a.m.", "the rape and murder of a 13-year-old girl", "seeking help", "Japan", "Too many glass shards left by beer drinkers in the city center", "\"Empire of the Sun\"", "England", "stronger", "Matthew Ward Winer", "Henry Fonda", "R\u00fcgen island,", "Mustique,", "green"], "metric_results": {"EM": 0.5, "QA-F1": 0.5957482621545122}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.45454545454545453, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.5, 0.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 0.10256410256410256, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-8883", "mrqa_squad-validation-7587", "mrqa_squad-validation-2844", "mrqa_squad-validation-872", "mrqa_squad-validation-1556", "mrqa_squad-validation-2091", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858"], "SR": 0.5, "CSR": 0.609375, "EFR": 0.96875, "Overall": 0.7890625}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "medical treatment for the sick and wounded French soldiers", "Roman Catholic", "\"Professor Conanarty to the Doctor's Sherlock Holmes\"", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday", "\"Journey's End\"", "immediate", "Levi's Stadium.", "decidedly Wesleyan", "art posters", "Elbegdorj", "Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz,", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone,", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington", "autonomy", "Islamic", "$24.1 million", "Fernando Gonzalez", "Graeme Smith", "$15 billion", "finance", "terminal brain cancer", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "the Employee Free Choice act", "separated", "Animal Planet", "crashing his private plane into a Florida swamp.", "there were no radar outages and said it had not lost contact with any planes", "54 bodies", "early detection", "Diversity", "$250,000", "met by Thursday.", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Briton Carl Froch", "Everglades", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans, Louisiana", "huybertsz.", "MIBs", "olympics", "Hockey"], "metric_results": {"EM": 0.625, "QA-F1": 0.7174868716809506}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.1, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-6300", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839"], "SR": 0.625, "CSR": 0.6111111111111112, "EFR": 1.0, "Overall": 0.8055555555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "His wife Katharina", "law", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Santa Clara, California.", "General Sejm", "Derek Jacobi", "net force", "\"larn\"", "50%", "French, and are entirely devoted to the English.", "United States", "CRISPR", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "its Video On Demand service to carry a modest selection of HD content.", "issues related to the substance of the statement.", "Edict of Fontainebleau", "15.", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.", "Ronaldinho", "cooperating with Turkey in engaging with the Taliban in Pakistan and Afghanistan.", "25", "a cardio to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "the couple's surrogate lost the pregnancy.", "environmental and political events", "people are starving, aid is scarce, and the only operating factories serve the military.", "two and a half hours.", "Elin Nordegren", "New York City", "6,000", "drug test after a Serie A game at Roma which returned a positive result.", "President Clinton.", "freedom from a bank to buy the guns.", "MDC head Morgan Tsvangirai.", "freedom and vote for the candidates of their choice.", "future relations between the Middle East and Washington.", "in a canyon", "Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.", "posting a $1,725 bail,", "school,", "strife in Somalia,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois,", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "UH-60 Blackhawk helicopters collided Saturday night while landing in northern Baghdad, killing one Iraqi soldier,", "London", "Abigail", "a social environment", "William Tell", "Andr\u00e9 3000", "Groundhog Day", "Cleopatra,", "Roustabout"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6189588189588189}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7777777777777778, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.060606060606060615, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2077", "mrqa_squad-validation-12", "mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-10185", "mrqa_squad-validation-9194", "mrqa_squad-validation-2972", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_hotpotqa-validation-2679"], "SR": 0.53125, "CSR": 0.603125, "EFR": 0.9666666666666667, "Overall": 0.7848958333333333}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley,", "pseudorandom", "John Wesley", "Genghis Khan's", "heating water to provide steam that drives a turbine connected to an electrical generator", "internal strife", "yellow fever", "a DC traction motor", "The Prince of P\u0142ock,", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Frederick William,", "within the premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Broncos last wore matching white jerseys and pants in the Super Bowl in Super Bowl XXXIII,", "the membrane of the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO.", "Brazil", "Monday", "a broken pelvis,", "issued his first military orders as leader of North Korea just before the death of his father was announced,", "heavy snow and ice", "Willem Dafoe", "Maude", "Phillip A. Myers.", "Kim Il Sung", "two weeks after Black History Month", "58 people", "two Metro transit trains", "last summer.", "Christopher Savoie", "Lance Cpl. Maria Lauterbach", "Dangjin", "Sharp-witted. Direct. In control.", "Hu Jintao", "Christian", "burns over about two-thirds of his body,", "11-month-old", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next", "President Bush", "Jeffrey Jamaleldine", "2,800", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "in straight sets to win the final of the Madrid Open.", "lightning strikes", "Bradley", "airlines around the world shut down every year.", "16 August 1975 at Kalkaringi", "Bonnie Aarons", "one", "is a wine made from grapes that have reached the first level of ripeness", "is a 2016 American-Indian-Irish computer-animated comedy-adventure film", "James Lofton", "is used especially for mystical, occult and spiritual viewpoints.", "Hair-like structures"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5319875710911894}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.10526315789473684, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5714285714285714, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-3418", "mrqa_squad-validation-7230", "mrqa_squad-validation-3237", "mrqa_squad-validation-492", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3284", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-1947", "mrqa_naturalquestions-validation-9422", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-1551", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.453125, "CSR": 0.5894886363636364, "EFR": 1.0, "Overall": 0.7947443181818181}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "the Victoria Department of Education", "the FBI ordered the Alien Property Custodian to seize Tesla's", "Manned Spacecraft Center", "economic inequality", "circumspect", "use of a decentralized network with multiple paths between any two points,", "Elway", "Philo of Byzantium", "36", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "the sieve of Eratosthenes", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "the Clangers", "a type of antelope", "The Least Qualified Person", "the Triassic Period", "\"The principles of cooperation and their...", "Anastasia Dobromyslova", "Gagapedia", "9", "The movie made $230.4 million worldwide,", "the Brassicaceae", "Robert Ludlum", "Fandom", "\"[8.7 megabytes)", "the largest showcase of Grand Prix racing cars in the world", "\u2018 Waynes World\u2019", "Hebrew Alephbet", "The London Underground Piccadilly Line", "British", "orangutan", "the jury of the 1863 Salon", "The book's sequel,", "The Oberlin College", "1920", "1969", "DodgeDodge", "Benny Hill", "Venice", "a shawl", "Enrico Caruso", "florence Nightingale Graham", "collapsible support assembly", "Sir Hardy Amies", "NATOThe North Atlantic Treaty Organization", "the 14th most common surname in Wales and 21st most common in England", "Can't Get You Out of My Head", "Ray Looze", "The 6-story building,", "The Islamic republic's", "the Golden Gate Yacht Club of San Francisco", "Raphael Taber Avila's", "The announcement of the country's national budget", "Buddhism"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5146734495080083}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.9411764705882353, 1.0, 0.8, 0.0, 0.7857142857142858, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444444, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-3790", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-2331", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983", "mrqa_searchqa-validation-13120"], "SR": 0.421875, "CSR": 0.5755208333333333, "EFR": 0.972972972972973, "Overall": 0.7742469031531531}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "90-60's", "Panini", "Bills", "anti-colonial movements", "a wide glacial alpine valley", "protein L", "he taught him to be suspicious of even the greatest thinkers and to test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor- patient relationship", "the case of an express wish of the people to withdraw from the social state principles", "1788", "weekend afternoons", "Roman Catholic", "Protestantism", "John Wesley", "the nationalisation law was from 1962, and the treaty was in force from 1958", "the Eternal Heaven", "Suffolk County Council", "Jessica Simpson", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Eric Pickles", "he was not a British soldier and did not have amnesia", "Vladivostok", "Sheryl Crow", "germanotta", "raphanus", "AFC Wimbledon", "Charles Hawtrey", "Malaysia", "astronomy", "malt wine", "George Clooney", "R C Sherriff", "James Chadwick", "\"Ah, look at all the lonely people.\"", "\"Water Works\"", "champagne", "rainwater flowing in a stream channel,", "the first Great Seal of the United States", "Brigit Forsyth", "germanotta", "\"Land of the Rising Sun\"", "london", "germanotta", "Kent", "germanotta", "germanotta", "white", "Switzerland", "germanotta", "Enlightening the World", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Jason Voorhees", "German state and municipal budgets", "David", "\"The Screening Room\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.49443530701754385}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.21052631578947367, 1.0, 0.75, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9126", "mrqa_squad-validation-6655", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-5827", "mrqa_squad-validation-3161", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-2501", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-3503", "mrqa_naturalquestions-validation-594", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_newsqa-validation-3207", "mrqa_searchqa-validation-8450", "mrqa_newsqa-validation-3860"], "SR": 0.453125, "CSR": 0.5661057692307692, "EFR": 1.0, "Overall": 0.7830528846153846}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "al-Biruni", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "had their own militia", "months after it happened", "90", "the quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "the concept of force", "mustelids", "John Connally", "saffron", "auguste america", "auguste renoir", "albinism", "suez Canal", "Brigit Forsyth", "america", "March 10, 1997", "gremlins", "the Battle of the Three Emperors", "Velazquez", "auguste america", "lizards", "pampa", "table tennis", "auguste america", "germanine Somerville", "auguste gove", "auguste america", "Jinnah International", "Sunday", "eric hisham Ibrahim", "auguste renoir", "soap", "liqueur", "auguste america", "auguste america", "Charlie Brooker", "germana", "auguste hampers", "2007", "william hoskins", "auguste america", "pale yellow", "auguste renoir", "bubba", "June 12", "Filipino American", "London", "gueuze", "Kindle Fire", "Steven Green", "auguste", "auguste america", "emperor", "Synchronicity"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4534722222222222}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-6463", "mrqa_squad-validation-4908", "mrqa_squad-validation-2875", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-4139", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-6547", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-972", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-3162", "mrqa_hotpotqa-validation-3487", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628"], "SR": 0.390625, "CSR": 0.5535714285714286, "EFR": 1.0, "Overall": 0.7767857142857143}, {"timecode": 14, "before_eval_results": {"predictions": ["seven months old", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "the soul don't leave their bodies to be threatened by the torments and punishments of hell", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "they were at least partly the product of a declining state of mind", "1898", "The Deadly Assassin and Mawdryn", "radioisotope", "Cody Fern", "Nicklaus", "Jim Gaffigan", "karthur", "2020", "1974", "332", "1997", "Authority", "the most junior enlisted sailor", "Spanish moss", "Chinese cooking", "Prague", "between 2 World Trade Center and 3 world Trade Center", "Kevin Spacey", "All Hallows'Day", "2.5", "the main type of cell found in lymph", "Bangladesh", "the President", "G minor", "the Hustons", "Chandan Shetty", "rock", "January 12, 2017", "United States", "Claims adjuster", "the female egg", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "the previous year's Palm Sunday celebrations", "vertebral column", "three", "robbie coltrane", "austerwurst", "kew Gardens", "krushchev", "$500,000", "Alexandros Grigoropoulos,", "conan reaper", "connoisseur", "BBC building in Glasgow, Scotland", "kidney"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6558224422015183}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9523809523809523, 1.0, 0.28571428571428575, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5333333333333333, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_squad-validation-2523", "mrqa_squad-validation-7670", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_hotpotqa-validation-2116", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-121", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.515625, "CSR": 0.5510416666666667, "EFR": 0.967741935483871, "Overall": 0.7593918010752688}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "Late Show", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "destroyed over 2,000 buildings", "Famous musicians", "ESPN Deportes", "Jean Ribault", "Tetzel", "Electorate of Saxony", "88%", "Necessity-based entrepreneurship", "950 pesos ( approximately $ 18 )", "octave systems", "Seattle", "Battle of Antietam", "Andy Cole and Shearer", "In Time", "by the early 3rd century", "Glenn Close", "four times", "Agostino Bassi", "five seasons", "One Direction spending time on a beach in Malibu", "Paul", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Jane Fonda", "Bhupendranath Dutt", "a warrior", "Dr. Lexie Grey ( Chyler Leigh )", "Matt Jones", "September 1972", "Uruguay", "Kelly Palmer", "Majandra Delfino", "National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "sheep were primarily raised for meat", "The pull - ring was replaced with a stiff aluminium lever", "Director of National Intelligence", "Remus Lupin", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson", "Thespis", "John Hill and Asa Taccone", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "boxelder bug", "Code 02 Pretty Pretty", "Joe Dever", "parliament", "child, Isaac, and daughter, Rebecca.", "Nevada", "Pablo Neruda", "Stage Stores, Inc.", "1881"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5133946782384282}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.2, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-7235", "mrqa_squad-validation-7141", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-2805", "mrqa_searchqa-validation-5103", "mrqa_hotpotqa-validation-1852"], "SR": 0.390625, "CSR": 0.541015625, "EFR": 0.9487179487179487, "Overall": 0.7448667868589743}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "the Arizona Cardinals 49\u201315", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "The high school student follows an education specialty track, obtain the prerequisite \"student-teaching\" time, and receive a special diploma to begin teaching after graduation", "six", "11", "hydrogen and helium", "Khitan", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland", "Google Docs, Sheets, and Slides", "SAVE", "Scandinavian Airlines", "1993 to 2001", "1951", "Southern Miss Golden Eagles", "Martin Truex Jr.", "Easter Rising", "45%", "more than two decades", "BAFTA TV Award", "Nardwuar the Human Serviette", "Battle of Culloden", "Burny Mattinson", "Julian Dana William McMahon", "cairns", "Mauser C96", "The story is set in the same world as \"Howl's Moving Castle\"", "CAC/PAC JF-17 Thunder", "Delacorte Press", "Neighbourhoods are the spatial units in which face-to-face social interactions occur", "Secretariat", "Minami-Tori-shima", "Hydrogen vehicle", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "the South", "Thomas Harold Amer", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Amway", "Capitol Records", "South Africa", "England", "The Girl in the romantic comedy \"My Sassy Girl\"", "Charles Russell", "Boyd Gaming", "Anthony Davis of the New Orleans Pelicans", "1966, 1967, 1968, 1970", "Glenn Close", "leontine Mary Welch", "Neighbours", "Ewan McGregor", "2011", "pippa passes", "an enslaved African American", "power-sharing talks", "Brown-Waite"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5400196158008659}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.8, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.25, 0.2857142857142857, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-76", "mrqa_squad-validation-4415", "mrqa_squad-validation-2191", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655"], "SR": 0.421875, "CSR": 0.5340073529411764, "EFR": 1.0, "Overall": 0.7670036764705882}, {"timecode": 17, "before_eval_results": {"predictions": ["force of gravity acting on the object balanced by a force applied by the \"spring reaction force\"", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter status", "1830", "nonfunctional pseudogenes", "the inner mitochondria membrane", "Charlie Harper", "Stevie Wonder", "beaver", "\"The Passage of the Red Sea\"", "formic acid", "welch", "Zimbabwe", "Wadsworth", "Edward \" Ted\" Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "welch", "Mercury", "gazelle", "Xenophon", "welch", "By Paul Bruno", "Nick Hornby", "\"Bard of Avon\"", "Charlemagne", "welch", "welch", "welch", "\"big house\"", "Hadrian", "US", "welch", "Australian comedian Barry Humphries", "Harris", "mulberry tree", "Tangled", "\"The French Connection\"", "CBS", "Manchester United", "Prokofiev", "Jessica Simpson", "welch", "Finland", "welch", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "The Union's forces had about 18,000 poorly trained and poorly led troops in their first battle", "New Jewel Movement", "Balaenidae", "north-south highway", "Goa", "Marius Ivanovich", "Oshkosh", "\"Papa's Got a Brand New Bag\"", "\"The Kansan\"", "\"The Sunday Thing\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.568795955882353}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false], "QA-F1": [0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10351", "mrqa_squad-validation-7089", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-767", "mrqa_hotpotqa-validation-1658", "mrqa_newsqa-validation-3031", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.484375, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.765625}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "high", "manakintown", "northwest", "10", "Middle Miocene", "new magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "1994", "Farrow / Previn / Allens", "Allison Janney", "chalkidice", "inability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Laura Jane Haddock", "1985", "775", "Aretha Franklin", "Gupta", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Coton", "( threshold 85 %, a distinction )", "Ella Eyre", "1995", "Identification of alternative plans / policies", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "aorta", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "png HTTP / 1.1", "lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "Rime is not valued or used always or everywhere or by everyone in the same way or for the same reasons", "September 2017", "The Canterbury Tales", "Rising Sun Blues", "Part 1", "dumbo", "the duke of Monmouth", "Christian", "Robert L. Stone", "2008", "Yemen", "bohemian manina", "Robert Langdon", "ABC1 and ABC2", "\"NBA 2K16\"", "Lord President of the Council and Lord Lieutenant of Ireland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6336532801526407}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.08695652173913042, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3193", "mrqa_squad-validation-6937", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-4613"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.9666666666666667, "Overall": 0.7489583333333334}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law,", "white", "New Orleans, Biloxi, Mississippi, Mobile, Alabama", "Jaime Weston", "1978", "\"Ein feste Burg ist unser Gott\" (\"A Mighty Fortress Is Our God\"", "warming", "1991", "more than 3 million tonnes of wheat", "Long troop deployments", "Joe Pantoliano", "The father of Haleigh Cummings,", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "nearly three out of four Americans are scared about the way things are going in the country today.", "sovereignty over them.", "this week when someone with a compatible organ died and their family asked that it be given to the singer, according to the organ procurement group that handled the donation.", "forgery", "Anil Kapoor", "19-year-old boy is sleeping in your bed, with your wife", "the first of 1,500 Marines will be part of the initial wave of President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "the new kid on the block in the modern art scene, a reputation that was cemented when the world's most prestigious modern art fair, Art Basel, arrived in the city in 2002.", "The Louvre", "snowstorm", "Stop, speed racers, stop.", "a lizard-like creature from New Zealand", "\"A lawyer for Gadhafi's son Saadi, who fled in September to Niger,\"", "two Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18,", "\"Pain is temporary, film is forever\"", "Russia", "Al Alcohol", "Atlantic Ocean", "President Sheikh Sharif Ahmed", "cortisone.", "a reported \u00a320 million ($41.1 million) fortune", "Kingman Regional Medical Center", "The United States", "Manmohan Singh", "Michael Jackson", "a slight girl of 11, living in a simple home in a suburb of Islamabad.", "40 militants and six Pakistan soldiers dead", "Spaniard Carlos Moya", "Stratfor,", "Louisiana", "the Southeast", "The father of Haleigh Cummings,", "Carol Browner", "\"A Mother For All Seasons.\"", "The Maraachlis", "back at work", "The Georgia Aquarium", "15", "Derek Hough", "John Adams", "borsht (Borsch)", "Zager and Evans", "Robert Matthew Hurley", "the Occoquan District supervisor", "\"community standards\"", "Cromwell", "Rovaniemi", "1937", "Emad Hashim"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4169342376373626}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_squad-validation-2400", "mrqa_squad-validation-7831", "mrqa_squad-validation-3028", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-1958", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-2013", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922"], "SR": 0.34375, "CSR": 0.521875, "EFR": 0.9761904761904762, "Overall": 0.7490327380952381}, {"timecode": 20, "before_eval_results": {"predictions": ["the late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhijn", "1331", "Death wish Coffee", "L", "Cameroon,", "1994", "paper ballots", "red carpet", "empty vodka", "urging the president not to rush to send more troops to Afghanistan", "Bobby Darin,", "German manufacturer Mercedes,", "16", "his former Boca Juniors", "her nipples rings", "the composer of \"Phantom of the Opera\" and \"Cats\"", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony,", "her niece", "well over 1,000 pounds", "development of a nuclear weapon", "bright blue-purple", "allegedly faking a doctor's note and was restricted from leaving his house in Tokyo,", "ceo Herbert Hainer", "his client, Brett Cummins,", "invited camps in the Philadelphia area to use his facility", "inmates", "Col. Elspeth Cameron-Ritchie,", "on the set at \"E! News\"", "a seven-member Spanish flight crew and one Belgian", "gas", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "A group calling itself the \"Pelotones Omega\"", "Senate Democrats", "joined the Baltimore Orioles", "An undated photo of Alexandros Grigoropoulos,", "\"ceremonial,\"", "self-righteously", "fire a missile toward Hawaii on July 4.", "Angola", "Gary Brooker", "the creation of an Islamic emirate in Gaza,", "\"Friday the 13th\"", "determining which Guant detainees should be tried by a U.S. military commision,", "in San Antonio,", "a guard in the jails of Washington, D.C.,", "50.57 knots", "the Ku Klux Klan", "1939", "Branford College", "Bury", "animal mating", "Malayalam", "August 17, 2017", "belt", "clone.", "Hodel", "The EU - US Umbrella Agreement", "the British rock group Coldplay"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4087283070279394}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5333333333333333, 0.0, 0.25, 0.23076923076923078, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3070", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_naturalquestions-validation-7987"], "SR": 0.3125, "CSR": 0.5119047619047619, "EFR": 0.9772727272727273, "Overall": 0.7445887445887446}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness,", "the poor.", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "\u201csplash\u201d", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair", "Illinois", "shoulder", "Madonna's", "Glasgow", "voice-based", "Australia", "roch", "Pearson PLC.", "dog", "American Civil War", "roch", "rochner and Cristina Fern\u00e1ndez de Kirchner", "New South Wales", "silvery blue", "China", "Harrisburg", "Mustela erminea,", "glockenspiel", "Dr John Sentamu", "CameroonCameroon", "roch White and the Man in Bambi", "Anne Boleyn", "Apple PXS 3045", "Holly Johnson", "Emma Chambers", "charlemagne", "the clubhouse,", "Russell Crowe", "roch", "ACC", "Robin Goodfellow", "Samuel Butler", "chamomile tea", "rochr\u00fcck barracks 1996", "tarn", "Normandie", "roch", "Newbury", "the Old Testament", "5 million square kilometres", "Target Corporation", "\"The Omega Man\"", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "\"concern\"", "John Jackson Dickison", "Amnesty International.", "Oprah Winfrey.", "Mother o' mine"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5755208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.5625, "CSR": 0.5142045454545454, "EFR": 1.0, "Overall": 0.7571022727272727}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin", "jesuit", "Robert Peary", "pearls", "Utah Territory", "Carrie Underwood", "Drambuie", "he made his horse a consul, his palace a brothel, and his", "Google", "Langston Hughes", "Pain tolerance", "jesuit", "Tito Puente", "london", "jesuit", "USS LST 325", "jesuit", "David Beckham", "Arturo Toscanini", "economics", "Miracle in the Andes", "jesuit", "jesuit", "discus", "london", "basidiomycota", "james", "Alison Parker", "Idi Amin", "Deere", "a body, body part, or personal object", "hard clay", "jesuit", "Rudy Giuliani", "masa", "two-minute", "the Vikings", "puck", "Bastille Day", "typhoid fever", "inlets", "london", "Williamsburg", "jesuit", "jesuit", "vinegar and baking soda", "John Knox", "the internal reproductive anatomy", "more than $1 billion", "study design, collection, and statistical analysis of data, amend interpretation and dissemination of results", "Jape", "Tesco", "Mallard", "Graham Hill", "the Battelle Energy Alliance", "IT", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of Quebradillas.", "$10 billion", "Trenton, Florida."], "metric_results": {"EM": 0.359375, "QA-F1": 0.4428308823529411}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-16257", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-1997"], "SR": 0.359375, "CSR": 0.5074728260869565, "EFR": 1.0, "Overall": 0.7537364130434783}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "the Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Doritos", "Atlantic Ocean", "domestic cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Switzerland", "The Argonauts", "prometheus", "the Altamont Speedway Free Festival", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "massively multiplayer", "Cyrenaica", "the first World War uniforms", "a rock", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "President George W. Bush", "the Earth", "Nafea Faa Ipoipo", "a pentatope number", "Mumbai", "Joan Rivers", "Jack Mogale", "New Netherland", "Justin Trudeau", "Radars", "Denis Law", "Love Is All Around", "William Golding", "Susan Helms", "a typhoon", "Rangers", "Money Saving Expert", "Adidas", "the \"Rabbit Hole\"", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "the two bishops", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "President Pervez Musharraf", "Tennis Channel", "a fox", "the first Saigon bureau chief for CBS News", "Jupiter"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6161917892156863}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-749", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4882", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-2978", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-4963", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-458", "mrqa_searchqa-validation-1439"], "SR": 0.59375, "CSR": 0.5110677083333333, "EFR": 1.0, "Overall": 0.7555338541666666}, {"timecode": 24, "before_eval_results": {"predictions": ["civil disobedience", "the chosen machine model", "Universal Studios and Walt Disney Studios", "1997", "a suite of network protocols created by Digital Equipment Corporation,", "sally Savoie", "15.", "the first home series", "North Korea announcing it would scrap peace agreements with the South, warning of a war on the Korean peninsula and threatening to test a missile capable of hitting the western United States.", "was acquitted of aggravated killing a limo driver on February 14, 2002.", "11", "change course shortly before it crashed into the sea,", "Alwin Landry's supply vessel Damon Bankston", "Chaffetz", "money or other discreet aid for the effort if it could be made available,", "Sarah,", "boats", "environmental videos", "Costa Rica", "Afghan security", "Saturday", "38,", "70,000", "carbon neutral", "\"E! News\"", "by his former Boca Juniors teammate and national coach Diego Maradona,", "Steve Williams", "McDonald's", "poetry", "Lifeway Christian Stores", "2008", "Diego Maradona", "Dog patch Labs", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "two", "Itawamba County School District", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "The EU naval force", "Plymouth Rock", "Liza Murphy,", "black civil rights leaders and prominent Democrats", "police", "former U.S. secretary of state. William S. Cohen", "30", "five", "get better skin, burn fat and boost her energy.", "cell phones", "alleged gang rape of a 15-year-old girl on the campus of Richmond High School in Northern California while 10 or more witnesses, most of them students,", "Damon Bankston", "karthik Rajaram", "Sunday", "death and destruction,", "an Irish feminine name", "the Rocky Mountains in southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "northern irish", "radar", "art", "All-Star Game", "the single-season touchdown reception record", "italy", "freestyle", "Florence Nightingale", "Belief"], "metric_results": {"EM": 0.375, "QA-F1": 0.4932301205738706}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.0, 0.06666666666666667, 0.26666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 0.5555555555555556, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.15384615384615383, 0.0, 0.2, 0.0, 0.5, 0.25, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-610", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-3826"], "SR": 0.375, "CSR": 0.505625, "EFR": 1.0, "Overall": 0.7528125}, {"timecode": 25, "UKR": 0.642578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.85546875, "KG": 0.42265625, "before_eval_results": {"predictions": ["50th anniversary special", "Thomas Savery", "Vicodin", "high-quality granulated sugar, and cotton", "22,000 years ago", "a violent separatist campaign", "Eleven", "269,000", "The three men loaded the paintings --", "40.8 feet", "Eintracht Frankfurt", "150", "an angry mob.", "Russian bombers", "41,", "Long Beach,", "super-yacht", "137", "a Kurdish militant group in Turkey", "3-2", "autonomy", "the north coast of Puerto Rico.", "the Russian air force,", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "son", "alleviation of their pain", "improve health and beauty.", "Tom Baer.", "some members of Pakistan's spy service", "think these planning processes are urgently needed and have been a long time in coming.", "either heavy flannel or wool", "Brian Mabry", "iTunes", "May 2000", "training", "Former detainees of Immigration and Customs Enforcement", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "some truly mind-blowing structures", "first name", "he was one of 10 gunmen", "2006", "San Diego", "women", "\"stand tall, stand firm.\"", "keystroke", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "state", "heart", "Hyderabad", "Asia", "to stay, abide", "Las Vegas", "Jackson Pollock", "wye", "Louisiana", "January 19, 1943", "King Duncan", "Georgia", "monopoly", "a son"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48748041979949874}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.09523809523809523, 0.0, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.2105263157894737, 0.1111111111111111, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-947", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-9476"], "SR": 0.40625, "CSR": 0.5018028846153846, "EFR": 0.9736842105263158, "Overall": 0.6792380440283401}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "\"I shall never take a wife,", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "Krishna Rajaram,", "43,000", "Booches Billiard Hall,", "finance", "Ross Perot", "in Hong Kong's Victoria Harbor", "2002", "at least seven", "the legitimacy of that race.", "think are the best.", "three", "Monday", "scarlett Keeling", "two years,", "50,000,", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "in July", "Akshay Kumar", "Alan Graham", "\"The FARC issued a statement", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in September,", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation", "Phil Spector", "Kim Jong Il's", "1994", "numerous suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "older than the industry average,", "raping her in a Milledgeville, Georgia, bar", "Pop star Michael Jackson", "Kingman Regional Medical Center", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.,", "same-sex civil unions,", "military veterans", "barter -- trading goods and services without exchanging money", "semi-autonomous organisational units", "at least a 6 - 6 record", "Matt Monro", "A Touch of Frost", "the innermost digit of the forelimb", "1974", "over 20 million", "American", "Hawaii", "KID-FRIendly 4- LETTER", "King Lear_CLAR Upon such sacrifices,", "Ottoman Empire"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6462549603174603}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.578125, "CSR": 0.5046296296296297, "EFR": 1.0, "Overall": 0.6850665509259259}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "alongside Deepwater Horizon", "John Dillinger", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "Seasons of my Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Lashkar-e-Tayyiba", "$1.5 million", "2006", "Rev. Alberto Cutie", "Los Angeles Angels", "Indian Army", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "a Starbucks", "\"BADBUL,\"", "98 people,", "2008", "Genoa, Italy", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Islamabad", "Iraq", "Iran", "November 26,", "people have chosen their rides based on what their", "in July", "Zoe's Ark", "two soldiers and two civilians", "an incestuous relationship between Elisabeth,", "\"Nothing But Love\"", "38 people", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "a store at the Form Design Center", "fractured pelvis and sacrum", "Wednesday", "the abduction of minors.", "gun", "Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "1954", "11 p.m.", "Charlotte Corday", "Thailand", "wheat", "Norwood, Massachusetts", "Manchester", "Drowning Pool", "Missouri", "Vermont's largest city", "a beta blocker"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7202886702617306}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0689655172413793, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.3636363636363636, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-436", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-4505", "mrqa_triviaqa-validation-3389", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-12398"], "SR": 0.609375, "CSR": 0.5083705357142857, "EFR": 1.0, "Overall": 0.6858147321428572}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author of the Fifth Assessment Report", "enforcing land officials into large debts that cannot be repaid, ownership of private industries thus expanding the controlled area, or having countries agree to uneven trade agreements forcefully.", "1981,", "forgery and flying without a valid license,", "\"It was a wrong thing to say,", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "former U.S. secretary of state.", "the North Korea has positioned what is thought to be a long-range missile on its launch pad,", "European Commission", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain,", "seven", "said the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry", "Anil Kapoor.", "kill members of the Zetas cartel from the state of Veracruz.", "\"The Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "Buddhism,", "the Dalai Lama's", "Sergey Lavrov and Secretary of State Hillary Clinton", "8 p.m. local time Thursday", "Passers-by", "one day,", "executive director of the Americas Division of Human Rights Watch,", "750", "300", "Matthew Fisher", "The Ski Train", "Big Brother.", "Michael brewers,", "Jonathan Tukel,", "some U.S. senators", "Atlanta, Georgia", "5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers", "environmental and political events", "$250,000", "100% of its byproducts", "School-age girls", "5,600", "700,000", "Sen. Arlen Specter (D-Pennsylvania)", "Deutschneudorf,", "on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "a deceased organ donor,", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "a vertebral column ( spine )", "January to May 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the Sidgwick Avenue arts faculty", "Douglas Hofstadter", "\"The Dark Tower\"", "American", "Little Women", "Castle Rock", "tomato sauce, mozzarella cheese and fresh basil"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6207229406369522}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.25806451612903225, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.09523809523809522, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 0.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.6666666666666667, 1.0, 1.0, 0.125, 0.0, 0.5, 0.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8823529411764706, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3692", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3839", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_searchqa-validation-9830"], "SR": 0.515625, "CSR": 0.5086206896551724, "EFR": 1.0, "Overall": 0.6858647629310346}, {"timecode": 29, "before_eval_results": {"predictions": ["downward pressure", "Kusala,", "438,000", "Erick Avari, Michael Mc Kean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "radio frequency connector", "Pakistan A", "Jacksonville Jacksonville Jacksonville", "9th, 10th, 11th & 12th Chinese People's Political Consultative Conference,", "the German Campaign of 1813", "James Fitz James,", "1965", "Paris at Charles de Gaulle Airport", "fifth level", "Culiac\u00e1n, Sinaloa,", "seven", "Syracuse", "1986", "coca wine", "puzzle video game", "Knoxville, Tennessee", "Washington, D.C.", "Cricetidae", "Bedrock Brands,", "2017", "Wayman Tisdale", "Mexico,", "Srinagar", "Northern Ireland", "and early 20th centuries", "political thriller novel", "22,500", "the Harpe brothers", "Eric Liddell", "23 March 1991", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Group", "Erinsborough", "Marine Corps", "Disney Company", "the Lost Battalion", "Miccosukee tribes", "Virginia", "Slam Dunk Contest", "$10\u201320 million", "January 28, 2016", "Kennedy Road,", "Washington, DC,", "J Records", "Colin George Blakely", "two Nobel Peace Prizes", "IB Diploma Program", "Richard Parker", "Chile and Argentina", "Cecil B. De Mille", "allergic reaction", "King George VI,", "3,000 kilometers", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist", "Russia", "shrimp", "Australia"], "metric_results": {"EM": 0.375, "QA-F1": 0.4894202152014652}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-4882", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-1144", "mrqa_hotpotqa-validation-1944", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585"], "SR": 0.375, "CSR": 0.5041666666666667, "EFR": 1.0, "Overall": 0.6849739583333333}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian statesman", "London", "Dave Thomas", "a farmers' co-op", "Danish", "1903", "From Here to Eternity", "other individuals, teams, or entire organizations", "ten years of probation", "In Pursuit", "Bolton", "How to Irritate People", "Kansas City, Missouri", "Dirk Werner Nowitzki", "lifetime", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200 Indians", "Theme Park World", "Virgin Group", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "United States and Canada", "a Canadian comedian", "apatosaurus", "1993", "American", "Frank Thomas' Big Hurt", "\"Stranger in Paradise\"", "Margarine Unie", "MGM Grand fire", "New York City", "The Seduction of Hillary Rodham", "2005", "Lambic", "Nintendo Switch, PlayStation 4, and Xbox One", "Argentina,", "Alphonso Johnson Jr.", "Michael Edward \" Mike\" Mills", "nuclear weapons", "Joseph E. Grosberg", "\"Chelsea Lately\"", "Allgemeine-SS", "Turkmenistan", "London,", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "state", "Moving Past the Pain to Record Wins", "Villa Park", "2005", "228 people", "\"We have cameras on board that have been able to image where the Apollo spacecraft landed, and you can literally see where they put down their scientific packages.", "Post Traumatic Stress disorder", "Copenhagen", "Chief Joseph"], "metric_results": {"EM": 0.546875, "QA-F1": 0.650811011904762}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6399999999999999, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-355", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-4684", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-672", "mrqa_newsqa-validation-3905"], "SR": 0.546875, "CSR": 0.5055443548387097, "EFR": 1.0, "Overall": 0.685249495967742}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "georgia", "georgia", "Iowa", "georgia", "Nassau", "a pearl oyster", "HIV", "Martin Van Buren", "Shink Hansen", "georgia", "aardwolf", "Beijing", "Lovelock", "New Jersey Devils", "Death Valley", "this house's 2008 collection", "reindeer", "Fortinbras", "the War of 1812", "Anna Mary Robertson", "Sailor Moon", "georgia", "New York Times Fiction Best Sellers", "a bear", "a whirlwind", "georgia", "georgia", "negative electrode", "Milton Berle", "George H.W. Bush", "Congolese", "a spacecraft", "georgia", "Dan Marino Jr.", "Mars", "a clownfish", "georgia", "The Love Guru", "Las Vegas", "georgia", "georgia", "heavy drinking", "a chimpanzee", "Baja California", "Soothsayer", "Yitzhak Rabin", "Anoints David", "Gettysburg", "Jack Gleeson", "Plank", "Buddhism", "Carl John,", "Portugal", "Graham Bond", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "scientists at the Idaho National Laboratory.", "it's very sad,\"", "12.3 million"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5024553571428572}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-11690", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-2051", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587"], "SR": 0.46875, "CSR": 0.50439453125, "EFR": 1.0, "Overall": 0.68501953125}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "1,100 tree species", "Henry Addington", "40", "the Republic of Sudan", "Shania Twain", "Hillsborough", "glucagon", "the New York Yankees", "wakefulness", "green", "A Singular Woman", "Ba'ath Party", "France", "charles pauline heston", "Ohio", "Francis Matthews Dies", "photographic", "iron", "Noah", "Royal Albert Hall", "New", "Sarah Ferguson", "Mercury", "watt", "Jennifer Pertwee", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "S\u00e3o Paulo", "his long-suffering wife, Emma,", "aged 75 or older", "Jennifer Lopez,", "1664", "Annie Lennox", "Fred Perry", "Downton Abbey", "Martina Hingis", "O\u2019Ahern, Hearne", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "a gulliver", "salford", "Italy", "The Streets", "Appalachian", "a black Ferrari", "algebra", "bears", "Michael Moriarty", "June 1992", "24", "1994", "Campbell Soup Company", "Kirkcudbright", "the soldiers", "cortisone.", "the United States can learn much from Turkey\\'s expertise on Afghanistan and Pakistan.", "An exotic dancer", "a Typeface", "a network of blood"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5497996794871796}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.15384615384615383, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4343", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-5228", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.46875, "CSR": 0.5033143939393939, "EFR": 1.0, "Overall": 0.6848035037878788}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "chicken", "Hudson Bay", "florida", "allergies", "stanley", "Asterix", "jim branley", "river of january", "wind", "fire", "Robin Hood", "West Point", "Andy Warhol", "london", "jim branley", "river of january", "the solar system", "potatoes", "Moldova", "Mitsubishi A6M Zero Fighter", "river of january", "jim branley", "1,000 Guineas", "baroudeur-rouleur", "clare", "london", "Madness", "river of january", "discretion", "yves Saint Laurent", "rudyard Kipling", "london", "river of january", "beaver", "stanley", "a frog", "clement", "stanley", "river of january", "5000", "racing", "micelles", "Newfoundland", "corvids", "Yellowstone", "St. Francis Xavier", "luzon", "stanley", "Buddhism", "jonny Buckland", "Toledo", "river of january", "\u00c6thelred", "Scarface", "forgery and flying without a valid license,", "Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0", "Liza Murphy", "Spock", "Astana", "Belgium, Germany, Italy, Luxembourg, Monaco, Spain and Switzerland"], "metric_results": {"EM": 0.34375, "QA-F1": 0.39722222222222214}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_triviaqa-validation-3524", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-7119", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_triviaqa-validation-2642", "mrqa_naturalquestions-validation-2068", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-11382"], "SR": 0.34375, "CSR": 0.4986213235294118, "EFR": 1.0, "Overall": 0.6838648897058823}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business", "tundras tundra", "spain", "george sandayana", "marsupials", "Alice Cooper", "Beta-Blockers", "trumpet", "woolen", "The Scream", "shildon", "appalachian mountain range", "Herald of Free Enterprise", "ballet", "disaster film", "george ababa", "snakes", "Blackburn Lancashire", "Frankie laine, 93", "The Mystery of Edwin Drood", "pommel horse", "scarlet", "Dick Van Dyke", "Egremont", "numb3rs", "paul jerome", "phrixus", "dupin", "Canada", "animal away from the danger,", "pears soap", "Some Like It Hot", "manhattan", "ireland", "Mike Meyers", "hippocampus", "plutonium", "igneous rock", "jules verne", "Thank you", "spain", "spain", "shrek", "26 miles", "Cleveland Brown", "fat Duck", "One Direction", "spain", "Saturn", "dupin", "george degeneff", "November 1999", "Baaghi", "lead dioxide", "boxer", "Wiltshire", "stoneware", "enka", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "ballet", "Howard Carter"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5203125}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6449", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4530", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-8994", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-78", "mrqa_searchqa-validation-4312"], "SR": 0.453125, "CSR": 0.4973214285714286, "EFR": 0.9714285714285714, "Overall": 0.6778906250000001}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Giovanni Boccaccio, Dante, Racine, Rabelais", "Matlock", "American Civil War", "jane agust\u00edn nicol\u00e1s ruiz de janeiro", "beetles", "Arafura Sea", "daedalus", "Tigris", "Bavarian Forest", "to make wrinkles in (land)", "spain", "carousel", "bullfight", "Mike Brady,", "Countertenor", "Whiskas", "fidelio", "Guys and Dolls", "jean Fellowes", "Denmark", "another Day in Paradise", "The Last King of Scotland", "agust\u00edn nicol\u00e1s ruiz de goya", "pembrokeshire", "G. Ramon", "jane Merrill", "rachmaninoff", "Finland", "stars", "Mille janeiro", "inks", "charles heston", "jean bannister", "Muriel Spark", "happy birthday", "seven", "opossum", "Pickwick", "presliced bread", "The Bridge", "raven", "jane marwan Khouri", "bPA", "jane Buckle", "etruscan army", "Ken Burns", "jane marcadilly", "Heather Stanning and Helen Glover", "Pyotr Ilich Tchaikovsky", "hebrew jane marc ruiz de janeiro", "sun", "Donna", "season four", "the sinoatrial node", "Lee Sunmi", "tomato", "the 2002 Senate election in Minnesota", "\"wildcat\" strikes, unsanctioned by national unions,", "the L'Aquila earthquake", "March 24,", "Duke of Edinburgh", "shubun no hi", "Pocahontas"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48645833333333327}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-400", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4758", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-1247", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.390625, "CSR": 0.49435763888888884, "EFR": 1.0, "Overall": 0.6830121527777778}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans to explain the Iranian Islamic Revolution and apolitical Islam", "jim", "city of a republic on the southeastern coast of Africa", "branson", "brady Ramsay", "city of january", "joan crawford", "sulfur dioxide", "joan crawford", "ravelway", "Portuguese", "Travelocity", "Avengers", "city of ethiopia", "comets", "sounds of Silence", "disciples", "canola", "joan crawford", "joan crawford", "raisbourg", "Bolivia", "julian donne", "Uranus", "river Rio Grande", "a Circus Parade", "comets", "30th anniversary", "joan crawford", "kings jim I", "One Foot in the Grave", "Bronx Mowgli", "joan crawow", "jorge agust\u00edn nicol\u00e1s ruiz de santayana", "bow and arrow", "river of january", "Krankies", "joan de torquemada", "julian branboim", "Canada", "rum and coke", "Lake Union", "ghee", "george", "comets", "Hyperbole", "oldpatrick", "90th birthday", "joan crawford", "Ceylon", "screwdrivers", "Kansas City Chiefs", "Great G minor symphony", "A Christmas Story", "1974", "\"The Breakfast Club", "Amberley Village", "if you don't have a cause of death,", "President Obama", "joan crawl", "Empress Dowager Cixi", "Brigham Young", "June", "a former chalk quarry"], "metric_results": {"EM": 0.265625, "QA-F1": 0.37760416666666663}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false], "QA-F1": [0.19999999999999998, 0.0, 0.2, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9574", "mrqa_triviaqa-validation-3858", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-3726", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-4817", "mrqa_triviaqa-validation-3788", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5196", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-858", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4108", "mrqa_hotpotqa-validation-5545", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-2849", "mrqa_hotpotqa-validation-1377"], "SR": 0.265625, "CSR": 0.48817567567567566, "EFR": 1.0, "Overall": 0.6817757601351351}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "neutral", "aisles", "Tess", "Sakshi Malik", "Columbia River Gorge", "a perceived harmful event, attack, or threat to survival", "49 cents", "1876", "William Whewell", "14 \u00b0 41 \u2032 34", "joy of living", "420", "George Strait", "sovereignty", "1989", "Shawn", "Kiss", "London, England", "Los Angeles", "February 10, 2017", "Kelly Reno", "financial information about a nonprofit organization", "1770 BC", "Chandan Shetty", "two", "Adam Carolla", "mitochondria", "Anakin and Obi - Wan", "Travis Tritt and Marty Stuart", "1976", "Bee Gees", "Matt Czuchry", "Pradyumna", "1902", "Isle Vierge", "seven heavenly virtues", "New Jersey Devils", "two", "0.30 in ( 7.6 mm )", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "Alcohol or smoking", "Gloria", "Hudson Bay", "Maginot Line", "pussia", "dumbo", "Purple Rain", "James A. Garfield", "Gettysburg", "iTunes", "$273 million", "India", "Gulf of Aden", "Desperate Housewives", "The Flying Dutchman", "Tlapa", "Tuesday"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5747919992145536}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.09523809523809523, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.7692307692307693, 0.8, 0.0, 0.6666666666666666, 0.0, 0.25, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07272727272727272, 0.0, 0.5454545454545454, 1.0, 0.3333333333333333, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-5491", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.46875, "CSR": 0.4876644736842105, "EFR": 0.9705882352941176, "Overall": 0.6757911667956658}, {"timecode": 38, "before_eval_results": {"predictions": ["Michelangelo", "roughly five hundred experts across the world", "the Taft -- Katsura Agreement", "Kim Basinger", "August 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Lava", "in positions Arg15 - Ile16", "Charles Crozat Converse", "Lady Gaga", "the Chicago metropolitan area", "The president", "Domhnall Gleeson", "eusebeia", "Pastoral farming", "Tottenham Hotspur", "a nobiliary particle indicating a noble patrilineality", "Stephen A. Douglas", "1984", "a loanword of the Visigothic word guma `` man ''", "Pakistan", "23 February", "El Filibusterismo", "Bryan Cranston", "the thylakoid membrane", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "Franklin and Wake counties", "By late 1922", "The Minneapolis Miracle was the final play of the 2017 / 18 Divisional Round game against the New Orleans Saints", "Yuma", "maximum energy of 687 keV", "between $10,000 and $30,000", "September 1980", "1931", "University of Oxford", "11 : 40 p.m. ship's time", "Norman Whitfield", "1959", "The `` Southern Cause", "Donna's", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795", "Joseph Stalin", "the intermembrane space", "a divergent tectonic plate boundary", "Idaho", "Sara Gilbert", "6", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "maurice rogers Major", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "his foreign policy approach,", "10 below", "General Motors'", "David McCullough", "Rendezvous with Rama", "CERN", "saudade"], "metric_results": {"EM": 0.5, "QA-F1": 0.5720255602240896}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.11764705882352941, 0.0, 0.23529411764705882, 0.0, 0.0, 0.32, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4685", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.5, "CSR": 0.4879807692307693, "EFR": 0.96875, "Overall": 0.6754867788461538}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale", "Jenny Slate", "ATP", "Philippe Petit", "September 1980", "January 2004", "provinces along the Yangtze River and in provinces in the south", "Toby Keith", "ARPANET", "17 - year - old", "rock", "Set six months after Kratos killed his wife and child", "Teri Hatcher as Mel Jones", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "A 30 - something man ( XXXX )", "Gestalt", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Sarah DiMaggio", "John Thornton", "Don Cook", "Lieutenant Templeton `` Faceman '' Peck", "Bonnie Aarons", "late 2018 or early 2019", "typically composed of roughly 70 % hydrogen by mass, with most of the remaining gas consisting of helium", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "Frederik Barth", "John F. Kelly", "Charles Sherrington", "1890", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "September, 2016", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "a defense against rain rather than sun", "1940", "Sarah Lavrof", "Mark Jackson", "Michael Buffer", "one baptism, one God and Father of all, who is over all and through all and in all", "location", "Second Continental Congress", "late - 17th century New England", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "about 0.04 mg / L several times during a day", "Juan Manuel de Ayala", "Joseph Smith, Jr.", "comedy Folks (1874 - 1894)", "1909", "John Duigan", "179", "Princess Diana", "Mikkel", "Jaipur", "Me and Bobby McGee", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5722293665827873}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4347826086956522, 0.07407407407407407, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.1818181818181818, 0.6666666666666666, 0.0, 0.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-1616", "mrqa_searchqa-validation-10341"], "SR": 0.4375, "CSR": 0.48671875, "EFR": 0.9722222222222222, "Overall": 0.6759288194444444}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "the Indian army and separatist militants", "Donald Trump.", "early detection and helping other women cope with the disease.", "glamour and hedonism", "2-0", "15,000", "58", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "planned attacks", "the U.S. and Britain", "since 2004.", "NATO", "Switzerland", "Monday", "500", "\" Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany", "\"To improve America's competitiveness, the senator from Illinois said he wants to spend $10 billion on childhood education, $150 billion over 10 years on developing alternative energy", "T.I.", "Arizona", "Robert Barnett", "$627,", "41,", "Adenhart", "a strict interpretation of the law", "Derek Mears", "Sylt", "2,700-acre", "Tuesday", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "near the grave.", "Ali Bongo", "Allred", "\"We connected meaningfully about the important issues that have emerged over recent days, and I offered him my sincere apologies for any offense to our veterans caused by this report.", "Two pages", "A Brazilian supreme court judge", "Derek Mears", "Operation Pipeline Express.", "improve a Social Security program for unemployed Mexicans, increasing from two months to six months the time they will receive medical and maternity coverage.", "East Java", "St. Louis, Missouri.", "NATO fighters", "Raymond Thomas", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2007", "P.V. Sindhu", "on location in Mexico", "snickers", "eagle", "capone", "Disney riverboat", "uncle Juan Nepomuceno Guerra", "Bergen", "embalming", "eagle", "a graphical user", "the American Kennel Club"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5567029469373219}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.375, 1.0, 0.5, 1.0, 1.0, 0.9333333333333333, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.08888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.29629629629629634, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.0, 0.10256410256410257, 1.0, 0.4, 1.0, 0.3333333333333333, 0.06666666666666667, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.40625, "CSR": 0.4847560975609756, "EFR": 1.0, "Overall": 0.6810918445121952}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "Directed distance is a positive, zero, or negative scalar quantity", "the Peace of Westphalia of 1648", "Megan Park", "The euro is the result of the European Union's project for economic and monetary union which came fully into being on 1 January 2002", "Kate Walsh", "September 14, 2008", "American country music artist Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "The pour point of a liquid is the temperature at which it becomes semi solid and loses its flow characteristics", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "international capital flows to developing countries", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000 and annual dues were estimated in 2009 to be less than $10,000 per year", "mining", "Cedric Alexander", "a unique species capable of reproduction", "David Joseph Madden", "initially registered with churches, who maintained registers of births", "Saint Nicholas", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Malloy as Pierre, Phillipa Soo as Natasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne", "collect menstrual flow", "pigs", "General George Washington", "Spanish", "Howard Rollins", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node travels through the right atrium to the atrioventricular node, along the Bundle of His and through bundle branches", "four", "Jack Nicklaus", "Norman Greenbaum", "Alan Menken", "six 50 minute ( one - hour with advertisements )", "creating complementary economic and political units for different ethnic groups", "the Jos Plateau", "Missouri River", "the right to vote", "brain", "10 June 1940", "Tandi, in Lahaul", "Richard Wagner", "ear", "Brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "September 21.", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\" Kristal Kraft,", "the southern Sadr City residents.", "Colombia", "King Arthur", "Howie Mandel", "Air Canada, Malaysia Airlines, and Thai Airways"], "metric_results": {"EM": 0.5, "QA-F1": 0.5937640202612144}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.47058823529411764, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.8, 0.21052631578947367, 0.7000000000000001, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8235294117647058, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08333333333333334, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_triviaqa-validation-6700", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_newsqa-validation-604"], "SR": 0.5, "CSR": 0.48511904761904767, "EFR": 0.96875, "Overall": 0.6749144345238095}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization", "within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "the population, serving staggered terms of six years", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "to bring", "Paul von Hindenburg", "Ceramic art", "the Soviet Union's 1976 achievement of thirteen gold medals, but 4 gold medals ( 13 overall ) were stripped later due to doping", "Covington, Kentucky", "New Mexico", "to reduce power availability for the majority of thermal power plants by 2040 -- 2069", "December 15, 2017", "between Polaris Avenue and Dean Martin Drive", "L.K. Advani", "differential erosion", "Jennifer Close", "in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "a long proboscis", "Norman Greenbaum", "the notion that an English p Larson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "a circular arc", "Charlotte Thornton", "the Indian Ocean", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Disha Vakani", "880,000 square kilometres ( 340,000 sq mi )", "parthenogenesis", "1926", "Tokyo", "31 December 1947", "Chris Masterson", "Leon Huff", "1765", "alberich", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "to ensure that auto owners comply with recalls.", "two", "prostate cancer,", "a dragon", "Fauntleroy", "a key chain", "yellow"], "metric_results": {"EM": 0.5, "QA-F1": 0.6164789558735242}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.7741935483870968, 0.2222222222222222, 1.0, 1.0, 0.15384615384615383, 1.0, 0.7272727272727272, 1.0, 0.9, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.18181818181818185, 1.0, 0.4666666666666667, 0.14814814814814814, 0.6666666666666666, 0.5, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.1904761904761905, 1.0, 0.7058823529411765, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-2518", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-1526", "mrqa_searchqa-validation-11152"], "SR": 0.5, "CSR": 0.48546511627906974, "EFR": 0.90625, "Overall": 0.662483648255814}, {"timecode": 43, "before_eval_results": {"predictions": ["2003", "2007", "she wrote the song after undergoing therapy, saying she was `` sick of keeping all these feelings inside and not speaking up for myself ''", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Michelle Ryan", "2013", "as the arms of the king of Ireland can be found in one of the oldest medieval rolls of arms", "Miami Heat", "1982", "After World War I", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Hazel Grace Lancaster", "Paul C\u00e9rusier", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier,", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "balsam", "Alex Ryan", "habitat", "2018", "Windows Media Video ( WMV )", "100 members", "Toledo, Bowling Green, and Mount Union", "Transvaginal ultrasonography", "During the last Ice Age", "in Haikou on the Hainan Island", "Robert Irsay", "in Paradise, Nevada", "Alicia Vikander", "annually in late January or early February", "Ashoka", "\"The first four, like the King's Chamber, have flat roofs formed by the floor of the chamber above, but the final chamber has a pointed roof", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Bumblebee", "the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "AMX - 30", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "Malibu, California beach", "2018", "Florida", "on the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "Puerto Rico ( Rich Port )", "in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "wintertime", "Pangaea", "Newcastle Brown Ale", "Australia", "vaclav Havel", "Mary Bon auto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"The facility sprawls across approximately 600 square miles of south-central Washington, an area roughly half the size of Rhode Island,\"", "'overcharged.'\"", "$60 billion on America's infrastructure.", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.5, "QA-F1": 0.6249670965102787}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.5, 0.19999999999999998, 1.0, 0.0, 1.0, 0.3, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.4444444444444444, 1.0, 0.08333333333333334, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.06896551724137931, 0.6666666666666666, 0.975609756097561, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-8027", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-1977"], "SR": 0.5, "CSR": 0.4857954545454546, "EFR": 1.0, "Overall": 0.681299715909091}, {"timecode": 44, "before_eval_results": {"predictions": ["set annual carriage fees of \u00a330m for the channels with both channel suppliers able to secure additional capped payments if their channels meet certain performance-related targets", "aluminum foil", "Laurel, Mississippi", "his writings about the outdoors, especially mountain-climbing", "Indianola", "Escorts Limited, an engineering company that manufacture agricultural machinery, machine construction and material handling equipment and railway equipment", "Jean Baptiste Point DuSable", "1964", "Cher", "Appalachian Mountains", "Jim Harrison", "Toronto", "the Tomorrowland section of the Magic Kingdom theme park at Walt Disney World Resort", "Fennec fox", "United States Army", "seasonal television specials", "Jean Acker", "4,530", "Democritus", "Caesars Entertainment Corporation", "Donna Karan, Givenchy, Guerlain, Chanel", "Reinhard Heydrich", "Karl Kraus", "Steve Howey", "Maria Brink", "Manitobaowoc County, Wisconsin", "the Northrop P-61 Black widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "a Pulitzer Prize", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "OutKast", "Richard Street", "Zaire", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "a pioneering New Zealand food writer", "South America", "2006", "four months in jail", "\"The Major of St. Lo\"", "Mary Elizabeth Hartman", "over 9,000 employees", "Bill Seaton", "potential of hydrogen", "San Antonio, Texas", "Stephen King Biography", "a tab", "Kent", "almost 9 million", "Kenya", "$83,27014", "terrorism", "Moses", "A summary of Chapter 5", "Wilson Pickett"], "metric_results": {"EM": 0.484375, "QA-F1": 0.55304201007326}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false], "QA-F1": [0.07692307692307693, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.15384615384615385, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2837", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-3458", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-1184", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.484375, "CSR": 0.4857638888888889, "EFR": 1.0, "Overall": 0.6812934027777778}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "early 20th century", "Australian", "September 1903", "interstate commerce", "Naomi Wallace", "Jenson buttons", "Tufts University", "Macau", "Azeroth", "Squam Lake", "New York", "Tayeb Salih", "King James II of England", "God Save the Queen", "526", "Scotland", "hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "gmbH", "Mick Jackson", "Lalit", "his virtuoso playing techniques and compositions in orchestral fusion", "Tampa Bay Lightning", "Steven Selling, directed by Kurt Wimmer", "Chesley Sullenberger III", "Manhattan Project", "Asia-Pacific War", "Romantic", "No. 16 Squadron", "AMC Entertainment Holdings, Inc.,", "New York Islanders", "fennec", "1978", "John Surtees", "French", "Pacific Place", "the Matildas", "\"Bad Blood\"", "\"is a song recorded by American singer-songwriter Justin Timberlake", "5320 km", "Andrea Maffei", "Jayalalithaa", "Sacramento Kings", "Walldorf", "Fife", "Fyvie Castle", "Faysal Qureshi", "the British Army", "80%", "boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Behati Prinsloo", "Saudi Arabia", "Joseph Crowley", "two"], "metric_results": {"EM": 0.453125, "QA-F1": 0.53359375}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-1503", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.453125, "CSR": 0.4850543478260869, "EFR": 1.0, "Overall": 0.6811514945652174}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican National Committee's website address is GOP.com", "18 February", "5", "Greenland sharks", "The Word", "Abraham Lincoln's", "St Jude", "Tasmania", "the Death Penalty", "xerophyte", "Jackie Robinson", "Brooklyn", "Dian Fossey", "MI5", "Harrow", "cr\u00e8me anglaise", "onions", "pork", "curling", "Victoria Coren Mitchell", "Gettysburg", "Chile", "Majorca", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "President Obama", "Canada", "O-s-c-a-are", "Cuba", "David Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "Spain", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "Daily Herald", "numerous", "halal", "2016", "the Supreme Court of Canada", "2017", "Chief of Protocol", "Diamond White", "1944", "Daniel Nestor, from Canada,", "Jeddah, Saudi Arabia,", "death of a pregnant soldier", "Beatrix Potter", "George Stephanopoulos", "Reader's Digest", "the king"], "metric_results": {"EM": 0.578125, "QA-F1": 0.63359375}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-4874", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-3458", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-6488"], "SR": 0.578125, "CSR": 0.48703457446808507, "EFR": 1.0, "Overall": 0.681547539893617}, {"timecode": 47, "before_eval_results": {"predictions": ["horse", "allergic reaction", "alex turner", "florida", "Runic", "Spain", "tennis", "alex korchhoff", "rotherham United", "heat transfer", "Misery", "Styal", "olek", "Blind Beggar", "Brainwash", "floroy Burrell", "parlophone", "Wild Atlantic Way", "hk", "Unseen Academicals", "alex blyton", "Lackawanna Six", "Brazil", "a domino game", "muezzin", "a window", "strake", "flaubert", "Apollo 11", "flit", "Nikola Tesla", "Nicky Henderson", "Evita", "albino sperm", "robert", "east fife", "St Pancras International Station", "social environment", "pre sliced bread", "dilbert", "casterbridge", "nunc dimittis", "French", "medea", "Burgundy", "cribbage", "paul McCartney", "Johannesburg", "France", "muffin man", "South Korea", "Prince James, Duke of York", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Galveston, Texas,", "carrier based in Texas.", "Robert Frost", "Henry VIII", "flour", "Mitsubishi Lancer"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6130952380952381}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-4781", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.578125, "CSR": 0.48893229166666663, "EFR": 1.0, "Overall": 0.6819270833333333}, {"timecode": 48, "before_eval_results": {"predictions": ["Route 66", "sesame Street", "marcella Hazan", "cabbage", "victoria", "mister magoo", "henchians", "Ash tree", "opossum", "New Zealand", "jimmy", "60", "jimmy", "1983", "frog", "Mongol", "1875", "collector", "pence", "Rod Stewart", "jimmy", "bagram", "maggie", "Chrysler", "ushanka", "jimmy Crighton", "chile", "us", "chile", "jimmy", "biathlon", "Boise", "Charlie Chan", "Vienna", "white", "vertebrates", "paul Rudd", "rabbit", "Scotland", "jimmy", "Orson Welles", "mr Nehru", "shabbat", "Dutch", "Texas", "marmole", "quant pole", "Ding Dong Bell", "Go West", "Rhododendron", "Ireland", "Chuck Noland", "Virginia", "in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park, Kenya", "2010", "Greece", "10 below in Chicago, Illlinois.", "Nearly all", "coins", "American Kennel Club", "Omaha", "Dick & Jane"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5040849673202614}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7058823529411764, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-1594", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-6306", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-11366"], "SR": 0.4375, "CSR": 0.48788265306122447, "EFR": 0.9722222222222222, "Overall": 0.6761616000566895}, {"timecode": 49, "UKR": 0.65234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.82421875, "KG": 0.42265625, "before_eval_results": {"predictions": ["shanghai", "Iran", "tobacco", "fr an-c", "jockey", "daniel Boone", "Thames Street", "the youngest president of the United States", "satyrs", "a fish", "libretto", "Microsoft", "wishbone", "garrick club", "Lackawanna 6", "barnaby Rudge", "soprano Susan Bullock", "the American Civil War", "noir", "dyan Cannon", "jimmy Robertson", "Florence", "dharm", "veruca salt", "shrewsbury", "Australian", "South Africa", "Bermuda", "Nicaraguan", "Churchill", "war of roses", "Chemnitz", "of spring training", "mackinaw", "ap\u00e9ritif", "Ken Norton", "Belize", "folklorist", "alopecia", "derny", "charles darlings", "Robin Hood", "Chris Martin", "Flintstones", "jennifer Ingleby", "rugby", "honda", "stanley", "3", "tobacco", "heifer", "free floating", "American Graffiti, Star Wars and The Empire Strikes Back", "New Orleans", "a pinball machine", "Texas Tech University", "leicestershire", "Herman Cain", "the United States", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\"", "George Babbitt", "Oklahoma", "Elaine Stritch", "four"], "metric_results": {"EM": 0.40625, "QA-F1": 0.451467803030303}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727274, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.40625, "CSR": 0.48624999999999996, "EFR": 0.9736842105263158, "Overall": 0.6718305921052632}]}