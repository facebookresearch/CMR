{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2240, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "the Grand Annual Steeplechase at Warrnambool", "the Paleoproterozoic eon", "the end", "1894", "French Rhin", "the Pacific", "the highest quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "teaching", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "from \u00a315\u2013100,000", "the Purus Arch", "the infected corpses", "the United Kingdom, Australia, Canada and the United States", "11", "forces", "in series 1", "the chief electrician position", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "outrage from child protection and parental rights groups", "the Masovian Primeval Forest", "the days, weeks and months after it happened", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "the problem of squaring an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "his granddaughter Susan Foreman", "Killer T cells", "Mr Ratti's solvent and varnish business", "More than 1 million", "2011", "inequality is driven by this price", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Buddhism", "Matthew 16:18", "the U.S. ship that was hijacked off Somalia's coast", "Wwanda", "the three-day festival has been canceled", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7017754814629815}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-7566", "mrqa_squad-validation-9243", "mrqa_squad-validation-9530", "mrqa_squad-validation-2052", "mrqa_squad-validation-7763", "mrqa_squad-validation-2732", "mrqa_squad-validation-4276", "mrqa_squad-validation-7728", "mrqa_squad-validation-1285", "mrqa_squad-validation-2520", "mrqa_squad-validation-2035", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-7717", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.578125, "CSR": 0.703125, "EFR": 0.9629629629629629, "Overall": 0.8330439814814814}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50% of the population to die", "mountainous areas", "the coast of Denmark", "quantum mechanics", "On Tesla's 75th birthday in 1931", "Distinguished Service Medal", "30", "Virgin Media", "the destruction of Israel and the establishment of an Islamic state in Palestine", "its main method of locomotion", "each six months", "Japanese", "the Electorate of Saxony", "Mark Twain", "the European Parliament and the Council of the European Union", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "a multi-party system", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "its compilation of a vast institutional compendium named Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two-page", "the Arizona Cardinals", "1991", "Chaffee", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "in the coming decades, pharmacists are expected to become more integral within the health care system", "declare martial law and sent the state militia to maintain order", "a customs union, and the principle of non-discrimination", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ricky Martin \u2013 \"Livin' la Vida Loca / The Cup of Life / Mar\u00eda\"  * Spice Girls", "the ten stages of corporate life cycle, starting with Courtship and Infancy and ending in Bureaucracy and Death?", "the company's factory in Waterford City, Ireland", "nitrogen", "Nolan, WB Reteam for Sci-Fi Actioner Inception, and a Producers Guild of America Award, with Thomas and Charles Roven.", "Haraboard", "six Oscars, including Best Picture, Best Actor in a Leading Role (Hanks) and Best Director (Robert Zemeckis)", "It always begins with the music, of course. The tune sticks with you long after the song is over; the sort of tune that makes it almost impossible to sit still", "Natural Opera  Rigoletto  TV Series performer", "Illinois", "Rafael Palmeiro Corrales", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7561076423576423}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4615384615384615, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.09999999999999999, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5067", "mrqa_squad-validation-1637", "mrqa_squad-validation-9600", "mrqa_squad-validation-4437", "mrqa_squad-validation-4546", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-8316", "mrqa_squad-validation-8159", "mrqa_squad-validation-7949", "mrqa_squad-validation-235", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936"], "SR": 0.6875, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "christopher saints", "Britain", "23", "a shortage of male teachers", "charleston", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President", "2000", "oxygen", "increase local producer prices by 20\u201325%", "the Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "multiple revisions", "the 50 fund", "integer factorization problem", "necessity", "Isel", "adapted quickly and often married outside their immediate French communities", "travis", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "charlasts", "the late 19th century", "the Channel Islands", "in no way", "Alberich", "charleston", "travis", "Churchill Downs", "charleston", "charleston", "1951", "Colombia", "study insects and their relationship to humans", "travis", "travis", "George Fox", "Washington, DC", "charleston", "24 hours a day and 7 days a week", "Sponsorship scandal", "\"Krabby Road\""], "metric_results": {"EM": 0.65625, "QA-F1": 0.6750868055555556}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-2032", "mrqa_squad-validation-801", "mrqa_squad-validation-9286", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-1581", "mrqa_hotpotqa-validation-3821"], "SR": 0.65625, "CSR": 0.6875, "EFR": 0.9545454545454546, "Overall": 0.8210227272727273}, {"timecode": 4, "before_eval_results": {"predictions": ["in plants that contain them", "Parliament of Victoria", "Zaha Hadid", "Fort Edward", "Science and Discovery", "Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "Battle of Olustee", "at the port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "Roman Catholic", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "In 1700 several hundred French Huguenots migrated from England to the colony of Virginia", "apicomplexan-related", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "freely available on the web", "the Song dynasty", "2010", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "guzzanti", "22", "terror groups", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "new model is simply out of their reach", "Muslim", "will auction off one of the earliest versions of the Magna Carta later this year", "a unit of Time Warner", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "FBI Special Agent Daniel Cain", "mike atherton", "one", "celebrity-inspired", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "mike atherton"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6979040138782786}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15999999999999998, 1.0, 1.0, 0.0, 0.8181818181818181, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-7094", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3195", "mrqa_squad-validation-3733", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.671875, "CSR": 0.684375, "EFR": 1.0, "Overall": 0.8421875}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a data network based on this voice-phone network", "roughly 500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches", "2011", "algae", "a way of reminding their countrymen of injustice", "June 1978", "Milton Latham", "1914", "Philippines", "Denver's Executive Vice President of Football Operations and General Manager", "the 1970s", "the political and economic advantage over a land and the indigenous populations they control,", "German Te Deum", "1795", "Bermuda 419", "air could be liquefied, and its components isolated, by compressing and cooling it", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "rudimentary immune system, in the form of enzymes that protect against bacteriophage infections", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "It is mainly for the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "an Ohio newspaper on 8 February 1925", "Herbert Hoover", "cannonball", "Panning", "Justin Timberlake", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey", "the un security council get troops for military actions", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "three", "9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "the present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past ; but some episodes are set in one time period or use multiple flashback time periods", "Morgan Freeman", "David Gahan", "The Stanley Hotel", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "Jonas", "Jaipur", "Jonas Olsson", "a small and fast naval ship designed to carry torpedoes into battle", "garland"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6879869599354894}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.56, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.45454545454545453, 0.888888888888889, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-376", "mrqa_squad-validation-9908", "mrqa_squad-validation-3473", "mrqa_squad-validation-6450", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.578125, "CSR": 0.6666666666666667, "EFR": 0.9259259259259259, "Overall": 0.7962962962962963}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "always more expensive than their public counterparts", "an antigen from a pathogen", "their disastrous financial situation", "making home craft tools, mechanical appliances, and the ability to memorize Serbian epic poems", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "\"social and political action,\" declared that \"To perform its mission in the society, a university must sustain an extraordinary environment of freedom of inquiry and maintain an independence from political fashions, passions, and pressures.\"", "1936", "New Birth", "gold", "a deficit", "178 Vivienne Westwood", "reduction", "disease", "GIIF", "Confucian propriety and ancestor veneration", "rediscovery of \"Christ and His salvation\"", "five", "the European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live and 5 Live Sports Extra", "1876", "a chain or screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "members, conducting hearings into allegations of professional misconduct and taking appropriate disciplinary action and accrediting teacher education programs", "end of the season", "10", "Jonas", "African-Americans", "will not support the Stop Online Piracy Act", "Chuck Bass", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said.", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Stuttgart on Sunday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "North Korea", "the first five Potter films have been held in a trust fund which he has not been able to touch.", "their cars have chosen their rides based on what their cars say about them", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "a \"stressed and tired force\" made vulnerable by multiple deployments", "James Whitehouse, has been quoted as saying she has terminal brain cancer, according to a blog called Manson Family Today.", "instability in the Maersk Alabama is being held by pirates on a lifeboat off Somalia.", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Jody Rosen of Rolling Stone", "Mike Gatting", "a private liberal arts college", "Church of Christ, Scientist", "a fat or fatty acid in which there is at least one double bond within the fatty acid chain", "New Testament"], "metric_results": {"EM": 0.53125, "QA-F1": 0.619559685965694}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.4444444444444445, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.2, 0.5, 1.0, 0.5, 0.25, 0.3636363636363636, 0.13333333333333333, 1.0, 0.16666666666666669, 0.0, 0.16, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.45454545454545453, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7114", "mrqa_squad-validation-1255", "mrqa_squad-validation-7950", "mrqa_squad-validation-5441", "mrqa_squad-validation-6001", "mrqa_squad-validation-486", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_naturalquestions-validation-7683", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.53125, "CSR": 0.6473214285714286, "EFR": 1.0, "Overall": 0.8236607142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["the 1970s", "his friendship", "increased", "187 feet", "pH or available iron", "90\u00b0", "materials", "$100,000", "Stanford Stadium", "baptism", "Jim Gray", "PSPACE", "July 1969", "hiding a Jew in their house", "prolamellar body", "spontaneous", "the courts of member states", "gold", "Time Lord", "Buckland Valley", "Scottish rivers", "ricks for Warsaw", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "their \"Freshman Year\" experience", "United States", "Benazir Bhutto,", "Lindsey oil refinery", "April 24", "Krishna Rajaram,", "early detection and helping other women cope with the disease.", "as many as 250,000", "Timothy Masters,", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jason Chaffetz", "William S. Cohen", "\"Dancing With The Stars\"", "some Guant Bay detainees", "Matthew Fisher", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "seeking help", "Japanese officials", "patrolling the pavement", "\"Empire of the Sun\"", "the Norman given name Robert", "the Olympics", "Matthew Ward Winer", "lenardo", "the Baltic Sea", "Mustique", "green"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6304533140470641}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.45454545454545453, 0.0, 1.0, 0.5333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.5714285714285715, 1.0, 0.3636363636363636, 0.4, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.05555555555555555, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-4067", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-8883", "mrqa_squad-validation-3938", "mrqa_squad-validation-7587", "mrqa_squad-validation-872", "mrqa_squad-validation-1556", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_searchqa-validation-7977"], "SR": 0.53125, "CSR": 0.6328125, "EFR": 0.9666666666666667, "Overall": 0.7997395833333334}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "that any French residents who chose to remain in the colony would be given freedom to continue worshiping in their Roman Catholic tradition, continued ownership of their property, and the right to remain undisturbed in their homes", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday afternoon", "\"Journey's End\"", "immediate", "Levi's Stadium", "decidedly Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "in variety of ways in different languages", "Aristotle and Archimedes", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th most-populous", "random access machines", "ensure that the prescription is valid", "the Stockton and Darlington Railway", "autonomy", "it is \"the largest center for breeding and exporting terrorism.\"", "$12.9 million", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie business.", "finance", "terminal brain cancer", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "the Employee Free Choice act", "it was to be reunited with his American father.", "Animal Planet", "fake his own death by crashing his private plane into a Florida swamp.", "there were no radar outages and said it had not lost contact with any planes during the computer glitches.", "54 bodies", "early detection", "Diversity", "$250,000", "break up ice jams.", "Nazi Germany", "March 27 at 4:30 p.m. ET", "The Kirchners", "involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "as soon as 2050", "Alfredo Astiz,", "Abdullah Gul,", "Briton Carl Froch", "Everglades,", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans", "Henrik van Oosting's", "\"The perfect is the enemy of the good\"", "that you were married to a very brilliant man?", "give away T.J. Oshie, a former first-round"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6575881736727325}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.6538461538461539, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-6300", "mrqa_squad-validation-10341", "mrqa_squad-validation-5586", "mrqa_squad-validation-1083", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.578125, "CSR": 0.6267361111111112, "EFR": 0.9629629629629629, "Overall": 0.794849537037037}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "\"hoos\"", "50%", "\"All I can say is that the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English.", "San Diego", "CRISPR", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "issues related to the substance of the statement", "the Edict of Fontainebleau", "15", "\"The U.S. program to assassinate terrorists in Iraq.", "Ronaldinho", "\"global security, prosperity and freedom.\"", "25", "a treadmill", "the couple's surrogate lost the pregnancy.", "environmental and political events", "\"he was preparing the country for war and death, and to hand power to Kim Jong Un,\"", "at least two and a half hours", "Elin Nordegren", "New York City", "6,000", "a drug test after a Serie A game at Roma which returned a positive result.", "President Clinton", "\"two sides to every story -- sometimes three\" and he was confident the legal system would work in Harris' favor.", "MDC head Morgan Tsvangirai.", "\" policing the world and Africa in particular?\"", "future relations with Washington", "a canyon in the path of the blaze", "Zuma", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail,", "school, their books burned,", "strife in Somalia", "Ayelet Zurer and Ewan McGregor", "Columbia Police Department.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "that a UH-60 Blackhawk helicopters crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen.", "London", "after Shawn's kidnapping", "a people", "William Tell", "OutKast", "Groundhog Day", "Cleopatra", "a fairground"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6147721723824664}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.1081081081081081, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16, 0.0, 0.0, 0.0, 0.8333333333333333, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-10185", "mrqa_squad-validation-2429", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-11812"], "SR": 0.546875, "CSR": 0.61875, "EFR": 0.9655172413793104, "Overall": 0.7921336206896552}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley, New York", "pseudorandom", "John Wesley", "Genghis Khan's", "heating water to provide steam that drives a turbine connected to an electrical generator", "internal strife", "yellow fever outbreaks", "DC traction", "The Prince of P\u0142ock,", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital.", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "July 18, 1994", "broken pelvis", "issued his first military orders as leader of North Korea", "heavy snow and ice", "Gainsbourg", "\"Maude\"", "Phillip A. Myers", "Korea", "two weeks after Black History Month", "58", "two Metro transit trains", "last summer.", "Noriko Savoie", "Spc. Megan Lynn Touma", "Hyundai Steel", "Sharp-witted. Direct. In control. Loyal.", "Chinese President Hu Jintao", "Christian", "injuries,", "October 3,", "is back at his Premier League club side Wigan Athletic in northern England.", "Larry Zeiger", "shock", "President Bush", "\"green-card warriors\"", "2,800", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Stanton", "bankruptcy", "16 August 1975", "Bonnie Aarons", "one", "is is the German Food Guide", "is a 2016 American-Indian-Irish computer-animated comedy-adventure film directed by Trevor Wall", "James Lofton", "is a glossary of spirituality terms", "whip-like"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5776405885780885}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.18181818181818182, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-3418", "mrqa_squad-validation-1299", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-1551", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.515625, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.8046875}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "Victoria Department of Education", "the FBI ordered the Alien Property Custodian to seize", "Manned Spacecraft Center", "higher economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks,", "Elway", "Philo", "36", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "the number 1 lacks", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "Agnes Wickfield", "the Clangers", "antelope", "nipples", "the Triassic", "Pio-  neers' Society, Ltd.", "Anastasia Dobromyslova", "gagapedia", "324", "Space Jam 2", "Radish", "Robert Ludlum", "giant grubs", "fltoff", "the largest showcase of Grand Prix racing cars in the world", "the film \u2018 Waynes World\u2019", "Hebrew", "The London Underground Piccadilly Line", "United States", "orangutan", "Manet", "The Magic Finger", "the US Constitution", "2005", "1971", "the \"Dodge Brothers\"", "dolt", "Venice", "a peplos", "Enrico Caruso", "florence Nightingale Graham", "collapsible support assembly", "Hardy Amies", "the Marshall Islands", "Wales", "Can't Get You Out of My Head", "Blake Pieroni", "Bloomingdale Firehouse", "the site of one of World War II's most notorious death camps.", "the Golden Gate Yacht Club of San Francisco", "Roger Vivier", "\"No woman, no cry\"", "Buddhism"], "metric_results": {"EM": 0.5, "QA-F1": 0.5570670215201465}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.8750000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6153846153846153, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983", "mrqa_searchqa-validation-13120"], "SR": 0.5, "CSR": 0.6002604166666667, "EFR": 0.96875, "Overall": 0.7845052083333334}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "70-50's", "Panini", "Bills", "anti-colonial movements", "Seeztal valley", "bacteria generate surface proteins that bind to antibodies", "suspicious of even the greatest thinkers and to test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor- patient relationship", "the case of an express wish of the people to withdraw from the EU", "1788", "Saturday", "a Roman Catholic archdiocese", "the Edict of Nantes", "John Wesley", "the Italian nationalisation law conflicted with the Treaty of Rome", "the Eternal Heaven", "Suffolk County Council and Waveney District Council", "Jessica Simpson", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "iceland", "saturday", "Vladivostok", "Sheryl Crow", "farthingale", "Camellia", "AFC Wimbledon", "Bob Monkhouse and Kenneth Connor", "Malaysia", "astronomy", "iceland", "George Clooney", "alfred hawthorne", "James Chadwick", "\"Father McKenzie writing the dirt off his hands as he walks from the grave\"", "Monopoly", "champagne", "an extended period of abundant rainfall lasting many thousands of years", "the United States", "Brigit Forsyth", "iceland", "sino-Japanese", "hawthorne", "david hawthorne", "Kent", "alfred hawthorne", "saturday", "white", "Switzerland", "gin", "the people of the United States", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Jason Voorhees", "state and municipal budgets", "David", "\"The Screening Room\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5426034902597403}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.75, 0.0909090909090909, 1.0, 0.0, 0.8, 0.3333333333333333, 1.0, 0.3, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9126", "mrqa_squad-validation-6655", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-5827", "mrqa_squad-validation-917", "mrqa_squad-validation-3161", "mrqa_squad-validation-4590", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-2501", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_newsqa-validation-3207", "mrqa_searchqa-validation-8450", "mrqa_newsqa-validation-3860"], "SR": 0.484375, "CSR": 0.5913461538461539, "EFR": 0.9696969696969697, "Overall": 0.7805215617715617}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "Huguenots had their own militia", "after the end of the Mexican War", "61", "quality of a country's institutions and high levels of education", "cilia", "gravity", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron", "thomas", "thomas", "albinism", "Straits of Tiran", "Brigit Forsyth", "call My Bluff", "March 10, 1997", "cuddly new pet", "the Battle of the Three Emperors", "Velazquez", "Arthur Ashe", "lizard", "strong cold southwest wind", "table tennis", "edward Smith papyrus", "thomas penhaligon", "thomas goveve", "thomas gove", "Jinnah International", "Monday", "Caracas", "The Loop", "Pears soap", "c Cocktail", "avro", "Genesis", "Charlie Brooker", "herbal tea", "Harrods", "2007", "cher", "scarface", "pale yellow", "thomas edward moult", "bubba", "June 12", "Filipino American", "London", "Lambic", "Kindle Fire", "Steven Green", "commas", "fortune", "principality", "Synchronicity"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6151041666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-6547", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-6628"], "SR": 0.5625, "CSR": 0.5892857142857143, "EFR": 1.0, "Overall": 0.7946428571428572}, {"timecode": 14, "before_eval_results": {"predictions": ["seven months old", "woodblocks", "New Orleans's Mercedes-Benz Superdome, Miami's Sun Life Stadium", "Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "slept after it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "a declining state of mind", "1898", "The Deadly Assassin and Mawdryn", "radioisotope thermoelectric generator", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1936", "Authority", "junior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "World Trade Center", "Kevin Spacey", "1 November", "2.5", "white blood cell in a vertebrate's immune system", "Bangladesh", "President", "minor key", "Hustons", "Chandan Shetty", "sedimentary rock", "October 1, 2014", "United States", "Claims adjuster", "head, neck, a midpiece and a tail", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "10 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "foreheads of participants", "vertebral column", "three", "flowers", "sausages", "kew Gardens", "Nikita Khrushchev", "$500,000", "Alexandros Grigoropoulos,", "Cyrus McCormick", "NYPD Shield", "BBC's central London offices", "a kidney transplant"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6446795203588682}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.608695652173913, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_squad-validation-7670", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-121", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.53125, "CSR": 0.5854166666666667, "EFR": 0.9666666666666667, "Overall": 0.7760416666666667}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "a special episode of The Late Show with Stephen Colbert", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "ESPN Deportes", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "60", "Seattle, Washington", "Battle of Antietam", "Andy Cole and Shearer", "In Time", "6th century AD", "Glenn Close", "four", "Agostino Bassi", "five", "a beach in Malibu, California", "Paul", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Sam Waterston", "Bhupendranath Dutt", "Darkspawn", "Dr. Lexie Grey ( Chyler Leigh )", "Matt Jones", "September 1972", "Uruguay", "Alex Skuby", "Thomas Mttyitch", "National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "wool for trading", "1970s", "Director of National Intelligence", "students", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson", "Thespis", "Portugal", "John Coffey", "Rachel Kelly Tucker", "Czech Republic", "a garage beetle", "Myst3ry", "game designer", "opposition group, also known as the \"red shirts,\"", "the abduction of minors.", "Nevada", "Chile", "a department store company specializing in retailing brand name apparel, accessories, cosmetics, footwear, and housewares", "1881"], "metric_results": {"EM": 0.546875, "QA-F1": 0.652130941974692}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.42857142857142855, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.5, 0.9090909090909091, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-1852"], "SR": 0.546875, "CSR": 0.5830078125, "EFR": 0.9310344827586207, "Overall": 0.7570211476293103}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "the New England Patriots", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "the US", "six", "11", "hydrogen and helium", "the Jin", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland", "Microsoft Office", "SAVE", "Scandinavian Airlines", "1993 to 2001", "February 23, 1898", "the NCAA Division I Football Bowl Subdivision", "Martin Lee Truex", "Easter Rising", "45% male", "more than two decades", "BAFTA TV Award", "a rash", "the Battle of Culloden", "Burny Mattinson", "Sir William McMahon", "the North Sea coast", "The 7.63\u00d725mm Mauser (.30 Mauser Automatic) round", "the Academy Award for Best Animated Feature", "the Pakistan Aeronautical Complex (PAC)", "Delacorte Press", "Neighbourhood", "Secretariat", "Wake Island", "hydrogen vehicle", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "\"Southern Living\" Reader's Choice Awards", "Thomas Harold Amer", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Amway", "Capitol Records", "South Africa", "Surrey", "The Girl in the romantic comedy \"My Sassy Girl\"", "Charles Russell", "Boyd Gaming", "shared Anthony Davis of the New Orleans Pelicans", "1989", "Glenn Close", "Florence Welch", "Neighbours", "Ewan McGregor", "2011", "pippa passes", "the leader of the late insurrection in Southampton, Virginia", "power-sharing talks", "Brown-Waite"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6469640602453102}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 0.25, 0.2857142857142857, 0.4, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-76", "mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655"], "SR": 0.484375, "CSR": 0.5772058823529411, "EFR": 1.0, "Overall": 0.7886029411764706}, {"timecode": 17, "before_eval_results": {"predictions": ["force of gravity acting on the object balanced by a force applied by the \"spring reaction force\"", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese imports became mass-market leaders with unibody construction and front-wheel drive,", "charter status", "1830", "nonfunctional pseudogenes", "the inner mitochondria membrane", "Charlie welch", "Stevie Wonder", "beaver", "\u201cThe Passage of the Red Sea.\u201d", "formic acid", "Toledo", "Zimbabwe", "\"Pressure of Speech\"", "Ted Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "MUDs", "Mercury", "gazelle", "Xenophon", "London Pride", "the Plimsoll line", "Nick Hornby", "\"The Comedy of Errors\"", "Charles V", "England", "welch", "Olympic Barbell", "\"big house\"", "Hadrian", "California", "typhus", "Birmingham Humphries", "Hamburg", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "Manchester United", "Prokofiev", "Jessica Simpson", "Culture Club", "Finland", "3000m race", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "Confederate", "New Jewel Movement", "Africa", "north-south highway at the crash scene in White Hills, Arizona, was shut in both directions.", "Anjuna beach in Goa", "Marius Petipa", "Oshkosh", "\"Papa's\"", "\"The World\"", "\"The Sunday Thing\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6211397058823529}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false], "QA-F1": [0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10351", "mrqa_squad-validation-3711", "mrqa_squad-validation-7089", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_hotpotqa-validation-1658", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.546875, "CSR": 0.5755208333333333, "EFR": 0.9655172413793104, "Overall": 0.7705190373563218}, {"timecode": 18, "before_eval_results": {"predictions": ["Jacksonville's low latitude", "1622", "high", "Manakintown", "northwest", "10 employees", "Middle Miocene", "new magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "2018", "The Hustons", "Allison Janney", "the Isthmus of Corinth", "ability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Alison", "1993", "19 state rooms", "Solange Knowles", "Gupta Empire", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Elms", "grades 1", "The Vamps", "2018", "the problems", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "the aorta", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "png HTTP / 1.1", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "universal significance", "September 2017", "moral", "Rising Sun Blues", "Part 2", "Dumbo", "the \u201cBloody Assizes\u201d of 1685", "Christian", "Robert L. Stone", "2008", "Jeddah", "Mentor", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.5, "QA-F1": 0.6029241134859741}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.08695652173913042, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7213", "mrqa_squad-validation-3193", "mrqa_squad-validation-6937", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_newsqa-validation-1493", "mrqa_hotpotqa-validation-4735"], "SR": 0.5, "CSR": 0.571546052631579, "EFR": 0.96875, "Overall": 0.7701480263157895}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law, even if it is Christ's life, Christ's death for sin, or God's goodness experienced in creation", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "1965", "270,000 tonnes", "Long troop deployments", "Joe Pantoliano", "the girl's stepmother, a key witness in the case, his attorneys told HLN's \"Nancy Grace.\"", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "eight in 10", "sovereignty over them.", "Tuesday in Los Angeles. Cole's search for a new kidney ended this week when someone with a compatible organ died and their family asked that it be given to the singer, according to the organ procurement group that handled the donation.", "forgery and flying without a valid license", "Anil Kapoor", "55-year-old Hogan's three-decade career, during which he held multiple championship titles and, during his heyday in the 1980s, was easily the most popular wrestler in the world.", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "unwanted baggage from the 80s", "The Louvre", "snowstorm", "Ferraris, a Lamborghini and an Acura NSX", "a lizard-like creature from New Zealand -- is now a dad.", "Mutassim", "her \"Nothing But Love\" comeback tour, her publicist said Wednesday.\" Doctors visited Whitney late last night in Paris and confirmed that she was suffering from an upper respiratory infection,\" a statement issued Wednesday said.", "\"Steamboat Bill, Jr.\"", "Russia's Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "alcohol", "Atlantic Ocean", "Ahmed,", "cortisone.", "fortune", "U.S. 93 in White Hills, Arizona, near Hoover Dam.", "Al Gore", "Manmohan Singh", "Michael Jackson", "be silent", "40 militants and six Pakistan soldiers dead, said military spokesman Gen. Athar Abbas.", "Roger Federer", "credit card information", "Louisiana", "the Southeast", "wife,Misty Cummings, then known asMisty Croslin,", "physicist Steven Chu as secretary of energy, and former EPA administrator Carol Browner to a new post in the White House to coordinate energy and climate policy.", "\"A Mother For All Seasons.\"", "the infant who became the center of an international end-of-life debate, died peacefully in his sleep at his Windsor, Ontario, home, a spokesperson for the family said Wednesday.", "back at work", "the initial necropsy or animal autopsy.", "27", "Amber Riley and her partner Derek Hough", "Thomas Jefferson", "Borsht (Borsch)", "Zager and Evans", "Bobby Hurley", "fourth term", "Adult Theatre", "the Ironsides", "Lapland", "1937", "Emad Hashim"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5170170703887809}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.05405405405405406, 1.0, 1.0, 0.0, 0.1904761904761905, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.631578947368421, 0.6666666666666666, 0.30769230769230765, 0.0, 1.0, 0.0, 0.14814814814814814, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_squad-validation-3028", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922"], "SR": 0.421875, "CSR": 0.5640625, "EFR": 0.972972972972973, "Overall": 0.7685177364864866}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death Wish Coffee", "L", "Cameroon,", "1994", "ballots", "a new fabric technique", "three empty vodka bottles,", "Eikenberry sent private cables to Obama last week,", "1959", "Di Montezemolo", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "\"Mandi is not a dangerous weapon.\"", "the composer of \"Phantom of the Opera\" and \"Cats\" and one of Britain's richest men,", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony,", "Amanda Knox's", "over 1,000 pounds", "Iran's development of a nuclear weapon", "a welcoming, bright blue-purple", "using recreational drugs", "ceo Herbert Hainer", "his client, Brett Cummins", "a nearby day care center whose children are predominantly African-American.", "inmates", "Cameron-Ritchie", "back on the set at \"E! News\" on Tuesday", "103 children", "jobs", "one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church, saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "a vigilante group whose goal is the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Republicans", "a businessman, team owner, radio-show host and author.", "An undated photo of Alexandros Grigoropoulos,", "a power-sharing deal with the opposition party's breakaway faction,", "a Prius", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Gary Brooker", "the creation of an Islamic emirate in Gaza,", "boogeyman Jason Voorhees", "determining which Guant detainees should be tried by a U.S. military commision,", "San Antonio,", "a guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "50", "Ku Klux Klan", "1939", "Branford College", "Wigan", "husbands", "Malayalam", "August 17, 2017", "a jacket, gloves or a briefcase", "mice", "Hodel", "access to US courts", "British rock group Coldplay"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4763163261546882}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.9523809523809523, 0.0, 0.14285714285714288, 0.23076923076923078, 1.0, 0.8, 0.8571428571428571, 0.28571428571428575, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.41379310344827586, 0.0, 0.0, 1.0, 0.9333333333333333, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.19047619047619047, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987"], "SR": 0.359375, "CSR": 0.5543154761904762, "EFR": 0.975609756097561, "Overall": 0.7649626161440186}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "the Treaties establishing the European Union", "\u201c Splash\u201d", "Nicola Adams", "copper and zinc", "eagle with extended wings", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "shoulders", "Madonna's", "Glasgow", "GPS receivers", "Australia", "glaze", "Pearson PLC.", "roch", "American Civil War,", "Loch Ness", "rochner", "roch", "a gentle cat with a somewhat shy nature around strangers.", "China", "Harrisburg", "roch", "glockenspiel", "Dr Sentamu", "women", "roch", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "charlemagne", "the A's west to Oakland,", "roch Crowe", "roch", "roch", "rochon", "roch Butler", "chamomile tea", "Ireland", "tarn", "Michel", "Albert Square", "Newbury", "the Old Testament", "70 million people", "Target Corporation", "\"The Omega Man\"", "Michelle Rounds", "Jackson told her that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "(INGO)", "John Jackson Dickison", "prisoners' rights and better conditions for inmates,", "Oprah Winfrey.", "his mother"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5376420454545454}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-4045", "mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_newsqa-validation-2971", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.453125, "CSR": 0.5497159090909092, "EFR": 0.9428571428571428, "Overall": 0.746286525974026}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials, and because they are a matter of having sufficient funds at a specific time, can arise even when the overall total is enough.", "Vicodin,", "london", "rusy", "pearl", "Utah Territory", "Carrie Underwood", "liqueur", "he made his horse a consul, his palace a brothel, and his...", "Google", "Langston Hughes", "pain tolerance", "samuel kramm", "puck", "riata", "it means to be 'waxing philosophical'?", "london", "rhodesian ridgebacks", "David Beckham", "arturo Toscanini", "economics", "Miracle in the Andes: 72 Days on the Mountain and my...", "proscenium arch", "Montenegro", "discus", "puck", "basidiomycota", "james", "puck", "Idi Amin", "deere", "a body, body part, or personal object", "hard clay", "plutarch", "Rudy Giuliani", "masa", "40 seconds", "the Vikings", "And to Think That I Saw It on Mulberry Street", "london", "typhoid fever", "river valley", "london", "Williamsburg", "\"OO\" 7- LETTER", "tualatin", "hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off", "jen Knox", "the internal reproductive anatomy", "more than $1 billion worldwide", "risk factors for disease and targets for preventive healthcare", "jape", "Tesco", "london and North Eastern Railway", "russell hill", "the Battelle Energy Alliance", "IT products and services,", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "$10 billion", "Trenton, Florida"], "metric_results": {"EM": 0.3125, "QA-F1": 0.42867133266350016}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 0.5652173913043478, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.28571428571428575, 0.14285714285714285, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_squad-validation-6887", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-16854", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1118", "mrqa_hotpotqa-validation-4804", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-1997"], "SR": 0.3125, "CSR": 0.5394021739130435, "EFR": 0.9772727272727273, "Overall": 0.7583374505928854}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Doritos", "Atlantic Ocean", "domestic cat", "the daughter of Tony Richardson", "Switzerland", "Argonauts", "prometheus", "the Altamont Speedway Free Festival", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "a MUDs", "Italy", "Khaki", "a magma", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "Mendip", "John McCain", "the Earth", "Nafea Faa Ipoipo", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "radar", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "a typhone", "Rangers", "business", "Adidas", "Snarked", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "the unoccupied square", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "Tennis Channel", "fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7383272058823529}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-749", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-458"], "SR": 0.703125, "CSR": 0.5462239583333333, "EFR": 1.0, "Overall": 0.7731119791666666}, {"timecode": 24, "before_eval_results": {"predictions": ["limited coercion", "the chosen machine model", "20th Century Fox", "1997", "a suite of network protocols", "Christopher Savoie", "11 to 12 year old", "the first home series defeat on Australia in almost 16 years", "the Pacific Ocean territory of Guam", "the gun in Christofi's hand, and even jumped in his pool,", "11", "change course", "Alwin Landry's supply vessel", "the 3rd District of Utah", "money or other discreet aid for the effort if it could be made available,", "Sarah Brown", "boatlift", "environmental", "Brazil", "the Taliban", "Saturday", "38", "70,000 or so", "the Single European Sky initiative", "\"E! News\"", "former Boca Juniors teammate and national coach Diego Maradona", "Steve Williams", "new restaurant", "crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan", "theory", "2008", "Diego Maradona", "Dog patch Labs Europe", "club-themed movies", "two", "Mississippi", "the former Massachusetts governor", "the EU naval force", "Plymouth Rock", "Liza Murphy,", "the nomination of Sonia Sotomayor", "police", "former U.S. secretary of state", "At least 33 people", "Samir Kuntar", "get better skin, burn fat and boost her energy.", "contraband", "how sociopathic brains develop", "Alwin Landry's", "Krishna Rajaram", "Sunday", "death and destruction,", "an Irish feminine name", "southwestern Colorado and northwestern New Mexico", "2018", "northern irish", "a(nd) r(anging)", "art", "the point guard position", "23", "Brazil", "freestyle", "the nightingale", "Belief"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4589968191530691}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5555555555555556, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.4, 0.0, 0.15384615384615383, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-610", "mrqa_squad-validation-4673", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-1545", "mrqa_searchqa-validation-3826"], "SR": 0.34375, "CSR": 0.538125, "EFR": 1.0, "Overall": 0.7690625}, {"timecode": 25, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.83203125, "KG": 0.43984375, "before_eval_results": {"predictions": ["50th anniversary special", "Thomas Savery", "Vicodin,", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago onward", "a violent separatist campaign", "36", "269,000", "The Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "38 feet", "Eintracht Frankfurt", "150", "a man had been stoned to death by an angry mob.", "Russian bombers", "41,", "Irvine,", "super-yacht designers", "137", "Kurdish militant group in Turkey", "3-2", "autonomy", "the shoreline of the city of Quebradillas.", "the Russian air force", "16", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "The mysterious disappearance of Flight AF 447 over the Atlantic Ocean has fueled speculation among aviation experts about what caused the state-of-the-art airliner to come down.", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "Many nutritionists actually caution against using injectable vitamin supplements because the quantities are not regulated.", "Tom Baer", "militants", "say these planning processes are urgently needed and have been a long time in coming.", "aikini", "Brian Mabry", "completely changed the business of music,", "Sunday", "60 euros -- $89 --", "Former detainees", "\"His treatment met the legal definition of torture.", "Some truly mind-blowing structures", "Crista", "he was one of 10 gunmen who attacked several targets in Mumbai", "2006", "San Diego,", "Emily Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "The Taliban has threatened to kill Bergdahl if foreign troops continue targeting civilians in the name of search operations in Ghazni and Paktika provinces,", "\"#JustSayin\"", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "\"bad guys\"", "spine", "Hyderabad", "between the Mediterranean Sea to the north", "to stay, abide", "Las Vegas", "Jackson Pollock", "wye", "McComb, Mississippi", "Janis Lyn Joplin", "King Duncan", "plutons", "a car, flatiron, racecar, shoe", "\"absorb\" (to choose, select) c.1200;"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4792868493091016}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.8, 0.0, 0.0, 0.3157894736842105, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.29411764705882354, 1.0, 1.0, 0.0588235294117647, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9, 0.1818181818181818, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9432", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-947", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2550", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-1982", "mrqa_searchqa-validation-11832", "mrqa_searchqa-validation-9476"], "SR": 0.390625, "CSR": 0.5324519230769231, "EFR": 1.0, "Overall": 0.7030528846153847}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "Karthik Rajaram", "43,000", "Booches Billiard Hall,", "finance", "by businessman Ross Perot.", "Hong Kong's Victoria Harbor", "2002", "people (clockwise from top left): Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "legitimacy of that race.", "think are the best.", "Fifty-two people and the four bombers", "Monday", "Scarlett Keeling", "two years,", "84-year-old", "regulators in the agency's Colorado office received improper gifts from energy industry representatives and engaged in illegal drug use and inappropriate sexual relations with them.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "in July", "Akshay Kumar", "Rihanna,", "\"The FARC statement dated February 11 saying the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation", "Phil Spector", "Kim Il Sung", "1994", "suicide attacks,", "Friday,", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Democrats", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "older than the industry average,", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "a Yemeni cleric and his personal assistant,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "at 9 a.m.", "same-sex civil unions,", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "6 - 6 with one win against a team from the lower Football Championship Subdivision ( FCS )", "Matt Monro", "Jack Frost", "the innermost digit of the forelimb; thumb", "1925", "over 20 million", "Peoria, Illinois", "Hawaii", "the gemstone", "King Lear", "Ottoman Empire"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5866945207570207}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.72, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586"], "SR": 0.484375, "CSR": 0.5306712962962963, "EFR": 1.0, "Overall": 0.7026967592592592}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab,", "trading goods and services without exchanging money", "alongside Deepwater Horizon", "John Dillinger,", "The remains of Cologne's archive building", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "a single mother with HIV in Brazil,", "Lashkar-e-Tayyiba (LeT),", "$1.5 million", "2006", "Rev. Alberto Cutie", "Los Angeles Angels", "Indian Army", "there's no chance of it being open on time.", "Karen Floyd", "14", "a Starbucks", "BADBUL,\"", "98", "2008", "Gulf of Aden,", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Swat Valley", "South Dakota State Penitentiary", "Iran", "November 26,", "people have chosen their rides based on what their", "July", "Zoe's Ark", "Four Americans", "a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death", "Saluhallen,", "fractured pelvis and sacrum", "Wednesday", "abduction of minors.", "a gun conviction,", "Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "wheat", "Norwood, Massachusetts", "Manchester", "Drowning Pool", "apteka", "Burlington", "beta blockers"], "metric_results": {"EM": 0.625, "QA-F1": 0.7080131673881673}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-436", "mrqa_naturalquestions-validation-6383", "mrqa_triviaqa-validation-3389", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-14535"], "SR": 0.625, "CSR": 0.5340401785714286, "EFR": 1.0, "Overall": 0.7033705357142858}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author of the Fifth Assessment Report", "rule", "1981,", "forgery and flying without a valid license,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force.", "shoot down the satellite", "European Commission", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain's", "seven", "the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry", "Anil Kapoor", "eradication of the Zetas cartel", "The Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "Zen Buddhism, a Buddhist practice founded in India,", "the Dalai Lama's", "Russia", "8 p.m. local time Thursday", "Passers-by", "in her apartment.", "executive director of the Americas Division of Human Rights Watch,", "750", "at least 300", "Matthew Fisher", "The Ski Train", "Big Brother.", "Ozzy Osbourne", "AbdulMutallab,", "some U.S. senators", "inconclusive", "5:20 p.m. at Terminal C", "caused Somalia's piracy problem was fueled by environmental and political events", "$250,000", "100% of its byproducts", "School-age girls", "5,600 people", "a million", "Sen. Arlen Specter", "the hunt for Nazi Gold and possibly the legendary Amber Room will end Friday after the two men leading the expedition had a disagreement.", "Whitmire, a Houston Democrat and the chair of the state senate's Criminal Justice Committee,", "a deceased organ donor,", "bragging about his sex life on television", "a vertebral column ( spine )", "January to May 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the Sidgwick Avenue arts faculty", "Douglas Richard Hofstadter", "The Dark Tower", "American", "Marmee", "Castle Rock", "a tomato"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6508949892951251}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.125, 0.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.06666666666666667, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3692", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.578125, "CSR": 0.5355603448275862, "EFR": 1.0, "Overall": 0.7036745689655173}, {"timecode": 29, "before_eval_results": {"predictions": ["stagnant", "his brother Kusala,", "438,000", "Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict,", "coaxial", "Pakistan A", "Jacksonville Jacksonville", "7", "French", "John Churchill", "1965", "Paris at Charles de Gaulle Airport", "fifth", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven", "Syracuse", "1963", "coca wine", "video game", "Knoxville, Tennessee", "Washington, D.C.", "Gal\u00e1pagos", "Tom Kartsotis", "2017", "Wayman Lawrence Tisdale", "Mexico", "Kolkata", "Northern ireland", "late 19th and early 20th centuries", "movie scripts written by ghost writers, nonfiction books on military subjects, and video games", "22,500", "the Harpe brothers", "Eric Liddell", "23 March 1991", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Films", "Erinsborough", "Marine Corps", "Robert Allen Iger", "Major Charles White Whittlesey", "Spanish", "Virginia", "NBA Slam Dunk Contest", "$10\u201320 million", "the 1970s", "Kennedy Road", "Somerset County, Pennsylvania,", "c.J. Pierce, bassist Stevie Benton and drummer Mike Luce,", "Colin George Blakely", "two Nobel Peace Prizes", "IB Middle Years Program", "Richard Parker", "the southernmost tip of the South American mainland", "royal Egyptian", "allergic reaction", "King Edward VIII,", "3,000 kilometers", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist", "Russia,", "shrimp", "Australia"], "metric_results": {"EM": 0.5, "QA-F1": 0.6128205128205129}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.5, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-4935", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-1144", "mrqa_hotpotqa-validation-1944", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585"], "SR": 0.5, "CSR": 0.534375, "EFR": 1.0, "Overall": 0.7034374999999999}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Germany's position in a Europe", "London", "Dave Thomas", "a farmers' co-op", "Danish", "1903", "From Here to Eternity", "other individuals, teams, or entire organizations", "ten years of probation", "In Pursuit", "Bolton", "Monty Python", "Kansas City Crime Family", "Dirk Werner Nowitzki", "lifetime", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "Springfield, Massachusetts", "British comedian", "Apatosaurus", "in 1885", "American", "Frank Edward Thomas Jr.", "\"Gliding Dance of the Maidens", "Margarine Unie", "Winecoff", "mentalfloss.com", "The Seduction of Hillary Rodham", "2005", "Lambic", "Massive Entertainment", "Argentina,", "Larry Alphonso Johnson Jr.", "Michael Edward \" Mike\" Mills", "veto power", "Joseph E. Grosberg", "Chelsea Does", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "Seattle", "U.S Olympic Trials", "Aston Lower Grounds", "2005", "228", "\"We're opening new chapters. And in fact, because some of this information is so new and it's so different from the way we use to think about the moon,\"", "Post Traumatic Stress disorder", "Copenhagen", "Chief Joseph"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6640719696969697}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6399999999999999, 0.0, 0.0, 0.0, 0.0, 1.0, 0.06060606060606061, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905"], "SR": 0.5625, "CSR": 0.5352822580645161, "EFR": 1.0, "Overall": 0.7036189516129032}, {"timecode": 31, "before_eval_results": {"predictions": ["New York City", "79", "Iceland", "Wyoming", "a huge terrestrial globe", "georgia", "Iowa", "georgia", "Nassau", "nacreous inner shell", "HIV", "Martin Buren", "a network of seven Shink Hansen systems", "La donna e mobile", "aardwolf", "Beijing", "georgia", "Inuk", "San Jose, California", "Yves Saint Laurent", "reindeer", "georgia", "the fleet", "Anna Mary Robertson Moses", "Sailor Moon", "georgia", "georgia", "a georgia bear", "a timeline", "a British-American supergroup consisting of Bob Dylan, George", "elderberries", "georgia", "Milton Berle", "georgia", "Congolese", "a lunar module", "georgia", "Dan Marino", "Mars", "a clownfish", "E/c^2", "The Love Guru", "Las Vegas", "georgia", "Butterflies", "a Connecticut Yankee", "orangutan", "Sonora", "Soothsayer", "Yitzhak Rabin", "Saul's", "Gettysburg", "Jack Gleeson", "Plank", "overland route hypothesis", "Jean Bernadotte", "Portugal", "Graham Bond", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "disrupt the system and cause a blackout.", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "12.3 million"], "metric_results": {"EM": 0.4375, "QA-F1": 0.47318948412698414}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4708", "mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-2051", "mrqa_naturalquestions-validation-5808", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587"], "SR": 0.4375, "CSR": 0.5322265625, "EFR": 1.0, "Overall": 0.7030078125}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "62 acres", "(Martin) Asquith", "40", "Libya", "Shania Twain", "Sheffield Wednesday", "glucagon", "The New York Yankees", "(rapid eye movement)", "green", "Ann Dunham", "Saddam Hussein", "French", "(John Altman)", "Ohio", "Francis Matthews", "photography", "iron", "Noah", "(EBU)", "New", "Sarah Ferguson", "Mercury", "a watt", "(Martin) Lake", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "(Placename)", "his long-suffering wife,", "aged 75 or older", "Jennifer Lopez", "(Lager)", "Annie Lennox", "(Martin) Lawford", "Downton Abbey", "Martina Hingis", "(Mac)Ahern", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "gulliver", "Pomona", "Italy", "The Streets", "Appalachian Mountains", "a black Ferrari", "(reunion of broken parts)", "bears", "Michael Moriarty", "June 1992", "24", "1994", "Campbell Soup Company", "Kirkcudbright", "the soldiers", "cortisone.", "recognizes the importance of Turkey and wants to engage with it from the start.", "(Martin) Cody", "Helvetica", "a lung"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6338541666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-4309", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5228", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-16567"], "SR": 0.578125, "CSR": 0.5336174242424243, "EFR": 1.0, "Overall": 0.7032859848484849}, {"timecode": 33, "before_eval_results": {"predictions": ["hymns", "deadly explosives", "knutsford", "insulin", "yankees", "Hudson Bay", "florida", "Allergic", "stanley", "Gauls", "1947", "Belfast", "wind", "fire", "Robin Hood", "yankees", "Andy Warhol", "london", "jim branley", "stanley", "the solar system", "tomato", "Moldova", "Mitsubishi", "a hummingbird", "rusedski", "Estimate", "baroudeur-rouleur", "clare", "Pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "london", "Manila", "rodents", "stanley", "stanley", "stanley", "stanley", "stanley", "5000 meters", "racing", "calcium phosphate", "Newfoundland", "corvids", "Yellowstone", "St. Francis Xavier", "Manila", "David Shore", "Buddhism", "Guy Berryman", "Ohio", "Melbourne", "\u00c6thelwald Moll", "Scarface", "forgery", "clare", "Liza Murphy", "Spock", "Turkmenistan", "Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain and Switzerland"], "metric_results": {"EM": 0.375, "QA-F1": 0.43675595238095233}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_squad-validation-2399", "mrqa_triviaqa-validation-3524", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_triviaqa-validation-2642", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-11382"], "SR": 0.375, "CSR": 0.5289522058823529, "EFR": 0.975, "Overall": 0.6973529411764706}, {"timecode": 34, "before_eval_results": {"predictions": ["the forts Shirley", "business districts", "spain", "spA", "george Santayana", "placental mammals", "Alice Cooper", "heart attacks", "trumpet", "spain", "The Scream", "spain", "Appalachian mountain range", "spain", "ballet", "philippines", "george ababa", "lizards", "blackburn", "george ababa", "The Mystery of Edwin Drood", "pommel horse", "a scarlet tanager", "Dick Van Dyke", "spain", "numb3rs", "spain", "phrixus", "spain", "george", "an \"ink sac\"", "soap", "Some Like It Hot", "spain", "ireland", "Eddie Murphy", "dolphins", "DeLorean", "igneous rocks", "spain", "Thank you", "spain", "spain", "spain", "26 miles", "Cleveland", "heston Blumenthal", "one Direction", "spain", "Uranus", "spain", "george manston", "the collapse of Ansett Australia in September 2001", "Baaghi", "lead dioxide", "boxer", "East Knidis", "stoneware", "Pittsburgh", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "ballet", "Howard Carter"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4817708333333333}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10251", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-6449", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-8994", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-78", "mrqa_searchqa-validation-4312"], "SR": 0.40625, "CSR": 0.5254464285714286, "EFR": 1.0, "Overall": 0.7016517857142858}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "John Forster", "Matlock", "American Civil War", "Ethiopia", "saturn", "Arafura Sea", "passe", "Tigris", "Bavarian", "to make wrinkles in one's face,", "Spain", "carousel", "bullfight", "jane brasse", "Tenor tessitura", "alpo", "fidelio", "armageddon", "jean Fellowes", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "goya", "pembrokeshire", "G. Ramon", "jane fonda", "marty feldmaninoff", "Finland", "stars with gravity", "Mille Miglia", "caves", "passe", "50p", "Muriel Spark", "happy birthday", "seven", "opossum", "Pickwick", "presliced bread", "armageddon noren", "bird", "jane", "bPA", "nelsons Column", "Etruscan army", "Ken Burns", "passe", "peter dorney", "pipa and Ivanov", "Mujib,", "saturn", "Donna", "season four", "the sinoatrial node", "Lee Sun-mi", "tomato", "the 2002 United States Senate election in Minnesota", "\"It has never been, and never will be, the policy of Total to discriminate against British companies or British workers.", "the earthquake", "March 24,", "Duke of Edinburgh", "Equinox Day", "Pocahontas"], "metric_results": {"EM": 0.390625, "QA-F1": 0.43339342948717946}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-400", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-3566", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-1247", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.390625, "CSR": 0.5217013888888888, "EFR": 1.0, "Overall": 0.7009027777777778}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "ghul", "city of acacias", "branson", "Gordon Ramsay", "nathan law", "Robert Kennedy", "sulfur dioxide and nitrogen oxides", "Annelies Marie Frank", "ringway", "Portuguese", "Travelocity", "Avengers", "city of raresborough", "ghane", "comets", "peter", "canola", "maurice ravers", "Sweeney Todd", "nathan", "Bolivia", "John Donne", "Uranus", "Rio Grande", "comets", "raon man", "30th anniversary", "julian Fontaine", "james VI of Scotland", "One Foot in the Grave", "mowgli", "comets", "George Santayana", "pliable leather", "brady bunch", "Wee Jimmy Krankie and his father", "Tomas De torquemada", "christopher farenboim", "Canada", "rum and cola", "Sleepless in Seattle", "ghee", "George III", "comets", "hyperbole", "oldpatricktoe-end", "90th birthday", "David Graham", "Ceylon", "screwdrivers", "Denver Broncos", "G minor", "A Christmas Story", "1974", "\"The Outsiders\"", "Amberley Village", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "Stephen Harper, left, and President Obama", "Josef Fritzl", "Pearl S. Buck", "Brigham Young", "pearl", "chalk quarry"], "metric_results": {"EM": 0.40625, "QA-F1": 0.50546875}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.25, 0.5, 0.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3858", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-2834", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-858", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-4108", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120"], "SR": 0.40625, "CSR": 0.5185810810810811, "EFR": 1.0, "Overall": 0.7002787162162163}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen", "aisles", "Tess", "Sakshi Malik", "Columbia River Gorge", "a physiological reaction", "49 cents", "1876", "the geologist James Hutton", "14 \u00b0 41 \u2032 34", "joy of living", "420", "George Strait", "the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "London, England", "Los Angeles", "February 10, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "1770 BC", "Chandan Shetty", "two", "John C. louis", "chromatin proteins", "Battle of Naboo", "Travis Tritt and Marty Stuart", "1976", "Barry and Robin Gibb", "Matt Czuchry", "Pradyumna", "The original purpose of vital statistics was for tax purposes and for the determination of available military manpower", "Isle Vierge", "Psychomachia", "New Jersey Devils", "two", "0.30 in ( 7.6 mm )", "Cress", "Superstition Mountains", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "epidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive, analytic ( aiming to further examine known associations or hypothesized relationships )", "Christian ( Josh Boswell )", "the Canadian Rockies continental divide", "Maginot Line", "pussia", "dumbo", "purple rain", "James A. Garfield", "The Gettysburg Address", "9 September 2014", "$273 million", "India", "Al Nisr Al Saudi", "Desperate Housewives", "(Pearl) Fawcett", "Morelos", "Tuesday"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5349228896103896}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 1.0, 0.9523809523809523, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.07272727272727272, 0.25, 0.5454545454545454, 1.0, 0.3333333333333333, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-5119", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.4375, "CSR": 0.5164473684210527, "EFR": 0.9166666666666666, "Overall": 0.6831853070175439}, {"timecode": 38, "before_eval_results": {"predictions": ["Godfrey Sykes", "25 years", "the Taft -- Katsura Agreement", "Kim Basinger", "In August 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in positions Arg15 - Ile16", "Charles Crozat Converse", "Lady Gaga", "in the Chicago metropolitan area in 1982", "President of the United States", "Domhnall Gleeson", "eusebeia", "Pastoral farming", "West Bromwich Albion", "f\u0254n", "Stephen A. Douglas", "1984", "`` man ''", "India", "21 February", "either Tagalog or English", "Bryan Cranston", "the stroma", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Alan Eustace", "Franklin and Wake counties in the U.S. state of North Carolina", "late 1922", "18 Divisional Round game", "623", "maximum energy of 687 keV", "between $10,000 and $30,000", "R2E Micral CCMC", "1931", "the `` Holy Club '' at the University of Oxford", "11 : 40 p.m. ship's time", "Norman Whitfield", "1959", "The statesmen", "Donna", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "in the intermembrane space", "divergent tectonic", "North Dakota", "Sara Gilbert", "6", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "Sir John Major", "roddy doddy rogers", "Daniil Shafran", "TD Garden", "Venus", "\"It's a really dumb thing to say,\"", "10 below", "General Motors'", "David McCullough", "a science fiction novel", "CERN", "london"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5652160973084885}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4347826086956522, 0.0, 0.3333333333333333, 1.0, 0.0, 0.32, 1.0, 1.0, 0.1, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4685", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-4506", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-5582", "mrqa_newsqa-validation-3486", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.484375, "CSR": 0.515625, "EFR": 0.9696969696969697, "Overall": 0.6936268939393939}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale : Sammy's Adventures", "Jenny Slate", "the root respiration", "Philippe Petit", "R2E Micral", "January 2004", "along the Yangtze River and in provinces in the south", "Toby Keith", "the development of electronic computers in the 1950s", "17 - year - old", "alternative rock", "Set six months after Kratos killed his wife and child", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "A 30 - something man ( XXXX )", "the Berlin School of experimental psychology", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Rachael Harris", "Richard Crispin Armitage", "Don Cook", "Lieutenant Templeton `` Faceman '' Peck", "Bonnie Aarons", "either late 2018 or early 2019", "typically composed of roughly 70 % hydrogen by mass, with most of the remaining gas consisting of helium", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "Frederik Barth", "John F. Kelly", "Santiago Ram\u00f3n y Cajal", "1890", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "related to the Common Germanic word guma", "September, 2016", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "Since 1940", "Professor Humbert", "Mark Jackson", "Michael Wrestling ( born November 2, 1944 )", "because it affirms the oneness of the body and has many members", "because we built sets, and they spend a lot of time in a forest", "the Second Continental Congress", "1958", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "about 0.04 mg / L several times during a day", "Juan Manuel de Ayala", "Prophet Joseph Smith, Jr.", "funny Folks (1874 - 1894)", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "a curfew", "\"Cry Baby,\"", "shark", "Fast Food Nation", "Hep Stars"], "metric_results": {"EM": 0.375, "QA-F1": 0.5363098877543849}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.07407407407407407, 0.967741935483871, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.47058823529411764, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.25, 0.24, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-4219", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-10341", "mrqa_triviaqa-validation-3500"], "SR": 0.375, "CSR": 0.512109375, "EFR": 1.0, "Overall": 0.698984375}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "two soldiers, a policeman and four militants", "Joan Rivers", "Olivia Newton-John", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "Zimbabwe President Robert Mugabe", "two weeks ago", "NATO", "Switzerland", "Monday", "second time", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany", "his likely Republican opponent said would slow economic growth with higher taxes.", "Clifford Harris,", "about 112 miles northeast of Eureka", "Robert Barnett", "$627", "41,", "Adenhart", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "Derek Mears", "Nieb\u00fcll", "on 112 acres about 30 miles southwest of Nashville,", "Tuesday", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "on the headstones", "Ali Bongo", "Allred", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages -- usually high school juniors who serve Congress as messengers", "Brazilian supreme court judge", "Derek Mears", "Operation Pipeline Express", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "A London, Ontario, hospital", "NATO fighters", "Raymond Thomas", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls", "in 2007", "P.V. Sindhu", "on location in Mexico", "snickers", "arctic whale whale", "capone", "Anaheim, California", "his uncle Juan Nepomuceno Guerra", "Bergen", "embalming ritual", "capone", "a graphical user interface", "a two - layer coat"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5950213148483132}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.30769230769230765, 1.0, 0.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444444, 0.06451612903225808, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 0.3076923076923077, 0.4, 1.0, 0.3333333333333333, 0.9411764705882353, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.40625, "CSR": 0.5095274390243902, "EFR": 1.0, "Overall": 0.6984679878048781}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "during the 1890s", "Stephen A. Douglas", "1998", "displacement", "sovereign states", "Megan Park", "euro", "Kate", "September 14, 2008", "American country music artist Trace Adkins", "on Mars Hill, 150 miles ( 240 km ) to the northeast", "1648", "before his 19th birthday", "they find cool, dark, and moist areas, such as tree holes or rock crevices", "The pour point of a liquid", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "by migrants competes with international aid as one of the largest financial inflows to developing countries", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "13,331", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "Kris Kringle", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Malloy as Pierre", "collect menstrual flow", "Swine influenza", "General George Washington", "Spanish", "Virgil Tibbs", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "16", "to direct black or non-white youth to the unskilled labour market", "the Jos Plateau", "the Platte River", "the right to vote", "prefrontal lobe", "10 June 1940", "Tandi", "alberich", "ear", "Brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "in the Kurdistan Region of Iraq", "through the Rockies and climb to 9,000 feet.", "\"A second employee from the U.S. Defense Department also died,", "CTU", "King Arthur", "No Deal", "Virgin America"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6184399393498659}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.08333333333333334, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.6666666666666666, 0.7000000000000001, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8235294117647058, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.13333333333333336, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5205"], "SR": 0.484375, "CSR": 0.5089285714285714, "EFR": 0.9393939393939394, "Overall": 0.6862270021645022}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water", "Middlesex County, Province of Massachusetts Bay, within the towns of Lexington", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Alexander Graham Bell's Volta Laboratory", "composition and powers of the Senate", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "victory", "Wilhelm Groener", "Ceramic art", "Norway", "Covington, Kentucky", "New Mexico", "to reduce power availability for the majority of thermal power plants by 2040 -- 2069", "December 15, 2017", "Paradise, Nevada", "L.K. Advani", "the Gulf of California opened and lowered the river's base level ( its lowest point )", "Glenn Close", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "Puerto Rico Energy Commission, another government agency whose board of directors is also appointed by the governor", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "the notion that an English p Larson may'have his nose up in the air ', upturned like the chicken's rear end", "groups of elements in the same column have similar chemical and physical properties, reflecting the periodic law", "ratio of the length s of the arc by the radius r of the circle", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson, himself a Southerner", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Disha Vakani", "880,000 square kilometres ( 340,000 sq mi )", "by producing an egg through parthenogenesis", "1926", "Durban, South Africa", "during the First World War", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "arrives of Norse god of the ocean,", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "The show went on without the self-proclaimed \"King of the South,\" whose car and home in the Atlanta suburb of College Park were searched after his arrest.", "prostate cancer,", "wyvern", "a normal 7 year old boy with a very kind heart growing up in America", "A rabbit with a fob", "yellow"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5931211195136159}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.6896551724137931, 0.2222222222222222, 1.0, 0.0, 0.1818181818181818, 1.0, 0.7272727272727272, 1.0, 0.9, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.18181818181818185, 1.0, 0.8, 0.14814814814814814, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.19047619047619047, 0.9767441860465117, 1.0, 0.7058823529411765, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.453125, "CSR": 0.5076308139534884, "EFR": 0.9142857142857143, "Overall": 0.6809458056478406}, {"timecode": 43, "before_eval_results": {"predictions": ["May 21, 2013", "2007", "she wrote the song after undergoing therapy, all located in the state of California, as well as MXM Studios in Stockholm, Sweden", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "as the arms of the king of Ireland", "Miami Heat", "1982", "After World War I", "October 1927", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Jean Antonin Merci\u00e9", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home", "Edward Kenway", "Haliaeetus", "a thick bunch of rootlets", "Alex Ryan", "a habitat", "2018", "Windows Media Video", "100", "Toledo", "Transvaginal ultrasonography", "During the last Ice Age", "the World Professional Billiards and Snooker Association", "Robert Irsay", "in Paradise, Nevada", "Alicia Vikander", "the 1930s", "Ashoka", "Relieving Chambers", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Bumblebee", "the Christian biblical canon", "New England", "Aelier de Construction d' Issy - Les - Moulineaux", "honey bees", "American country music artist Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "February 7, 2018", "New York City", "the fictional Iron River Ranch", "the island of Puerto Rico", "Bias is prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "wintertime", "Pangaea", "Newcastle Brown Ale", "Australia", "czechoslovakia", "Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\"", "\"pressing the reset button\"", "$60 billion on America's infrastructure.", "Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.359375, "QA-F1": 0.519282307604676}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.5, 0.10526315789473685, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.25, 1.0, 0.25, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6153846153846153, 0.0, 0.6, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.9545454545454545, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-9188", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-2425", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-8027", "mrqa_triviaqa-validation-4481", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977", "mrqa_searchqa-validation-13337"], "SR": 0.359375, "CSR": 0.5042613636363636, "EFR": 1.0, "Overall": 0.6974147727272728}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "lightweight aluminum foil", "Laurel, Mississippi", "about the outdoors, especially mountain-climbing", "Mississippi Delta", "Escorts Limited", "Jean Baptiste Point DuSable", "1992", "the Goddess of Pop", "eastern North America", "James Harrison", "Toronto", "Tomorrowland", "fennec", "United States Army", "seasonal television specials", "Jean Acker", "approximately 2 mi south from the city and county town of Lincoln, England", "Epicurus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "a night fighter", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "a composer and conductor", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Fletcher Avenue between Interstate 75 and the University of South Florida", "OutKast", "Richard Allen Street", "Zaire", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "French Cooking", "South America", "2007", "perjury", "Thomas D. Howie", "Mary Elizabeth Hartman", "over 9,000 employees", "Susan Jameson", "potential of hydrogen", "Alamodome in San Antonio, Texas", "Stephen King Biography", "a tab", "Kent", "almost 9 million", "Kenya", "$80,000", "'fair and square", "Moses", "a watch", "Wilson Pickett"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5546073717948717}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-993", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-1020", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_triviaqa-validation-1184", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.484375, "CSR": 0.5038194444444444, "EFR": 0.9696969696969697, "Overall": 0.6912657828282829}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "1858", "Australian", "1903", "the power to regulate interstate commerce", "Naomi Wallace", "McLaren-Honda", "Tufts University", "China", "Azeroth", "Squam Lake", "The Livingston family", "Tayeb Salih", "King James II", "God Save the Queen", "203", "Scotland", "\"rock and roll\"", "GmbH", "Mick Jackson", "Lalit", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "Chesley Sullenberger", "the Manhattan Project", "the Asia-Pacific War", "Romantic", "Hugh Dowding", "AMC Entertainment Holdings, Inc.", "New York Islanders", "Fennec fox", "1978", "six different constructors taking the first six positions", "French", "Pacific Place", "the Female Socceroos", "is a song by American rapper Kendrick Lamar", "2006", "about 5320 km", "Giuseppe Verdi", "\"Super Hit\"", "Sacramento Kings", "Walldorf", "Fife", "Fyvie Castle", "Faysal Qureshi", "the British Army", "80%", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Maroon 5", "Saudi Arabia", "\"is\"", "two"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6188244047619047}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.7499999999999999, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.53125, "CSR": 0.5044157608695652, "EFR": 1.0, "Overall": 0.6974456521739131}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "Guiana Highlands", "The King and I", "Republican", "1996", "5", "Greenland sharks", "Mancunian radio presenter Terry Christian", "President Abraham Lincoln", "St Jude Thaddeus", "1642", "the Death Penalty", "xerophyte", "Jackie Robinson", "Manhattan", "Dian Fossey", "MI5", "Harrow", "cr\u00e8me anglaise", "onions", "chicken", "curling", "Victoria Coren", "1863", "Chile\u2019s best wine producing regions", "Majorca", "Charles Dickens' novel, Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Uranus", "Venus", "President Obama", "Canada's Liberal Party", "Bologna Song Lyrics", "Cuba", "David Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "Great Britain returned Manila and Havana to Spain", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Daily Herald", "numerous", "\"permissible\"", "2016", "the Supreme Court of Canada", "2017", "Chief of Protocol", "Diamond White", "1944", "Umpire Louise Engzell.", "Jeddah, Saudi Arabia,", "Authorities in Fayetteville, North Carolina, are investigating the death of a pregnant soldier", "Beatrix Potter", "George Stephanopoulos", "How to Keep Young Mentally", "The king said, Divide the living child in two, and give half to the other"], "metric_results": {"EM": 0.5, "QA-F1": 0.5683035714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5875", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3458", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1581", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.5, "CSR": 0.5043218085106382, "EFR": 0.96875, "Overall": 0.6911768617021277}, {"timecode": 47, "before_eval_results": {"predictions": ["horse", "allergic reaction", "alex", "florida", "Runic", "canada", "cricket", "alex", "Millers", "mercury", "misery", "Styal", "olek", "alex bar", "alex", "alex have won nine Olympic gold medals", "parlophone", "Wild Atlantic Way", "canada", "alex", "alex", "oscar", "canada", "oscar", "muezzin", "a window", "oscar", "oscar", "a sub-orbital test flight", "Cellophane", "alex", "alex hurdle", "evita", "sperm", "oscar", "east fife", "st Pancras International Station", "social environment", "presliced bread", "Dilbert", "aristotelian Tragedy", "a canticle", "canada", "medea", "Burgundy", "cribbage", "alex", "Johannesburg", "France", "muffin man", "Seoul", "Prince James, Duke of York", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "the Greenbriar Boys", "Pansexuality", "Tony Ducks", "1754", "drugs", "Galveston, Texas,", "carrier based in Texas.", "Robert Frost", "Henry VIII", "Crescent", "Mitsubishi Eclipse"], "metric_results": {"EM": 0.40625, "QA-F1": 0.45372023809523804}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-5130", "mrqa_triviaqa-validation-555", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.40625, "CSR": 0.5022786458333333, "EFR": 1.0, "Overall": 0.6970182291666667}, {"timecode": 48, "before_eval_results": {"predictions": ["joplin, Missouri", "sesame Street", "chicken livers", "cabbage", "canada", "jimmy magoo", "fleece", "Ash tree", "opossum", "new Zealand", "jug band", "60", "goldfinger", "1983", "frog", "Mongols", "1875", "collector", "pennies", "jahrzehntealte Hits", "jimmy of anjou", "bagram collection Point", "jimmy gunderson", "Chrysler", "ushanka", "korky the Cat", "Bachelor of Laws", "united States", "Brazil\u2019s", "peking", "biathlon", "nampa", "Charlie Chan", "Vienna's", "white", "jawless fish", "jimmy henderson", "rabbit", "Scotland's flag", "jodhpurs", "Orson Welles", "jimmy have often remarked that many of the new concepts of nuclear physics or modern psychology are easy for them to grasp,", "menorah", "Dutch", "tesla", "Super Bowl Sunday", "quant pole", "Ding Dong Bell", "jimmy farfra", "Rhododendron", "ireland", "Chuck Noland", "the Colony of Virginia", "in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park, Kenya", "2010", "Vancouver 2010 Olympics", "10 below", "Nearly all of Britain's troops in Iraq will have left by the week's end.", "silver", "chow chow breed", "Omaha", "George Glenn Jones"], "metric_results": {"EM": 0.296875, "QA-F1": 0.40725511695906436}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8421052631578948, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-1594", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-6306", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-6607", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.296875, "CSR": 0.49808673469387754, "EFR": 1.0, "Overall": 0.6961798469387755}, {"timecode": 49, "UKR": 0.6484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.81640625, "KG": 0.44765625, "before_eval_results": {"predictions": ["Quin Ivy", "Iran", "tobacco", "Francis Ford Coppola", "dharm", "daniel boise", "Thames Street", "Theodore Roosevelt", "satyrs", "crabs", "dharm", "Microsoft", "scapulae", "garrick club", "jaber elbaneh", "master Humphrey", "Susan Bullock", "the American Civil War", "\"black\"", "clangers", "Frank Saul", "Florence", "saint Basil", "veruca salt", "severn", "dharm", "South Africa", "droughts", "Nicaragua", "Churchill", "war of roses", "clangers", "data", "trout", "ap\u00e9ritif", "dharm", "dharm", "folklorist", "alopecia", "derny", "charles darlings", "clangers", "Chris Martin", "noddy", "jennifer far Northerumberland", "rugby", "honda", "dharm", "11", "tobacco", "dharm", "fiat money", "Harrison Ford", "Chicago", "a pinball machine", "Texas Tech University", "Loughborough, Leicestershire, in the East Midlands of England", "Herman Cain", "the United States", "to prevent illegal immigration", "a prosperous real estate agent in Zenith, a Western city", "Oklahoma", "dharm", "four"], "metric_results": {"EM": 0.359375, "QA-F1": 0.39670138888888884}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-2670", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-1810", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-3615"], "SR": 0.359375, "CSR": 0.49531250000000004, "EFR": 1.0, "Overall": 0.6815625}]}