{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=100.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2050, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "Los Angeles Times", "the Broncos", "anticlines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "Pacific", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "squared integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "Swedex GmbH & Co KG", "More than 1 million", "2011", "market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Taoism", "Matthew 16:18", "the U.S.-flagged Maersk Alabama", "Rwanda", "revelry", "his health", "Hugo Vihlen", "the outskirts of a small Southern town"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7848958333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-5112", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-9655", "mrqa_squad-validation-7763", "mrqa_squad-validation-4772", "mrqa_squad-validation-7728", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.71875, "CSR": 0.7734375, "EFR": 0.7777777777777778, "Overall": 0.7756076388888888}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "the coast of Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel and the establishment of an Islamic state in Palestine", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "Arizona Cardinals", "1991", "Mercury/Gemini", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "pharmacists are expected to become more integral within the health care system", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "The Spice Girls", "Documents", "the company's factory in Waterford City, Ireland", "nitrogen", "Christopher Nolan", "the Florida current feeds the Gulf Stream that flows east of Cape Hatteras", "six", "It always begins with the music", "conductor", "Illinois", "Rafael Palmeiro Corrales", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.8125, "QA-F1": 0.8289835164835164}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9600", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-235", "mrqa_squad-validation-3967", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936"], "SR": 0.8125, "CSR": 0.7864583333333334, "EFR": 0.8333333333333334, "Overall": 0.8098958333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "monumental size", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "beautiful voice", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President", "2000 guests", "oxygen", "increase local producer prices by 20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "Edict of Nantes", "reserved to, and dealt with at, Westminster", "multiple revisions", "philanthropic initiative", "integer factorization", "necessity", "Isel", "adapted quickly and often married outside their immediate French communities", "U. S. Secretary of Housing and Urban Development", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "hydrogen ion gradient", "the late 19th century", "Channel Islands", "in no way", "Alberich", "charleston", "raising the spice level", "Churchill Downs", "the Ghent-Terneuzen Canal", "charleston", "\"shake Ambridge [the village where The Archers is set] to the core\u201d", "Germany", "study insects and their relationship to humans", "the limbic system", "Allan Border", "George Fox", "Maryland", "Great Expectations", "24 hours a day and 7 days a week", "Sponsorgate", "\"Krabby Road\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.6685763888888889}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-9286", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_hotpotqa-validation-437", "mrqa_hotpotqa-validation-3821"], "SR": 0.640625, "CSR": 0.75, "EFR": 0.8260869565217391, "Overall": 0.7880434782608696}, {"timecode": 4, "before_eval_results": {"predictions": ["higher plants", "Parliament of Victoria", "Zaha Hadid", "the Marquis de Vaudreuil", "Science and Discovery", "Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of ways", "Battle of Olustee", "the port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "2010", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "the Italian government", "22", "planned attacks in the southern port city of Karachi", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "a new model is simply out of their reach", "Muslim", "This will be the first time any version of the Magna Carta has ever gone up for auction", "a wedding", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "CNN's Campbell Brown", "a woman who may have been contacted through a Craigslist ad", "one", "celebrity-inspired names", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "colticelli"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7049163244843393}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.28571428571428575, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.15999999999999998, 1.0, 1.0, 0.0, 0.32, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-9456", "mrqa_squad-validation-7094", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.671875, "CSR": 0.734375, "EFR": 0.8571428571428571, "Overall": 0.7957589285714286}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "Connectional Table", "Deformational", "a high-level marketing manager", "500,000", "Ofcom", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence.", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches", "2011", "algae", "part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "the Philippines", "the Broncos", "1950s to 2011", "the characteristics of the conquering peoples", "German Te Deum", "1795", "Bermuda 419", "by compressing and cooling", "Infinity Broadcasting Corporation", "semi-legal", "1972", "rudimentary", "1957", "mother-of-pearl", "Gene Barry", "President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "used in a compact layout to combine keys which are usually kept separate", "from an Ohio newspaper on 8 February 1925", "President since Woodrow Wilson, with the notable exception of Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress", "radius R", "Panning", "Justin Timberlake", "the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "member states on a voluntary basis. As of 31 December 2013, the total size of the peacekeeping force is 98,200 police", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "Lowe's opened its first three stores in Canada", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman reprising his role as detective Alex Cross", "David Gahan", "the Overlook Hotel in his 1977 bestseller The Shining and its 1980 film adaption of the same name, as well as the location for the 1997 miniseries", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "people prepare for the long Lenten fast", "Jaipur", "Johan Persson and Martin Schibbye", "The anti-torpedo boat origin of this type of ship is retained in its name in...   capital ships[?] - Encyclopedia - Kids.Net.Au", "Newport County moved to Rodney Parade for the 2012/13 season after being at Newport Stadium for 18 years."], "metric_results": {"EM": 0.578125, "QA-F1": 0.676618118093844}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2666666666666667, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 0.1111111111111111, 0.0, 0.13793103448275862, 0.0, 1.0, 1.0, 0.8333333333333333, 0.4, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 0.3636363636363636, 0.5, 0.08333333333333334, 0.5833333333333334, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-9552", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-7488", "mrqa_squad-validation-3473", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.578125, "CSR": 0.7083333333333333, "EFR": 0.7777777777777778, "Overall": 0.7430555555555556}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "social and political action", "1936", "New Birth", "gold", "a 3\u20130 lead", "Vivienne Westwood", "reduction", "disease", "TGIF", "Confucian propriety and ancestor veneration", "the rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "a chain or screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "teachers in publicly funded schools", "at the end of the season", "10", "Jacob", "African-Americans", "\"creates the precedent and possibility for undue regulation, censorship and legal abuse.\"", "Chuck Bass", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Stuttgart on Sunday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "the United States, Japan, Russia, South Korea and China", "first five Potter films", "know what's important in life", "children ages 3 to 17", "two suicide bombers", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Her husband and attorney, James Whitehouse", "we want to ensure we have all the capacity that may be needed over the course of the coming days", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "Mike Gatting", "Colgate University", "The Church of Christ, Scientist", "unsaturated fats are comprised of lipids that contain?", "Luke 6 : 67 -- 71"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7437220614530397}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.08695652173913043, 0.0, 0.0, 1.0, 0.2, 0.5, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.7499999999999999, 0.5, 0.2666666666666667, 1.0, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.75]}}, "before_error_ids": ["mrqa_squad-validation-800", "mrqa_squad-validation-486", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-1267", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.640625, "CSR": 0.6986607142857143, "EFR": 0.782608695652174, "Overall": 0.7406347049689441}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "return to his side", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "unequal", "July 1969", "Hitler's secret police demanded to know if they were hiding a Jew in their house.", "a yellow chlorophyll precursor", "spontaneous", "the courts of member states", "gold", "TARDIS", "Buckland Valley near Bright", "Scottish rivers", "\"Bricks for Warsaw\"", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "Belgrade", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "\"Freshman Year\" experience", "India", "Zulfikar Ali Bhutto", "at the country's third-largest oil refinery", "April 24 through May 2.", "Krishna Rajaram", "early detection and helping other women cope with the disease.", "250,000", "Timothy Masters", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jared Polis", "William S. Cohen", "\"Dance Your Ass Off.\"", "military trials for some Guant Bay detainees.", "Gary Brooker", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Japan", "condition", "\"Empire of the Sun,\"", "Norman given name Robert", "stronger with an unfair advantage.", "Matthew Ward Winer", "Doc Holliday", "Baltic Sea", "Mustique", "green"], "metric_results": {"EM": 0.625, "QA-F1": 0.6982802983307016}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.9032258064516129, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.10256410256410256, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6115", "mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-3938", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991"], "SR": 0.625, "CSR": 0.689453125, "EFR": 0.7916666666666666, "Overall": 0.7405598958333333}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe.", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday afternoon", "Journey's End", "immediate", "Levi's Stadium", "Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "Chinggis Khaan", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "Islamic", "$12.9 million", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie business.", "finance", "terminal brain cancer.", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice act", "separated", "Animal Planet", "crashing his private plane into a Florida swamp.", "FAA said.Flights at Atlanta's Hartsfield-Jackson International Airport were delayed Tuesday afternoon.", "54 bodies", "early detection and helping other women cope with the disease.", "Diversity", "$250,000", "make sure water continues flow through the river channel and not spread out over land.", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "2050", "Alfredo Astiz, a former Navy captain whose boyish looks and deceitful ways", "Abdullah Gul", "Carl Froch", "The Everglades, known as the River of Grass,", "The original chromosome and the copy are now called sister chromatids", "Gibraltar territory currently contains an 800 m long section of the isthmus that links the Rock with mainland Spain.", "New Orleans, Louisiana", "early 1630s, a craze developed in Holland for this flower, with many investors paying huge sums for individual bulbs", "MIBs mission statement: protecting the earth from the scum of the universe.", "eat too many chockies, these, & you may find yourself crook, \"sick\"", "Get the latest St. Louis Blues news, scores, stats, standings, rumors, and more from ESPN."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6653798045922659}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0909090909090909, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407408, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.3076923076923077, 1.0, 0.0, 0.2857142857142857, 0.0, 0.11764705882352941, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-1710", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.59375, "CSR": 0.6788194444444444, "EFR": 0.6153846153846154, "Overall": 0.6471020299145299}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "\"hoos\"", "30%\u201350% O2 by volume", "very badly disposed towards the French, and are entirely devoted to the English.", "United States", "CRISPR sequences", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "the substance of the statement", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "providing the basic securities that Turkey can be a great partner.", "an average of 25 percent", "a gym", "Jennifer Arnold and husband Bill Klein, who both have skeletal dysplasia, a bone-growth disorder that causes dwarfism,", "environmental and political events.", "he fears a desperate country with a potential power vacuum that could lash out.", "at least two and a half hours.", "Elin Nordegren", "Europe, Asia, Africa and the Middle East", "6,000", "banned substance cortisone.", "President Clinton.", "delivered three machine guns and two silencers to the hip-hop star,", "Morgan Tsvangirai.", "policing the world and Africa", "future relations between the Middle East and Washington.", "in a canyon in the path of the blaze", "Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "strife in Somalia", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "they did not know how many people were onboard.", "London", "after Shawn's kidnapping", "the immediate physical and social setting in which people live or in which something happens or develops.", "William Tell", "OutKast", "Groundhog Day", "he really didn't mean t", "a singer who takes a job working with a struggling carnival."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6037884861302381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.08, 0.4, 0.0, 0.5217391304347825, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-9194", "mrqa_squad-validation-9484", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-11812"], "SR": 0.515625, "CSR": 0.6625, "EFR": 0.5806451612903226, "Overall": 0.6215725806451613}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley, New York", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever", "DC traction motor", "The Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "Friday", "pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "8 to 10 inches of snow", "Willem Dafoe", "Maude", "Phillip A. Myers.", "Koreans", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "58", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Touma", "Dangjin", "e-mails", "Chinese President Hu Jintao", "magazine", "burns over about two-thirds of his body, according to the hospital's associate director, Dr. Carl Schulman.", "October 3,", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next.", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "bankruptcy", "16 August 1975", "Bonnie Aarons", "one", "kabinett - Wines made with fully ripened grapes.", "Lionsgate.", "James Lofton", "Mysticism", "hair-like structures that help paramecium move around."], "metric_results": {"EM": 0.65625, "QA-F1": 0.7167850378787879}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-7230", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.65625, "CSR": 0.6619318181818181, "EFR": 0.9545454545454546, "Overall": 0.8082386363636364}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company under the Office of Alien Property (OAP) seal. John G. Trump,", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.", "Elway", "Philo of Byzantium", "36 acres", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "sum of divisors", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "The Soup Dragon", "antelope", "nipples", "Triassic Periods", "cooperative", "Anastasia Dobromyslova", "Lady Gaga", "9", "Blake Griffin", "radish", "Robert Ludlum", "a great power", "(.mov)", "The Donington Grand Prix Collection is, quite simply, the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew", "London Underground Piccadilly Line", "Wisconsin", "orangutan", "Manet", "The Magic Finger", "Massachusetts", "2005", "1969", "Chrysler", "dolt", "Venice", "a peplos.", "Enrico Caruso", "Elizabeth Arden", "collapsible support assembly", "Sir Hardy Amies", "Liechtenstein", "the 14th most common surname in Wales and 21st most common in England", "Rob Davis", "Cody Miller", "Brown Square Station", "Nazi Germany", "Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Jamaica", "Buddhism"], "metric_results": {"EM": 0.59375, "QA-F1": 0.670737333736004}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.23076923076923078, 1.0, 0.8, 1.0, 0.7234042553191489, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444444, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983"], "SR": 0.59375, "CSR": 0.65625, "EFR": 0.6538461538461539, "Overall": 0.6550480769230769}, {"timecode": 12, "before_eval_results": {"predictions": ["Southern Border Region", "70-50's", "Panini", "new laws or amendments to existing laws as a bill; a committee of the Parliament can present a bill in one of the areas under its remit", "anti-colonial movements", "Rhine Valley", "A", "by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "cases of an express wish of the people to withdraw from the EU", "1788", "2006", "Roman Catholic", "Henry of Navarre", "John Wesley", "because the nationalisation law was from 1962, and the treaty was in force from 1958, Costa had no claim.", "Eternal Heaven", "Ness Point", "John Mayer", "Leonard Cheshire", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Eric Pickles", "spy Who Loved Me", "Vladivostok", "Sheryl Crow", "Telstar", "Camellia sinensis", "AFC Wimbledon", "Charles Hawtrey", "Malaysia", "cosmology", "gin", "George Clooney", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "champagne", "extended period of abundant rainfall lasting many thousands of years.", "the United States", "Brigit Forsyth", "William Lamb, 2nd Viscount Melbourne", "state of Japan", "The History of Troilus and Cressida", "David Lean", "Kent", "Renoir\u00b4s", "Standard Motor Company", "white", "Switzerland", "soda water", "people of France to the people of the United States", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "federal states in the heart of...", "The Goat Amalthea", "\"The Screening Room\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.5989746980170367}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8387096774193548, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9452", "mrqa_squad-validation-6655", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-3503", "mrqa_naturalquestions-validation-594", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_searchqa-validation-9647", "mrqa_newsqa-validation-3860"], "SR": 0.546875, "CSR": 0.6478365384615384, "EFR": 0.7931034482758621, "Overall": 0.7204699933687002}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes)", "Georgia", "articles 1 to 7", "the Black Death", "their own militia", "days, weeks and months", "Over 61", "quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "force", "business", "John Connally", "saffron  Saffron", "hymenaeus", "Zeus", "a chemical", "the Suez Canal", "Brigit Forsyth", "call My Bluff", "March 10, 1997", "crazed Holiday", "The Battle of the Three Emperors", "Velazquez", "Arthur Ashe", "reptile", "strong cold southwest wind", "table tennis", "the journal of medicine", "penhaligon", "Gandalf", "thaddeus sholto", "Jinnah International Airport", "Monday", "capital of Venezuela", "beads", "soap", "highball", "Avro Lancaster", "\"Follow You Follow Me\"", "covey Brooker", "melon balm leaves and flowers", "harrods", "2007", "Christina Ricci", "Scarface", "pale yellow", "aluminium", "bubba", "June 12, 2018", "Filipino American", "London", "Lambic", "Nook", "Steven Green", "reedes", "cunY-CoLAG", "emperor", "\"Every Breath You Take\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5980034722222223}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-6463", "mrqa_squad-validation-4908", "mrqa_squad-validation-6933", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-4981", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-13371"], "SR": 0.484375, "CSR": 0.6361607142857143, "EFR": 0.8181818181818182, "Overall": 0.7271712662337663}, {"timecode": 14, "before_eval_results": {"predictions": ["seven", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "declining state of mind", "13 May 1899", "The Deadly Assassin and Mawdryn undead", "radioisotope thermoelectric generator", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1936", "Authority", "senior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "2 World Trade Center", "Kevin Spacey", "1 November", "2.5", "white blood cell", "Bangladesh -- India border", "President", "minor key", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shetty", "metamorphic rock", "January 12, 2017", "United States", "claims adjuster", "the nucleus", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column", "three", "nee", "sausages", "Kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "shIELD", "in the foyer of the BBC building in Glasgow, Scotland", "a thick stack of paper"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6556862408424908}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5333333333333333, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.2857142857142857, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4615384615384615, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-2339", "mrqa_squad-validation-1454", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.546875, "CSR": 0.6302083333333333, "EFR": 0.7586206896551724, "Overall": 0.6944145114942528}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "The Late Late Show", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "musicians", "CBS", "Jean Ribault", "Tetzel", "visitation of the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "60", "Seattle, Washington", "Battle of Antietam", "Nicolas Anelka", "In Time", "2nd century", "Glenn Close", "four times", "Agostino Bassi", "five", "Malibu, California", "the church at Philippi", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Bud '' Bergstein", "Bhupendranath Dutt", "a warrior", "Dr. Lexie Grey", "Majandra Delfino", "December 19, 1971", "Uruguay", "Alex Skuby", "Thomas Middleitch", "The National Legal Aid & Defender Association", "Monk's Caf\u00e9", "domesticated sheep goes back to between 11000 and 9000 BC", "1970s", "Director of National Intelligence", "Defence Against the Dark Arts", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Thespis", "Portugal", "burt Hammersmith", "Rachel Kelly Tucker", "Bohemia", "earwigs", "Code 02PrettyPretty", "musician", "the parliament", "the abduction of minors", "Nevada", "laureate", "Stage Stores,", "1881"], "metric_results": {"EM": 0.5, "QA-F1": 0.6092247596153846}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.375, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-551", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674", "mrqa_searchqa-validation-5103"], "SR": 0.5, "CSR": 0.6220703125, "EFR": 0.65625, "Overall": 0.63916015625}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "Arizona Cardinals", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "Western Xia", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Vancouver", "Office", "SAVE", "Scandinavian Airlines System Aktiebolag", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award Best Actor", "Jello Biafra drew on Nardwuar's face with a marker pen", "the 1745 rebellion of Charles Edward Stuart", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements,", "Sir William McMahon", "the North Sea", "7.63\u00d725mm Mauser", "Academy Award for Best Animated Feature", "the CAC/PAC JF-17 Thunder", "Delacorte Press", "Neighbourhoods", "Secretariat", "Wake Island", "Hydrogen vehicle", "Fort Valley, Georgia", "King of France", "\"Southern Living\" Reader's Choice Awards", "William Shakespeare", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Alticor", "Parlophone Records", "South Africa", "Surrey", "The Girl", "Charles Russell", "Boyd Gaming", "three different covers", "1991", "Glenn Close", "Florence Glenda Ballard", "Neighbours", "Ewan McGregor", "2011", "pippa passes", "a enslaved African American who led", "power-sharing talks", "Brown-Waite, along with Republican Rep. Shelley Moore Capito of West Virginia."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6474770021645022}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.16666666666666669]}}, "before_error_ids": ["mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3521"], "SR": 0.5625, "CSR": 0.6185661764705883, "EFR": 0.8571428571428571, "Overall": 0.7378545168067228}, {"timecode": 17, "before_eval_results": {"predictions": ["forces", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter", "1830", "nonfunctional", "inner chloroplast membrane", "Charlie Harper", "steveland Hardaway Morris", "beaver", "La Boh\u00e8me", "formic acid", "torc", "Zimbabwe", "Mr. Boddy", "Edward \"Ted\" Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "hound", "Plato", "Fuller's", "a reference mark", "Nick Hornby", "\"The Two Gentlemen of Verona\"", "Charles V", "England", "Lagertha", "weight plates", "\"big house\"", "Hadrian", "France", "flea", "Moonee Ponds, a suburb in Melbourne, Victoria", "Hamburg", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "In 2014/15, only six have won the title", "Robert Cummings", "John Mayer", "British public", "In 1906, Finland became the first country in the world to grant women full political rights.", "3000m", "Scotland", "Russia", "Travis Tritt and Marty Stuart", "The Union", "New Jewel Movement", "sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "\"s h! du'y van", "two", "jeopardy/1870_Qs.txt at master  jedoublen/jeopardy", "\"The Sunday Thing\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.5726934523809524}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-2812", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-3198", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.515625, "CSR": 0.6128472222222222, "EFR": 0.7741935483870968, "Overall": 0.6935203853046594}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "high", "Manakin Town", "northwest", "fewer than 10 employees", "Middle Miocene", "magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "1994", "Coppolas and, technically, the Farrow / Previn / Allens", "Anna Faris", "mainland greece", "inability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Meredith Quill", "1985", "19 state rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Alice", "20 November 1989", "Coton in the Elms", "55 -- 69 %", "Ella Eyre", "1995", "Identification of alternative plans / policies", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "arterioles", "July 21, 1861", "Dr. Addison Montgomery", "capital and financial markets", "an optional message body", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "universal significance", "September 2017", "moral", "Rising Sun Blues", "Part 2", "Dumbo", "the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "jegna", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6735148225957049}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 1.0, 0.2857142857142857, 0.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735"], "SR": 0.59375, "CSR": 0.611842105263158, "EFR": 0.5769230769230769, "Overall": 0.5943825910931174}, {"timecode": 19, "before_eval_results": {"predictions": ["Luther reviews and reaffirms, on the one hand, what has been called the \"second use of the law,\"", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "Long troop deployments", "CNN.com", "a Florida girl who disappeared in February, plans to file for divorce from the girl's stepmother, a key witness in the case,", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "32 percent", "sovereignty over them", "Tuesday", "forgery and flying without a valid license", "Anil Kapoor", "19", "President Obama", "unwanted baggage from the 80s", "The Louvre", "snowstorm", "exotic sports cars", "eggs", "Mutassim", "Thursday and Friday", "\"Steamboat Bill, Jr.\"", "NATO fighters", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "\u00a341.1 million", "Kingman Regional Medical Center", "CNN", "Manmohan Singh", "Michael Jackson", "be silent", "40 militants", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Louisiana", "Southeast", "the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home.", "Steven Chu", "tells Larry King her son has strong values.", "Moe and Sana Maraachli", "back at work", "the initial necropsy or animal autopsy", "27", "Derek Hough", "John Adams, a leader in pushing for independence, had persuaded the committee to select Thomas Jefferson to compose the original draft of the document", "parsley", "Zager & Evans", "Robert Matthew Hurley", "fourth term", "\"adult theatre - You must be 21 and able to prove it.", "(Oliver) Cromwell", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5629682239057239}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [0.14814814814814814, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-7208", "mrqa_searchqa-validation-328", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.453125, "CSR": 0.60390625, "EFR": 0.8571428571428571, "Overall": 0.7305245535714285}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death wish Coffee", "L", "Cameroon", "1994", "ballots", "polo", "three empty vodka bottles,", "strategy, plans and policy", "Bobby Darin", "Felipe Massa", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "humiliated", "composer", "the punishment", "Caylee Anthony, 2,", "Amanda Knox's aunt", "well over 1,000 pounds", "Iran's nuclear program", "12 shades of violet, including a welcoming, bright blue-purple during the day, a softer violet hue after dusk", "using recreational drugs", "ceo Herbert Hainer", "Brett Cummins,", "The Valley swim Club", "inmates", "Elspeth Cameron-Ritchie", "\"E! News\"", "six members of Zoe's Ark", "jobs", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "Mexican's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "goes into when I got out of the game.", "Saturday's killing of a 15-year-old boy", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "a 57-year old male", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Matthew Fisher", "jund Ansar Allah", "boogeyman", "referred the case of Mohammed al-Qahtani to prosecutors", "Sea World in San Antonio", "guard", "about 50 formal applications", "Ku Klux Klan", "MGM prohibited the release until The Wizard of Oz ( 1939 ) had opened", "Branford College", "Bolton, Bury, Oldham, Rochdale, Stockport, Tameside, Trafford, Wigan, and the cities of Manchester and Salford.", "stamens", "Malayalam movies", "August 17, 2017", "a jacket, gloves or a briefcase", "By the 1950s, scientists were able to do this to frogs; mice followed, in the '80s", "Hodel", "access to US courts", "Coldplay"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5438595547970547}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.0, 1.0, 0.9523809523809523, 0.5, 1.0, 0.5, 0.8, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.14814814814814814, 1.0, 0.0, 0.888888888888889, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.4, 1.0, 0.18181818181818182, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.390625, "CSR": 0.59375, "EFR": 0.6153846153846154, "Overall": 0.6045673076923077}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "Splash", "Nicola Adams", "copper and zinc", "Austria, Prussia, Russia, France, and also Poland,", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "breast", "Madonna's", "Glasgow", "satellite", "Australia", "giblet", "Pearson PLC", "Irish Setter", "American Civil War", "Loch Awe", "celibacy", "Tasmania", "medium-sized cat, fine-boned, long, and firmly muscled.", "Taiwan", "Harrisburg", "mink mink,", "glockenspiel", "Dr John Sentamu", "Baka hunter-gatherers", "Cruella de Vil", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "Charles V", "the community", "Russell Crowe", "Theodore Roosevelt", "skipper", "Puck", "Erewhon", "chamomile", "Ireland", "tarn", "Atlantic", "Albert Square", "Newbury", "the Old Testament", "70 million people, at that time 21 % of the world's entire population", "Target Corporation", "Sister, Sister", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "EYBistro", "Swamp Fox", "prisoners' rights and better conditions for inmates, like Amnesty International.", "talk show queen Oprah Winfrey.", "Mom"], "metric_results": {"EM": 0.5625, "QA-F1": 0.608974358974359}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_searchqa-validation-11802", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.5625, "CSR": 0.5923295454545454, "EFR": 0.75, "Overall": 0.6711647727272727}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin, generically known as hydrocodone", "Rome", "Robert Peary", "pearls", "blackbird Hundreds.", "Carrie Underwood", "liqueur liquor", "Nero", "mooglemb.com", "Langston Hughes", "Jimmy", "madge Larabee", "Orquestra", "riata", "unFINISHED", "USS LST 325", "rhodesian ridgebacks", "David Beckham", "Arturo Toscanini", "economics", "Miracle in the Andes", "arches", "Montenegro", "discus", "thick slice of bread", "basidiomycota", "james", "Ally McBeal", "Idi Amin Dada", "agricultural green & yellow", "a body, or a personal item associated with a saint", "terracotta", "Gaius Cassius Longinus", "lawyer, businessman, former politician", "masa harina", "50 seconds", "the Vikings.", "74 Fairfield Street", "summer", "typhoid fever", "a coastal inlet formed by the partial submergence of an unglaciated river valley.", "baviere-quebec.org", "Williamsburg", "\"Wire Rope Express\"", "University of Missouri-St. Louis", "hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off", "John Knox", "the internal reproductive anatomy", "$657.4 million in North America and $1.528 billion in other countries, for a worldwide total of $2.187 billion", "epidemiology is the study and analysis of the distribution and determinants of health and disease conditions in defined populations", "jape", "Tesco", "A4", "Graham Hill", "Battelle Energy Alliance", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services.", "debris", "$10 billion", "out in the woods"], "metric_results": {"EM": 0.328125, "QA-F1": 0.4098015761328455}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.21052631578947367, 0.45454545454545453, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16257", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1118", "mrqa_hotpotqa-validation-68"], "SR": 0.328125, "CSR": 0.5808423913043479, "EFR": 0.7906976744186046, "Overall": 0.6857700328614762}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "Atlantic", "cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Basel, Switzerland", "The Argonauts", "PromPromPrometheus", "Altamont Speedway Free Festival", "John F Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "a multi-user real-time virtual world described entirely in text", "Italy", "khaki", "magma", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "Barack Obama", "the Earth", "Nafea Faa Ipoipo?", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations", "Manchester City", "Love Is All Around", "William Golding", "Sally Ride", "Influenza", "Fife", "Money Saving", "Adidas", "the \"Rabbit Hole\"", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "the opponent's", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "The Wall Street Journal Europe", "fox", "60 Minutes", "Earth"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6995251225490196}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_searchqa-validation-1586"], "SR": 0.65625, "CSR": 0.583984375, "EFR": 0.8181818181818182, "Overall": 0.7010830965909092}, {"timecode": 24, "before_eval_results": {"predictions": ["limited coercion", "the chosen machine model", "Fox", "1997", "a suite of network protocols", "Noriko Savoie", "15", "the first home series defeat on Australia in almost 16 years", "between Pyongyang and Seoul", "killed a man, the latter cheated on his wife.", "11", "change course", "Damon Bankston", "Chaffetz", "money or other discreet aid for the effort if it could be made available,", "Sarah", "normal maritime traffic", "environmental", "Italy in the quarterfinals,", "Afghan security forces", "Saturday", "38", "70,000 or so", "climatecare, one of Europe's most experienced providers of carbon offset,", "E! News", "coach", "Steve Williams", "McDonald's", "writing her short stories (she has already published one book) and shows me a cartoon character she has created called \"Tomato Man.\"", "five female pastors", "2008", "Diego Maradona", "Dog patch Labs", "The drama of the action in-and-around the golf course", "two", "Itawamba County School District", "Romney", "EU naval force", "Plymouth Rock", "Liza Murphy", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "police", "former U.S. secretary of state", "At least 33", "five", "improve health and beauty.", "Texas is among a growing number of state governments going after them.", "that students often know ahead of time when and where violence will flare up on campus.", "Damon Bankston", "Krishna Rajaram", "Sunday, when a man wearing an explosives-laden vest drove a motorcycle rigged with bombs into a group of police recruits in eastern Baghdad.", "killing", "a feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "Northern Ireland", "radar", "art", "16th", "23", "South America", "freestyle", "Florence Nightingale", "the Kingdom of the Crystal Skull"], "metric_results": {"EM": 0.4375, "QA-F1": 0.621887912135757}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5555555555555556, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.12121212121212123, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.07692307692307693, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273]}}, "before_error_ids": ["mrqa_squad-validation-4673", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_searchqa-validation-3826"], "SR": 0.4375, "CSR": 0.578125, "EFR": 0.6666666666666666, "Overall": 0.6223958333333333}, {"timecode": 25, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.767578125, "KG": 0.4421875, "before_eval_results": {"predictions": ["50th anniversary special", "Thomas Savery", "Vicodin, generically known as hydrocodone", "Eastern crops", "22,000 years ago", "violent separatist campaign", "Eleven", "269,000", "the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "38 feet", "Eintracht Frankfurt", "150", "a man had been stoned to death by an angry mob.", "NATO fighters", "41", "Los Alamitos Joint Forces Training Base", "Wally", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "Isla Grande Airport in San Juan,", "the Russian air force", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "\"several pieces of aircraft equipment were at fault or had broken down.\"", "ensuring that all prescription drugs on the market are FDA approved", "customers", "Tom Baer", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "bikinis -- haute, bandeau-style little numbers", "Brian Mabry", "iTunes, which completely changed the business of music,", "Sunday.Maurice Clemmons, 37, was shot and killed early Tuesday by Seattle police.", "60 euros -- $89 --", "American Civil Liberties Union", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment", "Some truly mind-blowing structures are being planned for the Middle East.", "first name", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "2006", "San Diego County.", "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "The Taliban has threatened to kill Bergdahl if foreign troops continue targeting civilians in the name of search operations in Ghazni and Paktika provinces,", "@", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "number two, the U.N. forces", "heart", "Hyderabad", "Sinai Peninsula or simply Sinai ( / \u02c8sa\u026ana\u026a / ; Arabic : \u0633\u064a\u0646\u0627\u0621\u200e S\u012bn\u0101\u02bc", "to stay, abide", "Las Vegas Boulevard, commonly referred to as the Las Vegas strip, or the strip, is where many of the flashier and best known casinos operate.", "Jackson Pollock", "Lyrical", "McComb, Mississippi", "October 4, 1970", "King Duncan", "Brasstown Bald", "thimble", "\"to choose, select\" (take the long way home) late... event, at a gambling casino"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4675962894712895}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.27027027027027023, 1.0, 1.0, 0.0, 0.07407407407407408, 0.6666666666666666, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.1111111111111111, 0.0, 0.4, 0.888888888888889, 1.0, 0.8, 1.0, 0.1818181818181818, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_triviaqa-validation-1422", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-1982", "mrqa_searchqa-validation-11832", "mrqa_searchqa-validation-9476"], "SR": 0.390625, "CSR": 0.5709134615384616, "EFR": 0.6153846153846154, "Overall": 0.6362439903846154}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "pro-democracy activists clashed Friday with Egyptian security forces", "Krishna Rajaram,", "at least 25 dead", "Booches Billiard Hall,", "finance", "Ross Perot.", "Hong Kong's Victoria Harbor", "2002", "six prostitutes and a runaway involved in the drug trade.", "The first of the impeachment charges were brought against Arroyo in 2005 for vote-tampered. Congress rejected the charges.", "the sight of celebrity pontificating about the plight of the environment", "three", "Monday", "Scarlett Keeling", "two years", "Since 1980, the 84-year-old Mugabe has been the country's only ruler.", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "in July for A Country Christmas,", "Akshay Kumar", "Graham's wife", "\"against people who independent of their race, religion, ethnicity, social condition etc. accepted money and put themselves at the service of the army in an area that is the object of military operations.\"", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "David Bowie,", "the death of Prince George's County police Cpl. Richard Findley, who died Friday after being struck by a truck.", "Phil Spector", "Kim Il Sung", "1994", "planned attacks in the southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "dependable Camry know what's important in life,", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "in his 60s, is incarcerated at the Supermax prison in Florence, Colorado, as is Zayed.", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.", "\"don't ask, don't tell\"", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units within the National Health Service in England", "6 - 6 with one win against a team from the lower Football Championship Subdivision ( FCS )", "Matt Monro", "\"Jack\" Frost,", "the innermost digit of the forelimb; thumb", "1968", "25 million", "Peoria, Illinois", "Hawaii", "large water near the bottom, as deep as 477 m but typically 75-150 m;", "\" Upon such sacrifices, my Cordelia,/The gods themselves throw incense\"", "Ottoman Empire"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6267250658698027}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.578125, "CSR": 0.5711805555555556, "EFR": 0.7777777777777778, "Overall": 0.6687760416666667}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "bank robber John Dillinger,", "what caused the collapse of the building which contained the city's historical archives, bringing down parts of the two nearby structures.", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen", "a government-run health facility that provides her with free drug treatment.", "Lashkar-e-Tayyiba (LeT)", "$1.5 million", "2006", "Rev. Alberto Cutie", "Angels", "eight Indian army troopers, including one officer, and 17 militants,", "\"There's no chance of it being open on time.", "Karen Floyd", "14", "Starbucks", "BADBUL", "98", "2008", "near the Somali coast", "Paul Ryan", "state senators who will decide whether to remove him from office", "Dr. Jennifer Arnold and husband Bill Klein,", "Pakistan's combustible Swat Valley,", "South Dakota State Penitentiary", "Iran", "last month's Mumbai terror attacks", "know what's important in life,", "in July", "Sudanese nor orphans,", "Four Americans", "Josef Fritzl,", "Glasgow, Scotland", "At least 38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "Formagruppen", "fractured pelvis and sacrum", "Wednesday", "abduction of minors", "gun", "Aniston, Demi Moore and Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "Flag Day in 1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "apteka", "largest city", "beta blockers"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7584805819180819}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.38095238095238093, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-14535"], "SR": 0.65625, "CSR": 0.57421875, "EFR": 0.9090909090909091, "Overall": 0.6956463068181818}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author", "ownership of private industries", "1981", "forgery and flying without a valid license,", "comments he made after his new boss, golffer Adam Scott, defeated Woods at the Bridgestone Invitational in Ohio in August.", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "Genocide Prevention Task Force.", "shoot down the object whether it is a missile or a satellite.", "semiconductors", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "13", "\"face of the peace initiative has been attacked,\"", "misdemeanor assault", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations -- embraces this technology the same way the public has", "Anil Kapoor.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"The Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "Christianity and Judaism,", "the Dalai Lama's current \"middle way approach,\"", "Russia", "8 p.m. local time Thursday", "Passers-by", "\"My gut started feeling like something just wasn't right,\"", "executive director of the Americas Division of Human Rights Watch,", "750", "nearly 100 people", "Matthew Fisher", "The Ski Train", "Boys And Girls alone", "brewer", "AbdulMutallab,", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "inconclusive", "5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers", "environmental and political events", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600 people every year, and about 10 percent of those cases are hereditary.", "a million", "Sen. Arlen Specter", "Deutschneudorf", "state and federal level. in January, U.S. Rep. Kevin Brady, a Texas Republican, introduced legislation that would let prisons jam cell-phone signals within their walls.", "a deceased organ donor,", "bragging about his sex life", "a vertebral column ( spine )", "January to May 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "Festival of Britain on London's South Bank.", "Douglas Hofstadter", "\"The Dark Tower\" series", "American", "Marmee takes care of her 4 girls while her husband is away serving as an army chaplain in the Civil War", "Castle Rock", "anchovy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6643065566354966}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 0.125, 1.0, 0.25, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.186046511627907, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_hotpotqa-validation-5376", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.53125, "CSR": 0.5727370689655172, "EFR": 0.7333333333333333, "Overall": 0.6601984554597701}, {"timecode": 29, "before_eval_results": {"predictions": ["stagnant", "El Tem\u00fcr", "estimated 438,000 species", "Marty Ingels", "coaxial", "Pakistan A in their two-day match against England in October 2015.", "Everbank Field", "7 members appointed by the chief executive.", "Battle of Dresden", "Arabella Churchill,", "1965", "Charles de Gaulle Airport", "fifth", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven", "Province of Syracuse", "1963", "non-alcoholic", "puzzle", "Knoxville, Tennessee", "Washington, D.C.", "Oryzomyini", "Tom Kartsotis", "November 6, 2009", "Wayman Tisdale", "Mexico", "Srinagar", "Northern Ireland", "late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell", "2002", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Films", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey.", "Floridians", "Virginia", "1996 NBA Slam Dunk Contest.", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "Somerset County, Pennsylvania,", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes, once in 1954 and again in 1981.", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "Richard Parker", "southernmost tip of the South American mainland", "Charlotteton Heston", "allergic reaction", "Peter Townsend.", "3,000 kilometers (1,900 miles)", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "The Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "Russia", "peel and devein shrimp", "Australia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6314463268821822}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.15384615384615383, 0.3157894736842105, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4347", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-4716", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-4356", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-4033", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-6793"], "SR": 0.546875, "CSR": 0.571875, "EFR": 0.8275862068965517, "Overall": 0.6788766163793104}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a cooperative where farmers pool their resources in certain areas of activity.", "Danish", "1903", "the attack on Pearl Harbor.", "other individuals, teams, or entire organizations.", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Werner Nowitzki", "best in film and American television of 2013", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Hl\u00edn", "86,112", "Celtic", "Ouse and Foss", "United States and Canada", "British", "Apatosaurus", "1910s", "American", "Frank Thomas' Big Hurt", "\"Polovetskie plyaski\"", "Margarine Unie", "Winecoff Hotel fire", "mentalfloss.com", "The Seduction of Hillary Rodham", "2005", "Lambic", "Tom Clancy's The Division", "Argentina", "Larry Alphonso Johnson Jr.", "Mike Mills", "veto power", "Joseph E. Grosberg", "Chelsea Lately", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "The video was filmed along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "New York", "discus", "Aston Villa", "2005", "228", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6890630906255906}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.9189189189189189, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.578125, "CSR": 0.5720766129032258, "EFR": 0.7037037037037037, "Overall": 0.6541404383213859}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "a short story centering on the thoughts of a... At lunch one day, he ignores his mother when she asks him to pass a plate.", "log ride", "Iowa caucuses", "A People's History of the United States", "Nassau", "a pearl oyster", "Dr. Anthony Gallo", "Martin Van Buren", "a network of seven Shinkansen passenger and... in the fleet of France's Train  Grande Vitesse", "Rigoletto", "aardwolf", "Beijing", "Roger Bannister", "a few phone calls", "Death Valley", "Yves Saint Laurent", "reindeer", "a Norwegian crown prince", "the fleet", "Anna Mary Robertson", "a Sailor Moon", "Nevilles Superette", "The New York Times Fiction Best Sellers of 2004", "a bear", "a charleston", "George Harrison.", "a hamster and your father smelt of elderberries", "a polarized electron source consisting of a 3-electrode photocathode gun and a flashlamp-", "Milton Berle", "George Herbert Walker Bush", "Patrice Lumumba", "a lunar module", "a Spanish conquistador", "Dan Marino", "Mars", "a clownfish", "E/c^2", "Guru Pitka", "Las Vegas", "soy", "a butterfly", "heavy drinking", "orangutan", "New Mexico's", "Soothsayer", "Yitzhak Rabin", "David Becomes Israel's King", "Gettysburg", "Jack Gleeson", "Plank", "Buddhism", "Carl John", "Portugal", "Tom Evans", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\" leaving them \"vulnerable to disruption,\"", "knocking the World Cup off the front pages for the first time in days.", "12.3 million people worldwide"], "metric_results": {"EM": 0.5, "QA-F1": 0.5428574775112444}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-1768", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-2051", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.5, "CSR": 0.56982421875, "EFR": 0.90625, "Overall": 0.69419921875}, {"timecode": 32, "before_eval_results": {"predictions": ["Second World War", "62", "Clement Attlee", "40", "Libya", "Shania Twain", "Sheffield Wednesday", "insulin and glucagon", "The New York Yankees", "rapid eye movement", "green, red, white", "Ann Dunham", "Saddam Hussein", "French", "Jim Branning", "Ohio", "Francis Matthews", "photographic", "magnetite", "Noah", "London", "New", "Prince Andrew and Sarah Ferguson", "Mercury", "a power factor of one means that the real power is equivalent to the apparent power.", "Jack Douglas", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "Brazil", "optimism", "75", "Jennifer Lopez", "1664", "Morgan Choir", "P Baigneres", "Downton Abbey", "Martina Hingis", "a family of the Scottish highlands, known as MacLeish", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "Gulliver's Travels", "Pomona", "Milan", "Mike Skinner", "Appalachian Trail", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "Campbell's", "Kirkcudbright", "the soldiers", "cortisone.", "providing the basic securities", "PLEUNis", "a typeface", "lungs"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6183035714285714}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-6656", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.546875, "CSR": 0.5691287878787878, "EFR": 0.7931034482758621, "Overall": 0.6714308222309301}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "can contribute to other problems such as asthma, sinus or ear conditions, or trouble sleeping.", "abigail Masham", "Getafix", "Brighton", "Belfast", "wind", "fire insurance", "Robin Hood's A Holy Grail", "West Point", "Andy Warhol", "Spain", "robohunters", "smoky Bay", "solar system", "tomato and eggplant", "Moldova", "Mitsubishi A6M Zero Fighter", "Dartford Warblers", "Franz Liszt", "Estimate", "a mountain bike.", "clon", "Pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "Manila", "beaver", "Mel Blanc", "lion", "Moffitt", "Ellen Morgan", "phil Woolas", "5000 meters", "racing", "micelles", "Newfoundland and Labrador", "crow", "Yellowstone", "St. Thomas", "luzon", "Hugh Laurie", "Buddhism", "Chris Martin", "Ohio", "Port Melbourne", "\u00c6thelwald Moll", "Scarface", "forgery and flying without a valid license,", "In Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory", "Liza Murphy", "Spock", "Astana", "Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain and Switzerland"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6109375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-11382"], "SR": 0.578125, "CSR": 0.5693933823529411, "EFR": 0.7407407407407407, "Overall": 0.6610111996187363}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business", "steppes steppe", "Bologna, Italy", "George Santayana", "marsupials", "Alice Cooper", "diastolic", "trumpet", "Peter Kay.", "The Cry", "stockton & Darlington Railway", "Appalachian Mountains", "Herald of Free Enterprise", "ballet", "quake", "george knells", "lizard", "Blackburn Lancashire", "Frankie Laine", "The Mystery of Edwin Drood", "pommel horse", "bird", "Dick Van Dyke", "Egrement", "numb3rs", "Francisco de Goya", "phrixus", "tilly (n\u00e9e Katz; 1902\u20131977)", "Canada", "ink sac", "soap", "Some Like It Hot", "Mull", "Ireland", "Mike Meyers", "sea horse", "plutonium", "igneous", "Passepartout", "welcome", "Sweden", "Sweden", "shrek", "26 miles", "Cleveland Brown", "heston Blumenthal", "One Direction", "Flint", "Jupiter", "Mr. Stringer", "Charles Lindbergh", "September 2001", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "beer mugs", "Pittsburgh", "Pakistan's High Commission in India", "raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Hunter S. Thompson", "tchaikovsky", "Howard Carter"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5871428571428571}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.32, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6449", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-78"], "SR": 0.484375, "CSR": 0.5669642857142857, "EFR": 0.5757575757575758, "Overall": 0.6275287472943724}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Leonardo da Vinci", "Matlock", "American Civil War", "Ethiopia", "cetaceans", "Arafura Sea", "labyrinth", "Euphrates", "Austria", "to make wrinkles in one's face", "Spain", "Carousel", "bullfighting", "Mike Brady", "tenor", "alpo", "fidelio", "Guys and Dolls", "Julian Fellowes", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "Ghana", "sandstone Trail", "L. Pasteur", "jane Merrill", "rachmaninoff", "Finland", "stars with gravity", "Mille Miglia", "caves", "Billaley & His comets", "silver", "Muriel Spark", "Happy Birthday to You", "seven", "opossum", "Pickwick Papers", "presliced bread", "The Bridge", "raven", "Jordan", "genetically engineered", "nelsons Column", "Etruscans", "Ken Burns", "grosvenor crescent", "Great Britain's Heather Stanning and Helen Glover", "e. T. A. Hoffmann", "Mujib", "Aquarius", "Donna", "season four", "sinoatrial node", "Yubin, Yeeun", "tomato", "2002", "workers were hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "L'Aquila earthquake", "March 24,", "sesli Szlk", "chuseok", "Pocahontas"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6470833333333332}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.578125, "CSR": 0.5672743055555556, "EFR": 0.7777777777777778, "Overall": 0.6679947916666668}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "Kim", "Acacias", "branson", "Gordon Ramsay", "Manchester City", "joe skakel", "nitrogen oxides", "Margot Betti", "Manchester Airport", "Portuguese", "m McKinney", "police Surgeon", "Richmond", "Batman", "Paul Simon", "disciples", "rapeseed", "Tina Turner", "Benjamin Barker", "Nottingham Forest", "Bolivia", "John Donne", "Uranus", "Rio Grande", "Percheron", "The Graduate", "Greek", "Ginger Rogers", "James I", "One Foot in the Grave", "Bronx Mowgli", "Bob Anderson", "George Santayana", "Finger Tab", "windermere", "Wee Jimmy Krankie and his father", "muy Reverendo Se\u00f1or Fray Thomas de Torquemada", "Daniel Barenboim", "Canada", "a Cuba trapped in a time long forgotten", "Lake Union", "liquid butter", "George III", "Ed Sheeran", "Metonymy", "tobacco", "June", "lady Penelope", "Ceylon", "Screwdrivers", "Kansas City Chiefs", "G minor", "My Summer Story", "2004", "The Outsiders", "Amberley Village", "lack of a cause of death and the absence of any soft tissue", "President Obama", "kerstin and the rest of the family", "cixi", "Brigham Young", "June", "chalk quarry"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5252976190476191}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-2860", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_hotpotqa-validation-2330", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-2849"], "SR": 0.484375, "CSR": 0.5650337837837838, "EFR": 0.696969696969697, "Overall": 0.6513850711506961}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "neutral", "roofs of the choir side - aisles", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist James Hutton", "N 17 \u00b0 26 \u2032 48", "joy of living", "420", "George Strait", "the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "1990", "Shawn", "Kiss", "January to May 2014", "Los Angeles", "September 28, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "By 1770 BC", "Niveditha, Diwakar, Shruti", "two parties", "Jane Lynch", "cell nucleus", "Anakin", "Travis Tritt and Marty Stuart", "1976", "Bee Gees", "Matt Czuchry", "Pradyumna", "registration of births with the United Kingdom government is a practice that originated at least as far back as 1853", "Lands End", "Psychomachia", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates men's basketball", "two", "0.30 in ( 7.6 mm )", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "by January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria", "the Hudson Bay", "The Maginot Line", "France", "Dumbo", "purple", "Charles Guiteau", "Gettysburg Address", "iTunes, iTunes Radio, and iTunes Music", "$273 million", "India", "Jeddah, Saudi Arabia", "Desperate Housewives", "Cannonball Run", "guerrillas", "Tuesday"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6949540675383866}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4615384615384615, 0.0, 0.0, 1.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07272727272727272, 0.25, 0.7272727272727273, 1.0, 0.3333333333333333, 1.0, 0.782608695652174, 0.8, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-5119", "mrqa_searchqa-validation-2335"], "SR": 0.5625, "CSR": 0.564967105263158, "EFR": 0.8214285714285714, "Overall": 0.676263510338346}, {"timecode": 38, "before_eval_results": {"predictions": ["Isaac Newton", "25 years after the release of their first record", "the United States", "Kim Basinger", "fall of 2015", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in positions Arg15 - Ile16", "Joseph M. Scriven", "Lady Gaga", "Chicago metropolitan area", "president", "Domhnall Gleeson", "eusebeia", "horticulture", "Southampton ( 1902, then in the Southern League ) being the last finalist from outside the top two tiers", "nobiliary particle", "Stephen A. Douglas", "1984", "related to the Common Germanic word guma", "Pakistan", "the 2018 Winter Olympics was held at the Gangneung Ice Arena", "Tagalog or English", "Bryan Cranston", "thylakoid membranes", "two weeks of low mood that is present across most situations", "Felix Baumgartner", "Wake County", "December 1922", "18 Divisional Round", "602", "stable, non-radioactive rubidium", "Membership is believed to cost between $10,000 and $30,000", "the studies and developments department of the French firm R2E Micral", "1931", "University of Oxford", "Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "Southern Cause", "Randy", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "divergent tectonic", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "Margaret Thatcher", "Roddy Doyle", "Daniil Borisovich Shafran", "TD Garden", "Venus", "starting a dialogue while maintaining sanctions,", "10 below", "Henry Ford", "David McCullough", "Rendezvous with Rama", "CERN", "Portugal"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5793815253374077}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.09523809523809525, 0.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7857142857142858, 1.0, 0.2666666666666667, 0.0, 0.4, 1.0, 0.4, 0.4666666666666666, 0.3333333333333333, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-1411", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-3219"], "SR": 0.484375, "CSR": 0.562900641025641, "EFR": 0.7272727272727273, "Overall": 0.6570190486596736}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale : Sammy's Adventures", "Jenny Slate", "ATP", "Philippe Petit", "R2E Micral CCMC", "January 2004", "southwest and along the Yangtze", "Toby Keith", "development of electronic computers", "17 - year - old", "heavy metal", "Set six months after Kratos killed his wife and child,", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "XXXX", "the Berlin School of experimental psychology", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Phil Johnston", "Richard Crispin Armitage", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "either late 2018 or early 2019", "diffuse nebulae", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "McKim Marriott", "John F. Kelly", "Santiago Ram\u00f3n y Cajal", "1886", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "1960s", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "near temples", "1940", "Tad '' Stone", "Mark Jackson", "Michael Buffer", "`` There is one body and one Spirit just as you were called to the one hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all", "on location", "federal government", "New England", "Cody Fern", "questions about the name of the war, the tariff", "prophets and beloved religious leaders", "saliva", "Juan Manuel de Ayala", "Joseph Smith", "funny Folks (1874 - 1894)", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew", "Pearl", "Squalus acanthias", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.484375, "QA-F1": 0.612800376555379}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.3076923076923077, 0.967741935483871, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.47058823529411764, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0851063829787234, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-8323"], "SR": 0.484375, "CSR": 0.5609375, "EFR": 0.7272727272727273, "Overall": 0.6566264204545454}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"His treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "eight Indian army troopers, including one officer, and 17 militants,", "Donald Trump", "cancer awareness", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "Zimbabwe President Robert Mugabe", "since 2004", "NATO", "Switzerland", "Monday", "second", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "he wants to spend billions to revitalize the nation's economy, a plan the campaign of his likely Republican opponent said would slow economic growth with higher taxes.", "T.I.", "state of Oaxaca, Mexico", "Robert Barnett", "$627", "41", "Adenhart", "a strict interpretation of the law,", "Derek Mears", "Sylt", "rural Tennessee", "Tuesday afternoon", "southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "all faiths", "The son of Gabon's former president", "The Transportation Security Administration", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Derek Mears", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "St. Louis, Missouri", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2019", "P.V. Sindhu", "Mexico", "Snickers candy bars", "narwhal", "abbot", "Anaheim, California", "uncle", "Bergen", "embalming", "Cartagena", "Graphical", "German Shepherds have a two - layer coat which is close and dense with a thick undercoat"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6713045408831666}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.47058823529411764, 0.0, 0.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.3870967741935484, 0.13953488372093023, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 0.0, 1.0, 0.9523809523809523, 0.3076923076923077, 0.4, 1.0, 1.0, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.10526315789473682]}}, "before_error_ids": ["mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_naturalquestions-validation-10583"], "SR": 0.484375, "CSR": 0.5590701219512195, "EFR": 0.6666666666666666, "Overall": 0.6441317327235773}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "displacement", "the modern state system", "Megan Park", "It is the currency used by the institutions of the European Union", "Kate Walsh", "September 14, 2008", "American country music artist Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point of a liquid", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "A remittance is a transfer of money by a foreign worker to an individual in his or her home country", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "The hybridization of these species prevents healthy males from forming, whereas males exist in both parent species", "David Joseph Madden", "1902", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "A firm, flexible cup - shaped device worn inside the vagina to collect menstrual flow", "pigs", "General George Washington", "Spanish", "Virgil Tibbs", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the atrioventricular node, along the Bundle of His and through bundle branches", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six 50 minute ( one - hour with advertisements ) episodes", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "Intertropical Convergence Zone ( ITCZ )", "Missouri River", "the right to be served in facilities which are open to the public", "frontal lobe", "10 June 1940", "Tandi", "Alberich", "the ear canal", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "the Iraq's autonomous region of Kurdish.", "Denver, Colorado.", "State Department employee", "the American drama television series 24", "Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.5, "QA-F1": 0.6461703431372549}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.8, 0.6666666666666666, 0.7000000000000001, 0.11764705882352941, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_triviaqa-validation-2114", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-1237"], "SR": 0.5, "CSR": 0.5576636904761905, "EFR": 0.84375, "Overall": 0.6792671130952381}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water also absorbs heat ; it thereby cools the smoke, air, walls, and objects that could act as further fuel", "in Middlesex County, Province of Massachusetts Bay", "chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "its population", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "`` she who brings victory ''", "Hermann M\u00fcller", "Ceramic art", "Russia", "Covington, Kentucky", "New Mexico", "reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption", "December 15, 2017", "Paradise, Nevada", "L.K. Advani", "differential erosion", "Glenn Close", "Gospel of Matthew in the middle of the Sermon on the Mount, and the short form in the Gospel of Luke", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "Southampton ( 1902, then in the Southern League ) being the last finalist from outside the top two tiers", "2018", "electricity generation, power distribution, and power transmission on the island", "Tsetse can be distinguished from other large flies by two easily observed features", "Norman Greenbaum", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "compasses", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Daya Jethalal Gada", "2,140 kilometres ( 1,330 mi )", "by producing an egg through parthenogenesis", "1926", "Durban, South Africa", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Thor", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "two", "prostate cancer", "wyvern", "Little Lord Fauntleroy", "a key ring or a decorative key fob", "yellow"], "metric_results": {"EM": 0.5, "QA-F1": 0.63518612730167}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5384615384615384, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 0.9, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.09090909090909093, 1.0, 0.8, 0.14814814814814814, 0.6666666666666666, 1.0, 0.3157894736842105, 1.0, 1.0, 0.0, 1.0, 0.14285714285714285, 0.0625, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.5, "CSR": 0.5563226744186047, "EFR": 0.78125, "Overall": 0.666498909883721}, {"timecode": 43, "before_eval_results": {"predictions": ["1985", "February 27, 2007", "`` pick yourself up and dust yourself off and keep going ', female - empowerment song '' and `` kind of an epiphany song", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "Blue with a harp of gold", "Miami Heat", "1982", "After World War I", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Paul Signac", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "SURFACEARAA of ROOTS", "Alex Ryan", "a habitat", "2018", "Advanced Systems Format ( ASF )", "100 members, two from each of the 50 states", "Toledo", "embryo", "In the last Ice Age, sea levels were lower and the Solent was part of a river flowing south east from current day Poole Harbour towards mid-Channel", "Haikou on the Hainan Island", "Robert Irsay", "Paradise, Nevada", "Alicia Vikander", "annually in late January or early February", "Ashoka", "the name of a work gang", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority ( PREPA )", "Bumblebee", "into the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "armored fighting vehicle", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "during the winter of the 2017 -- 18 network television season on CBS", "Florida", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "in the southwestern part of the island", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "winter", "Pangaea", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson and the 1999 Vermont Supreme Court case \"Baker v. Vermont\"", "Chelsea", "North America", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site, once the center of the country's Cold War plutonium production.", "'We want to reset our relationship and so we will do it together.'\"", "Michigan", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6359477164164664}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3333333333333333, 0.0, 0.8333333333333333, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.21428571428571427, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 0.4444444444444444, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 1.0, 0.9189189189189189, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.25641025641025644, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977"], "SR": 0.46875, "CSR": 0.5543323863636364, "EFR": 0.7647058823529411, "Overall": 0.6627920287433156}, {"timecode": 44, "before_eval_results": {"predictions": ["fixed annual carriage fees of \u00a330m", "aluminum foil", "Laurel, Mississippi", "mountain-climbing", "Indianola", "Ritu Nanda Insurance Services (RNIS)", "Jean Baptiste Point DuSable", "1992", "Cher", "eastern", "Jim Harrison", "Montreal", "Tomorrowland", "fennec", "United States Army", "stop motion animation", "Jean Acker", "5,656", "Leucippus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "Northrop P-61 Black Widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony awards", "1991\u201392", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "OutKast", "Richard Street", "Zaire", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "a pioneering New Zealand food writer.", "South America", "2006", "perjury and obstruction of justice", "Operation Overlord", "Mary Elizabeth Hartman", "over 9,000 employees", "Rosalind Bailey", "potential of hydrogen", "Alamodome in San Antonio, Texas", "Carrie", "The Finger Tab", "Kent", "almost 9 million", "Kenya", "2008", "terrorism", "Moses", "Chapter 5", "Wilson Pickett"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6145776098901099}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2837", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.515625, "CSR": 0.5534722222222221, "EFR": 0.7741935483870968, "Overall": 0.6645175291218639}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "early 20th century", "Australian", "in September 1903", "the power to regulate interstate commerce", "Donna Wallace", "McLaren-Honda", "Tufts College", "People's Republic of China", "Azeroth", "Squam Lake", "Philip Livingston", "Tayeb Salih", "James II of England", "God Save the Queen", "526 people per square mile", "Scotland", "AC/DC founders Angus Young and Malcolm Young.", "GmbH", "Mick Jackson", "Lalit", "his virtuoso playing techniques and compositions in orchestral fusion.", "Tampa Bay Lightning", "Steven Selling", "Sully", "Manhattan Project", "Pacific War", "Romantic", "Hugh Dowding, (24 April 1882 \u2013 15 February 1970)", "AMC Entertainment Holdings, Inc.", "New York Islanders", "Fennec fox", "1978", "six different constructors taking the first six positions.", "French", "Pacific Place", "the Female Socceroos", "\"Bad Blood\"", "\"SexyBack\"", "5320 km", "Giuseppe Verdi", "\"Super Hit\"", "Sacramento Kings", "Walldorf, Baden-W\u00fcrttemberg", "Fife", "Fyvie Castle", "Aamina Sheikh, Hasan Ahmed, Neelam Muneer, Zaheen Tahira, Ismat Zaidi, Sami Khan and Faisal Qureshi.", "the British Army", "power directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Maroon 5", "Saudi Arabia", "Bob Turner", "two"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5942732869663017}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 0.5, 0.5714285714285715, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.25, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.23529411764705882, 1.0, 0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.484375, "CSR": 0.5519701086956521, "EFR": 0.7878787878787878, "Overall": 0.666954154314888}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican National Committee's website", "1996", "five", "Greenland sharks", "The Word", "Abraham Lincoln", "St Jude Thaddeus", "Anthoonij van Diemenslandt", "the death penalty", "xerophyte", "Jackie Robinson", "Staten Island", "Dian Fossey", "MI5", "harrow", "creme anglaise", "a sauce of lemon juice, parsley, salt, pepper, and drawn butter", "pork", "curling", "Victoria Coren Mitchell", "Gettysburg", "Chile\u2019s best wine producing regions", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck Schumann", "Mercury", "Venus", "Barack Obama", "Canada", "F-O-R-G-T-E-N", "Castro and the Cuban Revolution", "Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "all of its land in North America", "Mary Poppins", "Glyn Jones", "Port Moresby Harbour", "Connecticut", "Quentin Blake", "whooping cough", "The Daily Herald", "numerous", "slaughter of a halal animal is called \"zabihah\"", "2016", "the courts", "2017", "Chief of Protocol of the United States", "Diamond White", "1987", "a fan", "near Garacad, Somalia", "death", "Beatrix Potter", "Dan Eggen and Elizabeth Williamson", "How to Keep Young Mentally", "Divide the living child in two"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5854166666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4157", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-252", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.484375, "CSR": 0.550531914893617, "EFR": 0.7878787878787878, "Overall": 0.666666515554481}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "David Beckham", "on Scottish soil took place on a nearby moor at Culloden", "Runic", "Spain", "cricketer", "Max Planck", "rotherham United", "heat exchange", "Misery", "Styal", "a stately procession", "blind beggar", "Brainwash", "l Leroy Burrell (United States)", "parlophone", "Wild Atlantic Way", "John Denver", "Terry Pratchett", "noddy Goes To Toyland", "Lackawanna six", "Brazil", "pong", "muezzin", "a window", "a ship", "Madame Bovary", "Apollo 11", "flit gun", "Nikola Tesla", "Nicky Henderson", "Evita", "sperm whale", "Rocky Graziano", "east fife", "St Pancras International Station", "social environment", "presliced bread", "Dilbert", "mayor of Casterbridge", "dimittis", "French", "Medea", "Burgundy", "cribbage", "w/e 17th birthday in 2005", "Johannesburg", "France", "Muffin Man", "Seoul", "Prince James, Duke of York and of Albany", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "from Galveston, Texas, to Veracruz, Mexico", "if the airline doesn't perform, the credit card company still has your money and can give it right back to you.Periodically check in on your airline", "Robert Frost", "King Henry VIII", "pillsbury", "1995 Mitsubishi Lancer OZ Rally"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6529153138528139}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.07142857142857142, 1.0, 0.0, 0.0, 0.8000000000000002]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-5130", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.59375, "CSR": 0.5514322916666667, "EFR": 0.7307692307692307, "Overall": 0.6554246794871795}, {"timecode": 48, "before_eval_results": {"predictions": ["Route sixty-six", "sesame Street", "minced tomatoes", "cabbage", "south Australia", "Mr. Magoo", "mountain torrents", "Ash tree", "opossum", "New Zealand", "slide whistles", "60", "goldfinger", "1984", "a fish included in the pike family, Esocidae", "Mongol Empire", "1875", "tax collector", "penny", "maria", "Wars of the Roses", "Bagram", "maggie Gilkeson", "Chrysler", "fur hat", "korky the cat", "Civil Law", "United States", "Brazil", "pei Tang", "biathlon", "Idaho Falls", "Charlie Chan", "Vienna", "white", "jaws", "Paul Rudd", "mouse", "Scotland's", "jodhpurs", "Orson Welles", "Hindu Wisdom", "menorah", "Dutch", "Texas", "Super Bowl Sunday", "a quant pole", "Little Tommy Stout", "mzarts", "Rhododendron", "Ireland", "Chuck Noland", "the Colony of Virginia", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park", "2010", "through Greece, the birthplace of the Olympics,", "10 below", "Nearly all of Britain's troops in Iraq will have left by the week's end.", "silver", "Chow Chow", "Omaha, Nebraska", "How Rocket Learned to Read"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5193452380952381}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-6306", "mrqa_naturalquestions-validation-4803", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-6607", "mrqa_searchqa-validation-4136"], "SR": 0.453125, "CSR": 0.5494260204081632, "EFR": 0.7714285714285715, "Overall": 0.663155293367347}, {"timecode": 49, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.73828125, "KG": 0.46796875, "before_eval_results": {"predictions": ["sledge", "Iran", "alcohol", "frenchman", "jockey", "Daniel Boone", "Thames Street", "Theodore Roosevelt", "satyrs", "raw louvar", "La Boh\u00e8me", "amorance denied", "bone", "Garrick Club", "Lackawanna 6", "Barnaby Rudge", "Britten's", "American Civil War", "dark", "Cybill Shepherd", "Jimmy Robertson", "Florence", "Basil", "victoria", "Severn", "Australia", "South Africa", "perennial", "Nicaragua", "Churchill", "duke of Suffolk", "Chemnitz", "data", "trout", "ap\u00e9ro", "wali Muhammad", "Belize", "American Folk Song", "hair loss", "sprint", "Charlie Drake", "Robin Hood Men in Tights", "Chris Martin", "flinstone", "George Gently", "rugby", "honda", "deacon Blues", "11", "tobacco", "cows", "depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "Tom Selleck", "New Orleans", "comic book superhero", "Texas Tech University", "Loughborough Technical Institute", "Herman Cain", "the United States", "air support", "George Babbitt", "Oklahoma", "Tonight", "four"], "metric_results": {"EM": 0.375, "QA-F1": 0.45900297619047614}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857144, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-2670", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.375, "CSR": 0.5459375, "EFR": 0.85, "Overall": 0.678640625}]}