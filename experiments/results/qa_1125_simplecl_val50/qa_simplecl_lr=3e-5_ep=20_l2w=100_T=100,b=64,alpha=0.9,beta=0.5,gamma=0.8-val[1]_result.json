{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=20_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=100.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=20_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4100, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 0.9333333333333333, "Overall": 0.8494791666666667}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "1970s", "Sunni Arabs from Iraq and Syria", "P,NP-complete, orNP-intermediate", "Daniel Burke", "the highest terrace", "major national and international patient information projects and health system interoperability goals", "three", "net force", "12 January", "1976\u201377", "E. W. Scripps Company", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "Book of Discipline", "complicated definitions", "coordinating lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "main hall", "the Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation", "2014", "late 1970s", "30% less", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "teachers in publicly funded schools", "Stanford University Professor of Comparative Literature Richard Rorty", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts", "The Prisoners ( Temporary Discharge for Ill Health ) Act", "Carol Ann Susi", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8206106635237826}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-6282", "mrqa_squad-validation-3352", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "His wife Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger NFL", "1950s", "1980s", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "second use of the law", "free", "1973", "1971", "Mansfeld", "Warsaw Stock Exchange", "390 billion", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "13 May 1899", "Lunar Module Pilot", "citizenship", "Merritt Island", "accountants", "return home", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification", "Lituya Bay in Alaska", "120 m ( 390 ft )", "the eighth season will have only six episodes", "100 members", "photoelectric", "Welch, West Virginia", "26 January was chosen as the Republic day because it was on this day in 1930", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Hal David and Burt Bacharach", "five points", "The Monitor and Merrimac", "the Atlantic Ocean, the Gulf of Mexico and the Straits of Florida"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7859293050699301}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.888888888888889, 0.4444444444444445, 1.0, 0.4, 1.0, 0.4090909090909091, 1.0, 0.0, 0.5, 0.5, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-80", "mrqa_squad-validation-9173", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1454", "mrqa_squad-validation-1841", "mrqa_squad-validation-1220", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-2016", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-3996"], "SR": 0.703125, "CSR": 0.7447916666666667, "EFR": 0.8421052631578947, "Overall": 0.7934484649122807}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive", "principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "Abercrombie was recalled and replaced by Jeffery Amherst", "Egypt", "algae", "4,404.5 people per square mile", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "spy network and Yam route systems", "Stairs", "genetically modified plants", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Mark Ronson", "Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw)", "cloud storage", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum", "Denver's Executive Vice President of Football Operations and General Manager", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "Golden Gate Bridge", "Michael Schumacher holds the record for the most Grand Prix victories, having won 91 times", "10.5 %", "The Man", "President Gerald Ford", "Bud '' Bergstein", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "Forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "December 1971", "Land of the Valar", "the middle of the 15th century", "6 March 1983", "Viola Larsen", "horror fiction", "26,000"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7595238095238095}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.4, 0.0, 0.8571428571428571, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-4759", "mrqa_squad-validation-8763", "mrqa_squad-validation-6154", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-670", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433", "mrqa_hotpotqa-validation-454"], "SR": 0.65625, "CSR": 0.72265625, "EFR": 1.0, "Overall": 0.861328125}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Presque Isle", "wireless", "Coldplay", "the Yuan dynasty", "same-gender marriages with resolutions", "red algae red", "after their second year", "1960s", "narcotic drugs were controlled in all member states, and so this differed from other cases where prostitution or other quasi-legal activity was subject to restriction", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot", "50 fund", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "dissension and unrest", "Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "NDS, a Cisco Systems company", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "a thylakoid", "University College London", "to protect their tribal lands from commercial interests", "religious beliefs", "evading it", "the kettle and the Cricket", "Gandhi", "Vlad the Impaler", "\"Take us the foxes, the little... Alexandra - first cousins - as a means of getting Horace's money", "VHS", "Vincent van Gogh, in which Nimoy played Van Gogh's brother Theo.", "Earth's orbital period is 365 & this fraction of a day", "Tiger Woods won the 1994 U.S. Amateur in dramatic, history-making fashion.", "It was Arizona's territorial capital from 1867 to 1877,", "Marshal Dillon", "They come from our heartJi, from our...   14 - American Radio History  Mar 14, 1997", "The Best Hotels on Bali", "The new nightspot Teddy's made this presidential Hollywood hotel a happening", "Light Amplification by Stimulated Emission by radiation", "Jean Dapra", "Saturn", "Hundreds of species of peat mosses are found in bogs throughout Canada", "Why", "Daya", "fear of meat", "American", "Mexican military"], "metric_results": {"EM": 0.546875, "QA-F1": 0.653638230981981}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14814814814814814, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2666666666666667, 0.0, 0.3076923076923077, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.1818181818181818, 0.25, 0.0, 0.0, 0.15384615384615385, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-10204", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-10186", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-2804", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073"], "SR": 0.546875, "CSR": 0.6875, "EFR": 0.7586206896551724, "Overall": 0.7230603448275862}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "a Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "hunting", "the member state cannot enforce conflicting laws", "Graham Twigg", "a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals", "inversely", "Europe", "he was illiterate in Czech", "colonies", "$37.6 billion", "Kalenjin", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center in San Jose", "Variable lymphocytes receptors (VLRs)", "Edict of Fontainebleau", "Levi's Stadium", "ten million", "the Lippe", "Video On Demand content", "time and storage", "semester", "the courts of member states and the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "Lawrence, Nassau County, New York", "League of the Three Emperors", "science", "143,007", "Clinton", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "drawing the name out of a hat", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Br'er Rabbit", "corruption", "24 hours", "Dover Beach"], "metric_results": {"EM": 0.75, "QA-F1": 0.8212253891941392}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-4919", "mrqa_squad-validation-1187", "mrqa_squad-validation-8544", "mrqa_squad-validation-457", "mrqa_squad-validation-6676", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.75, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["1540s", "the Court of Justice of the European Union", "its circle logo", "three", "negative", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "distorting the grana and thylakoids", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "overthrow a government", "entertainment", "vote clerk", "high growth rates", "a vicious and destructive civil war", "Sony", "Stagecoach", "Silk Road", "San Diego", "a German Nazi colonial administration", "four public charter schools on the South Side of Chicago", "the means to invest in new sources of creating wealth", "Spanish", "Structural geologists", "president and CEO", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Ben Johnston", "a large green dinosaur", "Taylor Swift", "Eric Edward Whitacre", "Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "between 11 or 13 and 18", "Heather Langenkamp (born July 17, 1964)", "Henry Gwyn Jeffreys Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI) (Lithuanian: \"Vilniaus oro uostas\"", "It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index", "Charmed", "Lily Hampton", "English former international footballer", "Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "Ashanti", "79", "Algeria", "a novel", "the Eastern part", "Polar Bear", "Atlantic City"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7711046918767508}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.2, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3939", "mrqa_squad-validation-5774", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-9665", "mrqa_squad-validation-7983", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-5279", "mrqa_searchqa-validation-1971"], "SR": 0.65625, "CSR": 0.6919642857142857, "EFR": 0.8636363636363636, "Overall": 0.7778003246753247}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "the working fluid", "a suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "the solution", "those who already hold wealth", "center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "the papacy", "through homologous recombination", "canalized section", "in protest against the occupation of Prussia by Napoleon", "improved markedly", "northern (German) shore of the lake", "computer programs", "General Conference", "1996", "dreams", "The Judiciary", "single-tape", "Bart Starr", "allotrope", "Karluk Kara-Khanid", "Perth, Western Australia", "Ian Rush", "Gerry Adams", "New Orleans Saints", "2016", "four operas", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "capital of the Socialist Republic of Vietnam", "Sevens", "fennec fox", "Bart Conner", "fantasy", "Martin \"Marty\" McCorm (born 20 July 1983)", "Black Mountain College", "Atat\u00fcrk Museum Mansion", "Bothtec", "Cody Miller", "140 to 219 passengers", "John Locke", "Garth Jennings, co-directed by Christophe Lourdelet", "Pablo Escobar", "recent African descent", "Mexico City", "Sleeping Beauty", "PeopleMover", "8 December 1985", "Doddi in Iceland, Purzelknirps in Germany and Hilitos in Spain", "Ali Bongo", "Wheat Chex", "Ray Harroun", "Juice Newton", "David Tennant"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7663194444444444}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-9287", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-3413", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-3885", "mrqa_triviaqa-validation-1573", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869"], "SR": 0.71875, "CSR": 0.6953125, "EFR": 0.8888888888888888, "Overall": 0.7921006944444444}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Revolutionary", "higher efficiency", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "fire", "arms", "two", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "TeacherspayTeachers.com", "stratigraphic", "commensal flora", "a + bi", "General Conference in Dallas, Texas", "Central Asian Muslims", "from home viewers who made tape recordings of the show", "1330 Avenue of the Americas", "Alberta and British Columbia", "\"Pimp My Ride\"", "Don Johnson", "Section.80", "25 million", "8,515", "13 October 1958", "tailless", "Environmental Protection Agency", "between 1932 and 1934", "an English former international footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Adam Amin", "boxer", "Boston University", "Fulham", "A55", "Ranulf de Gernon, 4th Earl of Chester", "\u00c6thelstan", "Special economic zone", "44", "Division I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "center", "Shirley Horn", "Israel", "Bigfoot", "Papua New Guinea", "Edgar Degas", "Manchester"], "metric_results": {"EM": 0.625, "QA-F1": 0.7145292207792209}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.2, 0.0, 0.28571428571428575, 0.0, 0.8, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3391", "mrqa_squad-validation-9859", "mrqa_squad-validation-7643", "mrqa_squad-validation-5972", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-3553", "mrqa_triviaqa-validation-3170", "mrqa_newsqa-validation-1268", "mrqa_triviaqa-validation-1423"], "SR": 0.625, "CSR": 0.6875, "EFR": 0.7916666666666666, "Overall": 0.7395833333333333}, {"timecode": 9, "before_eval_results": {"predictions": ["$32 billion", "centrifugal governor", "Orange County", "The chloroplast peripheral reticulum", "1962", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "unpaired electrons", "French", "Museum of the Moving Image in London", "sent missionaries", "pyrenoid and thylakoids", "Woodward Park", "non-violent", "25 May 1521", "essentially holy people", "diplomacy or military force", "increase in the land available for cultivation", "the value of the spin", "pivotal event", "transgender teenage girl", "John Alexander", "David Michael Bautista Jr.", "Black Friday", "actor, singer and a DJ", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia, the Disneyland Monorail, and the Motor Boat Cruise", "Yasir Hussain", "USC Marshall School of Business", "Stephen James Ireland", "Marko Tapani \" Marco\" Hietala", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "2500 ft", "Central Park", "Robert John Day", "Afroasiatic", "James Tinling", "Italy", "79th Masters Tournament", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "a North African dish of small steamed balls of semolina, usually served with a stew spooned on top", "more than 22 million", "morphine sulfate oral solution 20 mg/ml", "The Firm (1993 film)", "a freshwater, airbreathing catfish"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6596118771922259}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4147", "mrqa_squad-validation-2943", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_triviaqa-validation-2522", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.578125, "CSR": 0.6765625, "EFR": 0.6666666666666666, "Overall": 0.6716145833333333}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "Timucuan", "suburban", "vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "Panthers", "Sanders", "economic instability", "Gamal Abdul Nasser", "Immunodeficiencies", "counterflow", "John B. Goodenough", "not covered in any newspapers", "arrows, swords, and leather shields", "Cybermen", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "Standard Model", "Tolui", "the Rhine-Ruhr region", "course of study", "Prevenient grace", "Kansas Jayhawks", "Captain Cook's Landing Place", "Chris Pine", "Yoo Seung-ho", "World War II", "NCAA Division I", "The The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Uzumaki", "Tom Jones", "Russell Humphreys", "Barbara Niven", "13\u20133", "Eliot Spitzer", "5,042", "European", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "Giuseppe Verdi", "Germanic", "New Jersey", "Bath, Maine", "Ector County", "Jim Davis", "Buck Owens", "World Health Organization", "Emmanuel Ofosu Yeboah", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Heather Stebbins", "ky", "Sir Giles Gilbert Scott", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills", "Comoros Islands", "Onomastic Sobriquets In The Food And Beverage Industry", "The Londoner"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7430986830514448}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 0.0, 1.0, 0.23255813953488372, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7269", "mrqa_squad-validation-797", "mrqa_squad-validation-6927", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.65625, "CSR": 0.6747159090909092, "EFR": 0.8636363636363636, "Overall": 0.7691761363636365}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate", "Battle of Olustee", "Spanish", "100\u2013150", "Philo of Byzantium", "cooler", "marine waters worldwide", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "his mother's genetics and influence", "shock", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "new element", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "There is a step in the right direction. It educates the consumer on how much they are paying for having a low-MPG car and encourages them to get into a more efficient vehicle", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "Larry King", "does not involve MDC head Morgan Tsvangirai", "there's no evidence as to the cause of death,... you could have an accidental death and a mother that panics", "200", "The number of students getting into drugs is bigger and also getting younger and younger", "opposition party members", "Missouri", "\"Racism and racist conversations have no place today in America.\"", "executive director of the Americas Division of Human Rights Watch", "Dominican Republic", "90", "KARK", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "in her home", "Employee Free Choice Act", "Bush administration", "more than 200", "This is not a project for commercial gain. It is done with the parents' full consent", "best-of-three series", "Kaka", "Japanese ex-wife", "Dan Parris, 25, and Rob Lehr, 26", "apartment near Fort Bragg in North Carolina", "two", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site", "Jacob", "Molotov cocktails, rocks and glass", "250,000", "Winehouse", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Jean F Kernel ( 1497 -- 1558 )", "Thomas Hardy", "Richmond", "1994", "The Conjuring", "The Gallipoli Campaign", "Georgian Bay", "Nowhere Boy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6329731995296211}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.0, 0.21052631578947367, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.38095238095238093, 0.0, 1.0, 0.0, 0.4444444444444445, 0.8333333333333333, 1.0, 0.2608695652173913, 1.0, 1.0, 0.0, 0.0, 0.6, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1257", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3068", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.546875, "CSR": 0.6640625, "EFR": 0.7586206896551724, "Overall": 0.7113415948275862}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "wealth", "every good work designed to attract God's favor", "Napoleon", "new technology and machinery", "James O. McKinsey", "private actors", "Bell Northern Research", "a body of treaties and legislation", "1227", "lower lake", "three", "Elders", "587,000", "Private Bill Committees", "Bruno Mars", "the Catechism", "Stagg Field", "Ian Botham", "Pyotr Tchaikovsky", "Kawasaki", "Al Shean", "Salvador Allende", "Marie Antoinette", "Redmond, United States", "Erik Thorvaldson", "Marsyas", "Pal Joey", "Mary Jane Grant", "\"The Homeland Security Advisory System\"", "Brunei", "supreme religious leader of all the subordinate priests", "Antonio", "European Economic Community (EEC)", "Christine Keeler", "Yehoshua", "Jack Nicholson Easy Rider", "four", "Netherlands", "\"Sugar Baby Love\"", "Coretta Scott King", "Sean", "Bill and Taffy Danoff", "zygote", "Travis", "Blue Peter", "Robert Kennedy", "Q", "umbrella", "an author and philosopher", "barber", "Rod Laver", "Murrah Federal Office Building", "Evita", "an old, unsavoury, and oily black clay pipe", "fortified complex", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "\"It is very easy for comments to be taken out of context and create unnecessary drama", "a delegation of American Muslim and Christian leaders", "Royal Wives", "the Greek Village", "Kim Clijsters"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5663039434523809}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0625, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-5431", "mrqa_squad-validation-9418", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_triviaqa-validation-712", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-5929", "mrqa_newsqa-validation-1150"], "SR": 0.515625, "CSR": 0.6526442307692308, "EFR": 0.7741935483870968, "Overall": 0.7134188895781638}, {"timecode": 13, "before_eval_results": {"predictions": ["Polignac's conjecture", "Bo'orchu", "Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "Fu\u00dfach", "Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees", "Dublin, Cork, Youghal and Waterford", "Tangled", "dernell", "moles", "diogenes Laertius", "concrete", "Anne Boleyn", "garon d. kuren", "Steve McQueen", "Portugal", "jazz tenor saxophonist", "one", "komando Pasukan Khusus", "Carlisle", "liquid", "Chillicothe and Zanesville", "Lucas McCain", "Antarctica", "mercury gilding", "aniridia", "Charles A. Carpenter", "River Forth", "woe", "NOW Magazine", "j Jesse James", "Italy", "Canada", "typhoid fever", "Pavarotti", "action figure", "Walt Kowalski-Gran Torino", "2010", "volume of a given mass of a gas", "Venezuela", "altinge Burlesque Theater in New York City in 1935", "temperature inversion", "40", "phrenology", "San Francisco", "Fall 1998", "Regulus", "Christopher James \" Chris\" Weidman", "Athletics Stadium", "one", "Virgin America", "an illegitimate daughter", "albergull florist", "Iran's parliament speaker", "In Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory"], "metric_results": {"EM": 0.5, "QA-F1": 0.5322916666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6078", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-2463", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.5, "CSR": 0.6417410714285714, "EFR": 0.6875, "Overall": 0.6646205357142857}, {"timecode": 14, "before_eval_results": {"predictions": ["in an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary civil disobedience", "Beijing", "three years", "27 July 2008", "chemically", "Aristotle", "St. George's Church", "Missy", "Strathclyde Regional Council debating chamber in Glasgow", "public official", "the most cost efficient bidder", "acorn", "Continent that spans 11 time zones", "thighbone", "Olympia", "Chornobyl", "Whatchamacallits", "Teri Hatcher", "a hopeful miner", "amber", "Yale Basmati", "The executioner's Song", "180 degree", "almaty", "Anamosa", "drei Akten", "Ephesus", "Camelot--Camelot", "film", "rope", "blinding light", "Cologne", "Leadership Academy for Girls", "an ingot", "Kosovo", "James Jeffords", "Prague", "tennis", "silk", "buffalo", "raoul", "rooileus virginianus", "japan", "\"A Beer Can Named Desire\"", "Thant", "boys", "windjammer", "Monsieur Verdoux", "George S. Klein", "Augusta", "counter clockwise", "2013", "Nick Hornby", "parachutes", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama", "memories of his mother", "Israel"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4833333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-6518", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-325", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3084"], "SR": 0.40625, "CSR": 0.6260416666666666, "EFR": 0.7894736842105263, "Overall": 0.7077576754385965}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes) (865\u2013915)", "river Deabolis", "April 20", "Rhenus", "1996", "wine", "German-Swiss", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "Arthur Wynne", "Eric Maguire", "Strongsville, Ohio", "(Flemish)", "Mastercard", "Roger B. Smith", "Nashville", "the olfactory nerve", "Ivan the Terrible", "Nancy Astor", "lentigo senilis", "rob a casino", "Toronto Maple", "Zsa Zsa Gabor", "acting", "Utah", "a liquor made from molasses or sugar cane", "(Rabbit) Angstrom", "Johann Strauss II", "joey", "pro bono", "the Universit degli Studi di Siena", "a candy store", "a brown", "Anthony Fokker", "Nacho Libre", "copper", "devils or demons", "hemlock", "Jeffrey Wigand", "Wordsworth", "a sesame seed bun", "a meager allowance", "1942", "blimps;blimp", "Robert Kirchhoff", "a geisha", "a mermaid", "altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "dark", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6390625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6463", "mrqa_squad-validation-9248", "mrqa_squad-validation-9270", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-7531", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-12729", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.515625, "CSR": 0.619140625, "EFR": 0.7741935483870968, "Overall": 0.6966670866935484}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite", "respiration", "1997", "late 1920s", "\u00a31.3bn", "27 July 2008", "unequal", "October 1973", "dragonnades", "Isiah Bowman", "assembly center", "Ominde Commission", "the Weser River", "in an open casket", "(Ho) Minh", "(circum) circumference", "an Inuit dwelling", "Detroit Rock City", "Toronto Blue Jays", "President Lincoln", "Ray Bradbury", "crimes committed out of hatred for someone's race", "King Julien XIII", "Nicolas Sarkozy", "Rubicon", "(Conello)", "17", "Louisa May Alcott", "Play-Doh", "Aphrodite", "Jesus Christ", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Clinton", "(Errol Lincoln)", "(Bellerophontes)", "Balaam", "business school of business", "the caine mutiny", "(The Allman Brothers Band)", "(The Clorox Company)", "(John) Coltrane", "peace sign", "oxygen", "Sphinx", "Jan Hus", "USA Network's (The Sing-Off)", "blue", "Onegin", "Macy's", "spinning jenny", "(Santa)", "(Denzel Washington)", "negligence", "judges", "attached to another chromosome", "Goosnargh", "Australia", "The Jefferson Memorial", "aged between 11 or 13 and 18", "Michoacan Family", "prisoners", "his salary", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5933264652014651}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.8, 0.5, 1.0, 0.2, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-12183", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-6610", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-9150", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-794", "mrqa_triviaqa-validation-4973", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1759"], "SR": 0.453125, "CSR": 0.609375, "EFR": 0.8, "Overall": 0.7046875}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Robert Lane and Benjamin Vail", "ring", "Party of National Unity", "22", "Dauphin", "Phillip Marlowe", "piracy", "Roger Clemens", "(Myuran Sukumuran", "Puerto Rico", "The Mausoleum", "The World Through More Than One lens", "Syria", "Deutsche Lufthansa", "The Old Man", "French", "Joe Louis", "Hydra", "the three Musketeers", "the Bayeux Tapestry", "a porch", "China", "Sunni", "notes placed at the bottom of a page", "Stephen Hawking", "Cicero", "Memphis", "soda", "Blanche DuBois", "quilt", "FRAM", "House of Representatives", "beer company", "Michael Moore", "Oman", "Chevy", "Ingenue", "Pennsylvania", "Don Juan", "Ian Fleming", "Headless Horseman", "London", "Yellowstone", "Summer", "Fiddler", "Ethiopian", "six", "1992", "a salt", "Bromley", "the Ruul", "Cartoon Network", "a small child", "know what's important in life", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "nuclear", "The drama of the action in-and-around the golf course"], "metric_results": {"EM": 0.625, "QA-F1": 0.6614583333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_squad-validation-1659", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-11215", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-1920", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-12814", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-4110"], "SR": 0.625, "CSR": 0.6102430555555556, "EFR": 0.7916666666666666, "Overall": 0.7009548611111112}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "1993", "June 1979", "friend", "tentacles", "Robert R. Gilruth", "complexity", "same-gender marriages", "2006", "mid-18th century", "hollandaise", "A Raisin in the Sun", "Sistine Chapel", "bielaruski", "a spread formation", "parsnips", "Big Bang", "Sex Pistols", "endodontist", "Saturn", "Cliffs of Dover", "Genoa", "Fanchette", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Indiana", "Sleepless", "polyantha", "pho- a popular Vietnamese dish", "21", "John Janetzko", "Copeina arnoldi", "Paul McCartney", "omega-3", "Pivot", "Bachman Turner Overdrive", "FEARnet", "caddy Shack", "Tokyo", "Panama", "Eppson Center for Seniors", "Narnia", "Finnegans Wake", "Wordsworth", "Norway", "The Bear", "nuclear bombs", "Betrayal of Christ", "elephants", "Mazur", "Denmark", "covert", "The Silver State", "May 2010", "in the United States", "Sugarloaf Mountain", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "the estate", "McFerrin, Robin Williams, and Bill Irwin", "ase", "the upper peninsula of Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts In northern Europe, a population was found in Moscow, Russia in 2003"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5270469961240309}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6976744186046512]}}, "before_error_ids": ["mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-4853", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-2870"], "SR": 0.421875, "CSR": 0.600328947368421, "EFR": 0.918918918918919, "Overall": 0.75962393314367}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "plants that are biologically contained", "Earth", "53,000", "one", "poet", "two points", "20,000", "the kip", "skeletal muscle and the brain", "2014", "peptide bonds", "Montreal", "the results show moved to Sunday evenings", "egg", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "arousal recognition", "Charlene Holt", "Buffalo Bill", "1991", "electron shells", "The Cornett family", "acid rain", "October 22, 2017", "Points that lie strictly to the left of the curve are said to be inefficient", "he cheated on Miley", "2001", "flawed democracy", "735 feet", "1871", "Rick Rude", "Ohio", "a form of business network", "a cylinder of glass or plastic", "Abraham Gottlob Werner", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Necator americanus and Ancy lostoma duodenale", "February 28", "Furious 7", "the American Civil War", "an electrical potential or ion concentration difference across the membrane", "Cecil Lockhart", "Mara Jade", "British and French Canadian fur traders", "semi-autonomous organisational units", "Lou Rawls", "Hermia", "Jupiter", "Latin", "15", "John Robert Cocker", "Israel", "a simple puzzle video game", "a palace", "the olfactory nerve", "Mallee", "a horse", "oxygen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.605859375}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.625, 0.0, 0.2, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8896", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-3747", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-2227"], "SR": 0.53125, "CSR": 0.596875, "EFR": 0.8666666666666667, "Overall": 0.7317708333333334}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "Cloth of St Gereon", "Thomas Sowell", "more than 70", "death of a heretic", "His Biblical ideal of congregations' choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape", "illegal crossings", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer", "well over 1,000 pounds", "political dead-end", "Mutassim", "Oklahoma", "Polo", "Joe Jackson", "in Amstetten", "computer problems", "Silvan Shalom", "Climatecare", "Steve Wozniak", "12-hour-plus shifts", "prisoners", "childbirth", "consumer confidence", "5:20 p.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "India", "1964", "Davidson", "Pakistan's combustible Swat Valley", "Friday", "1979", "the United States", "GospelToday", "chief executive officer", "\"There's no chance of it being open on time. Work has basically stopped,\" Bloomberg", "file papers shortly with an appeals court seeking an emergency stay", "non-European Union player in Frank Rijkaard's squad.Mexican forward Giovani dos Santos is set to take up the vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast midfielder Yaya Toure", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "orphans", "40", "Derek Mears", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter", "Whitsunday", "Dee and the Don", "Dumb and Dumber", "2004 Nokia Sugar Bowl", "Minton", "converging lens", "autu", "season five", "Revenge of the Sith"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5865172674547674}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.72, 0.16666666666666663, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-2466", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7266", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-3226", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.453125, "CSR": 0.5900297619047619, "EFR": 0.6, "Overall": 0.595014880952381}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress among teachers", "El Centro metropolitan area and San Diego-Carlsbad-San Marcos metropolitan area", "chief electrician", "Newton", "static friction", "the assassination of US President John F. Kennedy", "responsibility for the abductions", "Winter Park at Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Kenneth Cole", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", ", a North Korean Foreign Ministry spokesman described Rice as \"no more than an official of the most tyrannical dictatorial state in the world.", "\"Maude\"", "Climatecare, one of Europe's most experienced providers of carbon offset,", "Wednesday", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad", "Amanda Knox's aunt", "jazz", "$17,000", "Barney Stinson", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two", "Bill", "J.G. Ballard", "a nurse who tried to treat Jackson's insomnia with natural remedies", "Sarah", "\"I saw guys who were 34, 35, 36 years old -- still young guys -- about to get out of the game, and I wondered what will they do now?\"", "1981", "17 Again", "Nigeria", "$83,27014", "Republicans", "EU naval force", "Chris Robinson", "son of Gabon's former president", "steam-driven, paddlewheeled overnight passenger boat.", "Hyundai Steel", "a bone-growth disorder that causes dwarfism", "London Heathrow's Terminal 5.", "canceled the swimming privileges", "February 12", "more than 30", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "military action because we're getting frustrated", "The White House Executive chef", "Russell Huxtable", "Willy Russell", "Budapest", "\"The Expendables 2\" (2008)", "Northumbrian", "\"get thee to a nunnery\"", "wife Elena", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Otto Eduard Leopold, Prince of Bismarck, Duke of Lauenburg"], "metric_results": {"EM": 0.5, "QA-F1": 0.6027802620173944}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.4, 1.0, 1.0, 0.11764705882352941, 0.5454545454545454, 0.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.05714285714285714, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-729", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-4514", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-1056"], "SR": 0.5, "CSR": 0.5859375, "EFR": 0.8125, "Overall": 0.69921875}, {"timecode": 22, "before_eval_results": {"predictions": ["X-rays", "WMO Executive Council and UNEP Governing Council", "everyday Germans", "New York and Virginia, especially.", "two", "glowed even when turned off", "Pastor Paula White", "resources that could sustain future exploration of the moon and beyond.", "resources that could be found there", "April 6, 1994", "Prague", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards.", "a federal judge in Mississippi", "the department has been severely affected by the earthquake, with thousands of officers injured, killed or unaccounted for.", "22 million", "severe flooding", "a music video on his land.", "at the Lindsey oil refinery in eastern England.", "\"Slumdog Millionaire\" (No. 4)", "\"The Real Housewives of Atlanta\"", "18", "88", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide", "a president who understands the world today, the future we seek and the change we need.", "military trials for some Guant Bay detainees.", "Rany Freeman, an Egyptian living in Canada,", "Larry King", "Steven Chu", "racially motivated.", "Michael Partain", "women.", "the longest domestic relay in Olympic history,", "Zimbabwe's dire economic situation.", "No. 1 slot", "nine", "ash and rubble", "Friday", "some truly mind-blowing structures", "a Muslim with Lebanese heritage,", "the two-hour finale.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Damon Bankston", "researchers", "involvement during World War II in killings at a Nazi German death camp in Poland.", "opium", "warning", "84-year-old", "Robert Park", "Rima Fakih", "the Isthmus of Corinth", "Nalini Negi", "2017 - 12 - 10", "Runcorn", "collarbone", "horiz\u014dn kyklos", "UFC 50: The War of '04", "June 11, 1973", "San Diego County Fair", "Toy Story", "Viva Zapata", "# Quiz # Question"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5575515140267816}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.0, 0.8235294117647058, 0.0, 0.5, 0.19999999999999998, 0.967741935483871, 1.0, 0.6086956521739131, 1.0, 1.0, 0.5714285714285715, 0.4, 0.4444444444444445, 1.0, 0.0, 1.0, 0.923076923076923, 0.9565217391304348, 0.10256410256410256, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.390625, "CSR": 0.5774456521739131, "EFR": 0.717948717948718, "Overall": 0.6476971850613156}, {"timecode": 23, "before_eval_results": {"predictions": ["phycoerytherin", "lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition", "Behind the Sofa", "Tulsa, Oklahoma", "56", "Yemen", "2005", "Karen Floyd", "Four Americans", "those missing", "Haiti", "Susan Boyle", "Saturday", "Spain", "Jared Polis", "Janet and La Toya", "Dangjin", "30", "Michael Krane,", "lightning strikes", "Evans", "Italian government", "flooding", "threatening messages", "Noriko Savoie", "Citizens", "fake his own death", "\"in the interest of justice.\"", "martial arts", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "then-Sen. Obama", "Congress", "curfew", "Anne Frank, whose account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "once on New Year's Day and once in June, to mark the queen's \"official\" birthday.", "the government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.\"", "Zuma", "haute, bandeau-style little numbers", "five", "Iraqi Prime Minister Nouri al-Maliki", "September 11, 2001", "about 50", "a group of teenagers", "body bags on the roadway near the bus,", "al Fayed", "Desmond Tutu", "$17,000", "Toy Story", "$81,88010", "provide school districts with federal funds", "a transformiation, change of mind, repentance, and atonement", "Jason Lee", "phase of sleep", "substantive", "Kent", "beer and soft drinks", "five aerial victories.", "Cherokee River", "\"Manor Farm\"", "James A. Lovell", "Florida"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6476298418321444}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.5, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.15789473684210525, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-10787"], "SR": 0.578125, "CSR": 0.5774739583333333, "EFR": 0.9259259259259259, "Overall": 0.7516999421296295}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "The Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "Lindsey Vonn", "one of the shocks of the year", "him to step down as majority leader.", "The EU naval force", "gang rape", "ClimateCare, one of Europe's most experienced providers of carbon offset,", "The Louvre", "club", "like the video-game challenge of continuously trying to best your own fuel economy achievements,", "1979", "Iranian diplomat", "jazz", "an antihistamine and an epinephrine auto-injector", "Bangladesh's southern Bhola district.", "technology experts Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "one out of every 17 children under 3 years old in America", "President Sheikh Sharif Sheikh Ahmed", "a government-run health facility that provides her with free drug treatment.", "\"Britain's Got Talent\"", "military personnel", "behind the counter.", "11 healthy eggs", "one Iraqi soldier,", "Michael Partain,", "her fianc\u00e9", "racial intolerance.", "all animal products.", "\"El Viceroy\" and \"El General,\"", "Symbionese Liberation Army", "8.8 million", "work together to stabilize Somalia and cooperate in security and military operations.", "the ad would compromise the public broadcaster's appearance of unbiasedity.", "it -- you know -- black is beautiful", "$104,168,000", "Picasso's muse and mistress, Marie-Therese Walter.", "to provide alternative work for poor Afghan farmers to encourage them to give up opium production", "supplemental spending bill provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "off the coast of Dubai", "fallen comrades lost in the heat of battle.", "near Takoma Park, Maryland.", "27 Awa", "Mark Obama Ndesandjo", "\"Dance\" rated highly for Oxygen, with more than 1 million viewers tuning in.", "famous faces from the world visiting France, Russia, India, South Korea, China, South Africa, Brazil, Beirut and Poland.", "\"The Screening Room\"", "fatally shooting a limo driver on February 14, 2002.", "nucleus", "Treaty of Paris between France and the Sixth Coalition, and the Treaty of Kiel", "Sebastian Lund ( Rob Kerkovich )", "President Jimmy Carter", "Tom Watson", "Sandi Toksvig", "Hispania Racing F1 Team", "Viscount Cranborne", "Walt Disney World Resort in Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5458495312735243}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.5, 0.08695652173913043, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.23529411764705882, 0.33333333333333337, 0.0, 1.0, 0.0, 0.8, 0.5, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.6666666666666666, 0.0, 1.0, 0.07407407407407407, 0.34285714285714286, 1.0, 0.0, 0.0, 0.4, 0.5, 0.14285714285714288, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-4927", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.40625, "CSR": 0.5706249999999999, "EFR": 0.7894736842105263, "Overall": 0.6800493421052631}, {"timecode": 25, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-3952", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4548", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-4924", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-729", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-3637", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8638", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3254", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-49", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12150", "mrqa_searchqa-validation-12205", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-13756", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14282", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14700", "mrqa_searchqa-validation-14743", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-2355", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-4973", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6870", "mrqa_searchqa-validation-7227", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7584", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-8335", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-87", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10026", "mrqa_squad-validation-10026", "mrqa_squad-validation-10100", "mrqa_squad-validation-10254", "mrqa_squad-validation-10406", "mrqa_squad-validation-10418", "mrqa_squad-validation-1146", "mrqa_squad-validation-1166", "mrqa_squad-validation-1187", "mrqa_squad-validation-1218", "mrqa_squad-validation-126", "mrqa_squad-validation-1295", "mrqa_squad-validation-1313", "mrqa_squad-validation-1341", "mrqa_squad-validation-1407", "mrqa_squad-validation-1501", "mrqa_squad-validation-1549", "mrqa_squad-validation-159", "mrqa_squad-validation-1640", "mrqa_squad-validation-1662", "mrqa_squad-validation-1692", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1775", "mrqa_squad-validation-1877", "mrqa_squad-validation-1906", "mrqa_squad-validation-1960", "mrqa_squad-validation-2049", "mrqa_squad-validation-2059", "mrqa_squad-validation-2105", "mrqa_squad-validation-2113", "mrqa_squad-validation-2136", "mrqa_squad-validation-2207", "mrqa_squad-validation-2435", "mrqa_squad-validation-2466", "mrqa_squad-validation-2518", "mrqa_squad-validation-2530", "mrqa_squad-validation-281", "mrqa_squad-validation-2833", "mrqa_squad-validation-2858", "mrqa_squad-validation-2941", "mrqa_squad-validation-298", "mrqa_squad-validation-3091", "mrqa_squad-validation-3100", "mrqa_squad-validation-3127", "mrqa_squad-validation-3132", "mrqa_squad-validation-3149", "mrqa_squad-validation-3259", "mrqa_squad-validation-3260", "mrqa_squad-validation-3312", "mrqa_squad-validation-3319", "mrqa_squad-validation-3440", "mrqa_squad-validation-3454", "mrqa_squad-validation-3524", "mrqa_squad-validation-3632", "mrqa_squad-validation-3716", "mrqa_squad-validation-3813", "mrqa_squad-validation-3862", "mrqa_squad-validation-3865", "mrqa_squad-validation-3918", "mrqa_squad-validation-3943", "mrqa_squad-validation-4010", "mrqa_squad-validation-4047", "mrqa_squad-validation-4075", "mrqa_squad-validation-4078", "mrqa_squad-validation-4083", "mrqa_squad-validation-4102", "mrqa_squad-validation-4175", "mrqa_squad-validation-4315", "mrqa_squad-validation-4429", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-457", "mrqa_squad-validation-4673", "mrqa_squad-validation-4706", "mrqa_squad-validation-4770", "mrqa_squad-validation-4775", "mrqa_squad-validation-4844", "mrqa_squad-validation-4973", "mrqa_squad-validation-498", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5023", "mrqa_squad-validation-5037", "mrqa_squad-validation-5102", "mrqa_squad-validation-5135", "mrqa_squad-validation-5178", "mrqa_squad-validation-5194", "mrqa_squad-validation-5213", "mrqa_squad-validation-5226", "mrqa_squad-validation-526", "mrqa_squad-validation-5486", "mrqa_squad-validation-549", "mrqa_squad-validation-5513", "mrqa_squad-validation-5581", "mrqa_squad-validation-5741", "mrqa_squad-validation-5784", "mrqa_squad-validation-5812", "mrqa_squad-validation-5863", "mrqa_squad-validation-5871", "mrqa_squad-validation-5876", "mrqa_squad-validation-5972", "mrqa_squad-validation-6029", "mrqa_squad-validation-6059", "mrqa_squad-validation-6080", "mrqa_squad-validation-6121", "mrqa_squad-validation-6154", "mrqa_squad-validation-6166", "mrqa_squad-validation-6177", "mrqa_squad-validation-6242", "mrqa_squad-validation-6430", "mrqa_squad-validation-6588", "mrqa_squad-validation-6598", "mrqa_squad-validation-6614", "mrqa_squad-validation-6676", "mrqa_squad-validation-6685", "mrqa_squad-validation-6694", "mrqa_squad-validation-6721", "mrqa_squad-validation-6741", "mrqa_squad-validation-6789", "mrqa_squad-validation-6789", "mrqa_squad-validation-6801", "mrqa_squad-validation-6875", "mrqa_squad-validation-6921", "mrqa_squad-validation-7135", "mrqa_squad-validation-7159", "mrqa_squad-validation-716", "mrqa_squad-validation-7173", "mrqa_squad-validation-7229", "mrqa_squad-validation-7273", "mrqa_squad-validation-7434", "mrqa_squad-validation-7458", "mrqa_squad-validation-7576", "mrqa_squad-validation-7596", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-7967", "mrqa_squad-validation-7981", "mrqa_squad-validation-80", "mrqa_squad-validation-8035", "mrqa_squad-validation-8151", "mrqa_squad-validation-8176", "mrqa_squad-validation-8343", "mrqa_squad-validation-8356", "mrqa_squad-validation-8397", "mrqa_squad-validation-8420", "mrqa_squad-validation-8439", "mrqa_squad-validation-8485", "mrqa_squad-validation-8503", "mrqa_squad-validation-855", "mrqa_squad-validation-855", "mrqa_squad-validation-8608", "mrqa_squad-validation-8616", "mrqa_squad-validation-8719", "mrqa_squad-validation-8733", "mrqa_squad-validation-880", "mrqa_squad-validation-880", "mrqa_squad-validation-8833", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-890", "mrqa_squad-validation-8914", "mrqa_squad-validation-8924", "mrqa_squad-validation-9020", "mrqa_squad-validation-9066", "mrqa_squad-validation-913", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9220", "mrqa_squad-validation-9237", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9299", "mrqa_squad-validation-9333", "mrqa_squad-validation-940", "mrqa_squad-validation-9406", "mrqa_squad-validation-9436", "mrqa_squad-validation-9470", "mrqa_squad-validation-9559", "mrqa_squad-validation-962", "mrqa_squad-validation-9665", "mrqa_squad-validation-9686", "mrqa_squad-validation-9752", "mrqa_squad-validation-9753", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_squad-validation-9931", "mrqa_squad-validation-9960", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2794", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-339", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4945", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6753", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7367", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-980"], "OKR": 0.763671875, "KG": 0.4890625, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "concurring, smaller assessments of special problems instead of the large scale approach", "Anthony Hopkins", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "insect", "phylum", "Wayne Allwine", "St Pauls", "holography", "Pelias", "Sarah Collins", "Northumbria", "Harvard", "cricketer", "Seymour Hersh,", "long pole", "copper and zinc", "Tigris", "Cordelia", "pamphlets, posters, ballads", "dermatitis", "33", "spicy varietal", "Joseph Smith,", "Chicago", "palladium", "moon", "a number that can multiply with each other to form 169.", "petticoat", "The Virgin Spring", "Canada", "Winston Churchill", "Stockholm", "Peter Parker", "Goldie Myerson,", "Lesa Ukman", "bullfight", "Sparks", "Ginger Rogers", "Plymouth Rock", "Comedy Playhouse", "about 25 grams of citric acid per liter, or 1,000 grams, of juice", "Charles Darwin", "John", "Mr. Boddy", "Marie Van Brittan Brown", "Southern California Timing Association ( SCTA )", "1995", "Bourbon", "Taylor Swift.", "Clarence Coffee Jr., Kiesza, Charli XCX, Jacob Plant, and Jennifer Lopez.", "\"The three gunshot wounds to the head included two nonfatal rounds with entry points below the chin, and one fatal shot that entered Peterson through the right side of the head,\"", "a class to help women \" learn how to dance and feel sexy,\"", "Amy Bishop", "calathus", "the Louvre", "Seaver"], "metric_results": {"EM": 0.421875, "QA-F1": 0.505907154528478}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.9523809523809523, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 0.1764705882352941, 0.7777777777777778, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-8618", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-147", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-7521", "mrqa_naturalquestions-validation-1399", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.421875, "CSR": 0.5649038461538461, "EFR": 0.7567567567567568, "Overall": 0.6691758705821206}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Space Museum\"", "eight", "affordable housing", "Mao Zedong", "Verona", "New York", "elephant", "to cook in front of a large party of guests.", "Frank McCourt", "Harry Burton", "Judy Cassab", "Margo Leadbetter", "Schengen Area", "A", "city of Sheffield, England", "Famous Players", "the Beatles", "Gerald Durrell", "Jezebel", "Cork", "Pelias", "Arabian", "Halifax", "Noises Off", "jimmy osmond", "Frank Wilson", "Carlos the Jackal", "Edwina Currie", "Sonja Henie", "Robert Maxwell", "1768", "For Gallantry", "Tuesday", "Caucasus", "Cahaba", "The Good Life", "Tahrir Square", "uranium", "Count de La F\u00e8re", "27", "Jack Ruby", "tintoretto", "Eric Coates", "Saudi Arabia", "Lester", "Thailand", "Sydney", "dove", "Tunisia", "Prince Philip", "Wellington", "Tokyo", "Edgar Lungu", "49 cents", "heart rate that exceeds the normal resting rate", "672", "\"Linda McCartney's Life in Photography\", \"Some Like It Hot\", \"Kubrick's Napoleon: The Greatest Movie Never Made\", \"Saturday Night Live: The Book\",", "Stonecoast MFA Program in Creative Writing", "keystroke", "Juan Martin Del Potro.", "27", "Edgar Allan Poe", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.5, "QA-F1": 0.5440104166666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4166666666666667, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2797", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-733", "mrqa_newsqa-validation-279", "mrqa_searchqa-validation-12829"], "SR": 0.5, "CSR": 0.5625, "EFR": 0.78125, "Overall": 0.67359375}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "70", "forced Tesla out leaving him penniless. He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock.", "Benazir Bhutto", "Iran's nuclear program.", "27 Awa Indians", "Larry King", "Daniel Cain", "acid", "Wally", "2008", "after Wood went missing off Catalina Island, near the California coast,", "Behar", "Pakistan", "The Everglades, known as the River of Grass,", "Dilshan made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "1950s", "64", "Iran's parliament speaker", "27-year-old", "young self-styled anarchists", "about $163 million (180 million Swiss francs)", "baggage from the 80s and has grown beyond a resort town into something more substantial.", "around 3.5 percent of global greenhouse emissions.", "ensenada,", "Orbiting Carbon Observatory", "Switzerland", "Robert Redford", "Janet and La Toya", "more than 22 million people in sub-Saharan Africa", "hours", "returning combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "he's burned badly on the backs of his knees and every time he moves his knee, it pulls, and if it's healing, it starts to bleed.\"", "al-Shabaab", "posting a $1,725 bail", "sustain future exploration of the moon and beyond.", "his business dealings for possible securities violations", "Opry Mills,", "Number Ones", "normal maritime traffic", "he was diagnosed with skin cancer.", "al Qaeda", "Obama", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions,", "The oceans", "stomized him with a broomstick, a pair of scissors and a wooden dowel", "a nurse who tried to treat Jackson's insomnia with natural remedies", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "1932", "between 1923 and 1925", "gilda", "Nahum Tate", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire", "Benjamin Disraeli", "a rising sun"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5733029086275314}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.375, 0.4, 0.8, 0.3333333333333333, 0.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.06896551724137931, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.125, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1875, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.8, 0.8, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_squad-validation-1302", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2976", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-14496", "mrqa_searchqa-validation-15354"], "SR": 0.421875, "CSR": 0.5574776785714286, "EFR": 0.7297297297297297, "Overall": 0.6622852316602317}, {"timecode": 28, "before_eval_results": {"predictions": ["Bermuda 419 turf", "25-foot (7.6 m)", "symbols", "Hyundai Steel", "Monday night", "Bailey, Colorado", "Most of the 103 children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "40", "the Illuminati was this secret society which was brutalized by the Catholic Church in the 1600s.", "in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "toxic smoke from burn pits", "Lucky Dube,", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "space shuttle Discovery", "Gavin de Becker", "a nuclear weapon", "the IV cafe.", "Arizona", "between South America and Africa.", "Tetris", "outside influences", "aid to Gaza,", "flipped and landed on its right side,", "suppress the memories and to live as normal a life as possible;", "Tuesday", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the area was sealed off, so they did not know casualty figures.", "knocking the World Cup off the front pages for the first time in days.", "Cash for Clunkers", "project work", "one of the most influential, powerful and admired public figures of our time, by the most widely read biographer of our era.", "80 percent", "London's 20,000-capacity O2 Arena.", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Ozzy Osbourne", "$50", "Australian officials", "the Walk of Fame.", "husband Bill Klein,", "gun", "38", "Argentina", "mayor of Seoul from 2002 to 2004,", "Somalia's piracy problem was fueled by environmental and political events.", "17 Again", "Kabul", "22", "Steven Gerrard", "12.3 million people worldwide", "the area was sealed off, so they did not know casualty figures.", "Rima Fakih", "Old Trafford", "help bring creative projects to life", "season two", "Mary Elizabeth Patterson", "Tonight", "Fifth", "Nepal", "Merck Sharp & Dohme", "Fort Albany", "Knoxville, Tennessee", "Jawaharlal Nehru", "Transpiration", "hypomanic"], "metric_results": {"EM": 0.4375, "QA-F1": 0.559447496947497}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.2222222222222222, 0.8, 0.6666666666666666, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5100", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3575", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-79", "mrqa_hotpotqa-validation-4763", "mrqa_searchqa-validation-10091", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.4375, "CSR": 0.5533405172413793, "EFR": 0.6666666666666666, "Overall": 0.6488451867816092}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "ancient cult activity", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "sex organs", "Russian army", "diffuse nebulae", "August 6", "Doug Diemoz", "Colony of Virginia", "Monk's Caf\u00e9", "central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "maintenance utility", "July 4, 1776", "pick yourself up and dust yourself off and keep going", "John Garfield as Al Schmid   Eleanor Parker as Ruth Hartley   Dane Clark as Lee Diamond", "enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "1979", "Lorazepam", "the 2013 non-fiction book of the same name by David Finkel", "salami", "Brenda", "ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Husrev Pasha", "Stephanie Judith Tanner", "the palmar aspect of these fingers, including both front and back of the tips, perhaps as far back as the fingernail beds", "McFerrin, Robin Williams, and Bill Irwin", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "the first four caliphs ( successors )", "Lake Powell", "ornament", "September 6, 2019", "population", "substitute good", "Veronica", "over 74", "1987", "cunnilingus", "October 2000", "New York City", "Mamata Banerjee", "the United States economy first went into an economic recession", "the closing of the atrioventricular valves and semilunar valves", "Hermann Ebbinghaus", "The Miracles", "people in the 20th century who used obscure languages as a means of secret communication during wartime", "Donny Osmond", "Rome and Carthage", "43rd", "gmbH", "7.63\u00d725mm Mauser", "seven", "Muslim", "the two remaining crew members from the helicopter,", "Saturday's Hungarian Grand Prix.", "Rickey Henderson", "Baikal", "adventure park"], "metric_results": {"EM": 0.375, "QA-F1": 0.5387156977141558}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 1.0, 0.3076923076923077, 0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.7142857142857143, 0.23529411764705882, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.9333333333333333, 1.0, 1.0, 0.19354838709677422, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.375, "CSR": 0.5473958333333333, "EFR": 0.65, "Overall": 0.6443229166666666}, {"timecode": 30, "before_eval_results": {"predictions": ["address information", "Pleurobrachia", "1953", "AT&T", "pioneers.", "Hutter", "shoes", "an eighth", "Isaac Newton,", "acetylene", "'Archer' Jokes", "fiber.", "gray wolf", "Whats", "Winston Rodney", "sand", "Nanjing", "Montana", "John Cleese", "louis le Grand", "GILBERT & SullIVAN.", "Fox Network", "the Belgae", "Joe Lieberman", "the Boston Marathon", "fibreboard", "tin", "Song of Norway", "Frida Kahlo", "a diplomat, a Senator, and a member of the House of Representatives.", "\"Y\" 2 \"K\": An Eskimo...", "Fat Man", "Hair", "William Randolph Hearst", "basalt", "an ale", "homo", "telephone", "a song performed by English pop punk band Busted.", "Luther", "The New Colossus", "yelp", "Richard Wagner", "Princess Eugenie of York", "what those four years of college", "middleweight champion", "bronchodilator", "Forty", "fluorescent lights.", "Sixberry", "a Chenard", "Earl Long", "Neil Patrick Harris", "Wyatt and Dylan Walters", "1999", "vitamin D", "five", "Alberto juantorena", "R&B vocal group", "Awake", "Doctor of Philosophy", "Pakistan's", "Seoul.", "an African-American woman"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5300595238095238}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-3715", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_triviaqa-validation-7493", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-723"], "SR": 0.46875, "CSR": 0.544858870967742, "EFR": 0.7647058823529411, "Overall": 0.6667567006641366}, {"timecode": 31, "before_eval_results": {"predictions": ["non-Mongol physicians", "Prospect Park", "the macula", "volume", "a crossword puzzle clue", "Joseph Conrad", "Diners' Club Card", "Christian Dior", "August Wilson", "a drug problem", "Notre Dame", "Table cloth", "Tate", "Captain William Bligh", "Cecil Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "Pennsylvania Railroad", "Mike Huckabee", "Queen", "a megrim", "a card", "kozo", "Rex", "Samuel Beckett", "Rachel Carsons", "Vietnam", "sports", "David Geffen", "Franklin D. Roosevelt", "Prince William", "America Ferrera", "R", "a white dairy cattle", "New Jersey", "Lake Ontario", "Matthew Perry", "Marissa Jaret Winokur", "John", "fortune", "the Chocolate Factory", "artillery", "aluminum", "General McClellan", "Ned Kelly", "a piles of papers", "gravitational force", "Osiris", "quiveir", "Heroes", "on the two tablets", "organ transplant of a kidney into a patient with end - stage renal disease", "seven units", "Geheimrat Dr. Max", "Duke Ellington", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "February 20, 1978", "two years", "Arsene Wenger", "in an interview Tuesday on CNN's \"Larry King Live.\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.5499999999999999}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2]}}, "before_error_ids": ["mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-16496", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-9411", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-2228", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-10344", "mrqa_searchqa-validation-9799", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-4939", "mrqa_searchqa-validation-10042", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-10526", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-114", "mrqa_newsqa-validation-2123"], "SR": 0.4375, "CSR": 0.54150390625, "EFR": 0.6388888888888888, "Overall": 0.6409223090277778}, {"timecode": 32, "before_eval_results": {"predictions": ["the weight of the air that rushed back in", "Fresno Street and Thorne Ave", "the Black Death", "Sir Elton John", "John Stuart Mill", "Joshua Abraham Norton", "CIA", "piano", "Rickey Henderson", "Jawaharlal Bhutto", "D. carota ssp. maritimus", "John Grunsfeld", "Llados and Llanmad", "1976", "Galileo Descartes", "a quark", "The King Jesus Gospel", "Rudy Giuliani,", "the Espionage Act", "Virginia", "Sif", "New Jersey", "The Omega Man", "a walk-in pantry", "a barrel, cask, or tun", "the 1984 Summer Olympics", "Hugo Chvez", "Shamir", "Hinduism", "tin", "Maid Tells of Seeing Jackson", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Lindsey Davenport", "Los Angeles", "anemoi", "Richard III", "Labour", "pen", "Croatia", "Douglas Adams", "Strindberg", "Hawaii", "Stephen Crane", "Prussia", "Sophocles", "Mark Cuban", "Thought Police", "a bust", "Central Park", "Alice", "Part 2", "Coconut Cove", "aeoline", "trumpet", "Mel Gibson", "2.1 million", "Edward James Olmos", "ZZ Top,", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6283534356725147}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.7368421052631579, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-13862", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-4767"], "SR": 0.546875, "CSR": 0.5416666666666667, "EFR": 0.8620689655172413, "Overall": 0.6855908764367816}, {"timecode": 33, "before_eval_results": {"predictions": ["The BBC, in conjunction with animation studio Cosgrove Hall, reconstructed the missing episodes 1 and 4 of The Invasion (1968), using remastered audio tracks and the comprehensive stage notes for the original filming,", "pathogens, an allograft", "a large concrete block is next to his shoulder, with shattered pieces of it around him. Blood trickles down the road.", "hours", "28", "back at work", "Oxbow,", "201-262-2800", "opium", "\"I think she's wacko.\"", "the annual White House Correspondents' Association dinner Saturday,", "Hussein's Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar", "The station", "Hundreds of women protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "Little Rock military recruiting center", "Cash for Clunkers", "environmental", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers", "different women coping with breast cancer in five vignettes.", "a long-range missile", "Police", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "Roger Federer", "Miami Beach, Florida", "over 1000 square meters in forward deck space, allowing for such features as a full garden and pool, a tennis court, or several heli-pads.", "CNN", "no chance", "SSM Cardinal Glennon Children's Medical Center", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "two years ago", "two", "a very beautifully painted ruff of Italian lacework", "Symbionese Liberation Army", "he acted in self defense in punching businessman Marcus McGhee.", "two tickets to Italy on Expedia.", "Colombia", "in-cabin lighting system", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.Prasanna Jayawardene", "Antonio Maria Costa,", "$40 and a bread.", "Kgalema Motlanthe,", "the Ming dynasty", "George II ( George Augustus ; German : Georg II. August ; 30 October / 9 November 1683 -- 25 October 1760 )", "2014 -- 15", "November 5, 2013", "Javier Bardem", "Scotland", "Erika Girardi", "Terry the Tomboy", "Araminta Ross", "Mrs. Potts", "Spokescandy", "\"The Land of the Free and the Home of theBrave\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.6794090229482598}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.0689655172413793, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7058823529411765, 0.4, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1904761904761905, 0.0, 0.7499999999999999, 0.4444444444444445, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6]}}, "before_error_ids": ["mrqa_squad-validation-7682", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1379", "mrqa_naturalquestions-validation-7108", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.5625, "CSR": 0.5422794117647058, "EFR": 0.8214285714285714, "Overall": 0.6775853466386554}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "anti- strike", "Washington State's decommissioned Hanford nuclear site,", "Yemen", "bankruptcy", "nearly $2 billion in stimulus funds", "developing a youth ballpark in his hometown", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "Bahrain", "children of street cleaners and firefighters.", "Joan Rivers", "$3 billion, with further foreign direct investment exceeding $40 billion during the operations phase.", "hardship for terminally ill patients and their caregivers", "Honduras", "Brazil", "environmental", "strife in Somalia,", "Roy", "WBO welterweight title from Miguel Cotto", "relatives of the five suspects,", "Meredith Kercher.", "defense lawyers trying to save their client from the death penalty", "Aniston, Demi Moore and Alicia Keys", "work together to stabilize Somalia and cooperate in security and military operations.", "Friday,", "cancerous tumor.", "20", "Matthew Fisher", "$1.5 million", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "model of sustainability.", "glamour and hedonism", "a $158 green skirt and $298 bead and rhinestone cardigan", "Department of Homeland Security Secretary Janet Napolitano", "543 elected members, of which 58 are women.", "The woman who received the first-ever near-total face transplant in the United States", "Robert Gates", "Israel", "rural Tennessee.", "in critical condition", "Seoul", "Nicole", "always encouraging, too, even when she was finding fault.", "next week.", "Adam Lambert", "regulators in the agency's Colorado office", "early detection and helping other women cope with the disease.", "James Whitehouse,", "hopes the journalists and the flight crew will be freed,", "gentry Buddhism", "Lionel Hardcastle", "Stephen Lang", "Dick Van Dyke", "Noreg", "Beer", "Revengers Tragedy", "1754", "Black Elk", "Rye, New York", "hippopotamus", "St Paul"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5755359484265734}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.4444444444444445, 0.2222222222222222, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-2761", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_searchqa-validation-16463", "mrqa_searchqa-validation-7879"], "SR": 0.484375, "CSR": 0.540625, "EFR": 0.5454545454545454, "Overall": 0.6220596590909091}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts", "Border Reiver", "July 4, 1826", "rum", "Nantucket", "an Islamic leadership position", "sap", "Malibu", "Sisyphus", "measure of sound", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "horseshoe", "Caliban", "crayon", "the Aegean Sea", "Battle of the Little Bighorn", "Shakers", "a bellwether", "The Information Philosopher", "chips", "Boxer", "The Spiderwick Chronicles", "Mabel Harding", "Las Vegas", "a college the way a high school", "the Rose Bowl", "Norman Rockwell", "short cropped", "tunacanned light tunais", "Napa Valley", "Italy", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "The Bodyguard", "12 men", "Nancy Pelosi", "blogging", "Jupiter", "Sadat", "National Ice Cream Day", "Mary Shelley", "50 million cells per litre", "Volitan Lionfish", "Charlie Sheen", "(the three sons all have the middle", "Bonnie Aarons", "Wednesday, 5 September 1666", "a pop ballad", "Seth", "Lou Gehrig", "meaning and origin", "1949", "Aamir Khan", "My Gorgeous Life", "Argentina has always claimed sovereignty over the islands and invaded them in 1982, prompting a war in which more than 600 Argentinean and 255 British military personnel died.", "High Court Judge Justice Davis", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6204282407407408}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.07407407407407407, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-193", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-1389", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_searchqa-validation-12788", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884"], "SR": 0.53125, "CSR": 0.5403645833333333, "EFR": 0.8, "Overall": 0.6729166666666667}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "a scallop", "Nothing gained", "numismatic", "Supernanny", "Atlantic", "Cincinnati", "a mosque", "Henry Hudson", "a gun blast tubes", "Dry ice", "Taft", "Entourage", "eels", "Philadelphia", "The Museum of Modern Art", "the unicorns", "John C. Frmont", "Russia", "barbRA STREISAND", "Hermann Hesse", "the Taj Mittal", "English Monarchs", "The opera Carmen", "Margaret Mitchell", "Quasimodo", "Money for Nothing", "Pandarus", "(bumrushed)", "Burt Reynolds", "Sphinx", "Satchmo", "Saudi Arabia", "American new wave", "Arby's", "coffee", "The Lgion", "Burns", "The Incredible Hulk", "Winnipeg", "Memphis Belle", "Burkina Faso", "Central Pacific", "Attorney General", "Icelandic", "(Buffalo)", "(This Is SportsCenter)", "Edith Piaf", "Ivan I", "a prologue", "birch", "an investor couple", "Jack Gleeson", "Phil Hurtt", "animals", "Massachusetts", "City of Starachowice", "Fredric March", "2009", "Democratic", "meteorologist", "$104,327,006", "\"State of Play\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6677827380952381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-15899", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-13205", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-14520", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-11632", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-15286", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-5571", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-1525"], "SR": 0.609375, "CSR": 0.5422297297297297, "EFR": 0.76, "Overall": 0.665289695945946}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "KFC", "oats", "Romney", "Ivan the Terrible", "Sally Field", "May 21, 1927", "Egypt", "pi", "tin", "Lake Maurapas", "Cousin Eddie", "W", "Marriott International", "the Principality of Monaco", "Canada", "The Secret", "gold", "Collagen", "China", "a compound", "the cranes", "Jesse James", "Alzheimer", "the Gulf of Mexico", "Austin", "Euclid's Elements", "Eva Peron", "Cain", "Edward Asner", "X-Men: The Last Stand", "the Louvre", "King salmon", "Michael Scofield", "Mars", "Maine", "one of the best-known Greek cheeses,", "Meg", "Sonnets", "deuce", "Hans", "Peter Bogdanovich", "a #3 hit song", "Pilate", "British", "the Quaternary Period", "no contest", "Junior Walker", "Czech Republic", "tuna", "NIRA", "John Ernest Crawford", "beta decay", "France", "Priam", "Mariette", "Charles Quinton Murphy", "\"The Disaster Artist\"", "Australian", "the sins of the members of the church,", "$22 million", "\"17 Again,\"", "Nelson"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6182291666666666}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-10268", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-9713", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-8387", "mrqa_searchqa-validation-8068", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-5774"], "SR": 0.53125, "CSR": 0.5419407894736843, "EFR": 0.7666666666666667, "Overall": 0.6665652412280703}, {"timecode": 38, "before_eval_results": {"predictions": ["in whole by charging their students tuition fees", "Holden Caulfield", "Bill Hickok", "Leptospirosis", "recession", "Glashtyn", "Jay Silverheels", "Singapore", "tanks", "Marimba", "a boat", "forgetting Sarah Marshall", "Witness", "Martha Tabram", "3800", "Alan Shore", "taxonomy", "Spain", "cerebrospinal fluid", "William McMaster Murdoch", "William Shakespeare", "comedy", "Mary Poppins", "Casowasco", "Fresh Prince of Bel-Air", "Nod", "Nitrides of boron & silicon are used to make crucibles", "bathwater", "Second Marriage Ceremony", "Livin' On A Prayer", "Sherlock Holmes", "licorice", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "G", "Nunavut", "Katamari Damacy", "Bill Murray", "Margaret Thatcher", "The Queen of Spades", "tungsten", "forests", "Olympia", "Waylon Jennings", "The Bridge on the River Kwai", "Brazil", "British Columbia", "Sydney Pollack", "Scrapple", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "helps managers understand employees' needs in order to further employees' motivation", "one", "Norfolk Island", "Wright brothers", "sexual activity", "Sam tick", "Sandro Bondi refused to attend", "voluntary misdemeanor", "\"deep sorrow\"", "Pygmalion"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48854166666666665}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [0.4, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1, 0.5, 0.16666666666666669, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6983", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-14698", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-5541", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-379", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-6803", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-4043", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-4013", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-600"], "SR": 0.421875, "CSR": 0.5388621794871795, "EFR": 0.6756756756756757, "Overall": 0.647751321032571}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "Boogie Woogie Bugle Boy", "the European Economic Community", "Jack Nicholson", "Glory", "Sweeney Todd", "Wikis", "the capital of the Byzantine Empire", "Independence", "Jefferson Davis", "Ezra Pound", "Amazon River", "a ready-to-use cotton swab", "Alaska", "Dixie's Land", "RAND Corporation", "Warren Harding", "engrave", "Losrose Place", "Francis Crick", "Jay and Silent Bob", "Heath", "Abkhazia", "Twelfth Night", "Hawaii", "Marmaduke", "Tito", "Westies, Scotties, schnauzers", "Ratatouille", "neurons", "Calvin Coolidge", "Mark Cuban", "Rudy Giuliani", "eyes", "Tony Dungy", "the Danube", "Pocketknife Damascus Steel Blade", "a marathon", "life", "a plant", "chess", "GIGO", "Johannes Brahms", "Charleston Southern", "Italian", "The Grapes of Wrath", "a bicentennial", "Byzantium", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Tara", "March 15, 1945", "Charles Darwin", "Old Trafford", "Miles Morales", "Honey Irani", "global peace", "Kalahari Desert", "farmer Alan Graham", "Bob Dole", "Bollywood superstar", "managing his time"], "metric_results": {"EM": 0.4375, "QA-F1": 0.49516369047619047}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-459", "mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-11089", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-7830", "mrqa_searchqa-validation-14076", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-8155", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-15687", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4073"], "SR": 0.4375, "CSR": 0.536328125, "EFR": 0.8333333333333334, "Overall": 0.6787760416666667}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "South Korean horror film", "Oakdale", "Missouri", "FAI Junior Cup", "Flaw", "alt-right", "The Drudge Report", "15,000 people", "Yellow fever", "an all-female a cappella singing group", "1934", "13\u20133", "The Andy Williams Christmas Album", "Tsavo East National Park", "New York Islanders", "Algirdas", "nearly 80 years", "Jean Acker", "the Championship", "The Gettysburg Address", "Whitney Houston", "Premier League club Manchester United and the England national team.", "The Rite of Spring", "1", "26,000", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn.", "Mayor Ed Lee", "1958", "1993", "burlesque", "Afro-Russian", "Loretta Lynn", "England", "a Boeing B-17 Flying Fortress", "1 December 1948", "11", "XXIV Summer Universiade", "bronze", "1994", "Kansas City", "1999", "Pinellas County", "beer", "London", "Ployer Peter Hill", "Mindy Kaling", "1988", "Leonard Cohen", "Erika Mitchell Leonard", "Mase Dinehart", "Tevye", "Sir Tom Finney", "CameroonCameroon", "obtaining and proper handling of human blood", "toxic smoke from burn pits in Iraq", "two", "Iggy Pop invented punk rock.", "a pound of flesh", "a man", "Leonardo DiCaprio", "a narcissistic ex-lover who did the protagonist wrong"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6814695669934641}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.25, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.4, 0.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-16547", "mrqa_naturalquestions-validation-6326"], "SR": 0.59375, "CSR": 0.5377286585365854, "EFR": 0.9230769230769231, "Overall": 0.6970048663227018}, {"timecode": 41, "before_eval_results": {"predictions": ["a punt", "10", "cordoned off with yellow police tape.", "Les Bleus", "2005", "more than 4,000", "Specter", "an angry mob.", "normal maritime traffic", "Sri Lanka", "death", "an average of 25 percent", "he was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver", "Al Nisr Al Saudi", "as many as 50,000", "piano", "$250,000", "a \"prostitute\"", "the mammoth's skull", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Los Ticos", "psychiatric nurse-practitioner failed to diagnose the troubled infantryman", "Russia", "Facebook and Google,", "through a facility in Salt Lake City, Utah", "Manmohan Singh", "Haiti", "Tuesday afternoon", "militants", "23 years", "a head injury.", "Bahrain", "an open window that fits neatly around him", "Leo Frank", "Paul McCartney and Ringo Starr", "Washington", "Westernbert Gwashavanhu, political consul at Zimbabwe's embassy in Washington", "don't have to visit laundromats", "three", "United Kingdom Dance Championships.", "on-loan David Beckham claimed his first goal in Italian football.", "\"He is more American than a German, I don't know,\"", "\"Twilight\"", "forgery and flying without a valid license", "11", "A third beluga whale belonging to the world's largest aquarium has died", "Fayetteville, North Carolina,", "Indonesian military transport plane crashed into a residential area in East Java early Wednesday, killing 98 people, military officials said.", "al Qaeda", "Hillary Clinton", "Rihanna", "rate of angular rotation \u03c9, and the speed of the cannonball ( assumed constant ) v", "heart", "54 Mbit / s", "in the County of Gloucestershire", "the Model 32", "cereal only to be told by an off-screen voice that they'd have to eat several bowls (usually quantified by an exact number) of their cereal", "Oakdale", "Melbourne", "Guillermo del Toro", "stocks", "Monty Python and the Holy Grail", "Sweden", "Department of Transportation"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5958338970057719}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.4, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.5, 1.0, 0.7272727272727273, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.625, 1.0, 0.0, 0.0, 0.5333333333333333, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 0.11428571428571428, 0.0, 1.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-5552", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376"], "SR": 0.46875, "CSR": 0.5360863095238095, "EFR": 0.6176470588235294, "Overall": 0.6355904236694678}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Las Vegas", "Zimbabwe", "Italian Serie A", "Darrel Mohler", "dancing against a stripper's pole.", "Michoacan Family,", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "Morgan Tsvangirai.", "42", "takes on the swords of the Taliban.", "If huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica -- melt, then the rise is expected to be more dramatic.\"", "80 percent", "1979", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Elena Kagan", "CBS, CNN, Fox and The Associated Press.", "auxiliary lock", "1-1", "AbdulMutallab", "Myanmar", "Rambosk", "Marcus Schrenker,", "Philippine National Police.", "poems", "Channel 4 said the program was made with the parents' full consent.", "Evan Bayh", "The Red Cross, UNHCR and UNICEF", "MOSCOW, Russia", "debris", "not guilty", "capital murder and three counts of attempted murder", "Basel", "17", "Daytime Emmy Lifetime Achievement Award", "state senators", "31 meters (102 feet) long and 15 meters (49 feet) wide", "its tranquil beaches", "how preachy and awkward cancer movies can get.", "her father", "shark River Park in Monmouth County", "three out of four", "Islamabad", "partying", "Capitol Hill.", "\"theoretically\" Iran could develop a nuclear weapon", "1940's", "March 22", "ireport form", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Antichrist", "a major fall in stock prices", "Thomas Jefferson", "Alexander Salkind", "Orion", "brown", "SELFIE", "2002", "England", "Los Alamos National Laboratory", "the Rat", "rain", "Crawford", "Pyrenees"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6629601020915359}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.5, 0.11764705882352941, 0.0, 0.5714285714285715, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8235294117647058, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5989", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.546875, "CSR": 0.5363372093023255, "EFR": 0.8275862068965517, "Overall": 0.6776284332397755}, {"timecode": 43, "before_eval_results": {"predictions": ["north,", "legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees of Immigration and Customs Enforcement", "33-year-old", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "hardship for terminally ill patients and their caregivers,", "Jaime Andrade", "Zac Efron", "finance", "nearly $2 billion", "National Infrastructure Program, as he called it,", "1936,", "The station", "Krishna Rajaram,", "lifeless, naked body", "Robert Mugabe", "the wife of Gov. Mark Sanford,", "Camp Lejeune, North Carolina", "Saturday.", "$1.5 million", "a violent government crackdown seeped out.", "Iran could be secretly working on a nuclear weapon", "the fact that the teens were charged as adults.", "death squad killings carried out during his rule in the 1990s.", "Elena Kagan", "Hyundai Steel's", "100 percent", "Saturday", "Afghanistan,", "prisoners at the South Dakota State Penitentiary", "seven", "200", "Pakistan", "Seminole Tribe", "a Muslim with Lebanese heritage,", "South Africa", "Barack Obama", "helicopters and unmanned aerial vehicles", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah", "1,500", "a receptionist with a gunshot wound in her stomach", "$50", "$60 billion on America's infrastructure.", "ALS6,", "Malayalam", "Mad - Eye Moody and Hedwig", "1960 Summer Olympics in Rome", "Aston Park", "peasants, small and medium-size farmers, landless people, women farmers, indigenous people, migrants and agricultural workers", "pool", "1822", "The Dressmaker", "Trilochanapala", "crote", "a buffalo", "\"The Wizard of Oz\"", "the frontal lobe"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6276384320175439}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.2105263157894737, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 0.33333333333333337, 0.5, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8741", "mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.515625, "CSR": 0.5358664772727273, "EFR": 0.8387096774193549, "Overall": 0.6797589809384165}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf", "Los Angeles", "Christopher Livingstone", "Duval County", "Benj Pasek and Justin Paul,", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Araminta Ross", "Roger Staubach", "1944", "Highlands", "Franconia, New Hampshire", "Operation Watchtower", "Dan Crow", "War & Peace", "Amberley", "What Are Little Boys Made Of", "Berea College", "Omaha Nighthawks", "Luca Guadagnino", "Liesl", "native to Germany and other parts of Central Europe,", "New York Islanders", "Todd Phillips", "26,788", "the Troubles", "1967", "Marktown", "law", "Radcliffe College", "Charles Guiteau", "Ford", "If the citizen's heart was heavier than a feather", "India", "German", "armed", "25 million", "The Snowman", "Ella Fitzgerald", "Chris Claremont", "Rain Man", "Interscope Records", "British East India Company", "4,000", "\"the most influential private citizen in the America of his day\"", "I'm Shipping Up to Boston", "American", "Scottish singer and \"Britain's Got Talent\" winner Jai McDowall", "the most common sinitic name for China in modern times", "sixth - largest country by total area", "the first to develop lethal injection as a method of execution", "Nicola Adams", "\"Gansbaai\"", "Russia", "dependable Camry", "Steven Green", "in a hotel,", "Chaucer", "rattlesnake", "Riddles", "to bed"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6431042192760943}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8750000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-4454", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-13986", "mrqa_searchqa-validation-4414"], "SR": 0.5625, "CSR": 0.5364583333333333, "EFR": 0.7857142857142857, "Overall": 0.6692782738095239}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "United States, NATO member states, Russia and India", "30", "crocodile eggs", "Colorado prosecutor", "Jason Chaffetz", "the second she got back from Mexico,", "Carrefour", "in July for A Country Christmas, and the festivities run from mid-November until the holidays end.", "trail the illegal traffic.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Herman Cain", "\"17 Again,\"", "does not believe North Korea intends to launch a long-range missile in the near future,", "Wigan Athletic", "Romney", "two years ago.", "businessman", "Picasso's muse and mistress, Marie-Therese Walter.", "low-calorie", "Heshmatollah Attarzadeh", "the ireport form", "the government", "Nine out of 10 children", "police", "Sen. Joe Lieberman, I-Connecticut,", "a crocodile", "a bronze medal in the women's figure skating final,", "killed at least 63 people and wounded more than 200.", "Congress", "Susan Boyle", "\"Common Access Cards,\"", "Phillip A. Myers.", "Obama's", "Gyanendra", "homicide", "Casey Anthony, 22,", "officers at a Texas  airport", "10 municipal police officers", "UNICEF", "surrogate", "228", "Kerstin and two of her brothers, ages 18 and 5,", "since 2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Joan Rivers", "supermodel and philanthropist", "Jacob Zuma", "Oaxaca, Mexico", "Arsene Wenger", "slavery", "Kat ( Jesse Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Latin liberalia studia", "Enid Blyton", "Johnny Mathis", "Beverly Hills Cop", "Champion Jockey", "Luca Guadagnino", "Sleepy Brown", "the caged bird", "timing shapes and supports brain function", "a jigger", "a Bristol Box Kite"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6121618191591018}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.47619047619047616, 1.0, 1.0, 0.8695652173913044, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.6666666666666666, 0.7499999999999999, 1.0, 0.5, 1.0, 0.25, 0.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.484375, "CSR": 0.5353260869565217, "EFR": 0.6666666666666666, "Overall": 0.6452423007246377}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "bipartisan", "Nirvana", "phone calls or by text messaging", "without bail and will be arraigned June 25,", "12.3 million", "Mexico", "Napoli", "Michael Arrington,", "Brett Cummins,", "eight Indian army troopers, including one officer, and 17 militants,", "Saturday", "Nicole", "legitimacy of that race.", "Adidas", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "promise to improve health and beauty.", "Chinese", "Newcastle", "Nothing But Love", "allegedly involved in forged credit cards and identity theft", "June 6, 1944,", "almost all [Middle East and North Africa] countries, and censorship and self-censorship are prevalent throughout the region.", "twice", "October 19", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Seoul,", "promotes fuel economy and safety while boosted the economy.", "ALS6", "eight", "Siri", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback forest-firefighters", "children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "a U.S. helicopter crashed in northeastern Baghdad", "attempting illegal crossings", "American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "38", "Her husband and attorney, James Whitehouse,", "test scores and graduation rates", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel ( 1497 -- 1558 )", "a turtle", "British", "fox hunting", "New York", "travel diary", "16,116", "smoke", "sukkar", "bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6782317075285825}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.2222222222222222, 1.0, 0.8, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.47619047619047616, 0.0, 1.0, 0.625, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-2812", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-11573"], "SR": 0.59375, "CSR": 0.5365691489361701, "EFR": 0.7307692307692307, "Overall": 0.6583114259410803}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Airlines", "\"Parachutes\"", "5", "Chicago", "\"The Ones Who Walk Away from Omelas\"", "child actor", "Dennis H. Kux", "drawing the name out of a hat", "Brett Ryan Eldredge", "I-League", "two or three", "Badfinger", "Lady Frederick Windsor", "point-coloration", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1946 and 1947", "5,112", "1992", "many artists' lofts and art galleries, but is now better known for its variety of shops ranging from stylish upscale boutiques to national and international chain store outlets.", "143,372", "6'5\" and 190 pounds", "Mickey Gilley's", "Swiss federal popular initiative \"against mass immigration", "puppy", "Mexican", "December 24, 1973", "1933", "the backside", "Kristoffer Rygg", "1730", "London Luton Airport.", "the Salzburg Festival", "Mississippi", "Afghanistan", "1959", "Imelda Marcos", "Randall Boggs", "Part II", "Bunker Hill", "lion", "Royal", "World War II", "Knoxville", "\"Three's Company\"", "P.O.S,", "Labour", "\"Linda McCartney's Life in Photography\"", "Erich Maria Remarque", "September 14, 2008", "79", "Frank Theodore `` Ted '' Levine", "Romania", "Zephyr, Billy Cobham, Alphonse Mouzon, the James Gang, Deep Purple, and Moxy.", "Mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "ante rooms", "Lehman Bros International"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7062213827838828}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.13333333333333333, 1.0, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-4043", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-10434", "mrqa_triviaqa-validation-2701"], "SR": 0.609375, "CSR": 0.5380859375, "EFR": 0.76, "Overall": 0.6644609375}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "St Petersburg", "lime", "offensive", "Augustus", "the hexameters", "Fawn Hall", "relinquish", "Shakespearean Heroines", "Barnum", "Peter John", "cathode", "torque", "gold", "Marlon Brando", "high German", "Renoir, Degas", "University of Kentucky", "the ruddy", "Brussels", "Macbeth", "William", "piracy", "the death vestments", "Martin Luther", "Clue", "Poe", "Norway", "Andrew Johnson", "seventh", "Mike Connors", "Jungle Jim", "Jim Inhofe", "sancire", "Corpus Christi", "South Africa", "ostrich", "a \"rigid\" constitution", "8-hour", "mug", "Desperate Housewives", "Galileo Galilei", "Canada's", "the Princess Diaries", "Split", "the Grail", "West Virginia", "Thomas Jefferson", "movie house", "Renold", "critic", "Khrushchev", "1904", "young", "Bobby Tambling", "ambilevous", "chariot", "Humberside", "more than 265 million", "100 million", "freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "a head injury.", "Pope Benedict XVI", "Charles II"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4800045289855073}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-10470", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-15329", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-6959", "mrqa_searchqa-validation-3406", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-10801", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.40625, "CSR": 0.5353954081632653, "EFR": 0.8157894736842105, "Overall": 0.6750807263694952}, {"timecode": 49, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1056", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-211", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-785", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8428", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.7265625, "KG": 0.48203125, "before_eval_results": {"predictions": ["National Security Agency", "Heisman Trophy", "Brandi Chastain", "the Androscoggin", "C.J. Parker", "carnaval", "Treasure Island", "Pocahontas", "Improv", "Katzenbach", "low", "an inhalant or deodorant", "Great American Novel", "Ferris B Mueller's", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "the Percheron", "Lawrence Berkeley National Laboratory", "three Hills Rodeo", "a fresco", "Nevil Shute", "(Ulysses) Grant", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "leotard", "Bulworth", "a small intestine", "the mouthpiece", "Key West", "a pickle", "Have You Neverbeen Mellow", "repellents", "Manhattan", "130th anniversary", "Leontyne Price", "compost", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "(Darren) Star", "the Pierian spring", "Hungary", "the burnoose", "Philadelphia", "peanut butter", "Invisible Man", "diamond", "Alexander \"Lex\" Luthor", "food and clothing", "Schwarzenegger", "Master Christopher Jones", "Hebrew", "(Scottish)", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "Sunday evening forced the closure of a terminal for hours while authorities rescreened thousands of passengers.", "three out of four", "poems", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5715277777777777}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-16451", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12067", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301"], "SR": 0.5, "CSR": 0.5346875, "EFR": 0.84375, "Overall": 0.6720937499999999}]}