{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2060, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Ed Asner", "arrows", "1st century BC", "Marburg Colloquy", "Brookhaven", "ca. 2 million", "the Hungarians", "Mercury", "19th Century", "Art Deco style in painting and art", "The ability to make probabilistic decisions", "impact process effects", "1999", "phagosome", "the mass of the attracting body", "the Association of American Universities", "three", "allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "freight services", "up to four minutes", "the Little Horn", "Muslim and Chinese", "intracellular pathogenesis", "Santa Clara, California", "1784", "George Low", "Annual Conference Cabinet", "three", "Students", "Atlantic", "2001", "1887", "Chicago Bears", "John Harvard", "increase its bulk and decrease its density", "literacy and numeracy", "Christmas Eve", "the state", "Paris", "gender roles and customs", "outdated or only approproriate", "soy farmers", "United States", "Albert Einstein", "the number of social services that people can access wherever they move", "Tesco", "ABC Inc.", "1776", "wireless", "an electric current", "Warszowa", "the courts of member states", "supervisory church body", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church", "Manakin Episcopal Church", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Westminster", "Von Miller", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "Khwarezmia", "Queen Elizabeth II", "CBS", "Pittsburgh Steelers", "The chloroplast peripheral reticulum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8875363542546205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212122, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1826", "mrqa_squad-validation-4874", "mrqa_squad-validation-4283", "mrqa_squad-validation-1802", "mrqa_squad-validation-6210", "mrqa_squad-validation-3650", "mrqa_squad-validation-7430", "mrqa_squad-validation-6136"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["the Inland Empire", "New Zealand", "Jacksonville", "Newton's First Law", "the ability to pursue valued goals", "May 1888", "lecture theatre", "more than 28 days", "elliptical", "Boston", "Wednesdays", "Orange", "three", "Lampea", "San Jose State", "March 29, 1883", "between AD 0\u20131250", "Pleurobrachia", "eleven", "punts", "Solim\u00f5es Basin", "1474", "Arizona Cardinals", "Julia Butterfly Hill", "Orange", "Doctor in Bible", "left Graz", "waldzither", "over $40 million", "14th century", "6.7+", "end of the 19th century", "peace", "$40,000", "Cloth of St Gereon", "time and space", "7,000", "elementary particles", "indigenous", "3.5 billion", "New York City O&O WABC-TV and Philadelphia O&o WPVI-TV", "John Fox", "architectural", "Prime ideals", "Normant", "Leonardo da Vinci", "2003", "modern buildings", "Charles River", "KOA", "a disaster", "no contest", "Latin", "Manakin Town", "40,000", "After liberation", "\"winds up\" the debate", "1.1 \u00d7 1011 metric tonnes", "The Daily Mail", "Uncle Tom\u2019s Cabin", "The liver", "No man", "Martina Hingis", "Ukraine does not have real established and ratified borders with Russia"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8760416666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669]}}, "before_error_ids": ["mrqa_squad-validation-4458", "mrqa_squad-validation-1775", "mrqa_squad-validation-1001", "mrqa_squad-validation-696", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-7750", "mrqa_naturalquestions-validation-646"], "SR": 0.859375, "CSR": 0.8671875, "EFR": 1.0, "Overall": 0.93359375}, {"timecode": 2, "before_eval_results": {"predictions": ["$155 million", "CBS", "San Jose State", "Half", "evolution of the German language and literature", "a Latin translation of the Qur'an", "the Brotherhood", "high wages", "Tolui", "human law", "the object's weight", "over half", "1960s", "two months", "Johannes Bugenhagen and Philipp Melanchthon", "1805", "Elders", "30\u201375%", "45,000 pounds", "self molecules", "Taishi", "1960", "Captain America: Civil War", "political divisions", "D loop mechanism", "Monterey", "The Book of Common Prayer", "14", "Charleston", "fear of their lives", "hot winds blowing from nearby semi-deserts", "intracellular pathogenesis", "Safari Rally", "10,006,721", "Philip Segal", "the breadth of sizes", "1965", "whether this connection is relevant on microscales", "German Te Deum", "Stanford Stadium", "Jin", "Trevathan", "Doctor Who", "1206", "clinical services", "CRISPR sequences", "Queen Elizabeth II", "zero", "1992", "food security", "plasmas", "low ratio of organic matter to salt and water", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "time goes", "guardian", "guardian", "black", "50 feet"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6565002705627705}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2289", "mrqa_squad-validation-6099", "mrqa_squad-validation-6641", "mrqa_squad-validation-8360", "mrqa_squad-validation-2577", "mrqa_squad-validation-8747", "mrqa_squad-validation-5893", "mrqa_squad-validation-2906", "mrqa_squad-validation-1860", "mrqa_squad-validation-10427", "mrqa_squad-validation-6178", "mrqa_squad-validation-6405", "mrqa_squad-validation-1435", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11930", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-12889", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3857"], "SR": 0.609375, "CSR": 0.78125, "EFR": 1.0, "Overall": 0.890625}, {"timecode": 3, "before_eval_results": {"predictions": ["fewer than 10 employees", "1624", "Hangzhou", "in committee", "19th century", "1962", "dealing with patients' prescriptions and patient safety issues", "a group that included priests, religious leaders, and case workers as well as teachers", "Vistula River", "1290", "21 October 1512", "427,652", "double membrane", "August 1967", "German", "27-30%", "four", "the 50 fund", "Arizona Cardinals", "Peanuts", "\"ctenes\" or \"comb plates\"", "calcitriol", "Warsaw", "time", "since at least the mid-14th century", "the mitochondrial double membrane", "Mike Figgis", "in an adult plant's apical meristems", "isopentenyl pyrophosphate synthesis", "Associating forces with vectors", "Prime ideals", "The Three Doctors", "Malik Jackson", "four", "the Koori", "1910\u20131940", "pressure swing adsorption", "Johann Tetzel", "English", "a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "gauge bosons", "A1 (Gateshead Newcastle Western Bypass)", "Sun Life Stadium", "the Duchy of Prussia, the Channel Islands, and Ireland", "John Houghton", "February 2015", "draftsman", "a tiny snail", "a son should not kill his own mother", "the Galapagos Islands", "Connecticut", "the water vapor becomes liquid", "the best minds in the travel industry", "Amaranth", "a major raw", "Mycenaean civilization", "the body that regulates its rhythmic and periodic cycles", "the Empire State Building", "the Normandy Landings", "Evelyn \"Billie\" Frechette", "the shinbone or shankbone", "The Anvil Chorus", "Roald Amundsen", "the Royal Border Bridge"], "metric_results": {"EM": 0.625, "QA-F1": 0.6455627705627706}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6345", "mrqa_squad-validation-2192", "mrqa_squad-validation-3347", "mrqa_squad-validation-1036", "mrqa_squad-validation-3673", "mrqa_squad-validation-6737", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-22", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-8509", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-10694", "mrqa_searchqa-validation-6338", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-7003"], "SR": 0.625, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 4, "before_eval_results": {"predictions": ["Germany and Austria", "Centrum", "blue", "to spearhead the regeneration of the North-East", "Rhenus", "gambling", "the wing of the secular powers", "compiled a cubic interpolation formula for his astronomical calculations", "Zhongtong", "11.1%", "1538", "Deacons", "New Testament", "their prestige, \"experience, ideology, and weapons\",", "25 percent", "May 2013", "Torchwood (2006\u20132011) and The Sarah Jane Adventures (2007\u20132011", "capturing prey", "gana and thylakoids", "livestock pasture", "Ford", "1,300,000", "rubisco starts accidentally adding oxygen to sugar precursors", "two tumen (20,000 soldiers)", "eight", "algorithm", "WzzM and WOTV", "Orange", "tentilla", "gender roles and customs", "social unrest and violence", "Woodward Park", "1745", "Battle of Olustee", "observer status", "50-yard line", "3D printing technology", "Malkin Athletic Center", "24\u201310", "8.0", "empires", "domestic legislation of the Scottish Parliament", "a patient's quality of life", "New York City Mayor Michael Bloomberg", "the oceans are growing crowded, and governments are increasingly trying to plan their use", "innovative, exciting skyscrapers", "a lump in Henry's nether regions", "World War II", "semiconductors", "Tim Clark, Matt Kuchar and Bubba Watson", "fastest circumnavigation of the globe in a powerboat", "the man facing up, with his arms out to the side", "the foyer of the BBC building in Glasgow, Scotland", "Buddhism", "Manchester City", "Noriko Savoie", "three", "change course", "Tsvangirai", "A Lion Among Men", "the Federal Communications Commission required television stations to air anti-smoking advertisements at no cost to the organizations providing such advertisements", "a miracle food", "Chelsea Lately", "Luxembourg"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6838947510822511}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.2666666666666667, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7606", "mrqa_squad-validation-9250", "mrqa_squad-validation-8231", "mrqa_squad-validation-9645", "mrqa_squad-validation-7626", "mrqa_squad-validation-8874", "mrqa_squad-validation-4258", "mrqa_squad-validation-8832", "mrqa_squad-validation-6046", "mrqa_squad-validation-1643", "mrqa_squad-validation-4572", "mrqa_squad-validation-7094", "mrqa_squad-validation-2798", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2807", "mrqa_naturalquestions-validation-47", "mrqa_triviaqa-validation-1366", "mrqa_hotpotqa-validation-547"], "SR": 0.59375, "CSR": 0.7125, "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["with money from foreign Islamist banking systems, especially those linked with Saudi Arabia", "Doritos", "4000 years", "$37.6 billion", "Anglo-Saxons", "seven", "Golden Gate Bridge", "Southwest Fresno", "divergent boundaries", "chloroplasts are surrounded by a double membrane", "QuickBooks", "surface condensers", "clinical pharmacists", "a seal", "Philip Howard", "King Ethelred II of England", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "three", "Spanish", "Golden Super Bowl", "constitutional traditions common to the member states", "pharmacological effect", "Huguenots", "10\u20137", "Polish Academy of Sciences", "spherical", "Nurses", "New England Patriots", "Time magazine", "Class II MHC molecules", "two", "George Westinghouse", "by disrupting their plasma membrane", "internal combustion engines", "indirectly", "Religious and spiritual teachers", "B cells", "property damage", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "Goa", "How I Met Your Mother", "France's famous Louvre museum", "Leo Frank", "Thessaloniki", "Graziano Transmissioni", "opposition parties", "204,000", "Newcastle retained fourth place with a 3-1 victory over Blackburn, who remained in the relegation zone.", "release of the four men", "first lady Michelle Obama is weighing in on the issue by focusing on how health care can affect families.", "Ed McMahon", "this will be the first time any version of the Magna Carta has ever gone up for auction,", "Barack Obama", "at the University of Alabama in Huntsville", "Obama", "ballots", "Sodra nongovernmental organization", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "promotes fuel economy and safety while boosted the economy.", "heart rate that exceeds the normal resting rate", "Suffolk Punch", "Denmark", "Cincinnati", "Donald Sutherland"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7125277194211017}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.0, 0.11764705882352941, 0.09523809523809525, 0.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9691", "mrqa_squad-validation-8715", "mrqa_squad-validation-1090", "mrqa_squad-validation-3610", "mrqa_squad-validation-3075", "mrqa_squad-validation-827", "mrqa_squad-validation-6644", "mrqa_squad-validation-10444", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-3935", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-4043", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-4171", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-2465"], "SR": 0.609375, "CSR": 0.6953125, "EFR": 1.0, "Overall": 0.84765625}, {"timecode": 6, "before_eval_results": {"predictions": ["18 February 1546", "11", "neither conscientious nor of social benefit", "University of Chicago Press", "$2 million", "2015", "1762", "biased against Genghis Khan", "Warsaw Stock Exchange", "they are often branched and entangled with the endoplasmic reticulum", "computational resource", "to denote unknown or unexplored territory", "Nicholas Stone", "early Lutheran hymnals", "world line", "1991", "William Smith", "William Pitt", "geochemical component called KREEP", "the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "Japan", "Super Bowl Opening Night", "Working Group chairs", "laws of physics", "Broncos", "noisiest", "three-dimensional", "issues under their jurisdiction", "unsuccessful", "human", "they are homebound", "eliminate all multiples of 1", "nerves", "1700", "New Germany", "polysaccharides", "Governor of Vermont from 1991 to 2003", "the circulatory", "troggs", "God created everything in six days.", "Bratislava,", "Diana, the Princess.", "slave trade.", "vena cava", "a scallop", "bull's-eye", "Tartarus", "sky is known", "Nancy Reagan", "the Persian Achaemenid Empire.", "LAP", "Count Ferdinand von Zeppelin", "huge.", "Duke of Clarence", "a millimeter.", "\"Popeye\"", "Judas!", "survivors of Oceanic Flight 815", "pinball machine", "Love Is All Around", "Los Angeles Dance Theater", "Somalia", "field 14 unapproved narcotics that are widely used to treat pain.", "18"], "metric_results": {"EM": 0.5625, "QA-F1": 0.629431216931217}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.47619047619047616, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.07407407407407407, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6230", "mrqa_squad-validation-8765", "mrqa_squad-validation-5588", "mrqa_squad-validation-10477", "mrqa_squad-validation-4005", "mrqa_squad-validation-5054", "mrqa_squad-validation-383", "mrqa_squad-validation-6337", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4830", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-13281", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-1637", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-10057", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1064"], "SR": 0.5625, "CSR": 0.6763392857142857, "EFR": 0.9642857142857143, "Overall": 0.8203125}, {"timecode": 7, "before_eval_results": {"predictions": ["extinction of the dinosaurs", "oxygen", "encourage growth in richer countries", "K-9 and Company", "9.1 million", "little", "Asian, African and Caribbean countries", "cattle", "Mongol", "semantical problems and grammatical niceties", "five", "the \"gold standard\" of religion in minds of some or many Muslims.", "British", "Finsteraarhorn", "Abilene", "white", "Yosemite Freeway", "Thanksgiving", "874.3 square miles", "Two thirds", "the Privy Council", "well into the nineteenth century", "equality deprivation", "Daily Mail", "San Mateo", "Spanish", "around 300,000", "cryptomonads", "Swahili", "hymn-writer", "starch", "Bryant", "Earth", "tornado", "Rod Steiger", "hoo-hoo, the barn type", "John Sexton", "Kenny G", "coffee", "Chazz Michael Michaels", "Alberta", "Season", "kissanhnta", "bark beetles", "Allah", "bones", "Python", "Cecil B. DeMille", "Ada", "Faith Hill", "Ben Affleck", "U.S.", "V", "time", "Yardbird and Bird", "Sweden", "Vietnam", "hydrogen", "Alexandria", "Perfume: The Story of a Murderer", "New Jersey Economic Development Authority's 20% tax credit", "Georgetown", "Essex Eagles", "Alzheimer's disease"], "metric_results": {"EM": 0.515625, "QA-F1": 0.581737012987013}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7468", "mrqa_squad-validation-1938", "mrqa_squad-validation-6220", "mrqa_squad-validation-9588", "mrqa_squad-validation-4562", "mrqa_squad-validation-8189", "mrqa_squad-validation-7565", "mrqa_searchqa-validation-47", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-7869", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-943", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-3893", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-8990", "mrqa_searchqa-validation-4050", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10916", "mrqa_searchqa-validation-5178", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-7551", "mrqa_hotpotqa-validation-4891", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3468"], "SR": 0.515625, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 8, "before_eval_results": {"predictions": ["shocked", "lymphocytes", "producers of the show", "BSkyB", "Kawann Short", "Daidu in the north", "silent film", "22", "the park", "1965", "tidal currents", "Concentrated O2", "Ma Jianlong", "Demaryius Thomas", "Lake Constance", "the Orange Democratic Movement (ODM-K)", "Bannow Bay", "Red Army", "20th century", "ITT", "1966", "masses", "Linebacker", "high art and folk music", "four", "with six series of theses", "Happy Endings", "seven-eighths", "the cardinal de Richelieu", "the Atlas Mountains", "Madrid", "the Danube", "Yahweh", "leather", "George Pullman", "plums", "Jesus Christ", "Sappho", "Possession", "the tonka bean", "the divisor", "Hypnos", "Texas", "IHOP", "a white breed", "the Bill of Rights", "the ACT", "Brasilia", "Henry David Thoreau", "the Santa Ana winds", "Dick Cheney", "the sun", "Gustave Eiffel", "Edward Hopper", "the CIA", "d'Artagnan", "painted Caves", "1985 -- 1993", "apple", "its air-cushioned sole", "13", "Fort Worth", "Agent 99", "private"], "metric_results": {"EM": 0.625, "QA-F1": 0.6826202876984128}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.375, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3478", "mrqa_squad-validation-8421", "mrqa_squad-validation-1169", "mrqa_squad-validation-2474", "mrqa_searchqa-validation-15994", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-15182", "mrqa_searchqa-validation-523", "mrqa_searchqa-validation-15584", "mrqa_searchqa-validation-9386", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-10315", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-9179", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4461"], "SR": 0.625, "CSR": 0.6527777777777778, "EFR": 1.0, "Overall": 0.8263888888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["\"Provisional Registration\"", "August 15, 1971", "Levi's Stadium", "United Nations Framework Convention on Climate Change", "Inflammation", "Brown v. Board of Education of Topeka", "15 May 1525", "The Walt Disney Company", "Dundee", "Over 61 per cent", "During the Second World War", "the integer factorization problem", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence.", "Exploration is still continuing to determine if there are more reserves", "prep schools", "soft power", "strong Islamist outlook", "lengthening rubbing surfaces of the valve", "$32 billion", "keyed Northumbrian smallpipes", "the Dutch Republic", "Alex Haley", "three", "the honeyeater", "4:51", "Khrushchev", "Rhea", "the root", "Elton John", "Cuba", "the Battle of Thermopylae", "Preamble", "Kroc", "cricket", "white", "Wash.", "Preamble", "Italy", "a fifth part of 15", "tarn", "100; or 25", "the Buffalo bison", "Ann Widdecombe", "an equilateral triangle", "the Old Kent Road", "Tuesday", "Preamscopic", "Ab Fab", "Massachusetts", "Barrow", "California", "the Susquehanna River", "80\u2019s", "the maqui berry", "Singapore", "Wigan Warriors", "Davos", "eight", "Hoffa", "Home Rule Party", "Janet Napolitano", "J. Crew", "Deval Patrick", "Preamble"], "metric_results": {"EM": 0.5, "QA-F1": 0.5720486111111112}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.2666666666666667, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2920", "mrqa_squad-validation-9552", "mrqa_squad-validation-8273", "mrqa_squad-validation-9870", "mrqa_squad-validation-3013", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-5586", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2533", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-1203", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2672", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1553", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-7509"], "SR": 0.5, "CSR": 0.6375, "EFR": 0.96875, "Overall": 0.803125}, {"timecode": 10, "before_eval_results": {"predictions": ["-s", "environmental determinism", "4 August 2010", "King George III", "radio network", "Edsen Khoroo", "League of Augsburg", "Duarte Barbosa", "the People's Republic of China", "Roman Catholic", "Amazonia: Man and Culture in a Counterfeit Paradise", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "Sydney", "five", "January 18, 1974", "Spanish", "NFL", "extremely difficult", "student populations", "Catholic", "Parliament of the United Kingdom", "296", "the Ghent-Terneuzen Canal", "mulberry", "the mouth", "stone", "Ken Russell", "Dan Dare", "hera", "Smiths", "Mike Tyson", "hera", "Pesach", "Brian Deane", "kaleidoscope", "Uranus", "Apollon", "George Carlin", "Soviet Union", "Sydney", "Los Angeles", "the Underground Railroad", "puck", "\"beyond violet\"", "a\u00e7ai", "Portugal", "football", "Serena Williams", "63 to 144 inches", "the Titanic", "William Tell", "Christian Dior", "the snail", "Mendip Hills", "Wichita", "eukharisti\u0101", "New Croton Reservoir", "The Way You Move", "Leucippus", "Stephen King", "Venus Williams", "firefighter", "Ponty Mython", "Roman Polanski"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6866477272727273}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-6263", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-5874", "mrqa_triviaqa-validation-4926", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-2265", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-7138", "mrqa_newsqa-validation-2710", "mrqa_searchqa-validation-3397"], "SR": 0.671875, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.8203125}, {"timecode": 11, "before_eval_results": {"predictions": ["method by which the medications are requested and received", "salvation", "stoves", "they produce secretions (ink) that luminesce at much the same wavelengths as their bodies", "zaju variety show", "administration", "Chivas USA", "Edinburgh", "The Pink Triangle", "the dot", "Magdalen Tower", "an international data communications network", "public service", "Guy de Lusignan", "tiger team", "b cells", "The European Commission", "completed (or local) fields", "fundamental error", "Mongol and Turkic tribes", "hera", "five", "Whist", "caesar", "Toscana", "albinism", "black", "Pluto", "chromium", "black gold", "The Hague", "Vancouver Island", "Ironside", "gollancz", "nizhny Novgorod", "black spots", "Beyonce", "Wordsworth", "Man V Food", "Queen Elizabeth II", "Samuel Johnson", "Conrad Murray", "caesar", "Bennett Cerf", "green leaf", "morocco", "caesar", "Shrek", "morocco", "lions", "rhododendron", "Bob Fosse", "morocco", "Shanghai", "sweden", "boat lifts", "snake River Valley", "17 October 2006", "beer", "Las Vegas to Arizona", "Saturday's Hungarian Grand Prix", "Capuchin Church of the Immaculate Conception", "Edgar Allan Poe", "bobby"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5594494047619047}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5460", "mrqa_squad-validation-8252", "mrqa_squad-validation-6530", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-3550", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-2204", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-3846", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6189", "mrqa_triviaqa-validation-3023", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-890", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-8473"], "SR": 0.515625, "CSR": 0.6302083333333333, "EFR": 1.0, "Overall": 0.8151041666666666}, {"timecode": 12, "before_eval_results": {"predictions": ["a gift from God", "Greenland", "1724 to 1725", "placing them on prophetic faith.", "1.25 million", "1080i HD", "five", "Maria Goeppert-Mayer", "International Association of Methodist-related Schools, Colleges, and Universities", "one", "a majority in Parliament", "President Mahmoud Ahmadinejad", "Newcastle Eagles", "cholera", "General Pharmaceutical Council (GPhC)", "relative units of force and mass", "AD 14", "orogenic wedges", "his exploration and settlement of what is now Kentucky, which was then part of Virginia but on the other side of the mountains from the settled areas.", "The Handmaid's Tale", "chimpanzee", "The Fault in Our Stars", "car car", "video", "1961", "400 MW", "Total Nonstop Action Wrestling", "galt\u00fcr avalanche", "Archbishop of Canterbury", "1861", "Walt Disney World Resort in Lake Buena Vista, Florida", "David Villa", "Red and Assiniboine Rivers", "Bergen County, New Jersey,", "Continental Army", "Jack Kilby", "Ryan Babel", "Umar Israilov", "July 16, 1971", "1933", "The Heirs", "Baudot code", "1959", "1887", "Mark Dayton", "Marvel Comics", "The Weeknd", "Nick Cassavetes", "Lamar Hunt", "Sarah Winnemucca", "Jean Baptiste Point du Sable", "England", "Paul W. S. Anderson", "a basilica", "1994", "Ricky Nelson", "Wakanda and the Savage Land", "mercury", "phobias", "drug trade, which has increased dramatically since the American-led invasion to remove the hard-line Islamist government of the Taliban in October 2001.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "andrew johnson", "dapple-grey", "pre-Columbian times"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6955357142857143}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 0.19999999999999998, 0.04761904761904762, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4958", "mrqa_squad-validation-6320", "mrqa_squad-validation-10428", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-383", "mrqa_naturalquestions-validation-6015", "mrqa_triviaqa-validation-2685", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-7025", "mrqa_naturalquestions-validation-8227"], "SR": 0.640625, "CSR": 0.6310096153846154, "EFR": 1.0, "Overall": 0.8155048076923077}, {"timecode": 13, "before_eval_results": {"predictions": ["\u00a341,004", "The Rocket", "Tolui", "lower lake", "Gospi\u0107, Austrian Empire", "since 2001", "a maze of semantical problems and grammatical niceties", "\"Southwest Fresno\"", "5,000", "Huguenot", "ABC News Now", "sold Wardenclyffe for $20,000 ($472,500 in today's dollars)", "\u00c9mile Girardeau", "Brownlee", "The Five Doctors", "a certain number of teacher's salaries are paid by the State", "NCAA Division II", "Adrian Lyne", "his most brilliant student", "Las Vegas", "Ranulf de Gernon, 4th Earl of Chester", "2017", "Dealey Plaza", "Rudolf Schenker", "Shrek", "Lucille Ball", "\"Grimjack\" (from First Comics)", "16\u201321", "Vince Guaraldi", "Tony Burke", "Michael Redgrave", "6th", "Highlands Course", "Hawaii", "unclearambar styracif", "Marquis de Lafayette", "Bharatiya Janta Party (BJP)", "three", "Winter Haven", "four", "The Process", "Mindy Kaling", "Surrey", "Claudio Javier L\u00f3pez", "My Beautiful Dark Twisted Fantasy", "FCI Danbury", "a few", "\"Home of the Submarine Force\"", "Las Vegas", "Pope John X", "(2007)", "Arlo Looking Cloud", "Rwandan genocide", "Larnelle Harris", "The President of the United States negotiates treaties with foreign nations,", "2015, 2017", "1982", "rod", "Chris Robinson", "\"G unclear Girl\"", "shock wave", "unclear", "the Egyptian Goddess of Creation", "Eric Burdon"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6250496031746031}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3176", "mrqa_squad-validation-3287", "mrqa_squad-validation-1488", "mrqa_squad-validation-7792", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-3556", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-2732", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-16181", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-469"], "SR": 0.546875, "CSR": 0.625, "EFR": 1.0, "Overall": 0.8125}, {"timecode": 14, "before_eval_results": {"predictions": ["10,000", "perpendicular to the velocity vector", "Inherited wealth", "December 1963", "Only the series from 2009 onwards", "religious freedom in the Polish\u2013Lithuanian Commonwealth", "Spreading throughout the Mediterranean and Europe,", "mass", "ten times their own weight", "Quaternary", "1887", "other ctenophores", "symbiotic relationship with vitamin D", "mathematical models of computation", "Vistula River", "100 to 150", "Apple's new clock faces for folks who use the tiny player's 1.5-inch screen as a watch", "the school.", "March 8", "Democrats and Republicans", "the Catholic League", "Half Moon Bay", "\"He is obviously very relieved and grateful that the pardon was granted,\" Dean said.", "Friday", "Movahedi", "different women coping with breast cancer in five vignettes", "there were problems with the well and he should move his ship away.", "moody", "$40 and a quarter of bread", "Lance Cpl. Maria Lauterbach", "South Korea", "London", "more than 4,000 commercial farmers off their land, destroying Zimbabwe's once prosperous agricultural sector.", "in Lienz, France earlier this year.", "the results by a chaplain about 1:45 p.m., per jail policy.", "three people in connection with the attack, including a suspect \"fleeing the scene [who] tested positive for explosive residue", "$14.1 million", "1616", "Saturn", "Chile", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "Buddhism", "a $158 green skirt and $298 bead and rhinestone cardigan", "U.N. officials", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "boyhood experience in a World War II internment camp", "suppress the memories and to live as normal a life as possible", "about 4 meters (13 feet) high", "Dublin", "Democrat", "a Utah jail", "Mandi Hamlin and her lawyer, celebrity attorney Gloria Allred,", "Islamabad", "7 p.m.", "March 26, 1973", "Indian Ocean", "argument form", "the foot corresponding to which hand you intend to throw with perpendicular to the line", "Sevens", "England", "Yemen", "portraitist", "peter", "mercury"], "metric_results": {"EM": 0.359375, "QA-F1": 0.46031217046842043}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.27272727272727276, 1.0, 0.6666666666666666, 0.8750000000000001, 0.05, 0.0, 0.8000000000000002, 0.0, 0.0, 0.0, 0.13333333333333333, 0.2857142857142857, 0.1818181818181818, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10400", "mrqa_squad-validation-7770", "mrqa_squad-validation-4856", "mrqa_squad-validation-10458", "mrqa_squad-validation-6565", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-1329", "mrqa_naturalquestions-validation-6733", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-12191", "mrqa_triviaqa-validation-3839"], "SR": 0.359375, "CSR": 0.6072916666666667, "EFR": 1.0, "Overall": 0.8036458333333334}, {"timecode": 15, "before_eval_results": {"predictions": ["Prospect Park", "Khanbaliq", "Quaternary", "1870", "water", "prime", "50 fund", "Camisards", "over $40 million", "GTE.", "1,100 tree species", "spin", "Oligocene", "D.S. District Judge Dale Kimball", "Charles Darwin", "a Little Rock military recruiting center", "March 24,", "the Beatles", "Robert Park", "Amir Zaki", "Eleven", "2007", "a dozen", "\"She had a smile on her face, like she always does when she comes in here,\"", "56", "the National Football League", "\"The Lost Symbol\"", "Heshmat Tehran Attarzadeh", "IV cafe.", "18 federal agents and two soldiers", "Atlanta", "resources", "senior", "two Emmys", "\"To all of our valiant men and women, know that the American people believe in you, support you and are 100 percent behind you, and we thank God every day that you have our back.\"", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Rwanda", "75", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "closing these racial gaps.", "a bond hearing", "President Bush", "Amstetten,", "African National Congress Deputy President Kgalema Motlanthe,", "spiral into economic disaster.", "he lost his bid to Francisco de Narvaez, who leads a rival Peronist party, Union PRO, by a tally of 34.6 percent to 32.1 percent.", "Ralph Cifaretto", "a strict interpretation of the law,", "saying Chaudhary's death was warning to management.", "Iran", "20% tax credit", "July 23.", "70,000 or so are estimated to be there now.", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "Schleiden and Schawnn", "Tim McGraw", "Prussian 2nd Army", "cabbage", "a homebrew campaign setting", "Beno\u00eet Jacquot", "blue", "Capitol Building", "The Left Book Club", "holography"], "metric_results": {"EM": 0.4375, "QA-F1": 0.531619798026048}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.057142857142857134, 0.0, 0.29629629629629634, 0.0, 1.0, 1.0, 1.0, 0.6923076923076924, 0.2962962962962963, 0.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9016", "mrqa_squad-validation-4349", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2727", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-1335", "mrqa_triviaqa-validation-6296"], "SR": 0.4375, "CSR": 0.5966796875, "EFR": 1.0, "Overall": 0.79833984375}, {"timecode": 16, "before_eval_results": {"predictions": ["two thousand people", "address information", "high risk of a conflict of interest and/or the avoidance of absolute powers.", "to look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system.", "Thames River", "British East Africa (as the Protectorate was generally known) and German East Africa", "several hundred thousand, some 30% of the city", "Tower District", "Ted Ginn Jr.", "Catch Me Who Can", "John Fox", "the housing bubble", "137", "Adam Lambert and Kris Allen,", "Brian Smith", "\"Hillbilly Handfishin'\"", "Charles Lock", "voluntary homicide", "his enjoyment of sex and how he lost his virginity at age 14.", "his injuries,", "1979", "murder", "next year", "his plans to overhaul domestic policies,", "Christopher Savoie", "Anil Kapoor", "Afghanistan and India", "Dr. Albert Reiter,", "\"theoretically\" Iran could develop a nuclear bomb within the next year.", "\"whole ethos is one of violence\" and that it had \"made a brutal choice to step up attacks against innocent civilians,\"", "Matthew Fisher,", "cancer,", "Courtney Love,", "us to step up.\"", "\"Walk -- Don't Run\" and \"Haw Hawaii Five-O\"", "your own environmental videos", "two women", "Queen Elizabeth's birthday", "Monday,", "First Stop Resource Center and Housing Program", "Yusuf Saad Kamel", "hand-painted Swedish wooden clogs", "11 healthy eggs", "expressed concerns about the missile defense system.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Fullerton, California,", "September 28, 1918,", "1950s,", "U.S. troops", "a government-run health facility that provides her with free drug treatment.", "vegan bake sales from April 24 through May 2.", "\"The Rosie Show,\"", "al Fayed's", "Oxbow,", "gastrocnemius", "Ed Sheeran", "20", "Australia", "three-part", "2001", "vingtaines (or, in St. Ouen, cueillettes),", "Joe Louis", "George Blake", "Bogota"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6553498584748585}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.9743589743589743, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.27272727272727276, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714285, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-8570", "mrqa_squad-validation-914", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-2957", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-4647", "mrqa_searchqa-validation-8433", "mrqa_triviaqa-validation-6739"], "SR": 0.5625, "CSR": 0.5946691176470589, "EFR": 0.9642857142857143, "Overall": 0.7794774159663866}, {"timecode": 17, "before_eval_results": {"predictions": ["lower levels", "a pharmacy practice residency", "questions and answers", "\"anima non sic dormit),", "Captain Francis Fowke,", "12 January", "60,000", "Zagreus", "CBS", "17", "temperate", "Rod Blagojevich", "\"Sesame Street\"", "St. Louis, Missouri.", "$50 less,", "Afghanistan's restive provinces", "fled Zimbabwe and found his qualifications mean little as a refugee.", "7 Awa", "Iran", "Russian concerns that the defensive shield could be used for offensive aims.", "Sharon Bialek", "Matthew Fisher,", "books on the condition, which is characterized by bouts of diarrhea and constipation.\"", "in the north and west of the country,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "$250,000", "first or second week in April.", "Derek Mears", "a motor scooter", "Gary Player", "sieb\u00fcll", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "kite boards have developed has helped... but there is still a lot of people out there that want to achieve these records,\"", "Virgin America", "Palm Beach, Florida,", "Daniel Wozniak,", "22-year-old", "santa bin Laden", "how health care can affect families.", "santa Maria Costa,", "U.S. Food and Drug Administration", "Casa de Campo International Airport", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "2002", "at checkposts and military camps in the Mohmand agency,", "all the attackers were Pakistanis,", "Friday,", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "crocodile eggs", "from Geraldine Ferraro to Bill Clinton.", "slapped over and was beaten when he refused.\"", "$162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "senators", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "Jokai crater", "Arlene Phillips", "23 July 1989", "Ry\u016bkyuan people", "surrealism", "C. S. Lewis", "7", "rice wine", "Halifax"], "metric_results": {"EM": 0.5, "QA-F1": 0.5837174368772035}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3529411764705882, 0.5217391304347826, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.4444444444444444, 0.0, 1.0, 0.7692307692307693, 0.15384615384615388, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.07999999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7341", "mrqa_squad-validation-2408", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-157", "mrqa_naturalquestions-validation-132", "mrqa_triviaqa-validation-1659", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-6296"], "SR": 0.5, "CSR": 0.5894097222222222, "EFR": 1.0, "Overall": 0.7947048611111112}, {"timecode": 18, "before_eval_results": {"predictions": ["melatonin", "constant factors and smaller terms", "Shi Bingzhi", "New France's governor, the Marquis de Vaudreuil", "linear", "Advanced Steam movement", "Defensive ends", "the dot", "chastity", "European Court of Justice", "bronze medal in the women's figure skating final,", "\"trying to steal the election\" and \"intimidating the population and election officials as well.\"", "UK", "\" Teen Patti\" (\"Card Game\")", "Argentina", "Congress", "28", "New Haven, Connecticut, firefighter Frank Ricci,", "the project, which is designed to promote private sector investment in a variety of gas-related industries, on September 21.", "\"terrifying.\"", "Bill & Melinda Gates Foundation", "$106,482,500", "people give the United States abysmal approval ratings.", "not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures are necessary,", "\"political and religious\"", "$163 million (180 million Swiss francs)", "Afghan lawmakers", "Bahrain", "because that Somalia-based terrorist organization Al Shabaab may have been plotting an attack timed to coincide with the event,", "\"Zed,\"", "because this administration recognizes the importance of Turkey and wants to engage with it from the start.", "because the federal government isn't actively engaged in border enforcement is both dishonest and reckless.", "bricks and concrete on protesters from atop a six-story building.", "\"wildcat\" strikes,", "Ben Roethlisberger", "Dr. Christina Romete,", "Ewan McGregor", "Brazil", "Meira Kumar", "next week.", "Hong Kong from other parts of Asia, such as India and mainland China,", "Lindsey Vonn", "in an interview Tuesday on CNN's \"Larry King Live.\"", "organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "race", "Brazil", "Saluhallen,", "trying to detonate an explosive device in his underwear aboard a Christmas 2009 flight to Detroit,", "two people", "40-year-old", "outside the Iranian consulate in Peshawar", "Casey Anthony,", "the iconic Hollywood headquarters of Capitol Records,", "Emma Watson and Dan Stevens", "2002", "his finger.", "\"Sunny Afternoon\"", "Che Guevara", "Miller Brewing", "Elizabeth I", "John Fogerty", "Garonne", "giraffe", "cheese"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6345937375068567}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.21052631578947367, 0.2857142857142857, 0.8571428571428571, 1.0, 0.0, 0.9142857142857143, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714285, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.16, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2702702702702703, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1603", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9104", "mrqa_triviaqa-validation-7611"], "SR": 0.546875, "CSR": 0.587171052631579, "EFR": 0.896551724137931, "Overall": 0.741861388384755}, {"timecode": 19, "before_eval_results": {"predictions": ["1876", "1507", "Darian Stewart", "11", "unless he were removed from the school, Tesla would be killed through overwork.", "Japanese", "Muqali,", "2011 and 2012", "Pittsburgh Steelers", "apartment building", "Aung San Suu Kyi", "(The Frisky)", "the 3rd District of Utah", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Stephen Tyrone Johns", "30", "procedures", "acid attack by a spurned suitor.", "most of those who managed to survive the incident hid in a boiler room and storage closets", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Courtney Love,", "33-year-old", "to trail the illegal traffic.\"", "a book.", "he was in good health, contrary to media reports he was diagnosed with skin cancer.", "stand down.", "Ashley \"A.J.\" Jewell,", "at least 17", "from her father's home in Satsuma, Florida,", "to the southern city of Naples", "Hugo Chavez", "London's", "rural California,", "off the front pages for the first time in days.", "Old Trafford", "the area of the 11th century Preah Vihear temple", "the Delta Queen steamboat", "the Haeftling range.", "Pacific Ocean territory of Guam", "homicide", "The Ski Train", "Aniston, Demi Moore and Alicia Keys", "Lillo Brancato Jr.", "intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "Blagojevich", "Sen. Barack Obama", "protective shoes", "public-sector labor unions launching a general strike,", "U.S. President-elect Barack Obama", "Burhanuddin Rabbani, a former Afghan president who had been leading the Afghan peace council,", "was depressed over a recent breakup, grabbed the gun and  took her own life.", "glass shards", "Sedimentary rock", "2.45 billion years ago", "London", "Colorado", "Bangor International", "\"Grandmasters\"", "Suffragist", "Canterbury", "Tatooine", "the Lone Ranger", "Bonnie and Clyde"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6724033641796799}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.10526315789473685, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.8333333333333333, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.4, 1.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-816", "mrqa_squad-validation-1326", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2197", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-6758", "mrqa_hotpotqa-validation-2782", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-14663"], "SR": 0.53125, "CSR": 0.584375, "EFR": 0.9666666666666667, "Overall": 0.7755208333333333}, {"timecode": 20, "before_eval_results": {"predictions": ["Hostmen", "Greg Brady", "Fort Caroline", "Hungarians", "De Materia Medica", "John D. Rockefeller", "four", "mistreatment from government officials.", "Beijing, China,", "Virgil Tibbs", "Thaddeus Rowe Luckinbill", "up to 100,000", "United States, its NATO allies and others", "Virginia Dare", "JackScanlon", "Cathy Dennis and Rob Davis", "91.9 by 49.2", "Lalo Schifrin", "MGM Resorts International", "16 August 1975", "seawater pearls", "1962", "Buddhism", "1978", "1927, 1934, 1938, 1956 )", "1969", "Miami Dolphins", "Joseph Heller", "90 \u00b0 N 0 \u00b0 W", "1,350", "Leonard Bernstein", "25 September 2007", "Howard Caine", "Branford College", "62", "team", "September 2014 and PlayStation 3 and Xbox 360 in November 2014", "Archduke Franz Ferdinand of Austria, heir presumptive to the Austro - Hungarian throne", "in Christian eschatology", "October 1941", "peace between two entities", "mughal garden", "Cee - Lo", "after Shawn's kidnapping", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Labour Party", "three times", "November 25, 2002, by the Homeland Security Act of 2002", "December 27, 2015", "Peter Greene", "31 March 1909", "Ed Sheeran", "two", "Alberto Salazar", "live animals", "American", "Hoosick, Rensselaer", "CEO of an engineering and construction company with a vast personal fortune.", "1.2 million", "The Three Little Pigs", "Robert Louis Stevenson", "Samoa", "Russia and China", "Washington State's decommissioned Hanford"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6766109557327459}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8421052631578948, 0.625, 0.3333333333333333, 1.0, 0.4, 0.0, 0.0, 0.0, 0.967741935483871, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-6314", "mrqa_squad-validation-8027", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-5330", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-8934", "mrqa_triviaqa-validation-3886", "mrqa_hotpotqa-validation-2298", "mrqa_newsqa-validation-3687", "mrqa_searchqa-validation-13486", "mrqa_newsqa-validation-2446"], "SR": 0.546875, "CSR": 0.5825892857142857, "EFR": 0.896551724137931, "Overall": 0.7395705049261083}, {"timecode": 21, "before_eval_results": {"predictions": ["66 million years ago", "Spanish", "an attack on New France's capital, Quebec", "the Fresno Traction Company", "Westminster", "blue-green algae", "24 of the 32 songs", "their bearers", "the Washington metropolitan area", "the molar concentration, measured in units of moles per liter, of hydrogen ions", "the breast or lower chest of beef or veal", "the ruling city of the Northern Kingdom of Israel,", "Tagalog or English", "around 1600 BC", "July 2010", "Michael Phelps", "Rajendra Prasad", "Ren\u00e9 Georges Hermann - Paul", "Donna", "Keith Hernandez and Willie Stargell", "Orangeville, Ontario, Canada", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Janie Crawford, an African - American woman in her early forties", "the 2nd century", "in the pancreas", "Elk and Kanawha Rivers", "1961", "the latest version", "rocks and minerals", "Michael Schumacher", "the derivative financial instrument", "Introduced in 1957", "1775", "1995", "President Friedrich Ebert", "2018", "Ireland", "Kit Harington", "the teenage porcupine punk rocker", "the embryo", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Baker, California, USA", "Raja Dhilu", "In the 1979 -- 80 season", "Guy Berryman", "Canadian ice dancers Tessa Virtue and Scott Moir", "c. 497 / 6", "Tim Allen as Luther Krank", "thick skin", "in small packs, and in larger and smaller sizes", "India", "three", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "cricket", "the Major General of the Army", "Marktown", "14,000", "that Iran could be secretly working on a nuclear weapon", "Honduras", "pardoning Richard Nixon", "Ellen DeGeneres", "12 April 1961", "punk rock", "Westfield Old Orchard"], "metric_results": {"EM": 0.328125, "QA-F1": 0.4940361150287621}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.9333333333333333, 0.0, 0.30769230769230765, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.3636363636363636, 0.8, 0.0, 0.4, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.6666666666666666, 0.08, 0.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2387", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-4641", "mrqa_hotpotqa-validation-1675", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-3883", "mrqa_searchqa-validation-843", "mrqa_hotpotqa-validation-427", "mrqa_hotpotqa-validation-3984"], "SR": 0.328125, "CSR": 0.5710227272727273, "EFR": 1.0, "Overall": 0.7855113636363636}, {"timecode": 22, "before_eval_results": {"predictions": ["the mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania)", "Stanford University", "linebacker", "Mongol and Turkic tribes", "1859 and 1865", "Danny Lane", "in the New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16 )", "The Fixx", "Andrew Johnson", "Hellenism", "Mark Jackson", "Manhattan Island", "the Chainsmoker", "annual income of US $11,770", "al - khimar", "week 4", "L.K. Advani", "the President", "Zachary John Quinto", "Tanvi Shah", "the North Shore, at locations in Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "a single particle", "Thomas Jefferson", "the head of Lituya Bay in Alaska", "Manhattan, the Bronx, Queens, Brooklyn, and Staten Island", "Burbank, California", "Grace Zabriskie", "2014", "Yuzuru Hanyu", "Glenn Close", "Kanawha", "flawed democracy", "China", "Scott Belden", "Masha Skorobogatov", "February 27, 2007", "Felony Snicket", "8ft", "Owen Vaccaro", "food", "the lateral side", "Steve Trevor Jr.", "erosion", "90 \u00b0 N 0 \u00b0 W \ufeff / \ufefe 90 \u00b0N - 0 \u00b0 E", "London", "in the bloodstream or surrounding tissue following surgery, disease, or trauma", "2017", "March 1", "1840s", "9.7 m ( 31.82 ft )", "Montgomery", "if he was not named Romeo he would still be handsome and be Juliet's love", "the dark underbelly of the American Dream", "the Queen", "\u00ef\u00bf\u00bd", "the American rock band Pearl Jam", "2005", "Dan Tyminski", "the family", "in the southern port city of Karachi,", "at least nine", "Bashar al-Assad", "the New Revised Standard Version of the Bible", "Biathlon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5813616877761614}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.4210526315789474, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5714285714285715, 0.0, 0.5714285714285715, 0.14814814814814814, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.8, 0.4444444444444445, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8148148148148148, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.5, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2079", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-7486", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-1046", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-3470", "mrqa_triviaqa-validation-4546", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-2101", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1295", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-4138"], "SR": 0.453125, "CSR": 0.5658967391304348, "EFR": 0.9714285714285714, "Overall": 0.7686626552795031}, {"timecode": 23, "before_eval_results": {"predictions": ["internal strife", "a new stage in the architectural history of the regions they subdued", "Fresno", "castles and vineyards", "below 0 \u00b0C (32 \u00b0F)", "Von Miller", "Kansas", "Thaddeus Rowe Luckinbill", "December 25", "2002", "The Mandate of Heaven", "Geoffrey Zakarian", "Christopher Allen Lloyd", "in the central part of each developing bone", "Ali", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash in Hindi, Urdu and Punjabi", "Article 1, Section 2, Clause 3", "the Constitution of India came into effect on 26 January 1950", "Richard Bremmer", "Dick Rutan and Jeana Yeager", "in sequence with each heartbeat", "Ren\u00e9 Descartes", "James P. Flynn", "detritus", "September 27, 2017", "Ireland", "1978", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Tony Orlando and Dawn", "on February 10, 2017", "Alex Skuby", "Jamestown settlement in the Colony of Virginia", "March 2016", "from 1922 to 1991", "Tom Sawyer", "Bacon", "an explosion", "Heather Stebbins", "Redenbacher family", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "`` 0 '' trunk code", "April 1, 2016", "Friedman Billings Ramsey", "New York City", "cutting surfaces", "Massachusetts Compromise", "Justin Timberlake", "Andrew Moray and William Wallace", "Alamodome and city of San Antonio", "female - only species that reproduces by producing an egg through parthenogenesis", "Thomasmary DeCamp", "1871", "eye", "The History Boys", "sturgeon", "White Knights of the Ku Klux Klan", "five books, with two chapters in each book, with a cumulative total of 528 aphoristic sutras, about rules of reason, logic, epistemology and metaphysics", "Mot\u00f6rhead", "Kingman Regional Medical Center,", "Phillip A. Myers. A staff sergeant in the U.S. Air Force,", "Osama bin Laden", "Antarctica", "axon", "mushroom"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6053905122655123}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.28571428571428575, 0.0, 0.2857142857142857, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.2222222222222222, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333333, 0.2857142857142857, 0.18181818181818182, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.16, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1129", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-1207", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-3651", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-505", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-12624"], "SR": 0.484375, "CSR": 0.5625, "EFR": 0.9696969696969697, "Overall": 0.7660984848484849}, {"timecode": 24, "before_eval_results": {"predictions": ["research, exhibitions and other shows", "no damage", "William III of Orange", "1945", "faith alone, whether fiduciary or dogmatic, cannot justify man; justification rather depends only on such faith as is active in charity and good works", "Jim Thorpe", "1996", "the onset and progression of Alzheimer's disease.", "Disco", "Kingdom of Dalmatia", "the Indian School of Business", "New York", "Charles Whitman", "C. H. Greenblatt", "\"The Curious Case of Benjamin Button\" (2010)", "A55", "Corendon Dutch Airlines", "86", "Capella", "sea loch", "Fatih Ozmen", "U.S.", "Pacific Place", "served as the Attorney General of Michigan from 1999 to 2003", "Paradise, Nevada", "Westminster, London", "2016", "Wildhorn", "New York University School of Law", "Crips", "Harper's and Queen", "dementia", "Ferrara", "Guadalcanal Campaign", "Bishop's Stortford", "Starvation Is Motivation", "Barbara Lee Alexander", "Black Friday", "Archbishop of Canterbury", "TD Garden", "James Victor Chesnutt", "the Teutonic Knights", "Australian", "Julie Taymor", "Easy", "World War I", "79 AD", "musical research", "Portland", "Yoruba", "\"Lucky\"", "Charles Otto Puth Jr.", "2007 and 2008", "2001", "1966", "Jason Clarkson", "Ryan Harris", "Medellin", "Joe Jackson", "alternative-energy vehicles parked", "in the first near-total face transplant in the United States,", "genes", "olive", "Stockholm"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6041695283882784}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 0.5, 0.8571428571428571, 1.0, 0.7499999999999999, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.4, 0.4, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2153", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-4105", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-5348", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-979", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-431", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-2659", "mrqa_triviaqa-validation-3361", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1676"], "SR": 0.453125, "CSR": 0.558125, "EFR": 0.9428571428571428, "Overall": 0.7504910714285714}, {"timecode": 25, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-1123", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-4562", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3206", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-3654", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-11385", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11900", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12624", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-1335", "mrqa_searchqa-validation-13486", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14883", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-16181", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3783", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-4857", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8589", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-9756", "mrqa_squad-validation-10045", "mrqa_squad-validation-10069", "mrqa_squad-validation-10074", "mrqa_squad-validation-10086", "mrqa_squad-validation-10216", "mrqa_squad-validation-10228", "mrqa_squad-validation-10254", "mrqa_squad-validation-10310", "mrqa_squad-validation-10324", "mrqa_squad-validation-10338", "mrqa_squad-validation-10353", "mrqa_squad-validation-1036", "mrqa_squad-validation-10378", "mrqa_squad-validation-10477", "mrqa_squad-validation-1090", "mrqa_squad-validation-1320", "mrqa_squad-validation-1450", "mrqa_squad-validation-1603", "mrqa_squad-validation-1636", "mrqa_squad-validation-1672", "mrqa_squad-validation-1694", "mrqa_squad-validation-178", "mrqa_squad-validation-1802", "mrqa_squad-validation-1852", "mrqa_squad-validation-1855", "mrqa_squad-validation-1857", "mrqa_squad-validation-1938", "mrqa_squad-validation-1967", "mrqa_squad-validation-2040", "mrqa_squad-validation-2126", "mrqa_squad-validation-2153", "mrqa_squad-validation-2216", "mrqa_squad-validation-2289", "mrqa_squad-validation-2384", "mrqa_squad-validation-2400", "mrqa_squad-validation-2436", "mrqa_squad-validation-2460", "mrqa_squad-validation-2477", "mrqa_squad-validation-255", "mrqa_squad-validation-2577", "mrqa_squad-validation-2602", "mrqa_squad-validation-2619", "mrqa_squad-validation-268", "mrqa_squad-validation-2693", "mrqa_squad-validation-2773", "mrqa_squad-validation-2782", "mrqa_squad-validation-2798", "mrqa_squad-validation-282", "mrqa_squad-validation-2824", "mrqa_squad-validation-285", "mrqa_squad-validation-2929", "mrqa_squad-validation-3019", "mrqa_squad-validation-3041", "mrqa_squad-validation-3135", "mrqa_squad-validation-3185", "mrqa_squad-validation-320", "mrqa_squad-validation-3337", "mrqa_squad-validation-3476", "mrqa_squad-validation-353", "mrqa_squad-validation-3589", "mrqa_squad-validation-3709", "mrqa_squad-validation-383", "mrqa_squad-validation-3931", "mrqa_squad-validation-3948", "mrqa_squad-validation-3955", "mrqa_squad-validation-397", "mrqa_squad-validation-3993", "mrqa_squad-validation-4005", "mrqa_squad-validation-4079", "mrqa_squad-validation-4140", "mrqa_squad-validation-415", "mrqa_squad-validation-4181", "mrqa_squad-validation-427", "mrqa_squad-validation-4291", "mrqa_squad-validation-4305", "mrqa_squad-validation-4333", "mrqa_squad-validation-4338", "mrqa_squad-validation-4472", "mrqa_squad-validation-462", "mrqa_squad-validation-4686", "mrqa_squad-validation-4704", "mrqa_squad-validation-4835", "mrqa_squad-validation-4856", "mrqa_squad-validation-4870", "mrqa_squad-validation-5054", "mrqa_squad-validation-5088", "mrqa_squad-validation-5096", "mrqa_squad-validation-5154", "mrqa_squad-validation-5176", "mrqa_squad-validation-5238", "mrqa_squad-validation-5302", "mrqa_squad-validation-5326", "mrqa_squad-validation-5376", "mrqa_squad-validation-550", "mrqa_squad-validation-5537", "mrqa_squad-validation-5541", "mrqa_squad-validation-5588", "mrqa_squad-validation-5616", "mrqa_squad-validation-5672", "mrqa_squad-validation-5703", "mrqa_squad-validation-5767", "mrqa_squad-validation-5777", "mrqa_squad-validation-5913", "mrqa_squad-validation-60", "mrqa_squad-validation-60", "mrqa_squad-validation-607", "mrqa_squad-validation-6099", "mrqa_squad-validation-6126", "mrqa_squad-validation-6143", "mrqa_squad-validation-6178", "mrqa_squad-validation-6220", "mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6362", "mrqa_squad-validation-6395", "mrqa_squad-validation-6414", "mrqa_squad-validation-6564", "mrqa_squad-validation-660", "mrqa_squad-validation-6641", "mrqa_squad-validation-6737", "mrqa_squad-validation-6754", "mrqa_squad-validation-6782", "mrqa_squad-validation-68", "mrqa_squad-validation-6817", "mrqa_squad-validation-6915", "mrqa_squad-validation-696", "mrqa_squad-validation-7018", "mrqa_squad-validation-703", "mrqa_squad-validation-7069", "mrqa_squad-validation-707", "mrqa_squad-validation-7150", "mrqa_squad-validation-7161", "mrqa_squad-validation-7180", "mrqa_squad-validation-7198", "mrqa_squad-validation-7260", "mrqa_squad-validation-7399", "mrqa_squad-validation-754", "mrqa_squad-validation-7552", "mrqa_squad-validation-7597", "mrqa_squad-validation-7640", "mrqa_squad-validation-765", "mrqa_squad-validation-7678", "mrqa_squad-validation-7770", "mrqa_squad-validation-7782", "mrqa_squad-validation-7814", "mrqa_squad-validation-7856", "mrqa_squad-validation-7882", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-804", "mrqa_squad-validation-8056", "mrqa_squad-validation-8104", "mrqa_squad-validation-8115", "mrqa_squad-validation-8189", "mrqa_squad-validation-8226", "mrqa_squad-validation-8226", "mrqa_squad-validation-8285", "mrqa_squad-validation-8406", "mrqa_squad-validation-8480", "mrqa_squad-validation-8527", "mrqa_squad-validation-8629", "mrqa_squad-validation-8735", "mrqa_squad-validation-8760", "mrqa_squad-validation-8765", "mrqa_squad-validation-8832", "mrqa_squad-validation-884", "mrqa_squad-validation-8867", "mrqa_squad-validation-890", "mrqa_squad-validation-8957", "mrqa_squad-validation-898", "mrqa_squad-validation-9031", "mrqa_squad-validation-9066", "mrqa_squad-validation-9135", "mrqa_squad-validation-9186", "mrqa_squad-validation-9227", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9391", "mrqa_squad-validation-9392", "mrqa_squad-validation-9465", "mrqa_squad-validation-9504", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9652", "mrqa_squad-validation-9658", "mrqa_squad-validation-9771", "mrqa_squad-validation-979", "mrqa_squad-validation-9818", "mrqa_squad-validation-987", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-3051", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-469", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5568", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-6909", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-776"], "OKR": 0.845703125, "KG": 0.41875, "before_eval_results": {"predictions": ["progressive tax", "Jacksonville", "monophyletic", "forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other", "Fox Network", "the Anhaltisches Theater", "Anna Clyne", "Martin Scorsese and written by Terence Winter, based on the memoir of the same name by Jordan Belfort", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Formula E", "Eastern College Athletic Conference", "Kim Jong-hyun", "Peter Chelsom,", "The Ninth Gate", "heavy metal", "Cinderella", "Los Angeles", "American", "Acid house", "at the end of the 18th century", "The Five Boroughs", "Miranda Lambert", "Shenandoah National Park", "BBC Formula One coverage on TV, radio and online", "10 Years", "Haleiwa Ali'i Beach Park", "Armin Meiwes", "1886", "Rockhill Furnace, Pennsylvania", "northeastern", "coca wine", "Entrepreneur", "the lead roles", "PBS stations nationwide,", "second largest", "acid", "in 1911", "Lola Dee", "The Five", "Walt Disney Feature Animation", "The 2017\u201318 Premier League", "torpedoes", "1972", "Geographical Indication tag", "Ringo Starr", "Celtics", "World Championship Wrestling", "The 1994 United States Senate election in New York was held on November 8, 1994", "TD Garden", "the Chechen Republic", "Chrysler", "The Institute for Advanced Study", "2005", "Tenochtitlan", "The Tax Reform Act of 1986", "Lou Gehrig", "Mexico", "Thundercats", "he fears a desperate country with a potential power vacuum that could lash out.", "the Catholic League", "Krishna Rajaram,", "michael", "\"Death, be not\" this \"though some have called thee mighty and dreadful, for thou art not so\"", "green"], "metric_results": {"EM": 0.625, "QA-F1": 0.6898313492063493}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10397", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5167", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9487", "mrqa_newsqa-validation-2772", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4240"], "SR": 0.625, "CSR": 0.5606971153846154, "EFR": 0.9583333333333334, "Overall": 0.7000560897435898}, {"timecode": 26, "before_eval_results": {"predictions": ["bacteriophage T4", "1698", "Xingu", "The Ruhr", "Dar es Salaam", "Heinkel Flugzeugwerke", "Jesus", "Pope John X", "Harold Holt", "aged between 11 or 13 and 18", "\"Histoires ou contes du temps pass\u00e9\"", "Orchard Central", "Tom Hiddleston", "late eighteenth century", "The Snowman", "1979", "Premier League club", "port city of Aden", "British", "Prince Louis of Battenberg", "1985", "Archie Andrews", "2 May 2015", "17 December 177026 March 1827", "Crystal Dynamics", "Cleveland Cavaliers", "goalkeeper", "Debbie Harry", "\"media for the 65.8 million,\"", "John Joseph Travolta", "Hall & Oates", "the port of Mazatl\u00e1n", "successful racehorse breeder and owner", "unincorporated Clark County", "1919", "Jon Bernthal", "Love Streams", "Michael Edwards", "The Rite of Spring", "Lake Wallace", "England", "1993", "Boston Celtics", "Eisenhower Executive Office Building", "6,396", "Australian coast", "Frankie Bridge, Una Healy, Rochelle Humes, Mollie King and Vanessa White", "Attack the Block", "Leonarda Cianciulli", "Morse Field", "Tudor music and English folk-song", "CMYKOG", "1600 BC", "Anthony Daniels", "1963", "a peplos", "Car ferry", "Nutshell", "15,000", "10 to 15 percent", "\"It has never been the policy of this president or this administration to torture.\"", "Tarzan", "held", "postcards"], "metric_results": {"EM": 0.5, "QA-F1": 0.6051720848595848}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.923076923076923, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.28571428571428575, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-1352", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5102", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-1677", "mrqa_newsqa-validation-4143", "mrqa_searchqa-validation-3515"], "SR": 0.5, "CSR": 0.5584490740740741, "EFR": 1.0, "Overall": 0.7079398148148148}, {"timecode": 27, "before_eval_results": {"predictions": ["immediately north of Canaveral at Merritt Island", "pedagogic diversity", "Christian", "Extension", "Cinderella", "Dan Tyminski", "Guthred", "October 17, 2017", "Indian state", "Dumb and Dumber", "Boeing EA-18G Growler", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Paper", "Sir Matthew Alistair Grant", "Whitney Houston", "pneumatic tyres", "Bonkyll Castle", "Cheick Isma\u00ebl Tiot\u00e9", "Algernod Lanier Washington", "erotic romantic comedy", "leg injury", "Antonio Salieri", "American", "Europe", "cesario", "Brooklyn, New York", "Thriller", "Jesper Myrfors", "The Supremes", "Cersei Westerister", "Kalokuokamaile", "Brigadier General Raden Panji Nugroho Notosusanto", "Don Bluth", "2009", "Chief of the Operations Staff of the Armed Forces High Command", "Hong Kong Disneyland", "London", "Jim Diamond", "September 8, 2017", "FBI", "Christine MacIntyre", "1911", "Wildhorn, Bricusse and Cuden", "M1914 machine gun", "James Brolin", "Germany's position in a Europe", "January 2004", "co-founder and lead guitarist", "ten", "seven", "October 25, 1881", "J. Robert Oppenheimer", "Pradyumna", "Mark Jackson", "Road / Track", "The Colossus of Rhodes", "Equatorial Guinea", "c3H8O3", "identity documents", "Chris Robinson and girlfriend Allison Bridges", "off east  Africa", "bromide", "nasal septum", "Warp Drive"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5998329156223893}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3840", "mrqa_squad-validation-1916", "mrqa_squad-validation-1075", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3916", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3346", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3400", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-2957", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-6398"], "SR": 0.515625, "CSR": 0.5569196428571428, "EFR": 1.0, "Overall": 0.7076339285714286}, {"timecode": 28, "before_eval_results": {"predictions": ["public (government) funding", "boarding schools and day schools", "a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF)", "London", "Henry Mancini", "npl -nese", "Gordon Ramsay", "Gorbachev", "\"Stripes.\"", "a rose", "Charlotteton Heston", "Anna (Julia Roberts)", "scythe", "herbaceous", "Paddy Doherty", "smallpox major", "the Hanging Gardens of Babylon", "the Central African franc", "Chubby Checker", "F\u00fcr Elise", "Iran", "David Copperfield", "William Hickey", "Milady de Winter", "April", "Eric Morley", "ADHD and hypertension", "the Garrick Club", "short story/novella, picture book, film, and play format", "David Beckham", "New York City", "Tom Stoppard", "The Greatest", "a singer and performer", "in England", "a Scotsman\u2019s bonnet", "jazz saxophonist", "Seattle", "the Union Inn", "Cardiff", "Baton Rouge", "a borsets", "Tahrir Square", "Romanian", "bathtub curve", "Michael Caine", "Lord Snooty", "Borodin", "Jesse James", "Meerkat", "Greek", "passion fruit", "Thomas Lennon", "Haikou on the Hainan Island", "the British associationists", "Denmark and Norway", "1966", "North America", "Florida's Everglades", "Garth Brooks", "glamorous, sexy and international", "drive", "Glengarry Glen Ross", "Sharks"], "metric_results": {"EM": 0.46875, "QA-F1": 0.537624007936508}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.28571428571428575, 0.39999999999999997, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6891", "mrqa_squad-validation-6918", "mrqa_squad-validation-4846", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-6165", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-4619", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-2313", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-7182", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-9024", "mrqa_hotpotqa-validation-2910", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1004", "mrqa_searchqa-validation-15919"], "SR": 0.46875, "CSR": 0.5538793103448276, "EFR": 1.0, "Overall": 0.7070258620689656}, {"timecode": 29, "before_eval_results": {"predictions": ["Chicago Theological Seminary", "CBS and NBC", "$100,000", "Super Bowl LII", "starch", "Taylor Michel Momsen", "Kennedy Space Center ( KSC ) in Florida", "Bob Pettit", "James W. Marshall", "Blue laws", "Randy VanWarmer", "Both the Secretary of State and ambassadors are appointed by the President", "Emma Watson and Dan Stevens", "between 8.7 % and 9.1", "2018", "if the occurrence of one does not affect the probability of occurrence of the other", "Jason Flemyng", "Chesapeake Bay, south of Annapolis in Maryland", "northern China", "T.J. Miller", "in Pyeongchang County, Gangwon Province, South Korea", "status line", "in the eye", "jimmy heston", "Triple Alliance of Germany", "Andrew Lloyd Webber", "1955", "Charles Frederickson", "Buffalo Lookout", "Humpty Dumpty and Kitty Softpaws", "Charlene Holt", "close to 5,770 guaranies", "Leonard Nimoy", "1936", "Optimus", "10.5 %", "beneath the liver", "Andy Serkis", "West Norse sailors", "Kristy Swanson", "Anna Faris", "early to mid-2000s", "Fleetwood Mac", "improved economics of distribution", "Cairo, Illinois", "in the 1970s and'80s", "in the books of Exodus and Deuteronomy", "Wyatt `` Dusty '' Chandler ( George Strait )", "an epic poem written in the fifth century", "January 2, 1971", "The Miracles", "taxonomy", "forearm", "jimmy heston", "Charlie heston", "\"Twice in a Lifetime\"", "George Orwell", "Bardot", "teenager", "Long Island convenience store", "Mitt Romney", "rock", "high fever", "dancing with the stars"], "metric_results": {"EM": 0.5, "QA-F1": 0.5972397941970311}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.21052631578947367, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.5714285714285715, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5764", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-6865", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-3968", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-1958", "mrqa_newsqa-validation-1979", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7426"], "SR": 0.5, "CSR": 0.5520833333333333, "EFR": 1.0, "Overall": 0.7066666666666667}, {"timecode": 30, "before_eval_results": {"predictions": ["San Jose", "quality rental units", "cultural tourism and sports tourism", "a circle", "coast redwood", "corey", "Spanish Republic", "taximeter", "coyote", "The Sun", "Harry Reid", "Ray", "axis", "forge", "Kinetoscope", "No Country for Old Men", "flowers", "Blackbird", "Footprints", "caliban", "LA Kings", "U.S. Census Bureau", "Tommy Lee Jones", "Zacchaeus", "The Memory Keeper's daughter", "charlie", "hubris", "Yahtzee", "Tony Danza", "XML", "hives", "74.3", "William S. Hart", "Cain", "The First Impressions", "corey", "Kosher Wines", "Munich", "Michael Jordan", "February 2nd", "corey", "Hikaru Sulu", "tropical rainforests", "corey", "kysh", "honey", "Boston", "corey", "Arctic Ocean", "pizze Napoletane", "butternut squash", "Spain", "Thomas Chisholm", "May 2002", "1936", "Newfoundland and Labrador", "The Fortune cookie", "Monty Python's Spamalot", "1911", "Harlow Cuadra and Joseph Kerekes", "her translation of and commentary on Isaac Newton's book \"Principia\" containing basic laws of physics", "Israel", "corey Herbert Hainer", "anti-trust laws."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5453869047619047}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-315", "mrqa_squad-validation-2965", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-2374", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-11167", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-12408", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-15027", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-5162", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-1922", "mrqa_searchqa-validation-12415", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-13549", "mrqa_searchqa-validation-9773", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-9379", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-6412", "mrqa_naturalquestions-validation-10656", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3030", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-3054"], "SR": 0.46875, "CSR": 0.5493951612903225, "EFR": 1.0, "Overall": 0.7061290322580646}, {"timecode": 31, "before_eval_results": {"predictions": ["Rev. Paul T. Stallsworth", "blue", "Bill Cosby", "satirical erotic romantic comedy", "Ferengi", "Chancellor of Austria", "June 26, 1970", "Bloomingdale Firehouse", "Elena Marie Stefanik", "Fleetwood Mac", "Odense Boldklub", "the Supreme Court", "Bangkok", "Oklahoma Sooners", "Merrimack", "Charlie Wilson", "The Late Late Show", "Mark Anthony \"Baz\" Luhrmann", "two", "Indianapolis Motor Speedway", "Ravenna", "Anita Dobson", "a family member", "October 4, 1970", "The Worm", "Eliot Cutler", "Blackheart Records", "1970s and 1980s", "C. J. Cherryh", "Pablo Escobar", "Asbury Park, New Jersey", "Rockland", "Slaughterhouse-Five", "Adventures of Huckleberry Finn", "wine and cellar door", "Frank Sinatra", "Robert L. Stone", "goalkeeper", "Philadelphia", "New York", "Massapequa", "Sinngedichte", "The Highwaymen", "Fuenlabrada", "Kevin Spacey", "Arizona State University.", "Blue Grass Airport", "Kenneth Hood", "1952", "the Nebula Award, the Philip K. Dick Award, and the Hugo Award", "I'm Shipping Up to Boston", "the Royal Navy", "Isabella Palmieri", "Hathi Jr.", "1935", "a lion", "slow", "The Miracles", "Union Station in Denver,", "the European Commission", "Friday,", "the High Plains", "Franklin D. Roosevelt", "Ukraine"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6289663461538462}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.4, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-2607", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-5034", "mrqa_triviaqa-validation-6414", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-6055"], "SR": 0.5625, "CSR": 0.5498046875, "EFR": 1.0, "Overall": 0.7062109375000001}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh L. Dryden", "2004", "Kenya", "The Rocky Horror Picture Show", "Trainspotting", "Argentina", "Apollo 11 Lunar Module (LM) \"Eagle\"", "jellyfish", "March", "a shoe", "fauntleroy", "the World Health Organization", "Eat porridge", "Kofi Annan", "oxygen", "the right to print was strictly controlled in England", "Taggart", "i second that emotion", "the Gulf of Mexico", "Ladee-Lo", "Sepp Blatter", "the i second that emotion", "Jackson Street", "Brussels", "Flora MacDonald", "John Poulson", "Charles de Gaulle Airport", "the euro", "Jack \"Jack\" Frost", "Saskatchewan (province)", "Laurent Planchon", "the Solent", "vomiting", "i second that emotion", "Bristol Aeroplane Company", "Spinach", "Tony Meo", "i second that emotion", "Gemini", "Surrey", "1969", "the Fosse Way", "Budapest", "the Aconcagua Valley", "William Shakespeare", "borax", "a \"third track\"", "Jamaica", "Bri, a school teacher and his wife Sheila", "Diana Dors", "Kent", "Vickers-Armstrong's", "Ray Charles", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "USCS or USC ) developed from English units which were in use in the British Empire before the U.S. became an independent country", "Miller Brewing", "northwestern Italian coast", "Melbourne", "the Champs de Mars plaza", "her decades-long portrayal of Alice Horton", "it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "Peter Bogdanovich", "the collared dove", "the Federal Republic of Germany"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5039708253702819}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.08695652173913043, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.08695652173913043, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-2998", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-5360", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5129", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-5817", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-3368", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-5611"], "SR": 0.4375, "CSR": 0.5464015151515151, "EFR": 1.0, "Overall": 0.7055303030303031}, {"timecode": 33, "before_eval_results": {"predictions": ["New York and Virginia", "1887", "Lana Del Rey", "1,228 km / h ( 763 mph )", "New England Patriots", "Doc '' Brown", "Antarctica", "Mitch Murray, who offered it to Adam Faith and Brian Poole but was turned down", "blue", "`` Ultra Hand ''", "John Bull", "775 rooms", "eusebeia", "waiting tables at the Moondance Diner", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "a long line", "Jesus'birth", "a habitat", "Kirsten Simone Vangsness", "Central Germany", "Andrew Johnson", "Etienne de Mestre", "Aegisthus", "electors", "Julia Ormond", "Sauron", "1961", "ste\u026and", "2013", "`` leaper ''", "novelization", "a usually red oxide formed by the redox reaction", "Spain", "peter tylo", "Paul Lynde", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "`` Jocelyn Flores", "abdicated in November 1918", "paid monument", "erosion", "March 2, 2016", "stuffing", "1996", "Ray Charles", "16", "Ramones", "1800", "Norman French waleis", "Frank Theodore `` Ted '' Levine", "New Jersey", "May 2010", "France", "Heath Ledger", "Wilson Pickett", "centaur", "DJ", "cricket fighting", "Luis Edgardo Resto", "drama that pulls in the crowds", "a Ukrainian -- is accused of involvement during World War II in killings at a Nazi German death camp in Poland.", "Islamabad", "Tunisia", "RAND", "alberta"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5202720269254087}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false], "QA-F1": [0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.06451612903225806, 0.23529411764705882, 1.0, 0.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.7368421052631579, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8205128205128205, 0.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 0.09999999999999999, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3127", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-7802", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4561", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-1997", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2118", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-9333"], "SR": 0.453125, "CSR": 0.5436580882352942, "EFR": 1.0, "Overall": 0.7049816176470589}, {"timecode": 34, "before_eval_results": {"predictions": ["Venus", "Beyonc\u00e9 and Bruno Mars", "Zeebo", "7th century", "2018", "her abusive husband", "September 29, 2017", "interstellar medium", "transmission and final drive", "Universal Pictures", "Sukhvinder Singh", "March 14, 1942", "Nick Sager", "local authorities", "a biblical title of respect applied to prophets and beloved religious leaders", "state legislators of Assam", "digestive enzymes break down the long chains of amino acids", "Renishaw Hall, Derbyshire, England", "accomplish the objectives of the organization", "sport utility vehicles", "Isabella Palmieri", "101.325 kPa", "`` Mind your Ps and Qs", "Germany's failure to destroy Britain's air defences to force an armistice ( or even outright surrender )", "20 November 1989", "Tom\u00e1s de Torquemada", "Bob Gaudio", "1975", "Irene Bedard and Mel Gibson", "Procol Harum", "Erica Rivera", "lead - acid or 1.5 volts for zinc - carbon cells", "a four - page pamphlet", "2003", "Sebastian Lund", "Wednesday, 5 September 1666", "California State Route 1", "The management team", "a diffuse system of small concentrations of lymphoid tissue", "introduced the ActionScript 3.0 programming language, which supported modern programming practices and enabled business applications to be developed with Flash", "at Steveston Outdoor pool in Richmond, BC", "Robin Cousins", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Ukraine", "Lula", "introduced their working telegraph in 1839", "when the car comes to a halt", "early Christians of Mesopotamia", "Derrick Henry", "\u2212 93.2 \u00b0 C ( \u2212 135.8 \u00b0 F )", "a cliffhanger showing the first few moments of Sam's next leap", "on a bronze plaque and mounted inside the pedestal's lower level", "Cheerios", "kunigunde Mackamotski", "Brian Close", "a hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "Galleria Vittorio Emanuele II", "every aspect of public and private life", "Zelaya and Roberto Micheletti, the politician who was appointed president hours after Zelaya's June 28 removal, reached an agreement late Thursday to form a government of national reconciliation.", "Olympic medal", "Henry Ford", "Toyota", "Abraham Lincoln", "a mass of cells that grows slowly in"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5824905793970814}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.42857142857142855, 0.3333333333333333, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.06451612903225806, 0.8125000000000001, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.20000000000000004, 1.0, 0.0, 0.75, 0.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-10576", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-8873", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-1562", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-15641"], "SR": 0.4375, "CSR": 0.540625, "EFR": 0.9444444444444444, "Overall": 0.6932638888888889}, {"timecode": 35, "before_eval_results": {"predictions": ["its loss of prestige among these groups", "euphoric", "the Colossus of Rhodes", "Venezuela", "Mexico", "Ring Magazine", "Peter Pan", "sarto", "the Arctic Ocean", "the egg in front of the opening, the flame going out causes a vacuum effect, so the egg is sucked in by the differential in this between the inside & outside of the bottle", "dams", "Lafayette", "Elijah Muhammad", "the equatorial Pacific", "the Village People", "Alexander Pushkin", "Australia", "Munich", "Mexico", "a night shift", "the papacy", "Arkansas", "Night of the Living Dead", "Pierre-August Renoir", "mister", "Les Huguenots", "Innsbruck", "Lance Ito", "Microsoft", "a fern", "Sony", "the Roskilde 6", "Atlantic City's First Boardwalk", "Blackwater", "elephants", "American Airlines", "alberta", "Odysseus", "Geronimo", "Kensington Palace", "butch Cassidy", "Netherlands", "Pocahontas", "the Lion, the Witch and the Wardrobe", "Dagny Taggart", "the amygdala, the part of the brain", "Chicago Mercantile Exchange", "Las Vegas", "danskins are not just for dancing", "to the stars through difficulties", "Pablo Casals", "an ostrich or common ostrich", "1943", "Payaya Indians", "beneath the liver", "James I", "penrhyn", "psychological horror", "John Morgan", "Hungarian Rhapsody No. 2", "Henry II", "Senate Democrats", "63", "we are resetting,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5916666666666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9699", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-11675", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-5646", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-15775", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-5219", "mrqa_searchqa-validation-5473", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-3348", "mrqa_hotpotqa-validation-3745", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352"], "SR": 0.5, "CSR": 0.5394965277777778, "EFR": 1.0, "Overall": 0.7041493055555555}, {"timecode": 36, "before_eval_results": {"predictions": ["electric lighting", "James W. Marshall", "Terrell Suggs", "the Earth's axial tilt, which fluctuates within a margin of 2 \u00b0 over a 40,000 - year period, due to tidal forces resulting from the orbit of the Moon", "Lucknow", "2013 -- 14 television season", "National Industrial Recovery Act ( NIRA ), 1933", "The User State Migration Tool ( USMT )", "the Battle of Antietam", "William DeVaughn", "the National September 11 Memorial plaza", "Southend Pier", "Santa Monica", "sovereign states", "Raza Jaffrey", "31 January 1934", "Filipino", "1773", "modern random - access memory ( RAM )", "May 31, 2012", "April 1917", "Bart Cummings", "October 27, 1904", "Raghu", "Olivia Olson", "1990", "Billy Gibbons", "Bill Pullman", "BC Jean", "IMAX 3D", "Frankie Muniz", "stratum lucidum", "60", "Hasmukh Adhia", "four", "retinal ganglion cell axons and glial cells", "the 1980s", "in soils", "card verification data ( CSC ; also called card verification number, card verification value ( CVV )", "oversee the local church", "bohrium", "Britain", "Escherichia coli", "Archduke Franz Ferdinand of Austria", "June 1991", "2010", "he lost the support of the army, abdicated in November 1918, and fled to exile in the Netherlands", "in the basic curriculum -- the enkuklios paideia or `` education in a circle ''", "Mike Czerwien", "103", "Vienna", "Bajan", "Mexico", "the French Foreign Minister Pierre Laval", "$10.5 million", "Al Horford", "Andrew Johnson", "$30.6 million", "leftist Workers' Party", "his mother, Katherine Jackson, his three children and undisclosed charities.", "cotton", "Dennis Haysbert", "Quinn", "The Weatherbys Novices' Hurdle Race"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6145307845156636}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.12903225806451613, 1.0, 0.18181818181818182, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.08333333333333334, 0.0, 0.5, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.20000000000000004, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.5, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-4195", "mrqa_hotpotqa-validation-4351", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1953", "mrqa_searchqa-validation-13161", "mrqa_triviaqa-validation-5460"], "SR": 0.515625, "CSR": 0.5388513513513513, "EFR": 0.9032258064516129, "Overall": 0.6846654315605929}, {"timecode": 37, "before_eval_results": {"predictions": ["Joseph Swan", "the United States", "South Africa", "first among equals", "Scott Hicks", "a cappella", "albinism", "Henry Hunt", "aglet", "Saturday Night Live", "FC Bayern M\u00fcnchen", "winter", "Bonnie and Clyde", "english", "copper", "Dawn French", "David Bowie's", "b Benedict", "Doris Lessing", "Scooby-Doo", "Swaziland", "brazilia", "Kent", "the Humber", "a points based scoring system", "automobile", "Kent", "Richard Rodgers and Oscar Hammerstein", "Boy George", "Galileo Galilei", "Zelle", "Lee Ingleby", "Marilyn Manson", "brazil", "William Shakespeare's", "a carburetor", "brazilia", "Boulder Dam", "painkillers", "Israel", "Belle de Jour", "bognor Regis", "abba", "rain", "blue", "staph Hall", "France", "geena Davis", "kunsky", "death and dying", "Rosamund Pike", "the forces of Andrew Moray and William Wallace", "142,907", "mid November", "YouTube", "Theo James Walcott", "Ben Ainslie", "at least three bodies were trapped in a \"very compressed area.\"", "thunderstorms", "different women coping with breast cancer in five vignettes.", "&quot", "a sunflower", "Guy Ritchie", "March 24,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5673363095238095}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7074", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-541", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-6503", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-2982", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-8884", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-6291", "mrqa_searchqa-validation-7454"], "SR": 0.53125, "CSR": 0.5386513157894737, "EFR": 1.0, "Overall": 0.7039802631578947}, {"timecode": 38, "before_eval_results": {"predictions": ["the Rip", "tyne", "liver", "40", "sodium bicarbonate", "cuba", "cuba", "Bleak House", "Phil Redmond", "Stevie Wonder", "a head", "dogs", "hanover", "a moon", "king Charles I", "worked", "scales", "Dirty Dancing", "goddess of Revenge", "Diana Ross", "emperor", "a 1934 Austin seven box saloon", "Richie Unterberger", "cuba", "cuba", "cuba", "cuba sheep", "Jay-Z", "ligers", "cymbals", "air Bud", "cuba", "cuba", "Ticket Sarasota", "South Africa", "Christian Dior", "scrobbesbyrig", "Killer whales", "cuba", "France", "raspberries", "charity", "Cyprus", "speed camera", "Earl", "lizard", "bridge", "frauds", "a sea monster", "even numbers", "Tony Blair", "quartz or feldspar", "54 Mbit / s", "Manley", "Stacey Kent", "Eyes Wide Shut", "Anthony Ray Lynn", "piano", "loved tribute to pop legend Michael Jackson,", "cuba lewis", "French Guiana", "cablevision", "a bagwyn", "Tiger Woods"], "metric_results": {"EM": 0.5, "QA-F1": 0.5292410714285715}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2224", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-6161", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-213", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3351", "mrqa_naturalquestions-validation-655", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-5730", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-2594", "mrqa_searchqa-validation-9940", "mrqa_searchqa-validation-4817", "mrqa_newsqa-validation-3899"], "SR": 0.5, "CSR": 0.5376602564102564, "EFR": 1.0, "Overall": 0.7037820512820513}, {"timecode": 39, "before_eval_results": {"predictions": ["\"No, that's no good\"", "aldi", "Midnight Cowboy", "norma", "seborrheic dermatitis", "norman bristol", "a heavy barge", "chine-Niger", "central Stockholm", "Tangled", "dog", "norman Douglas", "Bulls Eye", "bar\u00e8re de Vieuzac", "bach", "Timothy Carroll", "Charles Darwin", "pembrokeshire Coast National Park", "Kevin macdonald", "peppers", "pangaea", "jimmy Boyd", "isambard Kingdom Brunel", "georgia", "1957", "norman", "roun", "salt", "normin micelles", "Ralph Vaughan Williams", "musical scale", "cats", "flannel", "e. T. A. Hoffmann", "Shanghai", "rome", "grows", "Tuesday", "Guru Nanak", "norman street", "inigo Montoya", "norman", "Little Jack Horner", "Indianapolis", "Dolores Haze", "cuckoo", "Miss Marple", "Dodge Ram", "Alice Cooper", "minorca", "transfusions", "Royal Bengal Tiger", "spherical boundary of zero thickness", "Max", "Chicago Tribune New York News Syndicate", "1999", "Sela Ann Ward", "Body Works", "forgery and flying without a valid license,", "183", "a cabin", "St. Patrick's Day", "linebacker", "Angel Recording Studios"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4601934523809524}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-10", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-7408", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5688", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-1116", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-2711", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-777", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-807", "mrqa_naturalquestions-validation-9755"], "SR": 0.390625, "CSR": 0.533984375, "EFR": 0.9743589743589743, "Overall": 0.6979186698717949}, {"timecode": 40, "before_eval_results": {"predictions": ["19th Century", "Famous Players", "Pacific Northwest", "Thomas de Quincey", "chile pestis", "horse", "buffalo", "smith", "a raven", "Sarajevo", "the Bill of Rights", "Fine", "Neighbours", "smith", "trumpet", "Westminster Abbey", "origami", "resistance of an unknown resistor", "scuba", "smith", "smith", "matricide", "smith", "\u201cTonight Is Another Day\u201d or \u201cTote the towy Load,\u201d", "smith", "Tomorrow Never Dies", "nubia", "Dane", "Washington", "niger", "devon", "James I", "smith", "kaulita", "purple rain", "Everyhit", "warblers", "Hell Upside Down", "spain", "10", "Southwest Airlines", "phone", "deaver", "Comedy of Errors", "smith", "Alex Turner", "smith", "dolls", "a shelf", "radicalization", "smith", "Humpty Dumpty", "1998", "Tanvi Shah", "the ENnie web site that has hosted the awards since their inception in 2001", "the 100th anniversary of the first \"Tour de France\"", "Mach number (M or Ma)", "Janet and La Toya", "2.5 million", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter", "smith", "devon humbert", "nibelung", "Inequality of opportunity"], "metric_results": {"EM": 0.40625, "QA-F1": 0.45416666666666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.25, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-2275", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-4716", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4538", "mrqa_triviaqa-validation-3844", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-863", "mrqa_triviaqa-validation-4337", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-7516", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-2372", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-15441", "mrqa_searchqa-validation-11519"], "SR": 0.40625, "CSR": 0.5308689024390244, "EFR": 1.0, "Overall": 0.7024237804878048}, {"timecode": 41, "before_eval_results": {"predictions": ["1220", "philippine", "hula hoops", "n Nissan", "henry fleece", "roddy dixon", "abacus", "Robin Hood", "saisyphus", "Velazquez", "South Africa", "caracas", "henie", "tchaikovsky", "oliver Twist", "angus", "usborne", "David Bowie", "Buzz Aldrin", "henry v. 2", "joseph", "dennis turpin", "rust", "james aniston", "pembroke County, Wales", "tbilisi", "henry vii", "othello", "sewing up of a small hole or tear in a piece of material", "glenn close", "cholderton estate", "us", "domestic cat", "Anita Brookner", "joseph vii", "golda meir", "Black Sea", "black jail", "Miss Dent", "a power outage", "Vienna", "The Archers", "Launcelot", "henry ochs", "henry vii", "james b Boyd", "shakespears", "The Four Marx Brothers", "tyne", "philippine", "Dry Ice", "Pat McCormick", "19 June 2018", "18 - season", "from 1993 to 1996", "james Gandolfini", "March 23, 2017", "he and the other attackers were from Pakistan", "June 6, 1944", "sniff out cell phones.", "bassoon", "the o.K. Corral", "butternut apple pie", "phoenicia"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5339364035087719}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5582", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-5080", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-3306", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-5773", "mrqa_triviaqa-validation-6097", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-2641", "mrqa_triviaqa-validation-7225", "mrqa_naturalquestions-validation-824", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-9161", "mrqa_searchqa-validation-233"], "SR": 0.453125, "CSR": 0.5290178571428572, "EFR": 1.0, "Overall": 0.7020535714285715}, {"timecode": 42, "before_eval_results": {"predictions": ["lack of reliable statistics", "a shortfall in their pension fund and disagreements on some work rule issues.", "eintracht Frankfurt", "Comoros Islands", "revolution of values", "Jeddah, Saudi Arabia", "40", "chest", "\"Big change is hard,\"", "Manny Pacquiao", "$250,000", "27,", "British Prime Minister Gordon Brown", "lima's ex-president", "Salt Lake City, Utah", "Marty Abrams", "Michoacan Family", "64", "new Delhi, India", "fastest", "Department of Homeland Security Secretary Janet Napolitano", "Iran's parliament speaker", "ended his playing career", "\"E! News\"", "South Florida", "Madeleine K. Albright", "ice jam", "toxic smoke", "Benazir Bhutto", "1971", "U.S. senators", "scotland", "Larry Ellison", "manhattan farmer Alan Graham", "kandi Burruss,", "cal Ripken", "Johannesburg", "cancer", "acid attack", "Vernon Forrest", "calling on NATO to do more to stop the Afghan opium trade", "one", "comfort those in mourning,", "Hyundai says it recycles 100% of its byproducts which supplies 80% of the operation energy at the plant.", "about 5:20 p.m.", "former Mobile County Circuit Judge Herman Thomas", "\"A salute to the martyrs of the massacre, and our condolences to their families.\"", "a man's lifeless, naked body", "\"release\" civilians,", "johnson fayed", "the shipping industry", "if a population temporarily exceeds the long term carrying capacity of its environment", "Barcelona", "emperor Cuauhtemoc", "scotland", "Misery", "jennifer purdy", "Antonio Lippi", "Thorgan", "River Clyde", "spain", "japan", "Cy Young", "Reese Witherspoon"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5158282689532689}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8000000000000002, 0.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2727272727272727, 1.0, 1.0, 0.4, 0.0, 0.6, 0.05714285714285715, 1.0, 1.0, 0.1818181818181818, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3979", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-5597", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-4313", "mrqa_hotpotqa-validation-727", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-5649"], "SR": 0.421875, "CSR": 0.5265261627906976, "EFR": 1.0, "Overall": 0.7015552325581396}, {"timecode": 43, "before_eval_results": {"predictions": ["largest Filipino American community, with 25,033 in the metropolitan area as of the 2010 Census", "House of Borromeo", "Washington, D.C.,", "1943", "Volvo 850", "The Mountain West Conference", "the Atlanta Hawks", "Western Europe", "political thriller", "Continental AG", "English football", "1989 until 1994", "the Distinguished Service Cross", "\"50 best cities to live in.\"", "Saint Michael, Barbados", "Pakistani cinema", "Emmanuel Ofosu Yeboah", "John Boyega, Nick Frost, Jodie Whittaker and Luke Treadaway", "Bhushan Patel", "1986", "1916", "Reginald Engelbach", "Vince Staples", "Archbishop of Canterbury", "Galway", "Lynyrd Skynyrd", "1988", "coaxial", "Northern Lights", "three different covers", "Malayalam fantasy comedy", "held in Kingdom of Dalmatia", "August 11, 1946", "Vincent Landay", "September 6, 1967", "Estadio de L\u00f3pez Cort\u00e1zar", "Nickelodeon Animation Studio", "Nicolas Vanier", "1985", "Gal Gadot", "Meghan Markle", "Texas Raiders", "Erika Girardi", "Joe Scarborough", "English", "76,416", "Bonkyll Castle", "second cousin once removed", "2012 Summer Olympics", "Studio 33 (PS) and Sony Studio Liverpool (PS2)", "Brig Gen Augustine Warner Robins", "United Nations", "Lewis Carroll", "two occasions", "under the UK\u2019s Trade Mark Registration Act 1875,", "blue", "elbow", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "the Employee Free Choice act in Lafayette Square in Washington", "the release of the four men", "a rake", "Jack the Ripper", "a carriage", "teak"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6934833466003543}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true], "QA-F1": [0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.08695652173913042, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.29629629629629634, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7278", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-1776", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7240", "mrqa_triviaqa-validation-2264", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2345"], "SR": 0.59375, "CSR": 0.5280539772727273, "EFR": 1.0, "Overall": 0.7018607954545455}, {"timecode": 44, "before_eval_results": {"predictions": ["British", "Sean Yseult", "Washington, D.C.", "over 12 million", "Lucha de Apuesta", "Conservatorio Verdi", "George Herbert Walker", "the backside", "Angelo Bruno", "The Future", "the Knight Company", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven and Andy Cohen", "Danish national men's ice hockey team", "December 31, 2015", "Margarine Unie", "death", "Fort Valley, Georgia", "Tom Hanks", "Vladimir Menshov", "Kramer", "the Dominican Republic", "Humberside Airport", "June 12, 2017", "Douglas Jackson", "wooden roller", "Blackpool Football Club", "William Lyon Mackenzie King", "Ted", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Chrysler", "Bruce Grobbelaar", "Honda Ballade", "Ascona", "Boston Celtics", "Austrian", "Australian Electoral Division", "Socrates", "American singer Toni Braxton", "Hindi", "Richard Masur", "Irish Chekhov", "311", "Dr. Gr\u00e4sler, Badearzt", "Alexandre Dimitri Song Billong", "Arizona Health Care Cost Containment System", "Mineola", "Gian Carlo Menotti", "bobsledder", "Mazda", "102,984", "Roscoe Lee Browne", "Super Bowl XVII", "John Goodman", "216", "The Spectator", "Easter Parade", "Enigma", "last summer.", "almost 100", "into the Southeast,", "the pie", "Great Balls of Fire", "heresy", "One Direction"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7129567736185383}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.7272727272727272, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-2525", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3087", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-15766"], "SR": 0.59375, "CSR": 0.5295138888888888, "EFR": 1.0, "Overall": 0.7021527777777778}, {"timecode": 45, "before_eval_results": {"predictions": ["to take charge of Methodist activities there", "quod erat demonstrandum", "Elizabeth II", "Celtic peoples", "Northern Exposure", "cocoa butter", "Kokomo", "Esther", "Warren Harding", "Monty Hall", "miniature golf", "Bill Hemmer", "Punxsutawney, Pennsylvania", "Bratislava", "yellow fever", "a sea otter", "the 13th letter of the alphabet", "\"franchise\"", "thy rod and thy staff", "President Richard M. Nixon", "dressage", "astronomer", "Mickey Mouse", "the stigma", "Assistant Professor", "by the Foot", "Medusa", "a spiral staircase", "a tabby", "the staff", "Voyager 1", "Farsi (Persian)", "glucose", "objects", "Mainland China", "Helen of Sparta,", "animal products", "the peace sign", "Morrie Schwartz", "English Monarchs", "Rajasthan", "\"retired\" safecracker Gal for one last job,", "a \"dirty blizzard\": oil in the water column began", "NFL", "zenith", "How shall he cut it", "\"to come before the meeting so it can be voted down\"", "William Wordsworth", "brushes", "a \"dwarf planet\"", "Arabian Nights", "Vincent Price", "Rugrats in Paris : The Movie", "Middle Eastern alchemy", "London", "Isle of Wight", "Peppercorn class A1 steam locomotive", "Queen In-hyun's Man", "Oneida Limited", "Michael Jordan", "Libreville, Gabon.", "two tickets to Italy", "The station", "Cahawba, Dallas County"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6210700757575758}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.18181818181818182, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9923", "mrqa_searchqa-validation-2593", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-13022", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-3132", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-975", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-70", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5006", "mrqa_searchqa-validation-11964", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-3322", "mrqa_triviaqa-validation-6557", "mrqa_hotpotqa-validation-2807", "mrqa_triviaqa-validation-888"], "SR": 0.53125, "CSR": 0.5295516304347826, "EFR": 1.0, "Overall": 0.7021603260869566}, {"timecode": 46, "before_eval_results": {"predictions": ["the position of people within the four-class system was not an indication of their actual social power and wealth,", "Jorge Lorenzo", "Frank McCourt", "Indiana Jones", "fungi", "the Venus flytrap", "Abraham", "a george orwell", "the faggot", "a gaggle", "California Chrome", "Pluto", "Route 66", "the Taklamakan Desert", "the Sutlej", "Astro-Santa Morph", "the Great Victoria Desert", "Germany", "Go West", "December 18, 1958", "Benjamin Franklin", "Portugal", "Operation Neptune", "Birmingham", "snakes", "Sedgefield", "Coral Sea", "Saddam Hussein", "Nadia Comaneci", "trenches", "South Korea", "pig", "superhero", "Carmen", "Kenya", "Stephen Potter", "Casa di Giulietta", "Anwar Sadat", "a hundred", "potomac", "Argentina", "Luke", "Frankfurt", "chipmunk", "Goldie Hawn", "a pulsar", "Belgium", "horses", "sugar", "Benfica", "Sun Lust Pictures", "Games played", "works in a bridal shop", "somatic cell nuclear transfer", "13th century", "1 January 1788", "Radcliffe College", "11", "\"Twilight\"", "The Carrousel du Louvre mall", "Speed Racer", "Henry Holt", "Queen Elizabeth", "Sir Walter Scott"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6451512303485987}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true], "QA-F1": [0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8093", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-445", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-4088", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-7773", "mrqa_naturalquestions-validation-5241", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3234", "mrqa_newsqa-validation-2953", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5788"], "SR": 0.578125, "CSR": 0.5305851063829787, "EFR": 1.0, "Overall": 0.7023670212765958}, {"timecode": 47, "before_eval_results": {"predictions": ["Arabah", "Venice", "Sinclair Lewis", "a bear", "The World is Not Enough", "tempera", "Jonathan Demme", "V\u00e1clav Havel", "Dick Van Dyke", "millais", "Tina Turner", "2010", "Coleraine", "glasses", "perfume", "Duke Orsino", "iron", "Copenhagen", "The Apprentice", "sahara", "Analytical Cubism", "sahara desert", "Commonwealth Institute of Science and Industrial Research", "eucharistia", "Charlotte's Web", "Spectre", "silks", "Charles Foster Kane", "Lorne Greene", "rowing", "not about nightingales", "Call My Bluff", "A", "Argentina", "Angela McCourt", "milk or water", "Caroline Aherne", "home Guard", "starch", "soap", "Donna Summer", "a balcony", "nottingham", "Poland", "Welcome Stranger", "Taggart", "April", "Chechnya", "Spot", "ateamautos", "football", "1,281,900", "Amartya Sen", "Sun Tzu", "bioelectromagnetics", "Foxborough, Massachusetts", "Speedway World Championship", "her most important work is her charity, the Happy Hearts Fund.", "Two of the dead were police officers.", "Michelle Obama", "kbenhavn", "The Proletariat", "saara", "Floxin"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6218312937062938}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-736", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6892", "mrqa_triviaqa-validation-1730", "mrqa_triviaqa-validation-4754", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-334", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-7020", "mrqa_searchqa-validation-15651"], "SR": 0.578125, "CSR": 0.5315755208333333, "EFR": 0.9629629629629629, "Overall": 0.6951576967592593}, {"timecode": 48, "before_eval_results": {"predictions": ["East Lothian", "Caesars Entertainment Corporation", "Supergirl", "king \u00c6thelred the Unready", "shaun the sheep", "Stephen Mangan", "William McKinley", "1905", "Vanilla Air", "Mineola, New York", "Serhiy Paradzhanov", "Strange Interlude", "Julia Compton Moore", "mash-Up", "1988", "early Romantic period", "Gettysburg address", "Harold Edward Holt", "Washington Street", "Lauren Lane", "Babylon", "Ford Falcon", "New York State Route 907E", "Roman \u00e0 clef (], ), French for \"novel with a key\"", "1827", "Kim Bauer", "United States Food and Drug Administration (FDA)", "william atlantic", "Suffolk", "prussian", "La Nouba", "1909 Cuban-American Major League Clubs Series", "86 ft", "American", "January 2004", "sulfur mustard H or HD blister gas", "The 45th Infantry Division", "2009", "5 Grammy Award nominations", "Anita Dobson", "City of Westminster, London", "Boyd Gaming", "February 14, 1859", "Texas Tech University", "John McClane", "Larry Wayne Gatlin", "Cayenne", "267.6 days", "North Carolina", "Selinsgrove, Pennsylvania", "Augusta Ada King-Noel, Countess of Lovelace (\"n\u00e9e\" Byron; 10 December 1815 \u2013 27 November 1852)", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "cake", "Salman Khan", "Challenger", "basil", "Clio Awards", "The Rosie Show", "California-based Current TV,", "over 1,000 pounds).", "Julius Caesar", "a volcano", "the Library of Congress", "thylakoid membranes"], "metric_results": {"EM": 0.59375, "QA-F1": 0.664740230331263}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8695652173913043, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-4008", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-3513", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1115", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5714", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-6806", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-1028"], "SR": 0.59375, "CSR": 0.5328443877551021, "EFR": 0.9615384615384616, "Overall": 0.6951265698587128}, {"timecode": 49, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-2508", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4095", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4589", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5192", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-564", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5755", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-1887", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2722", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3569", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-938", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-13836", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-15433", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-15641", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-16122", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-165", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1954", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4937", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6398", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-8410", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-975", "mrqa_squad-validation-10069", "mrqa_squad-validation-10086", "mrqa_squad-validation-1019", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-10397", "mrqa_squad-validation-10444", "mrqa_squad-validation-10449", "mrqa_squad-validation-1052", "mrqa_squad-validation-1129", "mrqa_squad-validation-1211", "mrqa_squad-validation-1265", "mrqa_squad-validation-1311", "mrqa_squad-validation-139", "mrqa_squad-validation-164", "mrqa_squad-validation-1672", "mrqa_squad-validation-1712", "mrqa_squad-validation-1916", "mrqa_squad-validation-2132", "mrqa_squad-validation-2155", "mrqa_squad-validation-2176", "mrqa_squad-validation-2326", "mrqa_squad-validation-2436", "mrqa_squad-validation-2467", "mrqa_squad-validation-264", "mrqa_squad-validation-2798", "mrqa_squad-validation-2824", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-2906", "mrqa_squad-validation-2914", "mrqa_squad-validation-294", "mrqa_squad-validation-2999", "mrqa_squad-validation-305", "mrqa_squad-validation-3337", "mrqa_squad-validation-3650", "mrqa_squad-validation-3742", "mrqa_squad-validation-3948", "mrqa_squad-validation-4025", "mrqa_squad-validation-4066", "mrqa_squad-validation-4135", "mrqa_squad-validation-4258", "mrqa_squad-validation-4338", "mrqa_squad-validation-4349", "mrqa_squad-validation-44", "mrqa_squad-validation-4472", "mrqa_squad-validation-4480", "mrqa_squad-validation-4605", "mrqa_squad-validation-4607", "mrqa_squad-validation-4686", "mrqa_squad-validation-4835", "mrqa_squad-validation-487", "mrqa_squad-validation-4897", "mrqa_squad-validation-4947", "mrqa_squad-validation-5088", "mrqa_squad-validation-5136", "mrqa_squad-validation-5238", "mrqa_squad-validation-5330", "mrqa_squad-validation-5672", "mrqa_squad-validation-594", "mrqa_squad-validation-60", "mrqa_squad-validation-6362", "mrqa_squad-validation-6562", "mrqa_squad-validation-6737", "mrqa_squad-validation-6737", "mrqa_squad-validation-6811", "mrqa_squad-validation-6918", "mrqa_squad-validation-696", "mrqa_squad-validation-703", "mrqa_squad-validation-7173", "mrqa_squad-validation-7435", "mrqa_squad-validation-754", "mrqa_squad-validation-7576", "mrqa_squad-validation-7598", "mrqa_squad-validation-7814", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-8285", "mrqa_squad-validation-8402", "mrqa_squad-validation-8406", "mrqa_squad-validation-8483", "mrqa_squad-validation-8607", "mrqa_squad-validation-8636", "mrqa_squad-validation-8715", "mrqa_squad-validation-8747", "mrqa_squad-validation-8760", "mrqa_squad-validation-879", "mrqa_squad-validation-8846", "mrqa_squad-validation-9015", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9691", "mrqa_squad-validation-9757", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1297", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2068", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2572", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2970", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3886", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-4904", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-5118", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-5505", "mrqa_triviaqa-validation-5607", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6785", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-92"], "OKR": 0.833984375, "KG": 0.453125, "before_eval_results": {"predictions": ["a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Jena Malone", "Cleveland, Ohio", "joined the utopian Ascona community", "John W. Henry", "Mos Def", "James Mitchum", "4 April 1963", "1995", "Steve Carell", "Wendell Berry", "Love Hina", "eastern", "novelty songs, comedy, and strange or unusual recordings", "OutKast", "44", "Jean Cayrol", "the Seasiders", "Musicology", "Dragon TV", "The Appalachian Mountains", "Bay Ridge, Brooklyn", "Nick Hornby", "over 1.6 million", "1968", "November 20, 1942", "September 26, 2010", "North Greenwich Arena", "1614", "Lucy Maud Montgomery", "Eminem, Bad Meets Evil, Akon, Christina Aguilera and Taio Cruz", "they typically last eight weeks and may include nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Saint Michael, Barbados", "Sleepy Hollow", "more than 26,000", "EN World web site", "Charles Russell", "KB", "Robert Jenrick", "Best Performance by an Actress in a Mini Series award", "southwest Denver, Colorado near Bear Creek", "Port Clinton", "Art of Dying", "Dallas", "Harvard", "fennec fox", "Dutch", "Terry Malloy", "Golden Calf", "Kal Ho Naa Ho", "Thorgan Ganael Francis Hazard", "the closing scene of the final episode of the first season of the AMC original series", "Everywhere", "The Jawaharlal Nehru Centre for Advanced Scientific Research ( JNCASR )", "Honda", "Edward Elgar", "the Republic of Upper Volta", "56,", "Madonna", "Eintracht Frankfurt", "the Marines", "the god of fire", "Amherst College", "two"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7425599216959511}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true], "QA-F1": [0.26666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-3430", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7692", "mrqa_newsqa-validation-616", "mrqa_searchqa-validation-12141", "mrqa_searchqa-validation-14102"], "SR": 0.640625, "CSR": 0.5349999999999999, "EFR": 0.9565217391304348, "Overall": 0.692444972826087}]}