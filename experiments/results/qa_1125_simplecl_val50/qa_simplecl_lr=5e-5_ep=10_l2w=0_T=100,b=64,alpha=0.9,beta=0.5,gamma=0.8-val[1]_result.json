{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2160, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "the 1970s", "Sunni Arabs from Iraq and Syria", "Graph isomorphism", "Thomas Murphy", "the highest terrace", "major national and international patient information projects", "three", "net force", "12 January", "1976\u201377", "Cleveland, Phoenix, Detroit and Denver", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "Book of Discipline", "complicated definitions", "lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "the main hall", "the Teaching Council", "confirmed", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation", "2014", "late 1970s", "30%", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Saul Bellow, political philosopher, literary critic and author of the New York Times bestseller \"The Closing of the American Mind\" Allan Bloom", "1991", "fossils in sedimentary rocks", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result", "The Prisoners ( Temporary Discharge for Ill Health ) Act", "Mrs. Wolowitz", "Daenerys", "Raabta"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7716021825396826}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-5952", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-6282", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_squad-validation-5178", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.734375, "CSR": 0.75, "EFR": 0.9411764705882353, "Overall": 0.8455882352941176}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "NP", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "acted in a 9 + 3 pattern, where the extra compact filament is suspected to have a supporting function", "7 West 66th Street", "patent archives", "all members of Parliament", "4-week period", "six", "Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger NFL", "1950s", "1980s", "Cologne, Germany", "second use of the law", "free", "1973", "1971", "Mansfeld", "Warsaw Stock Exchange", "390 billion individual trees divided into 16,000 species", "a suite of network protocols", "eighteenth century", "the journal Nature", "2009", "Franz Pieper", "the geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025 housing units", "1898", "Lunar Module Pilot", "citizenship", "immediately north of Canaveral at Merritt Island", "physicians, lawyers, engineers, and accountants", "return home", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification", "the head of Lituya Bay in Alaska", "120 m ( 390 ft )", "Game of Wester and A Dream of Spring", "100 members", "photo-electric", "Welch, West Virginia", "it was on this day in 1930", "twelve Wimpy Kid books", "Hal David and Burt Bacharach", "five points", "Merrimac", "Spain"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7946910294566545}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.375, 1.0, 0.8, 0.888888888888889, 0.0, 1.0, 0.4, 1.0, 0.3888888888888889, 0.38095238095238093, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-9559", "mrqa_squad-validation-2689", "mrqa_squad-validation-80", "mrqa_squad-validation-9173", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-4725", "mrqa_squad-validation-3840", "mrqa_squad-validation-1841", "mrqa_squad-validation-1220", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-2016", "mrqa_searchqa-validation-3996"], "SR": 0.671875, "CSR": 0.7239583333333333, "EFR": 0.9047619047619048, "Overall": 0.8143601190476191}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive; therefore, the defects justifying rebellion must be much more serious than those justifying disobedience, and if one cannot justify a civil disobedients' use of force and violence and refusal to submit to arrest", "the principle of inclusions and components", "the Dutch East Indies, the Caribbean, and several of the English colonies of North America, and Quebec, where they were accepted and allowed to worship freely", "12 December 2000", "six teams", "redistributive taxation", "a enzyme called rubisco", "recalled and replaced by Jeffery Amherst", "Egypt", "algae", "4,404.5 people per square mile", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "the economy", "Stairs", "genetically modified plants", "around 300,000", "three sites", "Von Miller", "Africa", "the clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Mark Ronson", "the Calvin cycle which uses rubisco", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw), who may sign them into law", "cloud storage service", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "the Solim\u00f5es Basin", "Charles Darwin", "23 November", "town of the Ubii", "Denver's Executive Vice President of Football Operations and General Manager", "Downtown San Bernardino", "Capital Cities Communications", "the lamprey and hagfish", "physicians and other healthcare professionals", "the Golden Gate Bridge", "Michael Schumacher", "10.5 %", "The Man", "President Gerald Ford", "Jane Fonda", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "in awe of Novalee, and had seen her enter the store at closing time, smashes through the window to help deliver her child", "December 1971", "the realm of the Valar in Aman", "the middle of the 15th century", "6 March 1983", "James G. Kiernan", "horror fiction", "26,000"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7334949515328719}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.46808510638297873, 1.0, 0.5161290322580645, 0.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.5714285714285715, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6798", "mrqa_squad-validation-3023", "mrqa_squad-validation-4108", "mrqa_squad-validation-180", "mrqa_squad-validation-10293", "mrqa_squad-validation-4759", "mrqa_squad-validation-8763", "mrqa_squad-validation-6154", "mrqa_squad-validation-126", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-670", "mrqa_squad-validation-8833", "mrqa_squad-validation-962", "mrqa_squad-validation-384", "mrqa_squad-validation-2644", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433"], "SR": 0.609375, "CSR": 0.6953125, "EFR": 1.0, "Overall": 0.84765625}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Fort Presque Isle (near present-day Erie, Pennsylvania)", "wireless power transmission", "Bruno Mars", "the Yuan dynasty", "same-gender marriages with resolutions", "red algae red", "after their second year", "the 1960s", "it was directly effective", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot (3.7 million cubic meter)", "The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "dissension and unrest", "the First Amendment or individual state Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "BSkyB", "shipping toxic waste", "anarchists", "Eastern crops such as carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "a thylakoid", "University College London", "to protect their tribal lands", "religious beliefs according to the Scottish census", "the spirit of protest should be maintained all the way, whether it is done by remaining in jail, or by evading it", "\"on the hearth is the luckiest thing in all the world!\"", "Gandhi in South Africa", "Vlad the Impaler", "The Little Foxes", "Samanyo, Beta-format video recorders", "Leonard Nimoy", "Yam + MP))1/3.", "1994", "1867 to 1877", "Marshall Dillon", "\"Wannabe\" and \"Say You'll Be There\"", "The Best Hotels on Bali", "Sam F. Kennedy", "LASER", "Marshall Flying Demi-Tasse", "Yahuah Reigns", "Samagaria radiata, the egret flower of the Far East - Botany Boy", "\"why\", perch", "Andrew Taggart, Emily Warren and Scott Harris", "Samaxophobia- Fear of riding in a car", "American", "Enrique Torres"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5326193164474415}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16, 0.1818181818181818, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.28571428571428575, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.5, 0.5714285714285715, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-1341", "mrqa_squad-validation-110", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-3703", "mrqa_squad-validation-390", "mrqa_squad-validation-10186", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-7088", "mrqa_squad-validation-2804", "mrqa_squad-validation-8068", "mrqa_squad-validation-5214", "mrqa_squad-validation-4315", "mrqa_squad-validation-9406", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073", "mrqa_newsqa-validation-496"], "SR": 0.421875, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.8203125}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (\"yes\" or \"no\")", "the Meuse", "a Western Union superintendent", "Super Bowl XLIV in 2010", "1891", "New Orleans", "fell from his horse while hunting", "the member state cannot enforce conflicting laws", "the work of British bacteriologist J. F. D. Shrewsbury", "a mouth that can usually be closed by muscles", "inversely to member state size", "Europe", "he was illiterate in Czech", "colonies", "$37.6 billion", "Kalenjin", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center in San Jose", "lymphocytes-derived molecule", "the Edict of Fontainebleau", "Levi's Stadium in the San Francisco Bay Area", "ten million people", "the Lippe", "Video On Demand content", "time and storage", "the top 5 percent", "the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "Lawrence", "the League of the Three Emperors", "science", "143,007", "Clinton", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Thomas Christopher Ince", "American Chopper", "from 1972 with drummer Rob Hirst, bass guitarist Andrew James and keyboard player/lead guitarist Jim Moginie", "German", "Fort Valley, Georgia", "American", "Easy (TV series)", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "American President Theodore Roosevelt", "corruption", "a doctor", "Dover Beach"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7422427000774368}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 0.5161290322580645, 0.9090909090909091, 1.0, 0.3846153846153846, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.8235294117647058, 0.8, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-159", "mrqa_squad-validation-6218", "mrqa_squad-validation-4919", "mrqa_squad-validation-4517", "mrqa_squad-validation-4210", "mrqa_squad-validation-1187", "mrqa_squad-validation-8544", "mrqa_squad-validation-457", "mrqa_squad-validation-6676", "mrqa_squad-validation-12", "mrqa_squad-validation-9753", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_squad-validation-3943", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-3996", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.609375, "CSR": 0.6354166666666667, "EFR": 1.0, "Overall": 0.8177083333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["1540s", "courts of member states", "around its circle logo", "three", "a negative long-term impact on the health of the city's residents", "to ensure that people aren\u2019t denied their functionings, capabilities, and agency and can thus work towards a better relevant income", "about 7.5% of primary enrollment, 32% of secondary enrollment and about 80% of tertiary enrollment", "1521", "Gibraltar and the \u00c5land islands", "The starch granules displace the thylakoids, but leave them intact", "exceeds any given number", "Hulagu Khan", "Israeli poet", "quality rental units", "Grover Cleveland", "to cause their repeal, or to exert pressure to get one's political wishes on some other issue", "entertainment", "A vote clerk", "high growth rates", "a vicious and destructive civil war", "Sony", "Stagecoach", "Silk Road", "San Diego", "Central Poland", "a program called the Council on Advanced Studies in the Social Sciences and Humanities", "to otherwise leverage the accumulation of wealth", "Spanish", "Structural geologists", "president and CEO of ABC", "indulgences for the living", "BSkyB and Virgin Media", "a terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Ben Johnston", "Sinclair Oil Corporation", "Taylor Swift", "Eric Edward Whitacre", "Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "11 or 13 and 18", "Heather Langenkamp (born July 17, 1964)", "Henry Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI)", "Bury St Edmunds, Suffolk, England", "The WB supernatural drama series \"Charmed\"", "Lily Hampton", "Liverpool and England international player", "The Ducks", "Ricky Skaggs", "48,982", "Obuasi Municipal district of the Ashanti Region of Ghana", "73", "Algeria", "a novel", "Biafra", "The School of Athens in the Vatican Stanza della Segnatura", "Atlantic City"], "metric_results": {"EM": 0.5, "QA-F1": 0.701613546338768}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 0.08695652173913043, 0.125, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8717948717948718, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 0.5454545454545454, 1.0, 1.0, 0.7499999999999999, 1.0, 0.4, 1.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.5, 1.0, 1.0, 0.8, 0.7499999999999999, 0.33333333333333337, 1.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-5213", "mrqa_squad-validation-7598", "mrqa_squad-validation-7029", "mrqa_squad-validation-8914", "mrqa_squad-validation-880", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-9665", "mrqa_squad-validation-913", "mrqa_squad-validation-7983", "mrqa_squad-validation-7543", "mrqa_squad-validation-5651", "mrqa_squad-validation-2840", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-5300", "mrqa_naturalquestions-validation-2159", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-1971"], "SR": 0.5, "CSR": 0.6160714285714286, "EFR": 0.96875, "Overall": 0.7924107142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "working fluid", "suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "solution", "means to invest in new sources of creating wealth", "center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "the papacy", "through homologous recombination", "a modern canalized section", "protest against the occupation of Prussia by Napoleon", "improved markedly", "along the entire length of the lake", "computer programs", "General Conference of the United Methodist Church", "1996", "dreams", "Judiciary", "single-tape", "Bart Starr", "allotrope", "Karluk Kara-Khanid ruler", "Perth", "Ian Rush", "Gerry Adams", "New Orleans Saints of the National Football League", "1974", "four", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "Hanoi", "Sevens", "fennec fox", "Bart Conner", "fantasy role-playing game", "Martin McCann", "Black Mountain College", "a historic house museum", "Bothtec", "Cody Miller", "140", "John Locke", "Garth Jennings", "Pablo Escobar", "Afro-Russian", "Mexico City", "Sleeping Beauty", "PeopleMover", "1985", "Doddi in Iceland, Purzelknirps in Germany and Hilitos in Spain", "Omar Bongo", "Honey Nut Chex", "Ray Harroun", "Emily Blunt", "David Tennant"], "metric_results": {"EM": 0.625, "QA-F1": 0.7254960317460317}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.6, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-7547", "mrqa_squad-validation-3091", "mrqa_squad-validation-9287", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-3413", "mrqa_hotpotqa-validation-3885", "mrqa_naturalquestions-validation-4388", "mrqa_triviaqa-validation-1573", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869", "mrqa_naturalquestions-validation-1618"], "SR": 0.625, "CSR": 0.6171875, "EFR": 0.9583333333333334, "Overall": 0.7877604166666667}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Non-revolutionary", "during the compression stage relatively little work is required to drive the pump", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "5th Avenue laboratory fire", "the history of arms", "two independent mechanisms", "minor", "Fringe or splinter movements", "17", "lower", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "teachers are now selling their lesson plans to other teachers through the web in order to earn supplemental income", "stratigraphic", "commensal flora", "a + bi", "Dallas, Texas", "Han Chinese and Khitans", "8 mm cine film", "1330 Avenue of the Americas in Manhattan", "Alberta and British Columbia", "noddy", "Don Johnson", "The song, which appears as the fifth track on the album", "25 million", "8,515", "13 October 1958", "tailless", "Environmental Protection Agency", "between 1932 and 1934", "an English professional footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Dusty Dvoracek", "boxer", "Boston University", "Fulham F.C.", "A55", "Ranulf de Gernon, 4th Earl of Chester", "Olaf Guthfrithson", "Madras Export Processing Zone", "44", "I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "A diastema ( plural diastemata )", "Alison Krauss", "Iran", "Bigfoot", "Papua New Guinea", "Renoir", "Hardman Square"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7149621212121212}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.5, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6789", "mrqa_squad-validation-1501", "mrqa_squad-validation-3405", "mrqa_squad-validation-2238", "mrqa_squad-validation-9859", "mrqa_squad-validation-8356", "mrqa_squad-validation-7643", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305", "mrqa_triviaqa-validation-4945"], "SR": 0.640625, "CSR": 0.6197916666666667, "EFR": 0.9565217391304348, "Overall": 0.7881567028985508}, {"timecode": 9, "before_eval_results": {"predictions": ["$32 billion", "cotton spinning", "Orange County", "chloroplast peripheral reticulum", "1962", "not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "unpaired electrons", "French", "the Museum of the Moving Image in London", "he sent missionaries, backed by a fund to financially reward converts to Catholicism", "pyrenoid and thylakoids", "Kearney Park", "force", "25 May 1521", "essentially holy people", "diplomacy or military force", "increase in the land available for cultivation", "the value of the spin", "a pivotal event in the Arab Muslim world", "to become a national transgender figure", "Trent Alexander-Arnold", "David Michael Bautista Jr.", "Black Friday", "American actor, singer and a DJ", "Prince Amedeo", "Lambic and Oud bruin", "Baja California Peninsula", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "the Disneyland Monorail, and the Motor Boat Cruise", "Yasir Hussain", "USC Marshall School of Business", "Stephen James Ireland", "Marco Hietala", "Basauri, Biscay", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "4145 ft above mean sea level", "Central Park", "Robert John Day", "Ouargli", "James Tinling", "Italy", "PGA Tour", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "manchester", "1.5 million", "morphine sulfate", "man from an impoverished man out of law", "manchester"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6396144602946073}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3371", "mrqa_squad-validation-4147", "mrqa_squad-validation-2943", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3487", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_triviaqa-validation-2522", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1061", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.546875, "CSR": 0.6125, "EFR": 0.9655172413793104, "Overall": 0.7890086206896552}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "the Mocama", "suburban", "early vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "tight end Owen Daniels and a 22-yard throw to receiver Andre Caldwell", "Sanders", "civil instability", "Gamal Abdul Nasser", "immune responses beginning to decline at around 50 years of age due to immunosenescence", "counterflow", "John B. Goodenough", "his arrest was not covered in any newspapers", "machine gun", "the Autons with the Nestene Consciousness and Daleks", "to attend school at the Higher Real Gymnasium", "Standard Model", "Chagatai", "the Rhine-Ruhr region", "pedagogy", "Prevenient grace", "Kansas State 52\u201321", "Captain Cook's Landing Place", "Chris Pine", "Yoo Seung-ho", "the Battle of the Philippines", "NCAA Division I", "The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Uzumaki (English version)", "Tom Jones", "the RATE project", "Barbara Niven", "13\u20133", "Eliot Spitzer", "5,042", "European culture", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "Giuseppe Verdi", "Central Europe", "New Jersey", "Bath, Maine", "Ector County", "Jim Davis", "Buck Owens", "the World Health Organization", "Emmanuel Ofosu Yeboah", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Heather Stebbins", "the cat", "Sir Giles Gilbert Scott", "get his ship away", "Nine fewer than an earlier count", "Onomastic Sobriquets In The Food And Beverage Industry", "the cat"], "metric_results": {"EM": 0.625, "QA-F1": 0.7022168803418803}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-797", "mrqa_squad-validation-7502", "mrqa_squad-validation-6495", "mrqa_squad-validation-9815", "mrqa_squad-validation-7729", "mrqa_squad-validation-1166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.625, "CSR": 0.6136363636363636, "EFR": 0.9583333333333334, "Overall": 0.7859848484848485}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate", "Battle of Olustee", "French", "100\u2013150", "Philo of Byzantium", "cooler", "marine waters worldwide", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "his mother", "shock", "cytotoxic natural killer cells and CDRs (cytotoxic T lymphocytes)", "violence", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "The Cash for Clunkers", "a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "McCartney", "a larger deal that has not been signed by anyone", "\"We could have an accidental death and a mother that panics,\"", "200", "The drug is legal for medical use, but it is trafficked into Hong Kong", "The Zimbabwe Electoral Commission", "Missouri", "to step down as majority leader", "executive director of the Americas Division of Human Rights Watch", "Casa de Campo International Airport", "90", "The station", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "her home", "Employee Free Choice", "Bush administration", "more than 200", "It is done with the parents' full consent", "best-of-three series", "Kaka", "Christopher Savoie", "U.S. filmmakers", "an apartment near Fort Bragg", "two", "nearly $2 billion", "Jacob", "Molotov cocktails, rocks and glass", "250,000", "Andrew Morris", "Ark of the Covenant", "Jean F Kernel ( 1497 -- 1558 ), a French physician", "The dusting Crowd", "The town of Selby", "1994", "The Conjuring", "The Gallipoli Campaign", "a large bay that protrudes northeast from Lake Huron into Ontario, Canada", "Nowhere Boy"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6801854395604396}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8666666666666666, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.1904761904761905, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 0.0, 1.0, 0.5714285714285714, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3087", "mrqa_squad-validation-6588", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2344", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-373", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-6176", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.5625, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.8046875}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British.", "wealth", "to be greater than sin", "Napoleon", "new technology and machinery", "Arley D. Cathey", "private actors", "Bell Northern Research", "a body of treaties and legislation", "1227", "lower lake", "three", "Elders", "587,000", "A further type of committee", "Bruno Mars", "Catechism", "beneath the university's Stagg Field.", "Ian Botham", "E. T. A. Hoffmann", "Vincent Motorcycle Company", "richmond", "Salvador Allende", "Harold Pinter", "Hawaii", "Erik Thorvaldson", "Apollon", "Rodgers & Hart", "Mary Seacole", "green", "richmond", "Moses", "Antonio", "European Monetary System", "Christine Keeler", "animal", "Nicholson", "four", "Germany", "Sugar Baby Love", "Rosa Parks Bus", "Sean", "John Denver", "Stage 1", "Travis", "Blue Peter", "Robert Kennedy", "Q", "a lightweight, folding version that, with added waterproofing materials, could protect users from rain and snow.", "Rudyard Kipling", "barber", "richmond", "Murrah Federal Office Building", "Evita", "oldpatricktoe-end of a Persian slipper", "Russian citadels", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley's Club", "\"Sex and the City's\"", "a delegation of American Muslim and Christian leaders", "The marriage to Henry VIII lasted less than a year", "University of South Carolina", "Del Potro."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5643836152882207}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.20000000000000004, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-5431", "mrqa_squad-validation-7974", "mrqa_squad-validation-9418", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_triviaqa-validation-712", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-729", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_newsqa-validation-1150"], "SR": 0.46875, "CSR": 0.5985576923076923, "EFR": 0.9705882352941176, "Overall": 0.7845729638009049}, {"timecode": 13, "before_eval_results": {"predictions": ["Cram\u00e9r's conjecture", "Bo'orchu", "Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "to counteract the constant flooding and strong sedimentation", "Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees.", "Dublin, Cork, Youghal and Waterford", "Tangled", "jinn", "moles", "Democritus", "Diego Garc\u00eda", "Anne Boleyn", "Calvin", "Steve McQueen", "Portugal", "jinni", "two", "komando Pasukan Khusus", "in the northwest of England", "dry Ice", "zanesville", "Lucas McCain", "phoctique", "mercury gilding", "aniridia", "t.S. Eliot", "the river", "woe", "NOW Magazine", "jinn", "netminder", "one", "typhoid fever", "Tina Turner", "g.I. Joe", "al Bundy", "2010", "netherlands", "Venezuela", "Laurel", "netherlands", "40", "phrenology", "San Francisco", "Fall 1998", "Regulus", "Christopher James \" Chris\" Weidman", "Drillers Stadium", "one", "Virgin America", "John Grisham", "netherlands", "Iran's parliament speaker", "netherlands"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5802083333333334}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8994", "mrqa_squad-validation-6078", "mrqa_squad-validation-9233", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.53125, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.796875}, {"timecode": 14, "before_eval_results": {"predictions": ["in an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary", "Beijing", "three years", "27 July 2008", "chemically", "Aristotle", "St. George's Church", "Michelle detroit", "Strathclyde Regional Council debating chamber in Glasgow, and to the University of Aberdeen", "public official", "the most cost efficient bidder", "ocular", "the one that spans 11 time zones", "the femur of Ardipithecus ramidus", "Olympia", "Ukraine", "toab", "andrew johnson", "to build a fire", "amber", "Princeton University", "The executioner's Song", "ocular", "bishkek Tajikistan", "Anamosa's Famous Artist", "andrew johnson", "The Comedy of Errors", "the asylum", "film", "shrews", "fiery light", "Cologne", "Oprah Winfrey", "fiery", "preevo", "James Jeffords", "detroit", "tennis", "ocular", "shrew", "andrew johnson", "odocoileus virginianus", "detroit", "andrew johnson", "the United Nations", "ocular", "accordion", "andrew johnson", "neurosis", "Augusta", "clockwise", "2013", "Nick Hornby", "oasis", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama", "goblin's Market", "Israel"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4692708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7000000000000001, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-405", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-14700", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-15235", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-325", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3084"], "SR": 0.390625, "CSR": 0.5802083333333333, "EFR": 1.0, "Overall": 0.7901041666666666}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes)", "Deabolis", "April 20", "R\u0113nos", "1996", "wine", "German-Swiss", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM's", "crossword", "Cuba Gooding Jr.", "John D. Rockefeller", "Flemish", "MasterCard", "Grant Wood", "Nashville", "the olfactory nerve", "Ivan the Terrible", "Nancy Astor", "lentigo", "Grant Russell", "Toronto Maple", "Zsa Zsa Gabor", "Vladimir Nemirovich-Danchenko", "Utah", "sugarcane", "(Rabbit) Angstrom", "Johann Strauss II", "a deer", "pro bono", "Politecnico di Bari", "a candy store", "a beer", "manfred von Richthofen", "Nacho Libre", "copper", "a representation in words or pictures of black magic or of dealings with the devil.", "a hemlock", "Jeffrey Wigand", "poetry", "a supplementary sauce", "supplementary", "Casablanca", "blimps", "Grant Kirchhoff", "a geisha", "a mermaid", "Altruism", "Frederic Remington", "S.A. de C.V.", "Grant Jennings", "a Tin Star", "\"Noir\"", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6175347222222222}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6463", "mrqa_squad-validation-9248", "mrqa_squad-validation-9270", "mrqa_searchqa-validation-4490", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-7531", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-7392", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.53125, "CSR": 0.5771484375, "EFR": 1.0, "Overall": 0.78857421875}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite tribe", "respiration", "1997", "the late 1920s", "\u00a31.3bn", "27 July 2008", "unequal", "October 1973", "dragonnades", "Isiah Bowman", "assembly center", "Ominde Commission", "the Rhine", "Eva Peron", "Ho Chi Minh", "circum", "an Inuit dwelling", "Detroit Rock City", "the (Montreal) Blue Jays", "Lilacs Last", "Ray Bradbury", "hate crimes", "King Julien XIII", "Nicolas Sarkozy", "Rubicon", "Steve Martin", "17", "Louisa May Alcott", "Play-Doh", "Aphrodite", "Thomas", "The Prince & Pauper", "cola", "Hillary Clinton", "King Philip", "Bellerophontes", "Balaam", "The Wharton School", "the Caine Mutiny", "(Robbie) Robertson", "(founded) Woolworth Company", "(John) Coltrane", "peace", "fire", "the Sphinx", "John Hus", "Nashville Star", "Mavericks", "Onegin", "Macy's", "a spinning jenny", "Santa Claus", "(Julius) Caesar", "Malpractice", "judges", "chromosome 21 attached to another chromosome", "Goosnargh", "Australia", "The Jefferson Memorial", "between 11 or 13 and 18", "Michoacan Family", "(Charlie) Blauser", "salary", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6353766025641026}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-15262", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-13161", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-794", "mrqa_triviaqa-validation-4973", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-1759"], "SR": 0.53125, "CSR": 0.5744485294117647, "EFR": 0.9666666666666667, "Overall": 0.7705575980392156}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Tesla", "the telephone ring", "the Orange Democratic Movement", "22", "the dauphin", "Phillip Marlowe", "piracy", "Roger Clemens", "The Crystal Method", "Puerto Rico", "Mausoleum", "Million Dollar Baby", "Syria", "Western Airlines", "The Old Man", "French", "Joe Louis", "the Nemean lion", "The Three Musketeers", "the Bayeux Tapestry", "a joey Porch", "China", "Sunni", "notes", "Stephen Hawking", "Mrcus Tullius Cicero", "Memphis", "Mountain Dew", "Blanche DuBois", "Quilt", "FRAM", "the House of Representatives", "a beer company", "Michael Moore", "Oman", "Chevy", "an artless girl", "Pennsylvania", "Don Juan", "Ian Fleming", "The Headless Horseman", "London", "Yellowstone", "Ronald Reagan", "a rich man", "Ethiopian", "six", "1992", "preston", "Bromley", "the Ruul", "Cartoon Network", "Caylee Anthony", "their rides based on what their cars say about them.", "the Taliban", "a nuclear weapon", "The drama of the action in-and-around the golf course"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6799242424242424}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1659", "mrqa_squad-validation-8420", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-11215", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-1920", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-14588", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-6024", "mrqa_searchqa-validation-12814", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-4110"], "SR": 0.640625, "CSR": 0.578125, "EFR": 0.9565217391304348, "Overall": 0.7673233695652174}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXII", "1993", "June 1979", "friend", "tentacles", "Robert R. Gilruth", "Complexity", "same-gender marriages", "2006", "the mid-18th century", "orange", "A Raisin in the Sun", "Moses", "White Russia", "a flanker", "a trowel", "Big Bang", "The Sex Pistols", "endodontist", "a bathtub", "White Cliffs of Denmark", "Genoa", "John Galt", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Kansas", "Seattle", "\"Little. White Pet.\"", "The Hampton Inn", "21", "the Civil War", "alevin", "Paul McCartney", "omega-6", "cavender Chang,1,Laws of Attractor,", "BTO", "Halloween", "\"Be the ball\"", "Tokyo", "Panama", "Confession", "Narnia", "Finnegans Wake", "(William) Wordsworth", "Iceland", "Berenstain Bears", "an earthquake", "Judas", "an elephant", "Mazurka", "Finland", "clandestine", "\"All for our Country\"", "May 2010", "Camp David", "Guanabara", "Thailand", "gender queer", "Minister for Social Protection", "Germany", "the estate", "Robin Williams, and Bill Irwin", "ase", "Michigan and surrounding states and provinces"], "metric_results": {"EM": 0.5, "QA-F1": 0.5798751293995859}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.1739130434782609]}}, "before_error_ids": ["mrqa_squad-validation-499", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-4973", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-4853", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-2870"], "SR": 0.5, "CSR": 0.5740131578947368, "EFR": 0.96875, "Overall": 0.7713815789473684}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "genetically modified", "Earth", "53,000", "one", "poet", "two", "20,000", "the kip", "skeletal muscle and the brain", "2014", "peptide bonds", "Montreal", "Sunday evenings", "sperm and ova", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "mindfulness", "Charlene Holt", "Captain Leland Stottlemeyer", "1991", "electron shells", "The Cornett family", "acid rain", "October 22, 2017", "inefficient", "he cheated on Miley", "2001", "democracy", "735", "1913", "Rick Rude", "Toledo, Bowling Green, and Mount Union", "board of trade", "a cladding of a different glass", "Abraham Gottlob Werner", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another", "Necator americanus and Ancy Lostoma duodenale", "March 1", "Forza Horizon 3", "Abraham Lincoln's war goals", "oxygen", "Cecil Lockhart", "Mara Jade", "British and French Canadian fur traders", "semi-autonomous organisational units", "Lou Rawls", "Hermia", "Jupiter", "east", "15", "John Robert Cocker", "Israel", "video", "a palace", "the olfactory nerve", "Eucalyptus", "a lion", "oxygen"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5628720238095238}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.2857142857142857, 0.28571428571428575, 0.26666666666666666, 0.0, 0.2, 0.8, 0.5, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8896", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-2379", "mrqa_triviaqa-validation-2227"], "SR": 0.453125, "CSR": 0.56796875, "EFR": 0.9714285714285714, "Overall": 0.7696986607142857}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "the Cloth of St Gereon", "Thomas Sowell", "more than 70", "death of a heretic", "Biblical ideal of congregations' choosing their own ministers", "1886", "\"Blue Harvest\"", "Jacob Zuma", "gang rape of a 15-year-old girl on the campus of Richmond High School in Northern California", "\"safe housing\"", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer", "over 1,000 pounds", "Mubarak's", "Saadi", "Texas and Oklahoma", "Polo", "Joe Jackson", "uranus", "computer problems left travelers across the United States waiting in airports,", "Silvan Shalom", "Jonathan Breeze", "Steve Jobs", "12-hour", "prisoners", "September", "consumer confidence", "5:20 p.m.", "North vs. South,", "India", "1964", "Davidson", "Swat Valley", "Friday", "1979", "the United States", "uranus", "Akio Toyoda", "The museum has been halted over a budgetary dispute, delaying its opening,", "the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanus Bay, Cuba.", "Giovani dos Santos", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "children that a French charity attempted to take to France from Chad for adoption", "40", "\"Friday the 13th\"", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "two years", "Doctor Who", "winter festivals", "Pentecost", "Aberdeen", "\"Dumb and Dumber\"", "The 2003 LSU Tigers football team represented Louisiana State University (LSU)", "Earl Warren", "A converging lens", "autompne", "uranus", "The Force Fighters"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5397218233155734}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.923076923076923, 0.8571428571428571, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.05, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2466", "mrqa_squad-validation-7937", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7266", "mrqa_hotpotqa-validation-1094", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.453125, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.78125}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress among teachers.", "San Diego-Carlsbad-San Marcos", "chief electrician", "Newton", "the applied force is opposed by static friction, generated between the object and the table surface.", "the assassination of US President John F. Kennedy", "\"an affront to Somalia's territorial sovereignty.\"", "Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Awearness Fund", "10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "\"no more than an official of the most tyrannical dictatorial state in the world.\"", "Maude", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Wednesday.", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad, Iran.", "Amanda Knox's aunt", "jazz", "$17,000", "Barney Stinson,", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two contestants.", "Bill", "J.G. Ballard", "nurse who tried to treat Jackson's insomnia with natural remedies", "Sarah", "saving and planning for retirement long before his career neared its end.", "1981", "17 Again", "Nigeria", "$81,8709.", "Republicans", "EU naval force", "Chris Robinson", "Omar Bongo,", "steam-driven, paddlewheeled overnight passenger boat.", "Hyundai Steel", "skeletal dysplasia, a bone-growth disorder that causes dwarfism,", "London Heathrow's Terminal 5.", "\"I underestimated the number of swimmers who would come to swim at the club.\"", "February 12", "30 Latin American and Caribbean nations", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Tiger Woods", "military strike", "White House Executive chef", "Russell Huxtable", "Willy Russell", "Budapest", "\"Mortal Kombat X\"", "Northumbrian", "Ophelia", "the country's last Communist leader", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Duke of Lauenburg"], "metric_results": {"EM": 0.5, "QA-F1": 0.5972360972360973}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.2666666666666667, 0.15384615384615383, 0.0, 0.5714285714285715, 1.0, 0.0, 0.4, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.1, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-10313", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2810", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-4514", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-1056"], "SR": 0.5, "CSR": 0.5596590909090908, "EFR": 0.96875, "Overall": 0.7642045454545454}, {"timecode": 22, "before_eval_results": {"predictions": ["\"instrument will... enable one to generate Roentgen rays of much greater power than obtainable with ordinary apparatus.\"", "WMO Executive Council and UNEP Governing Council resolutions", "Saxon chancellery", "New York and Virginia", "two", "glowed even when turned off.", "a number of celebrities and ministers,", "scientists know about Earth's closest neighbor.", "\"It's public knowledge that the United Kingdom has started the search for hydrocarbon resources in the Falkland Islands area,\"", "April 6, 1994", "Prague", "\"The work is the hardest and least rewarding work we have ever tried to do.", "a federal judge in Mississippi", "\"The mob scatters as the police officers in military style camouflage fire shots in the air and apprehend a few stragglers, some with a kick or a punch.", "$22 million", "the impact of severe flooding", "a music video on his land.", "at the Lindsey oil refinery in eastern England.", "$55.7 million", "The Real Housewives of Atlanta", "18", "88", "\"I haven't seen any violence.", "a president who understands the world today, the future we seek and the change we need.", "the commissions", "Sachina Verma", "Larry King", "Barack Obama", "racially motivated.", "Michael Partain", "male veterans struggling with homelessness and addiction.", "the longest domestic relay in Olympic history", "Zimbabwe's main opposition party", "$27.5 million", "nine", "four bodies", "Friday", "'City of Silk' in Kuwait", "Rima Fakih", "Tuesday night", "\"It is believed to be the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Alwin Landry's supply vessel Damon Bankston", "researchers", "involvement during World War II in killings at a Nazi German death camp in Poland.", "opium", "\"The new language will strengthen the existing warnings,\"", "84-year-old", "Robert Park", "Rima Fakih", "the Isthmus of Corinth", "Nalini Negi", "2017 - 12 - 10 )", "Runcorn", "paris", "horizon", "UFC 50: The War of '04", "June 11, 1973", "The Del Mar Fairgrounds", "Toy Story", "paris", "\"The Cricket on the Hearth: A Fairy Tale of Home\""], "metric_results": {"EM": 0.34375, "QA-F1": 0.4878681545081801}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.9333333333333333, 0.0, 0.22222222222222224, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.5, 0.19999999999999998, 0.07142857142857144, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9565217391304348, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-8587", "mrqa_squad-validation-2356", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-1475", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4449", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.34375, "CSR": 0.5502717391304348, "EFR": 1.0, "Overall": 0.7751358695652174}, {"timecode": 23, "before_eval_results": {"predictions": ["phycobilin phycoerytherin", "was lost in the 5th Avenue laboratory fire", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition", "Behind the Sofa", "Tulsa, Oklahoma.", "a former general secretary of the Communist Party,", "in Yemen", "2005", "Karen Floyd", "six Iraqis and wounded 10 others,", "those missing were slowed by ammonia leaks and a fire that was not extinguished until afternoon.", "Haiti", "Susan Boyle", "Saturday", "The Interior Ministry", "Jared Polis", "Janet and La Toya,", "Dangjin", "three", "Miriam Brown", "lightning strikes", "Evans", "Italian government", "the flooding was so fast that the thing flipped over,\"", "threatening messages", "\"I have never thought about taking children away from their father, never,\"", "drafting a new constitution after three decades of Mubarak's rule.", "fake his own death", "the charges \"in the interest of justice.\"", "martial arts", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "then-Sen. Obama", "Congress", "a curfew", "her account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "June,", "the government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.", "Zuma", "haute, bandeau-style little numbers", "nine", "Iraqi Prime Minister Nouri al-Maliki", "2000", "about 50", "15-year-old", "in body bags on the roadway near the bus,", "al Fayed's", "Desmond Tutu", "$1.4 million", "Jobs", "$81,880", "to provide school districts with federal funds, in the form of competitive grants, to establish innovative educational programs for students with limited English speaking ability", "`` a transformiation, change of mind, repentance, and atonement ''", "Jason Lee", "sleep", "noun", "Kent", "beer and soft drinks", "five aerial victories.", "Cherokee River", "Animal Farm", "Apollo 13", "Florida"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5828499194465235}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.07692307692307691, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15789473684210525, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1506", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-957", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458"], "SR": 0.53125, "CSR": 0.5494791666666667, "EFR": 1.0, "Overall": 0.7747395833333334}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "Yuan dynasty", "manually suppress the fire.", "compound", "Nigeria", "U.S. team medical director Richard Quincy", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "him to step down as majority leader.", "United Nations World Food Program", "gang rape of a 15-year-old girl on the campus of Richmond High School in Northern California", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "ancient Egyptian antiquities in the world,", "his club", "would be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "1979", "one of its diplomats in northwest Pakistan", "jazz", "an antihistamine and an epinephrine auto-injector for emergencies,", "Bangladesh,", "Michael Arrington,", "17", "U.N. High Commissioner for Refugees", "HIV/AIDS fight", "\"Larry King Live,\"", "military personnel.", "behind the counter.", "11", "one", "40 former U.S. Marines or sons of Marines who lived at Camp Lejeune", "her fianc\u00e9,", "racial intolerance.", "dairy and eggs,", "Vicente Carrillo Leyva,", "two police cars", "$8.8 million", "to stabilize Somalia and cooperate in security and military operations.", "who is responsible for causing it and what should be done about it", "black is beautiful,\"", "$104,168,000", "Picasso's muse and mistress, Marie-Therese Walter.", "opium poppies,", "$162 billion in war funding", "off the coast of Dubai", "fallen comrades lost in the heat of battle.", "Springfield, Virginia,", "27", "Mark Obama Ndesandjo", "Oxygen,", "Russian residents and worldwide viewers,", "\"Bill, Jr.", "fatally shooting a limo driver on February 14, 2002.", "nuclear matrix", "Vienna", "Sonja Percy ( Shalita Grant )", "President Woodrow Wilson", "Tom Watson", "Sandi Toksvig", "Etihad Aldar Spyker F1 Team", "Viscount Cranborne", "Lake Buena Vista, Florida", "Iceland", "wedlock", "catalytic converters"], "metric_results": {"EM": 0.359375, "QA-F1": 0.48449989306670344}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.0, 0.25, 0.18181818181818182, 0.0, 0.4, 0.06896551724137931, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8428", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4128", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-4927", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.359375, "CSR": 0.541875, "EFR": 1.0, "Overall": 0.7709375}, {"timecode": 25, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-3952", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4548", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-4924", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-729", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-3637", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8638", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3254", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-49", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12150", "mrqa_searchqa-validation-12205", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-13756", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14282", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14700", "mrqa_searchqa-validation-14743", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-2355", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-4973", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6870", "mrqa_searchqa-validation-7227", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7584", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-8335", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-87", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10026", "mrqa_squad-validation-10026", "mrqa_squad-validation-10100", "mrqa_squad-validation-10254", "mrqa_squad-validation-10406", "mrqa_squad-validation-10418", "mrqa_squad-validation-1146", "mrqa_squad-validation-1166", "mrqa_squad-validation-1187", "mrqa_squad-validation-1218", "mrqa_squad-validation-126", "mrqa_squad-validation-1295", "mrqa_squad-validation-1313", "mrqa_squad-validation-1341", "mrqa_squad-validation-1407", "mrqa_squad-validation-1501", "mrqa_squad-validation-1549", "mrqa_squad-validation-159", "mrqa_squad-validation-1640", "mrqa_squad-validation-1662", "mrqa_squad-validation-1692", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1775", "mrqa_squad-validation-1877", "mrqa_squad-validation-1906", "mrqa_squad-validation-1960", "mrqa_squad-validation-2049", "mrqa_squad-validation-2059", "mrqa_squad-validation-2105", "mrqa_squad-validation-2113", "mrqa_squad-validation-2136", "mrqa_squad-validation-2207", "mrqa_squad-validation-2435", "mrqa_squad-validation-2466", "mrqa_squad-validation-2518", "mrqa_squad-validation-2530", "mrqa_squad-validation-281", "mrqa_squad-validation-2833", "mrqa_squad-validation-2858", "mrqa_squad-validation-2941", "mrqa_squad-validation-298", "mrqa_squad-validation-3091", "mrqa_squad-validation-3100", "mrqa_squad-validation-3127", "mrqa_squad-validation-3132", "mrqa_squad-validation-3149", "mrqa_squad-validation-3259", "mrqa_squad-validation-3260", "mrqa_squad-validation-3312", "mrqa_squad-validation-3319", "mrqa_squad-validation-3440", "mrqa_squad-validation-3454", "mrqa_squad-validation-3524", "mrqa_squad-validation-3632", "mrqa_squad-validation-3716", "mrqa_squad-validation-3813", "mrqa_squad-validation-3862", "mrqa_squad-validation-3865", "mrqa_squad-validation-3918", "mrqa_squad-validation-3943", "mrqa_squad-validation-4010", "mrqa_squad-validation-4047", "mrqa_squad-validation-4075", "mrqa_squad-validation-4078", "mrqa_squad-validation-4083", "mrqa_squad-validation-4102", "mrqa_squad-validation-4175", "mrqa_squad-validation-4315", "mrqa_squad-validation-4429", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-457", "mrqa_squad-validation-4673", "mrqa_squad-validation-4706", "mrqa_squad-validation-4770", "mrqa_squad-validation-4775", "mrqa_squad-validation-4844", "mrqa_squad-validation-4973", "mrqa_squad-validation-498", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5023", "mrqa_squad-validation-5037", "mrqa_squad-validation-5102", "mrqa_squad-validation-5135", "mrqa_squad-validation-5178", "mrqa_squad-validation-5194", "mrqa_squad-validation-5213", "mrqa_squad-validation-5226", "mrqa_squad-validation-526", "mrqa_squad-validation-5486", "mrqa_squad-validation-549", "mrqa_squad-validation-5513", "mrqa_squad-validation-5581", "mrqa_squad-validation-5741", "mrqa_squad-validation-5784", "mrqa_squad-validation-5812", "mrqa_squad-validation-5863", "mrqa_squad-validation-5871", "mrqa_squad-validation-5876", "mrqa_squad-validation-5972", "mrqa_squad-validation-6029", "mrqa_squad-validation-6059", "mrqa_squad-validation-6080", "mrqa_squad-validation-6121", "mrqa_squad-validation-6154", "mrqa_squad-validation-6166", "mrqa_squad-validation-6177", "mrqa_squad-validation-6242", "mrqa_squad-validation-6430", "mrqa_squad-validation-6588", "mrqa_squad-validation-6598", "mrqa_squad-validation-6614", "mrqa_squad-validation-6676", "mrqa_squad-validation-6685", "mrqa_squad-validation-6694", "mrqa_squad-validation-6721", "mrqa_squad-validation-6741", "mrqa_squad-validation-6789", "mrqa_squad-validation-6789", "mrqa_squad-validation-6801", "mrqa_squad-validation-6875", "mrqa_squad-validation-6921", "mrqa_squad-validation-7135", "mrqa_squad-validation-7159", "mrqa_squad-validation-716", "mrqa_squad-validation-7173", "mrqa_squad-validation-7229", "mrqa_squad-validation-7273", "mrqa_squad-validation-7434", "mrqa_squad-validation-7458", "mrqa_squad-validation-7576", "mrqa_squad-validation-7596", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-7967", "mrqa_squad-validation-7981", "mrqa_squad-validation-80", "mrqa_squad-validation-8035", "mrqa_squad-validation-8151", "mrqa_squad-validation-8176", "mrqa_squad-validation-8343", "mrqa_squad-validation-8356", "mrqa_squad-validation-8397", "mrqa_squad-validation-8420", "mrqa_squad-validation-8439", "mrqa_squad-validation-8485", "mrqa_squad-validation-8503", "mrqa_squad-validation-855", "mrqa_squad-validation-855", "mrqa_squad-validation-8608", "mrqa_squad-validation-8616", "mrqa_squad-validation-8719", "mrqa_squad-validation-8733", "mrqa_squad-validation-880", "mrqa_squad-validation-880", "mrqa_squad-validation-8833", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-890", "mrqa_squad-validation-8914", "mrqa_squad-validation-8924", "mrqa_squad-validation-9020", "mrqa_squad-validation-9066", "mrqa_squad-validation-913", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9220", "mrqa_squad-validation-9237", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9299", "mrqa_squad-validation-9333", "mrqa_squad-validation-940", "mrqa_squad-validation-9406", "mrqa_squad-validation-9436", "mrqa_squad-validation-9470", "mrqa_squad-validation-9559", "mrqa_squad-validation-962", "mrqa_squad-validation-9665", "mrqa_squad-validation-9686", "mrqa_squad-validation-9752", "mrqa_squad-validation-9753", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_squad-validation-9931", "mrqa_squad-validation-9960", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2794", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-339", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4945", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6753", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7367", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-980"], "OKR": 0.8984375, "KG": 0.46953125, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "concurring, smaller assessments of special problems instead of the large scale approach", "jodie Foster", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "phylum", "phylum", "Ub Iwerks", "St Pauls", "holography", "Pelias", "Joshua Radin", "Northumbria", "Harvard", "cricket", "Ron Ridenhour", "quant pole", "copper and zinc", "Tigris", "Cordelia", "pamphlets, posters, ballads", "dermatitis", "four", "Rh\u00f4ne Grape Varietal", "Prophet Joseph Smith, Jr.", "Huntington Beach,", "gold", "moon", "a number", "jimmy", "The Virgin Spring", "Canada", "Winston Churchill", "Stockholm", "Peter Parker", "Golda Meir", "Salvatore Ferragamo,", "bullfight", "Sparks", "Ginger Rogers", "Plymouth Rock", "Comedy Playhouse", "citric", "Charles Darwin", "John Denver", "Miss Scarlet", "Marie Van Brittan Brown", "California", "1995", "Bourbon", "Taylor Swift", "\"Home\"", "\"The three gunshot wounds to the head included two nonfatal rounds with entry points below the chin, and one fatal shot that entered Peterson through the right side of the head,\"", "The Detroit, Michigan, radio station promotion held three years ago was like a class to help women \" learn how to dance and feel sexy,\"", "Amy Bishop Anderson,", "calathus", "the Louvre", "an American private, not-for-profit, coeducational research university"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5116397421037381}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.9523809523809523, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.1764705882352941, 0.4827586206896552, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-8618", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-82", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5832", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_naturalquestions-validation-1399", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.421875, "CSR": 0.5372596153846154, "EFR": 0.972972972972973, "Overall": 0.7209527676715177}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Day of the Doctor\"", "ranked third in a list of the 20th century,", "affordable housing", "Mao Zedong", "Verona", "Silverdome", "elephants", "a charcoal powered grill, stove or hot plate", "Frank McCourt", "jules Verne", "joseph smith", "seymour hersh", "Schengen Area", "\u201cA\u201d", "city of Sheffield, England", "Famous Players-Lasky Corporation", "the Monkees", "Gerald Durrell", "Jezebel", "Ireland", "jason", "Arabian", "Halifax", "Noises Off", "jason smith", "Frank Wilson", "first hit international headlines in 1975 when he led a commando raid in Vienna on an Opec oil cartel meeting,", "Edwina Currie", "st Moritz", "jonathan smith", "1768", "\u201cFor Gallantry;\u201d", "full of woe", "england", "attor for a more centrally located capital.", "apartment", "Tahrir Square", "osmium", "d'Artagnan", "27", "Jack Ruby", "Tintoretto", "Eric Coates", "Saudi Arabia", "arson", "Thailand", "Sydney", "a dove", "Tunisia", "Prince Philip", "canterbury", "Tokyo", "Edgar Lungu", "49 cents", "over 100 beats per minute", "672", "\"Linda McCartney's Life in Photography\"", "Franconia, New Hampshire, United States", "Twitter", "Juan Martin Del Potro.", "27,", "man", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5763020833333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7774", "mrqa_squad-validation-8026", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-733", "mrqa_newsqa-validation-279", "mrqa_searchqa-validation-12829"], "SR": 0.53125, "CSR": 0.537037037037037, "EFR": 0.9666666666666667, "Overall": 0.7196469907407408}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "70", "eventually forced Tesla out leaving him penniless.", "Benazir Bhutto,", "nuclear program.", "Awa", "Larry King", "Rod Blagojevich", "acid attack", "Wally", "2008", "after Wood went missing off Catalina Island,", "Fakih", "Afghanistan.", "Everglades", "closed on 366 for eight wickets on the opening day.", "1950s", "64,", "parliament speaker", "27-year-old", "anarchists", "$4.5 million", "baggage from the 80s", "greenhouse emissions", "oaxaca", "Orbiting Carbon Observatory", "Switzerland", "Kenneth Cole", "Janet and La Toya", "11.4 million orphans", "hours", "combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "injuries,", "al-Shabaab", "booked on an outstanding arrest warrant relating to a domestic violence case,", "sustain future exploration of the moon and beyond.", "his business dealings for possible securities violations", "Opryland", "Number Ones", "attempting illegal crossings into U.S. waters.", "he was diagnosed with skin cancer.", "al Qaeda,", "jeremy Obama", "\"gotten the balance right\"", "oceans", "\"scream and howl in pain\"", "doctors", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "1932", "between 1923 and 1925", "gilda", "jeremy wilson", "table tennis", "Tamil", "stephen armson", "Indianola", "the Empire State Building", "Disraeli", "sun"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6002500971250971}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.923076923076923, 0.4, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.5, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.6, 0.0, 1.0, 0.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_squad-validation-1302", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816"], "SR": 0.46875, "CSR": 0.5345982142857143, "EFR": 1.0, "Overall": 0.7258258928571429}, {"timecode": 28, "before_eval_results": {"predictions": ["a hybrid Bermuda 419 turf", "25-foot", "symbols", "Hyundai Steel's", "Monday night", "Florida to Colorado", "journalists and the flight crew will be freed,", "40", "brutalized by the Catholic Church", "in a public housing project", "toxic smoke from burn pits", "Lucky Dube,", "two Israeli soldiers,", "space shuttle Discovery", "World-renowned security expert Gavin de Becker", "a nuclear weapon", "Japan", "polygamist", "in the Intertropical Convergence Zone", "simple puzzle video game,", "outside influences in next month's run-off election,", "aid to Gaza,", "rolled over Tuesday near Campbellton, Texas, killing two people and injuring more than a dozen,", "suppress the memories and to live as normal a life", "February 2008", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the helicopter went down in Talbiya,", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England", "Cash for Clunkers program", "\"project work\"", "One of the most influential, powerful and admired public figures of our time,", "80 percent of the woman's face", "London's", "organizing the distribution of wheelchair,", "Ozzy Osbourne", "$199", "Australian officials", "the iconic Hollywood headquarters of Capitol Records,", "Dr. Jennifer Arnold and husband Bill Klein,", "gun", "At least 38", "Argentine", "the underprivileged.", "Somalia's piracy problem was fueled by environmental and political events.", "\"17 Again,\"", "Kabul", "22", "Steven Gerrard", "12.3 million", "two Interior Ministry officials said that a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.M.-backed Iraqi forces and gunmen.", "Rima Fakih", "Old Trafford", "help bring creative projects to life ''", "eight episodes of the first season of NCIS", "Mary Elizabeth Patterson", "West Side Story", "The Fifth Amendment", "Nepal", "Merck & Co", "Fort Albany", "Knoxville, Tennessee", "grey", "Transpiration", "psychosis"], "metric_results": {"EM": 0.40625, "QA-F1": 0.55949461996337}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.25, 0.0, 0.0, 0.0, 0.3333333333333333, 0.4799999999999999, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8205128205128205, 0.6666666666666666, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1178", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2380", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-9595", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-79", "mrqa_searchqa-validation-10091", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.40625, "CSR": 0.5301724137931034, "EFR": 0.9736842105263158, "Overall": 0.7196775748638838}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "oxygen", "Betty Meggers", "ancient cult activity", "primarily in Polk County, Florida", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Russian army", "near the end of their main sequence lifetime", "August 6", "Doug Diemoz", "Virginia", "Monk's Caf\u00e9", "central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "a computer maintenance utility included in Microsoft Windows designed to free up disk space", "July 4", "pick yourself up and dust yourself off and keep going '", "Anthony Caruso as Johnny Rivers", "sailing ships to cross the world's oceans for centuries", "October 12, 1979", "Lorazepam", "2013 non-fiction book of the same name by David Finkel", "a Cadillac", "Brenda ''", "a ranking used in combat sports,", "Husrev Pasha", "Stephanie Judith Tanner", "ulnar collateral ligament of elbow joint", "Robin Williams", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "Abu Bakr, Umar, Uthman ibn Affan, and Ali of the Rashidun Caliphate", "Lake Powell", "a decorative ornament", "September 6, 2019", "two", "a substitute good", "Archie Marries Veronica", "over 74", "1987", "fingers on either side of the mouth", "October 2000", "New York City", "Prafulla Chandra Ghosh of the Indian National Congress", "the United States economy first went into an economic recession", "in sequence with each heartbeat", "Hermann Ebbinghaus", "Marvin Gaye", "people in the 20th century who used obscure languages as a means of secret communication during wartime", "\"Donny Osmond\"", "Carthage", "\"George Herbert Walker Bush", "Gesellschaft", "7.63\u00d725mm Mauser", "seven", "Muslim", "helicopters and boats, as well as vessels from other agencies,", "Saturday", "Rickey Henderson", "Lake Baikal", "White Castle"], "metric_results": {"EM": 0.375, "QA-F1": 0.5275137239552562}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1111111111111111, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8387096774193548, 0.8, 0.7142857142857143, 0.5714285714285715, 0.15999999999999998, 0.4, 0.0, 0.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.0, 0.25, 1.0, 0.5, 0.5, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.19354838709677422, 1.0, 0.5, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3937", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2194", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-10459", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.375, "CSR": 0.525, "EFR": 0.925, "Overall": 0.7089062500000001}, {"timecode": 30, "before_eval_results": {"predictions": ["a setup phase in each involved node before any packet is transferred to establish the parameters of communication", "Pleurobrachia", "1953", "AT&T", "North Carolina", "the Delaware Chingachgook,", "shoes", "nine", "Rashid Akmaev,", "acetylene", "\"being obscure\"", "cereal", "gray fox", "a rose by any other name", "Winston Rodney", "\"shovel\"", "Nanjing", "Montana", "walker", "the Sun King", "GILBERT & SullIVAN", "Barnes & Noble", "Julius Caesar", "Joe Lieberman", "the Boston Marathon", "fibreboard", "tin", "The Song of Norway", "Frida Kahlo", "walker", "\"Y\" 2 \"K\": An Eskimo", "The Hustler", "Hair the Musical", "William Randolph Hearst", "pumice", "ale, lager, stout, fruit beer, cider, perry including organic beer", "shrews", "first woman telephone operator", "\"Year 3000\"", "walker", "The New Colossus", "yelped in pain when the bee stung.", "walker", "Sarah, Duchess of York", "The Graduate", "\"Tom Terrific\"", "bronchoconstriction", "Four", "Neon and Argon Glow Lamps", "Upper Red Lake", "Le Mans", "walker", "Neil Patrick Harris", "Wyatt", "2001", "vitamin D", "three", "al Alberto juantorena", "Boyz II Men", "Awake", "Doctor of Philosophy", "Pakistan", "in Seoul.", "Elena Kagan"], "metric_results": {"EM": 0.328125, "QA-F1": 0.38960077751196176}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [0.052631578947368425, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-11550", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-7137", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-3715", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-3579", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-14012", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-8480", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-8290", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-282", "mrqa_triviaqa-validation-6657", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-154"], "SR": 0.328125, "CSR": 0.5186491935483871, "EFR": 1.0, "Overall": 0.7226360887096774}, {"timecode": 31, "before_eval_results": {"predictions": ["traditional Mongol shamans", "Prospect Park", "the iris", "the volume", "a squint", "Joseph Conrad", "Credit Corporation", "Christian Dior", "Pittsburgh", "Romeo", "Notre Dame", "Table cloth", "(Peter) Piper", "Bligh", "British South Africa", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "Pennsylvania", "Mike Huckabee", "Queen", "one of these", "a brass Monopoly plate", "mulberry", "Edmund Hillary", "Samuel Beckett", "Rachel Carson", "Vietnam", "sports", "David Geffen", "Franklin D. Roosevelt", "Prince William", "Betty Suarez", "an R", "a horse with a predominantly white face.", "New Jersey", "the Elbow River", "Matthew Perry", "Baltimore", "John Ford", "kismet", "Willy Wonka", "heavy", "aluminum", "Matthew Brady", "Ned Kelly", "a piles of papers", "gravity", "Isis", "a quiver", "Heroes", "placed in the ark of the covenant ; and Deuteronomy 5", "depending on the source of the donor organ", "seven", "Planck", "Rocky Marciano", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "Chelsea Peretti", "two years,", "Lee Probert", "Tuesday"], "metric_results": {"EM": 0.546875, "QA-F1": 0.63359375}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8204", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-8269", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-12813", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-9411", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-16714", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_hotpotqa-validation-513", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-2123"], "SR": 0.546875, "CSR": 0.51953125, "EFR": 1.0, "Overall": 0.7228125000000001}, {"timecode": 32, "before_eval_results": {"predictions": ["the weight of the air that rushed back in", "Fresno Street and Thorne Ave", "the Black Death", "Elton", "John Stuart Mill", "Oblivion", "the CIA", "piano", "Rickey Henderson", "Sirimavo Bandaranaike", "in the Pigment Power of Vegetables", "John Grunsfeld", "Angkor Wat", "1976", "Galileo Descartes", "a quark", "Dust jacket", "Rudy Giuliani", "the Constitution", "Virginia", "Thor", "New Jersey", "The Omega Man", "a walk-in pantry", "the barrel", "the Summer Olympics", "Hugo Chvez", "Shamir", "Hinduism", "tin", "Diana", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Neil Jordan", "Tiger Woods", "Los Angeles", "Astraeus", "Richard III", "the Labour Party", "the pen", "Mexico", "Douglas Adams", "A Doll\\'s House", "Hawaii", "Stephen Crane", "France", "Sophocles", "Mark Cuban", "the FBI", "a bust", "Central Park", "The Queen of Hearts", "Deathly Hallows", "Florida and into the town of Coconut Cove, where his classmate Dana Matherson starts bullying him", "an aeoline", "trumpet", "Mel Gibson", "2.1 million", "Edward James Olmos", "Lynyrd Skynyrd", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.609375, "QA-F1": 0.71007241114149}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.7368421052631579, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.6363636363636364, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-14835", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-4767"], "SR": 0.609375, "CSR": 0.5222537878787878, "EFR": 1.0, "Overall": 0.7233570075757576}, {"timecode": 33, "before_eval_results": {"predictions": ["the BBC on VHS, on MP3 CD-ROM, and as special features on DVD", "immunity and the self/nonself vocabulary", "a large concrete block is next to his shoulder, with shattered pieces of it around him.", "approximately 35 million", "28", "back at work", "Oxbow,", "the Emerson Police Department's tip line", "opium", "\"I think she's wacko.\"", "Saturday,", "Hussein's Revolutionary Command Council", "a self-interest in supporting Afghan forces in destroying drug labs, markets and convoys,", "the Dalai Lama", "Myanmar at a demonstration in New Delhi, India", "the station", "protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "Conway, Arkansas,", "fuel economy", "the environmental efforts", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers", "different women coping with breast cancer in", "the country only intends short-range missiles can be \"rolled out on a dime,\"", "Police", "a cancer-causing toxic chemical.", "(Roger) Federer", "Brooklyn, New York,", "over 1000 square meters in forward deck space,", "(CNN)", "no chance", "(Baby) Joseph", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "in the past year", "two", "a portrait", "Symbionese Liberation Army", "the fracas in a nightclub bar in the north-western of England city", "two tickets to Italy on Expedia.", "Colombia", "a softer violet hue", "the resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka, seeking a win to level the series at 1-1,", "the U.N.", "$40 and a bread.", "Kgalema Motlanthe,", "the Ming dynasty", "George II ( George Augustus ; German : Georg II. August ; 30 October / 9 November 1683 -- 25 October 1760 )", "2014 -- 15", "1919", "Javier Bardem", "Scotland", "Bremen, Germany in a family of Portuguese descent", "Terry the Tomboy", "Araminta Ross", "Mrs. Potts", "Peanuts Chocolate Candies", "the country"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5841904623154623}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.6666666666666666, 1.0, 0.0, 0.0, 0.25, 1.0, 0.18181818181818182, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.7499999999999999, 0.4444444444444445, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7682", "mrqa_squad-validation-6585", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2192", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1379", "mrqa_naturalquestions-validation-7108", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.484375, "CSR": 0.5211397058823529, "EFR": 1.0, "Overall": 0.7231341911764706}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "taking it well.", "nearly $2 billion in stimulus funds", "Yemen,", "bankruptcy", "nearly $2 billion in stimulus funds", "is a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spanish Davis Cup hero Fernando Verdasco,", "Kenya", "children of street cleaners and firefighters.", "Rivers", "$3 billion", "hardship for terminally ill patients and their caregivers", "Honduran", "Brazil", "three different videos", "strife in Somalia", "Roy", "WBO welterweight title", "relatives of the five suspects,", "Meredith Kercher", "three months before the crimes \"had the answers in front of her that clearly marks all the symptoms of acute stress disorder or post- traumatic stress disorder.\"", "Alicia Keys", "a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "Monday.", "a lump in Henry's nether regions was a cancerous tumor.", "20", "Matthew Fisher", "$1.5 million.", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "a model of sustainability.", "glamour and hedonism", "a dress from an American designer.", "Department of Homeland Security Secretary Janet Napolitano", "543", "patients who underwent a near-total face transplant", "he is protecting him and is with him,", "Israel", "rural Tennessee.", "in critical condition", "Seoul", "Nicole", "her mom would always convince her that she was going to be on the Olympic medals podium.", "next week.", "Ryan Seacrest", "inspectors general's", "early detection and helping other women cope with the disease.", "James Whitehouse,", "hopes the journalists and the flight crew will be freed,", "gentry Buddhism", "Geoffrey Dyson Palmer", "Stephen Lang", "Dick Van Dyke", "Noreg", "beer", "Revengers Tragedy", "1754", "Black Elk Speaks", "Rye, New York", "the hippopotamus", "St Paul"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5765151515151515}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.5, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.0, 0.3636363636363636, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-4378", "mrqa_searchqa-validation-16463", "mrqa_searchqa-validation-7879"], "SR": 0.46875, "CSR": 0.5196428571428571, "EFR": 1.0, "Overall": 0.7228348214285715}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts which they returned to Earth.", "Border Reiver", "July 4,", "wine", "nantucket", "an Islamic leadership position", "sap", "Malibu", "Sisyphus", "a unit of acoustic absorption", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "crabs", "a drunken monster", "Purple", "the Black Sea", "The Battle of the Little Bighorn", "the Shakers", "a bellwether", "The Information Philosopher", "potato chips", "Boxer", "The Spiderwick Chronicles", "Florence Mabel Harding", "Las Vegas", "Acting Out the Bible", "the Rose Bowl", "Norman Rockwell", "Her clothing", "a tunacanned light tunais", "Napa Valley", "Eurail France", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "Saturday Night Fever", "12 men", "Nancy Pelosi", "the Art of French Cooking", "Jupiter", "Sadat", "National Ice Cream Day", "Mary Shelley", "50 million", "Lionfish", "Charlie Sheen", "Baby Boys", "Bonnie Aarons", "Wednesday, 5 September 1666", "pop ballad", "Seth", "Lou Gehrig", "nine", "1949", "Aamir Khan", "My Gorgeous Life", "Buenos Aires", "High Court Judge Justice Davis", "fluoroquinolones"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6628063725490196}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12788", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-1807"], "SR": 0.59375, "CSR": 0.5217013888888888, "EFR": 1.0, "Overall": 0.7232465277777778}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "Virginia", "nothing gained", "gold", "Supernanny", "Atlantic", "Cincinnati", "mosque", "(Henry) Hudson", "a Peashooter", "anhydride", "Elihu Root", "Entourage", "an eel", "Philadelphia", "The Museum of Modern Art", "the Unicorn", "John C. Frmont", "Russia", "Peabodys", "Hermann Hesse", "the Taj Mahal", "Harry", "Carmen", "Margaret Mitchell", "(Cla) Frollo", "Mark Knopfler", "( Troilus)", "primary", "(Burt) Reynolds", "Sphinx", "Louis Armstrong", "Saudi Arabia", "(Jackie) Harrison", "Arby\\'s Restaurant Group", "coffee", "(Jackie) Pershing", "(Robert) Burns", "The Hulk", "Winnipeg", "Memphis Belle", "Burkina Faso", "the Central Pacific", "solicitor general", "(Island)", "a bison", "(Jackie) Lewan", "Edith Piaf", "Ivan", "a prologue", "clay", "Anthony Mayfield", "Jack Gleeson", "(Phil) Hurtt", "animals", "Massachusetts", "Star", "W. P. Lipscomb", "2009", "Democratic", "meteorologist", "$104,327,006", "\"Hairspray,\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-11899", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-3131", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-9131", "mrqa_searchqa-validation-4107", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2000", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-1525"], "SR": 0.59375, "CSR": 0.5236486486486487, "EFR": 1.0, "Overall": 0.7236359797297298}, {"timecode": 37, "before_eval_results": {"predictions": ["Switzerland", "Impressionists", "John Y. Brown Jr.", "oats", "Mitt Romney", "Ivan the Terrible", "Sally Field", "Charles A. Lindbergh", "Eritrea", "pi", "tin", "Lake Pontchartrain", "Clark", "\"w\"", "Marriott", "France", "Canada", "The Secret", "the gold rush", "collagen", "China", "a compound", "the cranes", "Jesse James", "Alzheimer", "the Rio Grande", "Stephen", "Euclid", "Eva Peron", "Cain", "Lou Grant", "X-Men", "the Louvre", "chinook", "Prison Break", "one", "Maine", "one", "Meg", "the Sonnet", "deuce in tennis", "Hans Christian Andersen", "Bogdanovich", "Barry White", "Jesus Christ", "\"Britain redeemed", "the Quaternary Period", "nolo contendere", "Junior Walker", "Czech Republic", "the Chicken of the Sea", "the NIRA", "Mark McCain", "beta decay", "France", "Priam", "Mariette", "Ike Barinholtz", "\"Milk\"", "Australian", "the sins of the members of the church", "$52.4 million", "\"17 Again,\"", "Bardstown"], "metric_results": {"EM": 0.5625, "QA-F1": 0.621875}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-6358", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_naturalquestions-validation-2668", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_newsqa-validation-3646", "mrqa_hotpotqa-validation-5774"], "SR": 0.5625, "CSR": 0.524671052631579, "EFR": 1.0, "Overall": 0.7238404605263158}, {"timecode": 38, "before_eval_results": {"predictions": ["tuition fees", "Holden Caulfield", "Wild Bill Hickok", "Leptospira", "a recession", "a mermaid", "Jay Silverheels", "Singapore", "the M1 Abrams", "a drumstick", "a canoe", "Pineapple Marshall", "Witness", "Martha Tabram", "3800", "Rene Auberjonois", "non-celled", "Spain", "the brain", "non-Gnther Ltjens", "Macbeth", "non-spirited", "Mary Poppins", "non-hitter", "The Fresh Prince of Bel-Air", "non-Nod", "watermelon", "a non-sounding 'don't throw the baby out with the bathwater phrase", "a wedding", "Livin\\'s", "non-nemesis", "Pineapple", "Marie Antoinette", "Ford", "non-Fiction", "Roger B. Taney", "non-union", "German", "Katamari Damacy", "Bill Murray", "Margaret Thatcher", "Harry Potter and the Philosopher Stone", "Pine Manganese", "non-successional forest", "Olympia", "Tommy Allsup", "Doctor Zhivago", "Brazil", "British Columbia", "Sydney Pollack", "non-solid congealed loaf", "Oona Castilla Chaplin", "September 24, 2017", "John Cooper Clarke", "the different levels of importance of human psychological and physical needs", "one", "Fraser Island", "the Wright brothers", "jurisdiction", "Sam Pumpkin", "the film was shown as part of the festival\\'s special screening program.", "voluntary depletion", "the need for reconciliation in a country that endured a brutal civil war", "non-marital"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4607682291666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.16, 0.5, 0.06250000000000001, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-3282", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-6033", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-7477", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-4043", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-402", "mrqa_naturalquestions-validation-6347", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-2005", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-600", "mrqa_triviaqa-validation-2744"], "SR": 0.390625, "CSR": 0.5212339743589743, "EFR": 1.0, "Overall": 0.7231530448717949}, {"timecode": 39, "before_eval_results": {"predictions": ["South America", "\"Boogie Woogie Bugle Boy\"", "Europe", "Charlton Heston", "Glory", "Sweeney Todd", "The Bridge on the River Kwai", "the Fall of Constantinople", "Independence in a marriage", "President Jefferson", "Ezra Pound", "the tributaries", "a toothpick", "California", "Dixie", "the Vietcong", "Stanley Winck", "engrave", "Shue", "Francis Crick", "Jay and Silent Bob", "Heath", "South Ossetia", "Twelfth Night", "South Dakota", "a key", "Tito", "a conformation", "Ratatouille", "circadian rhythms", "Calvin Coolidge", "Mark Cuban", "Rudy Giuliani", "eyes", "Tony Dungy", "the Danube", "Andrew Johnson", "26.2", "life", "precipitation", "chess", "GIGO", "Johannes Brahms", "Charleston Southern", "Italian", "The Grapes of Wrath", "a bicentennial", "Byzantium", "Mayo", "Led Zeppelin", "a Tesla coil", "Schleswig - Holstein", "Anna Murphy", "March 15, 1945", "Charles Darwin", "Old Trafford", "Spider-Man", "Honey Irani", "Disneyland", "Kalahari", "Rihanna", "Bob Dole,", "Ben Kingsley", "managing his time"], "metric_results": {"EM": 0.5, "QA-F1": 0.5602678571428572}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4408", "mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-3796", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-15687", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-4073"], "SR": 0.5, "CSR": 0.520703125, "EFR": 1.0, "Overall": 0.723046875}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Freiburg", "James Weldon Johnson", "a 2003 South Korean horror film", "Oakdale", "Missouri", "the FAI Junior Cup", "Flaw", "alt-right", "The Drudge Report", "15,000", "yellow fever", "a cappella singing group", "1934", "a record of 13\u20133, and the NFC West Championship", "We Need a Little Christmas", "a lion population in the Tsavo East National Park, Kenya", "the New York Islanders", "1345 to 1377", "nearly 80 years", "Jean Acker", "the Championship", "the Gettysburg Address", "its most awarded female act of all-time", "(Jefferson) Rashford", "Stravinsky\\'s \"The Rite of Spring\"", "1", "over 26,000", "Kristin Scott Thomas", "Edwin Mah Lee", "1958", "1993", "American burlesque", "Afro-Russian", "Loretta Lynn", "England", "the Boeing B-17 Flying Fortress", "1994\u201395", "11", "the 2007 Summer Universiade", "2012", "Medgar Wiley Evers", "Kansas City", "1999", "Pinellas County", "beer", "London", "the B-17 Flying Fortress bomber", "Mindy Kaling", "1988", "Leon Uris", "Erika Mitchell Leonard", "Mason Alan Dinehart", "Golde", "Sir Tom Finney", "Cameroon", "its responsibility to assure that all collection instruments and environments are sterile and of first use", "by military personnel to hazardous materials", "two", "Iggy Pop formed a blues band called the Prime Movers.", "the Duke has attempted to persuade Shylock to spare Antonio", "The Mayor of Casterbridge", "(Jefferson) DiCaprio", "'go f * * k yourself '"], "metric_results": {"EM": 0.5, "QA-F1": 0.6634521677260647}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6, 0.25, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1749", "mrqa_hotpotqa-validation-2352", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-3027", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-1786", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-13997", "mrqa_naturalquestions-validation-6326"], "SR": 0.5, "CSR": 0.5201981707317074, "EFR": 1.0, "Overall": 0.7229458841463415}, {"timecode": 41, "before_eval_results": {"predictions": ["a sack", "10", "a minivan ran a red light and struck two vehicles at an intersection", "Les Bleus", "in 2005.", "4,000", "The Valley Swim Club", "an angry mob.", "normal maritime", "Sri Lanka", "death", "an average of 25 percent", "fatally shooting a limo driver", "The Al Nisr Al Saudi", "as", "piano", "$250,000", "a \"prostitute\"", "the mammoth", "tax", "Brazil", "acute stress disorder in Iraq", "Russia and China", "Facebook and Google,", "a facility in Salt Lake City, Utah,", "Manmohan Singh", "Haiti", "in his sleep at his Windsor, Ontario, home,", "Pakistan", "23 years", "an injury.", "Tim Cahill", "an open window that fits neatly around him", "Leo Frank", "Paul McCartney and Ringo Starr", "it has witnessed only normal maritime traffic around Haiti,", "President Robert Mugabe intends to rig", "don't have to visit laundromats", "13", "Susan Boyle", "on Sunday", "\"He hears what I'm saying, but there's just no coming through,\"", "\"Twilight\"", "forgery and flying without a valid license", "11,", "A third beluga whale belonging to the world\\'s largest aquarium has died,", "Fayetteville, North Carolina,", "Suwardi,", "30,000 additional U.S. troops", "Hillary Clinton", "Rihanna", "radius R of the turntable", "deoxygenated blood from the right side of the heart to the lungs", "54 Mbit / s, plus error correction code", "Janet Royall", "the B-24 Bomber Crash Landings", "cereal", "England", "Victoria", "Guillermo del Toro", "Wall Street", "Monty Python and the Holy Grail", "King of Sweden", "FMCSA"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5705962873931625}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.9523809523809523, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.625, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6153846153846153, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-6603", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-403", "mrqa_searchqa-validation-2346", "mrqa_searchqa-validation-1519", "mrqa_searchqa-validation-10945"], "SR": 0.40625, "CSR": 0.5174851190476191, "EFR": 1.0, "Overall": 0.7224032738095238}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Arizona", "Zimbabwe", "Italian Serie A", "Darrel Mohler", "showing her dancing against a stripper's pole.", "the \" Michoacan Family,\"", "WTA Tour titles", "MDC head Morgan Tsvangirai.", "42 years old", "taking on the swords of the Taliban.", "\"bleaching\"", "80 percent", "1979", "\"Follow the Sun,\"", "Elena Kagan", "CBS, CNN, Fox and The Associated Press.", "an auxiliary lock", "1-1", "\"underwear bomber\" Umar Farouk AbdulMutallab", "Myanmar", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides,", "his business dealings", "\"Fourteen\" gunmen", "poems telling of the pain and suffering of children", "the program was made with the parents' full consent.", "the Democratic VP candidate", "The International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "Moscow", "debris", "\"Can I just say how pleased I am with today's verdict,\"", "capital murder and three counts of attempted murder", "Basel", "17", "a Daytime Emmy Lifetime Achievement Award.", "state senators", "31 meters (102 feet)", "its nude beaches.", "\"Five\" tells stories of different women coping with breast cancer", "Florida girl", "shark River Park in Monmouth County", "three", "Islamabad", "partying", "Capitol Hill,", "Israel does not have a nuclear weapon but does have enough low-enriched uranium", "1940's", "March 22,", "think are the best.", "in the Mediterranean Sea.", "\"Antichrist\"", "World War II", "Thomas Jefferson", "Jeff East ( born October 27, 1957 )", "Orion the Hunter", "terra rosa", "Selfie", "2002", "England", "Los Alamos National Laboratory", "Rat", "rain", "Crawford", "pyrenees"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5829890289449112}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.5000000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.4, 0.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3073", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-932", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-800", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5989", "mrqa_triviaqa-validation-1492", "mrqa_triviaqa-validation-1225", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.453125, "CSR": 0.5159883720930232, "EFR": 1.0, "Overall": 0.7221039244186047}, {"timecode": 43, "before_eval_results": {"predictions": ["the north,", "the legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees", "33-year-old", "cast doubt on Woodward's assertion Tuesday in a conversation with \"American Morning\" host John Roberts.", "hardship for terminally ill patients and their caregivers,", "Jaime Andrade", "Zac Efron", "finance", "$2 billion", "pesos", "After the war,", "The station", "Krishna Rajaram", "The Arkansas weatherman", "Robert Mugabe", "the governor's", "in the elements in Southern California,", "Saturday.", "$1.5 million", "authorities cracked down on journalists after protests surfaced when President Mahmoud Ahmadinejad was declared the winner in the June 12 presidential election,", "the report should spur U.S. diplomacy to prevent Iran from developing nuclear weapons", "the fact that the teens were charged as adults.", "death squad killings", "Sonia Sotomayor", "Hyundai", "100 percent", "Saturday", "in Afghanistan,", "prisoners at the South Dakota State Penitentiary", "seven", "200", "declined its support after the September 11, 2001,", "the Seminole Tribe", "a Muslim with Lebanese heritage,", "in a tide of migrants washing up in South Africa.", "Barack Obama", "declined at the switch,", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "the radical Islamist group Jund Ansar Allah,", "1,500", "most of those who managed to survive the incident hid in a boiler room and storage closets", "$50", "$60 billion", "a new gene,", "Malayalam", "Mad - Eye Moody and Hedwig", "1960 Summer Olympics in Rome", "Aston Lower Grounds", "peasant", "pool", "1822", "The Dressmaker", "Anandapala", "sugar", "a buffalo", "ruby slippers", "the parietal"], "metric_results": {"EM": 0.46875, "QA-F1": 0.576065340909091}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.07142857142857144, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.7499999999999999, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-228", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8741", "mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-2281"], "SR": 0.46875, "CSR": 0.5149147727272727, "EFR": 0.9705882352941176, "Overall": 0.7160068516042781}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf", "Los Angeles", "Chris Eubank Jr.", "Florida", "Benj Pasek and Paul,", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Araminta Ross", "Roger Staubach", "1944", "Highlands Course", "Franconia, New Hampshire", "Operation Watchtower", "Dan Crow", "\"War & Peace\"", "Amberley", "a popular nursery rhyme dating from the early 19th century", "Berea College", "the Bears", "Luca Guadagnino", "Charmian Carr", "Germany and other parts of Central Europe", "the New York Islanders", "Todd Phillips", "26,788", "the Troubles", "1967", "Marktown", "jus sanguinis", "Radcliffe College", "James A. Garfield", "Ford", "weighed against the feather of truth", "India", "Lutheranism", "\"Heathers\"", "25 million", "The Snowman", "Ella Jane Fitzgerald", "Chris Claremont", "Rain Man", "Interscope Records", "Robert Grosvenor", "3,672", "the most influential private citizen in the America of his day", "I'm Shipping Up to Boston", "Switzerland", "American singer and \"Britain's Got Talent\" winner Jai McDowall", "\"Central '' or `` middle '', and gu\u00f3 ( \u570b / \u56fd )", "the mainland of the Australian continent, the island of Tasmania and numerous smaller islands", "the beginning of the American colonies", "Nicola Adams", "\"bay of geese,\"", "Russia", "shows the world that you love the environment and hate using fuel,\"", "Steven Green", "in a hotel,", "Chaucer", "rattlesnake", "Riddles", "healthy, wealthy, and wise"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6518303571428572}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.56, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4454", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-1044", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-13986"], "SR": 0.5625, "CSR": 0.5159722222222223, "EFR": 1.0, "Overall": 0.7221006944444446}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "Indian Ocean waters near the Gulf of Aden,", "30", "crocodile eggs", "Colorado prosecutor", "Polis", "on Saturday.", "Haiti has been providing virtually nonstop reports about the devastation from Tuesday's earthquake and tracking down information on others serving there.", "in July for A Country Christmas,", "sniff out cell phones.", "near Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "women who complained about his conduct.", "\"17 Again,\"", "the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future,", "Premier League club side Wigan Athletic in northern England.", "Mitt Romney", "two years ago.", "the Magna Carta", "\"Nude, Green Leaves and Bust,\" or \"Nu au Plateau de Sculpteur,\"", "low-calorie meals", "Heshmatollah Attarzadeh", "environmental", "the Middle East and North Africa,", "Nine out of 10 children", "police", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "the jaws of a crocodile", "the Olympic medal", "wounded more than 200.", "Congress", "Susan Boyle", "military ID cards,", "Phillip A. Myers.", "Obama's", "Gyanendra,", "the cause of the child's death will be listed as homicide by undetermined means,", "Casey Anthony,", "officers at a Texas  airport", "10 municipal police officers", "UNICEF", "the couple's surrogate", "228", "Kerstin and two of her brothers,", "2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Joan Rivers", "women in jeans wearing very high heels and a short puffy jacket.", "Jacob Zuma,", "in the Oaxacan countryside of southern Mexico", "Arsene Wenger", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "the eldest", "the enkuklios paideia or `` education in a circle ''", "Enid Blyton Books For Children", "Johnny Mathis", "Grazer,", "Champion Jockey", "Luca Guadagnino", "So Fresh, So Clean", "the Caged Bird Sings", "the shortest month of the year", "1.5 ounces", "a Bristol Box Kite"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5508388874197698}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.09523809523809523, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.7692307692307693, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 0.11764705882352942, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.09523809523809523, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5640", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.453125, "CSR": 0.5146059782608696, "EFR": 1.0, "Overall": 0.7218274456521739}, {"timecode": 46, "before_eval_results": {"predictions": ["\"spectacular\"", "\"We need a president who understands the world today, the future we seek and the change we need.", "Nirvana", "\"Dancing With the Stars\"", "without bail and will be arraigned June 25,", "12.3 million", "Mexico", "United", "Michael Arrington,", "Brett Cummins,", "Indian Army", "Saturday", "Nicole", "the legitimacy of that race.", "Adidas", "Dennis Davern,", "Africa.", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "get better skin, burn fat and boost her energy.", "Chinese", "Newcastle", "\"Nothing But Love\"", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "June 6, 1944,", "the Middle East and North Africa,", "2-1", "October 19,", "\"It was a wrong thing to say,", "Seoul,", "promotes fuel economy and safety while boosted the economy.", "ALS6,", "eight", "Siri.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback Forestry", "the children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "a U.S. helicopter crashed in northeastern Baghdad as", "attempting illegal crossings", "American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.", "38,", "Her husband and attorney, James Whitehouse,", "\"Toward Excellence with Equity: An emerging vision for closing the achievement gap,\"", "five", "a time-lapse video showing some of the most gigantic pumpkins in the world,", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel", "Discworld", "Japan", "fox hunting", "Dobbs Ferry, New York", "travel", "16,116", "\"Juno\"", "a sugar base, natural or artificial", "a bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7576035076035077}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1764", "mrqa_hotpotqa-validation-2136", "mrqa_hotpotqa-validation-2280", "mrqa_searchqa-validation-11573"], "SR": 0.703125, "CSR": 0.5186170212765957, "EFR": 0.9473684210526315, "Overall": 0.7121033384658455}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Airlines", "A Rush of Blood", "5", "Wilmette, Illinois", "The Wind's Twelve Quarters", "Child actor", "Dennis Kux", "drawing the name out of a hat", "Chris DeStefano", "Hero Indian Super League", "two or three", "Nimbus School of Recording Arts", "fascinating Winkleman", "a pale body and relatively darker extremities,", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1946 and 1947", "5,112", "1979", "retail, office and residential", "14,677", "6'5\" and 190 pounds", "Mickey Gilley", "relations", "German shepherd", "Mexican", "December 24, 1973", "1933", "the backside", "Ulver and the Troms\u00f8 Chamber Orchestra", "1730", "London", "the Salzburg Festival", "Mississippi", "India", "1959", "Francisco P. Felix,", "Steve Buscemi", "oratorio", "Bunker Hill", "lion", "the Royal", "World War II", "Knoxville", "Three's Company", "Dessa, Cecil Otter, P.O.S, Sims, Mike Mictlan, Paper Tiger, and Lazerbeak", "Labour", "TASCHEN", "Erich Maria Remarque", "September 14, 2008", "79", "Buffalo Bill", "Romania", "the James Gang and Deep Purple", "Mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "cloaks", "Salans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5640252976190476}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-1874", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-3675", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-4643", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5435", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-5531", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-10434", "mrqa_triviaqa-validation-2701"], "SR": 0.453125, "CSR": 0.5172526041666667, "EFR": 1.0, "Overall": 0.7223567708333334}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "Helsinki", "gari", "a offensive", "Vulcan", "mating rituals", "Fawn Hall", "waive", "Wanda", "The Barnum & Bailey Circus", "Johnny Weissmuller", "negative electrode", "a torque wrench", "gold", "Marlon Brando", "m.H.G. Middle High German", "impressionism", "Kentucky Wildcats men's basketball", "ruddy liar", "Brussels", "Macbeth", "General Lee", "piracy", "Dostoevsky", "Martin Luther", "Clue", "London", "German", "Abraham Lincoln", "seven years", "Mike Connors", "Tarzan of the Apes", "Senator Obama", "sancire", "Corpus Christi", "power and influence in Africa", "the eider-duck", "a preamble", "the night shift", "deaf-blind", "Desperate Housewives", "Galileo Galilei", "Canada", "Anne Hathaway", "a strike", "the baseball bat", "West Virginia", "Thomas Jefferson", "home theater", "the deep purple logo", "kritiks", "Khrushchev", "1904", "the young girl ( an illustration by Everest creative Maganlal Daiya back in the 1960s )", "Jimmy Robertson", "ambidevous", "chariots", "Humberside Airport", "more than 265 million", "100 million", "employment aid, family finances, competitiveness, infrastructure, and actions toward public spending that is more transparent and efficient.", "head injury.", "\"clear moral prohibition\"", "Charles II"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5769196428571428}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.9, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.07999999999999999, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-4061", "mrqa_searchqa-validation-5735", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-3952", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_searchqa-validation-15062", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.46875, "CSR": 0.5162627551020409, "EFR": 1.0, "Overall": 0.7221588010204082}, {"timecode": 49, "UKR": 0.671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1056", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-211", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-785", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8428", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.806640625, "KG": 0.44453125, "before_eval_results": {"predictions": ["the NSA", "Heisman Trophy", "Brandi Chastain", "the Colorado", "C.J. Parker", "carioca", "a treasure map", "Pocahontas", "Drew", "(Whizzer) White", "an octave", "Aerosol", "a mythical magnum opus", "(Matthew) Broderick", "(Joseph) Campbell", "Margaret Mitchell", "Charles Busch", "the Clydesdale horse", "Ernest Teller", "rodeo", "fresco", "Nevil Shute", "Grant", "Jesse Jackson", "(P. Abt. 1256", "Department of Homeland Security", "the Black Sea", "leotard", "Bulworth", "the small intestine", "the mouth", "Cuba", "(P.R. TOLkiEN", "Olivia Newton-John", "bug spray", "Manhattan island", "7:24 a.m. Tuesday", "Leontyne Price", "compost", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "(P.T. Bradshaw", "the Castalian Spring", "Prague", "the burnoose", "Philadelphia", "peanut butter", "Edgar Allan Poe", "cork", "Lex Luthor", "coffee, macadamia nuts, pineapple, livestock, sugarcane and honey", "( Schwarzenegger )", "Master Christopher Jones", "hieroglyphic", "(P.8 million)", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "The incident", "three out of four", "poems", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5418402777777778}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9727", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-16920", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-10510", "mrqa_searchqa-validation-16451", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-7250", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-15005", "mrqa_searchqa-validation-1364", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14796", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-1897", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301"], "SR": 0.453125, "CSR": 0.515, "EFR": 1.0, "Overall": 0.687609375}]}