{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2180, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Ed Asner", "arrows", "1st century BC", "Marburg Colloquy", "Brookhaven", "ca. 2 million", "the Hungarians", "Mercury", "19th Century", "Art Deco style in painting and art", "The ability to make probabilistic decisions", "impact process effects", "1999", "phagosome", "the mass of the attracting body", "the Association of American Universities", "three", "allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "freight services", "up to four minutes", "the Little Horn", "Muslim and Chinese", "intracellular pathogenesis", "Santa Clara, California", "1784", "George Low", "Annual Conference Cabinet", "three", "Students", "Atlantic", "2001", "1887", "Chicago Bears", "John Harvard", "increase its bulk and decrease its density", "literacy and numeracy", "Christmas Eve", "the state", "Paris", "gender roles and customs", "outdated or only approproriate", "soy farmers", "United States", "Albert Einstein", "the number of social services that people can access wherever they move", "Tesco", "ABC Inc.", "1776", "wireless", "an electric current", "Warszowa", "the courts of member states", "supervisory church body", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church", "Manakin Episcopal Church", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Westminster", "Von Miller", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "Khwarezmia", "Queen Elizabeth II", "CBS", "Pittsburgh Steelers", "The chloroplast peripheral reticulum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8875363542546205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212122, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1826", "mrqa_squad-validation-4874", "mrqa_squad-validation-4283", "mrqa_squad-validation-1802", "mrqa_squad-validation-6210", "mrqa_squad-validation-3650", "mrqa_squad-validation-7430", "mrqa_squad-validation-6136"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["the Inland Empire", "New Zealand", "Jacksonville", "Newton's First Law", "the ability to pursue valued goals", "May 1888", "lecture theatre", "more than 28 days", "elliptical", "Boston", "Wednesdays", "Orange", "three", "Lampea", "San Jose State", "March 29, 1883", "between AD 0\u20131250", "Pleurobrachia", "eleven", "punts", "Solim\u00f5es Basin", "1474", "Arizona Cardinals", "Julia Butterfly Hill", "Orange", "Doctor in Bible", "left Graz", "waldzither", "over $40 million", "14th century", "6.7+", "end of the 19th century", "peace", "$40,000", "Cloth of St Gereon", "time and space", "7,000", "elementary particles", "indigenous", "3.5 billion", "New York City O&O WABC-TV and Philadelphia O&o WPVI-TV", "John Fox", "architectural", "Prime ideals", "Normant", "Leonardo da Vinci", "2003", "modern buildings", "Charles River", "KOA", "a disaster", "no contest", "Latin", "Manakin Town", "40,000", "After liberation", "\"winds up\" the debate", "10%", "The Daily Mail is mentioned in The Beatles\u2019 hit single Paperback Writer  The Yorkshire Post was the first British newspaper to report on The Abdication Crisis", "Uncle Tom's Cabin", "The liver", "No man", "Martina Hingis", "Ukraine borders with seven countries"], "metric_results": {"EM": 0.875, "QA-F1": 0.8942708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4458", "mrqa_squad-validation-1775", "mrqa_squad-validation-1001", "mrqa_squad-validation-696", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-7750", "mrqa_naturalquestions-validation-646"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 2, "before_eval_results": {"predictions": ["$155 million", "CBS", "San Jose State", "Half", "the evolution of the German language and literature", "Qur'an", "Brotherhood", "high demand", "Tolui", "legon, the current King of Thebes, who is trying to stop her from giving her brother Polynices a proper burial", "the object's weight", "over half", "1960s", "two months", "his friends Johannes Bugenhagen and Philipp Melanchthon", "1805", "Elders", "30\u201375%", "45,000 pounds", "self molecules", "Taishi", "1960", "Captain America: Civil War", "political divisions", "D loop mechanism", "Monterey", "The Book of Common Prayer", "14", "Charleston", "fear of their lives", "hot winds blowing from nearby semi-deserts", "intracellular pathogenesis", "Safari Rally", "10,006,721", "Philip Segal", "decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases", "1965", "quantum gravity", "German Te Deum", "Stanford Stadium", "Jin", "Trevathan", "Doctor Who", "1206", "clinical services", "CRISPR sequences", "Queen Elizabeth II", "zero", "1992", "food security", "plasmas", "their low ratio of organic matter to salt and water", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "time goes", "guardian", "guardian", "black", "leg leg"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6806074134199134}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.9714285714285714, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7408", "mrqa_squad-validation-6099", "mrqa_squad-validation-6641", "mrqa_squad-validation-2547", "mrqa_squad-validation-8360", "mrqa_squad-validation-2577", "mrqa_squad-validation-8747", "mrqa_squad-validation-5893", "mrqa_squad-validation-2906", "mrqa_squad-validation-1860", "mrqa_squad-validation-10427", "mrqa_squad-validation-6178", "mrqa_squad-validation-6405", "mrqa_squad-validation-1435", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11930", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-12889", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3857"], "SR": 0.59375, "CSR": 0.78125, "EFR": 1.0, "Overall": 0.890625}, {"timecode": 3, "before_eval_results": {"predictions": ["fewer than 10 employees", "1624", "Hangzhou", "in committee", "the 19th century", "1962", "dealing with patients' prescriptions and patient safety issues", "a group that included priests, religious leaders, and case workers as well as teachers", "Vistula River", "1290", "21 October 1512", "427,652", "a double membrane", "August 1967", "German", "27-30%", "four", "the 50 fund", "Arizona Cardinals", "Peanuts", "cilia", "calcitriol", "Warsaw", "time", "since at least the mid-14th century", "the mitochondrial double membrane", "Mike Figgis", "in an adult plant's apical meristems", "isopentenyl pyrophosphate synthesis", "Associating forces with vectors", "prime ideals", "The Three Doctors", "Malik Jackson", "four", "the Koori", "from 1910\u20131940", "pressure swing adsorption", "Luther", "English", "a lack of remorse", "the fundamental means by which forces are emitted and absorbed", "the A1 (Gateshead Newcastle Western Bypass", "Sun Life Stadium", "the Duchy of Prussia, the Channel Islands, and Ireland", "John Houghton", "February 2015", "draftsman", "a tiny snail", "Orestes", "the Galapagos Islands", "the U.S. government", "the movement", "the Travel Detective", "a very liquid cereal", "a major raw", "the Mycenaean civilization", "a biological clock", "the Belasco Theatre", "the Normandy Landings", "a bank robber", "fibula", "Il Trovatore", "the South Pole", "the East Coast Main Line"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6403544372294372}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6345", "mrqa_squad-validation-2192", "mrqa_squad-validation-3347", "mrqa_squad-validation-4730", "mrqa_squad-validation-1036", "mrqa_squad-validation-7435", "mrqa_squad-validation-3673", "mrqa_squad-validation-2132", "mrqa_squad-validation-6737", "mrqa_squad-validation-10310", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-8509", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-10694", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-7003"], "SR": 0.609375, "CSR": 0.73828125, "EFR": 0.96, "Overall": 0.849140625}, {"timecode": 4, "before_eval_results": {"predictions": ["Germany and Austria", "Centrum", "blue police box", "to spearhead the regeneration of the North-East", "the Gaulish name", "the Gramme dynamo", "under the wing of the secular powers", "the Yuan dynasty", "Zhongtong", "11.1%", "1538", "Deacons", "the New Testament", "their prestige, \"experience, ideology, and weapons\"", "25 percent", "May 2013", "the Sarah Jane Adventures", "in capturing prey", "a four-carbon compound", "livestock pasture", "Ford", "1,300,000", "the end result of ATP energy being wasted and CO2 being released, all with no sugar being produced", "two tumen (20,000 soldiers)", "eight", "a computational problem", "WzzM and WOTV", "Orange", "tentilla (\"little tentacles\")", "gender roles and customs", "social unrest and violence", "Woodward Park", "1745", "the Battle of Olustee", "observer status", "the 50-yard line", "3D printing technology", "The Malkin Athletic Center", "24\u201310", "magnitude 6.7+", "the European seaborne empires", "the domestic legislation of the Scottish Parliament", "tweed", "Bloomberg", "the oceans are growing crowded, and governments are increasingly trying to plan their use", "innovative, exciting skyscrapers", "a lump in Henry's nether regions", "the war years", "the computer processing unit (CPU) market", "Matt Kuchar and Bubba Watson", "the fastest circumnavigation of the globe in a powerboat", "the man facing up, with his arms out to the side", "the foyer of the BBC building in Glasgow, Scotland", "Christianity", "Manchester City", "Savoie", "three", "change course", "Tsvangirai", "\"A Lion Among Men", "January 2, 1971", "a delicacy fit for the kings and queens", "the late-night talk show \"Chelsea Lately\" on the E! network", "Luxembourg"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6837908798576902}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.06896551724137931, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 0.2, 0.0, 0.8333333333333333, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9250", "mrqa_squad-validation-1320", "mrqa_squad-validation-2288", "mrqa_squad-validation-8231", "mrqa_squad-validation-9645", "mrqa_squad-validation-7626", "mrqa_squad-validation-4947", "mrqa_squad-validation-4258", "mrqa_squad-validation-8832", "mrqa_squad-validation-6046", "mrqa_squad-validation-4943", "mrqa_squad-validation-4572", "mrqa_squad-validation-7094", "mrqa_squad-validation-2798", "mrqa_squad-validation-10045", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2807", "mrqa_triviaqa-validation-1366", "mrqa_hotpotqa-validation-547"], "SR": 0.5625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 5, "before_eval_results": {"predictions": ["with money from foreign Islamist banking systems, especially those linked with Saudi Arabia", "Anheuser-Busch InBev", "4000", "$37.6 billion", "Anglo-Saxons", "seven", "Golden Gate Bridge", "Southwest Fresno", "divergent boundaries", "the fact that chloroplasts are surrounded by a double membrane", "QuickBooks", "surface condensers", "clinical pharmacists", "seal illegally", "Philip Howard", "King Ethelred II of England", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "three", "French", "Golden Super Bowl", "the constitutional traditions common to the member states", "pharmacological effect", "Huguenots", "10\u20137", "Polish Academy of Sciences", "roughly spherical and highly refractive bodies", "Nurses", "New England Patriots", "Time magazine", "Class II MHC", "two plastid-dividing rings", "Westinghouse", "by disrupting their plasma membrane", "reciprocating (piston) steam engines", "indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons,", "Religious and spiritual teachers", "B cells", "property damage", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops", "Goa", "How I Met Your Mother", "France's famous Louvre museum", "Leo Frank", "Thessaloniki", "Graziano Transmissioni", "opposition parties", "1.2 million", "United's", "the release of the four men", "how health care can affect families", "Ed McMahon", "at least $20 million to $30 million", "No. 2 man (or woman)", "Friday", "Obama", "ballots", "Sodra nongovernmental organization", "cancer-causing toxic chemical", "fuel economy and safety", "resting heart rate over 100 beats per minute", "heavy breeds", "Denmark", "Cincinnati", "Donald Sutherland"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6755970285841609}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [0.7000000000000001, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.888888888888889, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.8181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9691", "mrqa_squad-validation-754", "mrqa_squad-validation-8715", "mrqa_squad-validation-3408", "mrqa_squad-validation-1090", "mrqa_squad-validation-3087", "mrqa_squad-validation-3610", "mrqa_squad-validation-3075", "mrqa_squad-validation-827", "mrqa_squad-validation-8826", "mrqa_squad-validation-8867", "mrqa_squad-validation-6644", "mrqa_squad-validation-3263", "mrqa_squad-validation-10444", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-4043", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-4171", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-2465"], "SR": 0.5625, "CSR": 0.6796875, "EFR": 1.0, "Overall": 0.83984375}, {"timecode": 6, "before_eval_results": {"predictions": ["2:45 a.m.", "11", "neither conscientious nor of social benefit", "University of Chicago Press", "$2 million", "2015", "1762", "biased against Genghis Khan", "The Warsaw Stock Exchange", "they are often branched and entangled with the endoplasmic reticulum", "a computational resource", "to denote unknown or unexplored territory", "John Michael Rysbrack", "in early Lutheran hymnals", "world line", "In the autumn of 1991", "William Smith", "William Pitt", "geochemical component called KREEP", "Theory of the Earth to the Royal Society of Edinburgh", "Japan", "Super Bowl Opening Night", "the Working Group chairs", "laws of physics", "Denver's Executive Vice President of Football Operations and General Manager", "noisiest", "independent of each other", "issues under their jurisdiction", "unsuccessful", "human", "more convenient and private method rather than traveling to a community drugstore where another customer might overhear about the drugs that they take", "eliminate all multiples of 1", "nerves", "1687", "fabric", "poly-Carbohydrates", "Democratic National Committee (DNC)", "heart, blood, and blood vessels", "Troggs", "six", "Bratislava", "Diana, the Princess", "slave trade", "vena cava", "Virginia's", "bullseye", "Tartarus", "This kind of backdrop that often represents the sky is known", "Nancy Reagan", "Achaemenid Empire", "LAP", "German", "net worth", "64", "Datson, H., Birch,", "Dies at 65", "Judas", "Dr. Jack Shephard, Kate Austen, Sayid Jarrah, Hugo \" Hurley", "comic book", "Love Is All Around", "dance-oriented production company", "French", "morphine elixir is widely used to treat pain.", "18"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5415426587301588}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.5, 0.8571428571428571, 1.0, 1.0, 0.4, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2545", "mrqa_squad-validation-6230", "mrqa_squad-validation-8765", "mrqa_squad-validation-5588", "mrqa_squad-validation-2384", "mrqa_squad-validation-10477", "mrqa_squad-validation-2921", "mrqa_squad-validation-4005", "mrqa_squad-validation-5054", "mrqa_squad-validation-383", "mrqa_squad-validation-10398", "mrqa_squad-validation-6337", "mrqa_squad-validation-3113", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4830", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-13281", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-1637", "mrqa_searchqa-validation-12770", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-10057", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1064"], "SR": 0.453125, "CSR": 0.6473214285714286, "EFR": 1.0, "Overall": 0.8236607142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["the extinction of the dinosaurs", "oxygen", "reduce growth", "K-9 and Company", "9.1 million", "a lower level of economic mobility than all the continental European countries", "School corporal punishment", "cattle", "Mongol", "a maze of semantical problems and grammatical niceties", "five", "the \"gold standard\" of religion", "British", "Finsteraarhorn", "Abilene", "white", "Yosemite Freeway/Eisenhower Freeway", "Thanksgiving", "874.3 square miles", "Two thirds", "the Ministry of War", "well into the nineteenth century", "\u201ccapability deprivation\u201d", "Daily Mail", "San Mateo", "Spanish", "around 300,000", "cryptomonads", "The Swahili", "hymn-writer", "a site of starch accumulation in plants that contain them", "Bryant", "Earth", "Utica, Ill.", "Rodeo", "hog", "Barack Obama", "Kenny G", "a small, half size cup used for serving espresso", "a peacock unitard", "the Federated States of Micronesia", "spring", "strudel", "insecticides", "Allah", "bones", "Python", "the Bible", "Ada Monroe", "Faith Hill", "Ben Affleck", "U.S.", "V", "time", "the jazz saxophonist", "Sweden", "Indonesia", "atomic numbers", "Alexandria", "Perfume", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "Georgetown", "Essex County Cricket Club", "Alzheimer's disease"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6226425438596491}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7399", "mrqa_squad-validation-6220", "mrqa_squad-validation-9588", "mrqa_squad-validation-4562", "mrqa_squad-validation-8828", "mrqa_searchqa-validation-47", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-7048", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-943", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-8990", "mrqa_searchqa-validation-4050", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10916", "mrqa_searchqa-validation-5178", "mrqa_searchqa-validation-4457", "mrqa_naturalquestions-validation-10073", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3468"], "SR": 0.59375, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.8203125}, {"timecode": 8, "before_eval_results": {"predictions": ["shocked", "lymphocytes", "producers", "BSkyB", "Kawann Short", "Daidu", "silent", "22", "the park", "1965", "tidal currents", "Concentrated O2", "Ma Jianlong", "Demaryius Thomas", "Lake Constance", "the Orange Democratic Movement", "Irish", "Red Army", "1700", "ITT", "1966", "masses", "Linebacker", "high art and folk music", "four", "with six series of theses", "the midseason forensic investigation drama Body of Proof", "seven-eighths", "the cardinal de Richelieu", "the Atlas Mountains", "Madrid", "the Danube", "eagles", "leather", "George Mortimer Pullman", "red", "the Messiah", "Sappho", "the law", "plumeria", "the divisor", "Dreams", "Texas", "Rooty Tooty", "a black breed", "the eagles", "the SAT", "e nihilo", "Henry David Thoreau", "Santa Ana", "Dick Cheney", "an eagles", "Gustave eiffel", "Edward Hopper", "the CIA", "d'Artagnan", "green", "1993", "apple", "its air-cushioned sole", "13", "Fort Worth", "Agent 99", "private"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6161024305555556}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7803", "mrqa_squad-validation-3478", "mrqa_squad-validation-1169", "mrqa_squad-validation-9837", "mrqa_squad-validation-2474", "mrqa_squad-validation-5926", "mrqa_searchqa-validation-15994", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-15182", "mrqa_searchqa-validation-523", "mrqa_searchqa-validation-15584", "mrqa_searchqa-validation-9386", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-14478", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-9179", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4461"], "SR": 0.5625, "CSR": 0.6319444444444444, "EFR": 1.0, "Overall": 0.8159722222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["\"Full Registration\" status after a year if there is sufficient evidence to show that the \"Standard for Full Registration\" has been met", "August 15, 1971", "Levi's Stadium", "Framework Convention on Climate Change", "Inflammation", "Brown v. Board of Education of Topeka", "15 May 1525", "The Walt Disney Company", "Dundee", "Over 61", "During the Second World War", "the integer factorization problem", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence", "Exploration is still continuing to determine if there are more reserves", "prep schools", "its soft power", "strong Islamist", "lengthening rubbing surfaces of the valve", "$32 billion", "keyed Northumbrian smallpipes", "the Dutch Republic", "Alex Haley", "three", "the helmeted honeyeater", "4:51", "Khrushchev", "Hera", "Preamble", "Elton John", "Cuba", "the Battle of Thermopylae", "Ukraine", "Kroc", "cricket", "white", "Washington", "Carmen", "Genoa", "one third", "C14", "972", "Yellowstone", "Ann Widdecombe", "a triangle", "the Old Kent Road", "Tuesday", "sodium pyroborate", "Ab Fab", "Massachusetts", "Scotland", "California", "the Susquehanna River", "80", "a Preamble", "Singapore", "Warriors", "Davos", "eight", "actor and filmmaker", "the Home Rule Party", "Secretary Janet Napolitano", "J. Crew", "Mitt Romney", "Grover"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6139204545454546}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.09090909090909091, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 0.6, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2040", "mrqa_squad-validation-8784", "mrqa_squad-validation-2920", "mrqa_squad-validation-9552", "mrqa_squad-validation-8273", "mrqa_squad-validation-9870", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-2533", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-1203", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2672", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1553", "mrqa_searchqa-validation-7509"], "SR": 0.53125, "CSR": 0.621875, "EFR": 1.0, "Overall": 0.8109375}, {"timecode": 10, "before_eval_results": {"predictions": ["tenggis", "environmental determinism", "4 August 2010", "King George III", "radio", "esen Khoroo", "the League of Augsburg", "Duarte Barbosa", "the People's Republic of China", "Roman Catholic Church", "Amazonia: Man and Culture in a Counterfeit Paradise", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "Sullivan Bay on Port Phillip", "five", "January 18, 1974", "Spanish", "Professional", "extremely difficult", "student populations", "Catholic", "the Parliament of the United Kingdom", "296", "terneuzen", "mulberry", "the Virus", "a binnate or bipinnate", "Ken Russell", "Dan Dare", "mucia", "the Smiths", "Mike Tyson", "Turkey", "Pesach", "Brian Deane", "kaleidoscope", "Uranus", "crimean", "crimea", "Ukraine", "Sydney", "Los Angeles", "Underground", "puck", "UV", "passion fruit", "Portugal", "cricket", "Serena Williams", "63 to 144 inches", "Titanic", "William Tell", "Jean Dess\u00e8s", "the snail", "Mendip", "Wichita", "the Passover", "New Croton Reservoir", "yuri", "Epicurus", "Stephen King", "Venus Williams", "a firefighter", "Ponty Mython", "Roman Polanski"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6371685606060606}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6263", "mrqa_squad-validation-2998", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-2265", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-7138", "mrqa_hotpotqa-validation-2340", "mrqa_newsqa-validation-2710", "mrqa_searchqa-validation-3397"], "SR": 0.59375, "CSR": 0.6193181818181819, "EFR": 1.0, "Overall": 0.8096590909090909}, {"timecode": 11, "before_eval_results": {"predictions": ["method by which the medications are requested and received", "salvation", "jugs", "they produce secretions (ink) that luminesce at much the same wavelengths as their bodies", "zaju", "administration", "Chivas USA", "Edinburgh", "The Pink Triangle", "the dot", "Magdalen Tower", "an international data communications network", "public service", "Guy de Lusignan", "tiger team", "Killer T cells", "The European Commission", "completed (or local) fields", "fundamental error", "Mongol and Turkic", "hez-buh-lah", "five", "Whist", "Nile River", "tuscany", "achromatopsia", "black", "Pluto", "chromium", "copper", "The Hague", "Vancouver Island", "Ironside", "george smiley", "Maxim Gorky", "brown trout", "Beyonce", "Wordsworth", "Man V Food", "Queen Elizabeth II", "Prince of Abissinia", "Conrad Murray", "Mary Poppins", "Sid Ziff", "black leaf", "rod", "caesar", "shrek", "Oslo", "lions", "Rhododendron", "sweden", "Franklin D. Roosevelt", "Shanghai", "caesar", "an elevator with a counterbalance", "Billy Colman", "17 October 2006", "beer", "Hoover Dam", "2006", "Capuchin Church of the Immaculate Conception, Rome, Italy", "Edgar Allan Poe", "Sir Robert Peel"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6644345238095238}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5460", "mrqa_squad-validation-8226", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-481", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-3846", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6189", "mrqa_triviaqa-validation-3023", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-890", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-8473"], "SR": 0.609375, "CSR": 0.6184895833333333, "EFR": 1.0, "Overall": 0.8092447916666666}, {"timecode": 12, "before_eval_results": {"predictions": ["a gift from God", "Greenland", "1724 to 1725", "placing them on prophetic faith", "1.25 million", "720p high definition", "five", "Maria Goeppert-Mayer", "the International Association of Methodist-related Schools, Colleges, and Universities", "three", "an majority in Parliament, a minority in the Council, and a majority in the Commission", "President Mahmoud Ahmadinejad", "Newcastle Eagles", "cholera", "other senior pharmacy technicians", "relative units of force and mass", "AD 14", "orogenic wedges", "his exploration and settlement of what is now Kentucky, which was then part of Virginia but on the other side of the mountains from the settled areas", "The Handmaid's Tale", "chimpanzee", "The Fault in Our Stars", "car car", "a handheld game console", "1898", "400 MW", "Total Nonstop Action Wrestling", "galt\u00fcr avalanche", "the last Roman Catholic Archbishop of Canterbury", "1861", "Disneyland theme park in Anaheim, California", "David Villa", "Red and Assiniboine Rivers", "New Jersey", "Continental Army", "Jack Kilby", "Ryan Gunoemen", "Umar S. Israilov", "July 16, 1971", "1933", "The Heirs", "the Baudot code", "1959", "1887", "and Governor of Minnesota Jesse Ventura", "Marvel Comics", "The Weeknd", "Nick Cassavetes", "Lamar Hunt", "Sarah Winnemucca", "Jean Baptiste Point DuSable", "England", "glenn fishburne", "a basilica", "1994", "Ricky Nelson", "in Wakanda and the Savage Land", "mercury", "phobias", "drug trafficking is a transnational threat, and they poison economies and governments, and it is in everyone's interest to stop this proliferation.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties", "andrew johnson", "a dame", "pre-Columbian times"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6407315340909091}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.125, 0.04761904761904762, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4958", "mrqa_squad-validation-5889", "mrqa_squad-validation-937", "mrqa_squad-validation-4079", "mrqa_squad-validation-10428", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-2706", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-383", "mrqa_naturalquestions-validation-6015", "mrqa_triviaqa-validation-2685", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-7025", "mrqa_naturalquestions-validation-8227"], "SR": 0.5625, "CSR": 0.6141826923076923, "EFR": 1.0, "Overall": 0.8070913461538461}, {"timecode": 13, "before_eval_results": {"predictions": ["\u00a341,004", "the Catch Me Who Can", "Tolui", "lower lake", "Gospi\u0107, Austrian Empire", "since 2001", "a maze of semantical problems and grammatical niceties", "Southwest Fresno", "5,000", "Huguenot", "ABC News Now", "sold Wardenclyffe for $20,000 ($472,500 in today's dollars)", "\u00c9mile Girardeau", "Brownlee", "partial funding", "relatively low in Ireland compared to the rest of the world", "NCAA Division II", "Adrian Lyne", "his most brilliant student", "Las Vegas", "Ranulf de Gernon", "2017", "Dallas", "Rudolf Schenker", "William Steig's 1990 fairy tale picture book of the same name", "Lucille Ball", "\"Grimjack\" (from First Comics)", "16\u201321", "Vince Guaraldi", "Tony Burke", "Michael Redgrave", "8th", "Johns Creek", "Hawaii", "unclear, without any central line of frass", "Gilbert du Motier", "Gujarat", "three", "Winter Haven", "four", "Joel Embiid", "Mindy Kaling", "Surrey", "Claudio Javier L\u00f3pez", "\"All of the Lights\" by Jay-Z", "FCI Danbury", "a few", "the US Naval Submarine Base New London submarine school", "Las Vegas", "Pope John X", "(2007)", "Arlo Looking Cloud", "Rwandan genocide of 1994", "Larnelle Harris", "Commander in Chief of the United States Armed Forces", "2014 -- 2018", "1982", "6", "Chris Robinson", "unclear", "unclear", "unclear", "the Egyptian Goddess of Creation", "Richie Unterberger"], "metric_results": {"EM": 0.5, "QA-F1": 0.5848958333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3287", "mrqa_squad-validation-1488", "mrqa_squad-validation-7036", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-3556", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-5454", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-1091", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-1441", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-16181", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-469"], "SR": 0.5, "CSR": 0.6060267857142857, "EFR": 1.0, "Overall": 0.8030133928571428}, {"timecode": 14, "before_eval_results": {"predictions": ["10,000", "perpendicular to the velocity vector", "Inherited wealth", "December 1963", "only the series from 2009 onwards", "religious freedom in the Polish\u2013Lithuanian Commonwealth", "the Silk Road", "the kilogram-force", "ten times their own weight", "Quaternary", "1887", "other ctenophores", "symbiotic relationship", "mathematical models of computation", "Vistula River", "quarter", "iCloud service will now be integrated into the iOS 5 operating system", "a dorm parent mistreated students", "March 8", "Mike Meehan", "the Catholic League", "1,000 pounds", "\"This is probably not the best time to repeat the passage that was found to be offensive,\"", "Friday", "Movahedi", "stories of different women coping with breast cancer in five vignettes", "\"black rain\" of drilling fluid and a roar of escaping gas\"", "peter", "money", "Lance Cpl. Maria Lauterbach and her fetus", "Hyundai Steel", "London", "400", "Val d'Isere, France", "the test results by a chaplain about 1:45 p.m., per jail policy.", "a municipal building in Baghdad's Sadr City,", "12.3 million", "1616", "Sky", "Chile", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls", "Buddhism", "J.Crew", "U.N. Security Council resolution in 2006", "Depression-era bank robber", "boyhood experience in a World War II internment camp", "suppress the memories and to live as normal a life as possible", "the worst might not yet be over.", "the Irish capital", "Republican", "Spanishfork,", "Mandi Hamlin and that officer called over another officer, who told her she would need to remove them", "Islamabad", "9 a.m.", "March 26, 1973", "Indian Ocean", "argument form", "quarter", "Sevens", "England", "Yemen", "Spanish painting", "peterfunk", "mercury"], "metric_results": {"EM": 0.375, "QA-F1": 0.4587171052631579}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.16666666666666669, 1.0, 0.6666666666666666, 0.7777777777777778, 0.15789473684210525, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10400", "mrqa_squad-validation-7770", "mrqa_squad-validation-10458", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-3798", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-391", "mrqa_naturalquestions-validation-6733", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-12191", "mrqa_triviaqa-validation-3839"], "SR": 0.375, "CSR": 0.590625, "EFR": 0.975, "Overall": 0.7828124999999999}, {"timecode": 15, "before_eval_results": {"predictions": ["Prospect Park", "Khanbaliq", "Quaternary", "1870", "water", "prime", "50 fund", "Camisards", "over $40 million", "GTE", "1,100", "spin", "Oligocene", "Melodie Rydalch", "Charles Darwin", "a Little Rock military recruiting center", "March 24,", "the Beatles", "Robert Park", "Adriano", "11ven people died and 36 were wounded in the Monday terror attack,", "2007", "new clashes", "\"They pretty much asked me if she was depressed,... how she acted around the baby, if she seemed stressed out,\"", "56", "Pittsburgh Steelers", "albino monk", "one of its diplomats", "These intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on", "18 federal agents and two soldiers", "Atlanta", "resources", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "two Emmys for work on the 'Columbo' series starring Peter Falk.", "\"To all of our valiant men and women, know that the American people believe in you, support you and are 100 percent behind you, and we thank God every day that you have our back.\"", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Rwanda", "75", "eradication of the Zetas cartel", "closing these racial gaps.", "his bodyguard-turned-informant", "President Bush", "Amstetten,", "African National Congress Deputy President Kgalema Motlanthe", "\"It's hard for everyone... I thought it was better for me here,\"", "\"The Kirchners have been weakened by this latest economic crisis,\"", "\"The Sopranos,\"", "sharia law", "\"The minister later apologized, telling CNN his comments had been taken out of context.", "Iran", "20% tax credit", "July 23.", "70,000", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "Robert Remak", "Tim McGraw", "Prussian 2nd Army", "cabbage", "a homebrew campaign setting", "Beno\u00eet Jacquot", "blue", "Capitol", "The Left Book Club", "holography"], "metric_results": {"EM": 0.5, "QA-F1": 0.5767016895141894}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818185, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.09523809523809525, 1.0, 0.08, 0.2666666666666667, 1.0, 1.0, 1.0, 0.1111111111111111, 0.2962962962962963, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9016", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2727", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-11133", "mrqa_triviaqa-validation-6296"], "SR": 0.5, "CSR": 0.5849609375, "EFR": 1.0, "Overall": 0.79248046875}, {"timecode": 16, "before_eval_results": {"predictions": ["two thousand people", "address information", "high risk of a conflict of interest and/or the avoidance of absolute powers", "to look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system.", "Thames River", "British East Africa (as the Protectorate was generally known) and German East Africa", "several hundred thousand, some 30% of the city", "the Tower District", "Ted Ginn Jr.", "Catch Me Who Can", "John Fox", "the housing bubble", "a total of 183 people, including 137 children, have been taken away since law enforcement officers raided the compound Thursday night,", "Adam Lambert and Kris Allen,", "Brian Smith", "\"Hillbilly Handfishin'\"", "Robert Mugabe", "voluntary wrongful after witnesses identified him and he was interviewed by police.", "his enjoyment of sex and how he lost his virginity at age 14.", "his injuries,", "1979", "murder", "next year", "his plans to overhaul domestic policies,", "Paul Bruno,", "Anil Kapoor", "Afghanistan and India", "Dr. Albert Reiter,", "\"From Terror to Nuclearollah: The Significance of the Iranian Threat,\"", "brutal choice", "Matthew Fisher", "cancer,", "Courtney Love,", "to step up.\"", "\"Perfidia,\" \"Walk Don't Run '64\" and \"Diamond Head.\"", "its own environmental videos", "two women", "once in June,", "Monday,", "Roy Foster", "Yusuf Saad Kamel", "hokeriet,", "11 healthy eggs", "Russia and some European countries have expressed concerns about the missile defense system.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Fullerton, California,", "around 1918 or 1919.", "in the 1950s,", "U.S. troops working in support of Iraqi soldiers were attacked by small-arms, machine-gun and RPG fire from buildings overlooking the road.", "it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "vegan bake sales from April 24 through May 2.", "\"The Rosie Show,\"", "al Fayed's", "Oxbow,", "gastrocnemius", "Ed Sheeran", "\"clock\"", "Australia", "three-part", "2001", "vingtaines (or, in St. Ouen, cueillettes),", "California", "George Blake", "Cundinamarca"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5720371123818815}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.9743589743589743, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.27272727272727276, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6874999999999999, 0.17391304347826086, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-8570", "mrqa_squad-validation-914", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-2957", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-4647", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-10239", "mrqa_triviaqa-validation-6739"], "SR": 0.453125, "CSR": 0.5772058823529411, "EFR": 1.0, "Overall": 0.7886029411764706}, {"timecode": 17, "before_eval_results": {"predictions": ["relatively equal distributions of wealth", "a pharmacy practice residency", "questions and answers", "Genesis", "the same architect,", "12 January 1943,", "60,000", "Zagreus", "CBS", "17", "temperate", "Rod Blagojevich,", "\"Sesame Street's\"", "Windsor, Ontario,", "$50 less,", "Afghanistan's", "fled Zimbabwe and found his qualifications mean little as a refugee.", "\"executioners\"", "Israel and the United States", "concerns about the missile defense system.", "Sharon Bialek", "Gary Brooker", "the exact cause of IBS remains unknown,", "Helmand province", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "$250,000", "first or second week in April.", "Derek Mears", "braving elements ranging from rain to wind and even one speeding ticket", "Player's", "Nieb\u00fcll", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "kite boards", "Virgin America", "Palm Beach in Florida.", "Daniel Wozniak,", "22-year-old", "bin Laden", "how health care can affect families.\"", "United Nations", "U.S. Food and Drug Administration", "Casa de Campo International Airport", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "2002", "checkposts and military camps", "all the attackers were Pakistanis,", "Friday,", "\"Taxman,\"", "crocodile eggs", "more than 20 times", "hid his money", "congressional Democrats vowed to put into place since they took control of Congress", "senators", "five - year time jump", "JHU-APL", "Arlene Phillips", "23 July 1989", "Ry\u016bkyuan sailors", "surrealism", "C. S. Lewis", "7", "sake", "Nova Scotia"], "metric_results": {"EM": 0.375, "QA-F1": 0.4864233385123331}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.33333333333333337, 0.0, 0.0, 0.12500000000000003, 1.0, 0.0, 0.5, 0.15384615384615385, 0.5217391304347826, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 0.4444444444444444, 0.0, 1.0, 0.7692307692307693, 0.15384615384615388, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2408", "mrqa_squad-validation-5326", "mrqa_squad-validation-1570", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-1858", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-157", "mrqa_naturalquestions-validation-132", "mrqa_triviaqa-validation-1659", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-4857"], "SR": 0.375, "CSR": 0.5659722222222222, "EFR": 1.0, "Overall": 0.7829861111111112}, {"timecode": 18, "before_eval_results": {"predictions": ["melatonin", "constant factors and smaller terms", "Shi Bingzhi", "Fort Edward and Fort William Henry.", "linear", "Advanced Steam movement", "Defensive ends", "the dot", "chastity", "European Court of Justice", "a bronze medal in the women's figure skating final,", "\"trying to steal the election\" and \"intimidating the population and election officials as well.\"", "UK", "\"Gandhi,\"", "Argentina", "Congress", "28", "Frank Ricci,", "\"Kurdistan Gas City.\"", "\"greatly moved\" by meeting victims of abuse in Valletta, Malta.", "Bill & Melinda Gates Foundation", "$106,482,500", "because everybody around me likes Obama,\"", "not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "\"political and religious\"", "$163 million (180 million Swiss francs)", "Afghan lawmakers", "North Korea", "\"Otherwise the weekend, we've effectively dealt with record-breaking crowds,\"", "\"Zed,\"", "\"global security, prosperity and freedom.\"", "because the federal government is asleep at the switch,", "Molotov cocktails, rocks and glass.", "\"wildcat\" strikes,", "Ben Roethlisberger", "Dr. Christina Romete,", "Ewan McGregor", "Costa Rica", "Meira Kumar", "next week.", "clubs and bars in Hong Kong and Shenzhen,", "Lindsey Vonn", "\"They were nothing,\"", "Amnesty International.", "President Obama's race", "Los Ticos", "Form Design Center", "AbdulMutallab was in the bathroom for about 15 to 20 minutes, which seemed long to the passenger, Tukel said.", "two people", "40-year-old", "outside the Iranian consulate in Peshawar", "Casey Anthony,", "the iconic Hollywood headquarters of Capitol Records,", "Emma Watson and Dan Stevens", "2002", "a leak in a Dike", "\"Sunny Aftermath\"", "Che Guevara", "Miller Brewing", "Elizabeth I", "John Fogerty", "Garonne", "giraffe", "cheese"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6046919484419484}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.9696969696969697, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2702702702702703, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1603", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9104", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-254"], "SR": 0.53125, "CSR": 0.5641447368421053, "EFR": 0.9333333333333333, "Overall": 0.7487390350877193}, {"timecode": 19, "before_eval_results": {"predictions": ["1876", "1507", "Danny Trevathan", "11", "would be killed through overwork", "Japanese", "Muqali,", "2011 and 2012", "Pittsburgh Steelers", "apartment building in Cologne, Germany", "Aung San Suu Kyi", "Hank Moody", "the 3rd District of Utah.", "that suggested returning combat veterans could be recruited by right-wing extremist groups.", "Stephen Tyrone Johns", "30", "procedures", "acid", "most of those who managed to", "that the Bainbridge would be getting backup shortly.\"", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Courtney Love,", "33-year-old", "cell phones", "a book", "he was in good health, contrary to media reports he was diagnosed with skin cancer.", "to stand down.", "Ashley \"A.J. Jewell,", "17", "from her father's home in Satsuma, Florida,", "to the southern city of Naples", "Hugo Chavez", "London's", "home in rural California,", "has been killed in an attempted car-jacking as he dropped his children off at a relative's house,", "Old Trafford", "the area of the 11th century Preah Vihear temple", "steam-driven, paddlewheeled overnight passenger boat", "clothing", "about 3,000 kilometers (1,900 miles),", "homicide", "The Ski Train", "Alicia Keys", "Robert De Niro", "intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "The governor", "$60 billion on America's infrastructure.", "protective shoes", "to defend our territory and our laws and our homeland and our government.\"", "U.S. President-elect Barack Obama", "Burhanuddin Rabbani,", "was depressed over a recent breakup,", "glass shards", "Sedimentary rock", "2.45 billion years ago", "London", "Colorado", "Bangor International", "\"Grandmasters\"", "Suffragist", "Cobblestone", "Abu Dhabi", "Silver", "Bonnie and Clyde"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6612280537445011}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4210526315789474, 0.8, 1.0, 0.5, 0.8333333333333333, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.09523809523809522, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4444444444444445, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-2197", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-6758", "mrqa_hotpotqa-validation-2782", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-12322"], "SR": 0.515625, "CSR": 0.56171875, "EFR": 0.967741935483871, "Overall": 0.7647303427419355}, {"timecode": 20, "before_eval_results": {"predictions": ["the Hostmen", "Greg Brady", "Fort Caroline", "the Hungarians under Ferenc De\u00e1k", "Greek physician Pedanius", "John D. Rockefeller", "four", "mistreatment from government officials.", "Beijing, China,", "Virgil Tibbs", "Thaddeus Rowe Luckinbill", "up to 100,000", "the United States, its NATO allies and others", "Virginia Dare", "JackScanlon", "Kylie Minogue", "94 by 50", "Lalo Schifrin", "MGM Resorts International", "16 August 1975", "seawater pearls", "1962", "Buddhism", "1978", "1927, 1934, 1938, 1956", "1969", "New England Patriots", "Joseph Heller", "the north pole", "1,350", "Leonard Bernstein", "25 September 2007", "Howard Caine", "Yale University, her grandfather's alma mater,", "three", "the team", "November 2014", "Gavrilo Princip", "a central place in Christian eschatology", "October 1941", "peace between two entities ( especially between man and God or between two countries )", "hanjore Gardens", "Cee - Lo", "after Shawn's kidnapping", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Conservative Party", "three times", "November 25, 2002", "October 29, 2015", "Peter Greene", "31 March 1909", "Ed Sheeran", "two", "Alberto Salazar", "live animals", "American", "Rensselaer County,", "CEO of an engineering and construction company", "more than 1.2 million", "The Three Little Pigs", "Robert Louis Stevenson", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "United States, Britain and France", "south-central Washington,"], "metric_results": {"EM": 0.5, "QA-F1": 0.6293570788530466}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.6666666666666666, 0.4, 0.0, 0.25, 1.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.967741935483871, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.25, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6314", "mrqa_squad-validation-8027", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-8934", "mrqa_triviaqa-validation-3886", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3167", "mrqa_searchqa-validation-13486", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-2446"], "SR": 0.5, "CSR": 0.5587797619047619, "EFR": 0.96875, "Overall": 0.7637648809523809}, {"timecode": 21, "before_eval_results": {"predictions": ["as far back as the early Cambrian, about 515 million years ago", "Spanish", "an attack on New France's capital, Quebec", "the Fresno Traction Company", "Westminster", "the ancestors of chloroplasts", "24 of the 32 songs", "Sauron", "in the Washington metropolitan area", "the base 10 logarithm of the molar concentration", "the breast or lower chest of beef or veal", "Sargon II", "Spanish", "around 1600 BC", "From 1976 to 1983", "American swimmer Michael Phelps", "Rajendra Prasad", "Ren\u00e9 Georges Hermann - Paul", "his parents", "the St. Louis Cardinals", "Orangeville, Ontario, Canada", "medical abnormalities, activation level, or recruitment order, or to analyze the biomechanics of human or animal movement", "Janie Crawford,", "by the early 3rd century", "positions 14 - 15, 146 - 147 and 148 - 149", "Elk and Kanawha Rivers", "1961", "`` classic '' Mac OS,", "in rocks and minerals", "Michael Schumacher", "finance", "1957,", "1776", "1963,", "Field Marshal Paul von Hindenburg", "2018", "Ireland", "Kit Harington", "her boyfriend Lance", "Transvaginal ultrasonography", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Baker, California, USA", "a Raja Dhilu", "In the 1979 -- 80 season,", "Guy Berryman", "Tessa Virtue and Scott Moir", "c. 497 / 6 -- winter 406 / 5 BC", "Tim Allen", "thick skin", "biscuit - sized cakes", "India", "three", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "cricket", "the Major General of the Navy", "Marktown", "14,000", "Iran could be secretly working on a nuclear weapon", "Honduras", "Pardon of Richard Nixon", "Ellen DeGeneres", "12 April 1961", "American punk rock", "Westfield Old Orchard"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5247100122100122}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true], "QA-F1": [0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.25, 0.0, 0.6666666666666666, 0.0, 1.0, 0.30769230769230765, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 0.8, 0.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.6666666666666666, 0.08333333333333333, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4472", "mrqa_squad-validation-8780", "mrqa_squad-validation-2387", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-4641", "mrqa_hotpotqa-validation-1675", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-3883", "mrqa_hotpotqa-validation-427", "mrqa_hotpotqa-validation-3984"], "SR": 0.390625, "CSR": 0.5511363636363636, "EFR": 1.0, "Overall": 0.7755681818181819}, {"timecode": 22, "before_eval_results": {"predictions": ["the mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania).", "Stanford University and stayed at the Santa Clara Marriott", "linebacker", "Mongol and Turkic tribes", "1859", "Danny Lane", "in the New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16", "new wave rock band The Fixx", "President Andrew Johnson", "Hellenism", "Mark Jackson", "New York", "the Chainsmoker", "2015", "al - khimar", "week 4", "L.K. Advani", "the President", "Zachary John Quinto", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash", "the North Shore, at locations in Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "two", "Thomas Jefferson", "the head of Lituya Bay in Alaska", "Manhattan", "a sound stage in front of a live audience in Burbank, California", "Grace Zabriskie", "2014", "Yuzuru Hanyu", "Glenn Close", "Elk", "flawed democracy", "China, and features Kung Fu instead of Okinawan Karate", "Anthony Quinn", "Masha Skorobogatov", "February 27, 2007", "Felina Weissman", "6ft 1in Coltrane", "Owen Vaccaro", "bacteria", "on the lateral side of the tibia", "Lynda Carter", "erosion", "90 \u00b0 N 0 \u00b0 W \ufeff / \ufefe 90 \u00b0N - 0 \u00b0 E \ufef5 / 90", "Canterbury to Canterbury", "in the bloodstream or surrounding tissue", "2015", "February 29", "1840s", "9.7", "Alabama", "Juliet", "\"I know, it's so boring. It's a song about the dark underbelly of the American Dream, and about excess in America which was something we knew about.\"", "the Queen", "eagles", "vocalist Eddie Vedder", "2005", "Dan Tyminski", "Elisabeth", "southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "at least nine", "Bashar al-Assad", "the New Revised Standard Version", "Biathlon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5905794615413491}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.14814814814814814, 1.0, 1.0, 0.4444444444444445, 0.13333333333333333, 1.0, 1.0, 0.8, 1.0, 0.9523809523809523, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7586206896551724, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-322", "mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2079", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-7486", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-4139", "mrqa_triviaqa-validation-4546", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-2101", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1295", "mrqa_searchqa-validation-15510"], "SR": 0.453125, "CSR": 0.546875, "EFR": 0.9714285714285714, "Overall": 0.7591517857142858}, {"timecode": 23, "before_eval_results": {"predictions": ["internal strife", "a new stage in the architectural history of the regions they subdued", "Fresno", "castles and vineyards", "below 0 \u00b0C (32 \u00b0F)", "Von Miller", "Division I history ( 61 )", "Thaddeus Rowe Luckinbill", "December 25", "2002", "The Mandate of Heaven", "Geoffrey Zakarian", "Christopher Allen Lloyd", "prenatal development in the central part of each developing bone", "Ali", "Vijay Prakash", "Article 1, Section 2", "the Constitution of India came into effect on 26 January 1950", "Thomas Marvolo Riddle", "Dick Rutan and Jeana Yeager", "in sequence with each heartbeat", "Ren\u00e9 Descartes", "James P. Flynn", "detritus", "September 27, 2017", "Brendan Graham", "1985", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Joyce Vincent Wilson", "on February 10, 2017", "Alex Skuby", "Colony of Virginia", "5 September, at which point it formally acceded to the community", "from 1922 to 1991", "Tom Goodman - Hill", "Bacon", "an explosion", "Heather Stebbins", "Redenbacher family", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "`` 0 '' trunk code", "April 1, 2016", "Friedman Billings Ramsey", "New York City", "cutting surfaces", "The Massachusetts Compromise", "Justin Timberlake", "Andrew Moray and William Wallace", "Alamodome and city of San Antonio", "asexually", "John Garfield", "In 1871 A.D. Pt. Buddhiballav Pant", "eye", "The History Boys", "caluga", "the White Knights of the Ku Klux Klan", "five", "Mot\u00f6rhead", "Kingman Regional Medical Center,", "A staff sergeant in the U.S. Air Force,", "the most-wanted man in the world", "Antarctica", "california", "california"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5655528499278499}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.8, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.28571428571428575, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.2222222222222222, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333333, 0.2857142857142857, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1129", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1445", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-1207", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-3651", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-505", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-12624"], "SR": 0.453125, "CSR": 0.54296875, "EFR": 0.9714285714285714, "Overall": 0.7571986607142858}, {"timecode": 24, "before_eval_results": {"predictions": ["research, exhibitions and other shows", "no", "Stadtholder William III of Orange", "1933\u20131953", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Jim Thorpe", "1996", "the onset and progression of Alzheimer's disease.", "Disco", "Kingdom of Dalmatia", "McKinsey", "Rockefeller Center", "Charles Whitman", "C. H. Greenblatt", "The Curious Case of Benjamin button", "Mold-Denbigh", "Corendon Dutch Airlines", "86 ft long", "Minneapolis, Minnesota", "Scottish Highlands", "Fatih Ozmen", "the U.S. military designation for a steel disintegrating link", "Pacific Place", "the Attorney General of Michigan from 1999 to 2003", "Paradise, Nevada", "Westminster, London", "2016", "Wildhorn", "New York University School of Law", "Crips", "Harper's and Queen", "dementia", "50 km north-northeast of Bologna, on the Po di Volano, a branch channel of the main stream of the Po River, located 5 km north", "Guadalcanal Campaign", "Bishop's Stortford", "Starvation Is Motivation", "Barbara Lee Alexander", "Black Friday", "Archbishop of Canterbury", "TD Garden", "James Victor Chesnutt", "the Prussian Crusade", "Australian", "Julie Taymor", "Easy", "World War I", "79 AD", "Musicology", "Portland, OR", "Yoruba", "No Surprises", "Julie Frost", "2007 and 2008", "2001", "1966", "Cameron Diaz", "Allan Border", "Medellin", "his father's parenting skills.", "alternative-energy vehicles", "two weeks ago", "genes", "olive", "Stockholm"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5452752976190476}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.08333333333333334, 1.0, 0.5, 0.8571428571428571, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.8, 0.6666666666666666, 0.0, 1.0, 0.25, 1.0, 0.0, 0.5, 0.6666666666666666, 0.4, 0.4, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7092", "mrqa_squad-validation-2153", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-4105", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-5348", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-979", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-1697", "mrqa_hotpotqa-validation-431", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-2659", "mrqa_triviaqa-validation-3361", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1676"], "SR": 0.390625, "CSR": 0.536875, "EFR": 0.9743589743589743, "Overall": 0.7556169871794871}, {"timecode": 25, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-1123", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-4562", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3206", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-3654", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-11385", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11900", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12624", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-1335", "mrqa_searchqa-validation-13486", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14883", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-16181", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3783", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-4857", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8589", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-9756", "mrqa_squad-validation-10045", "mrqa_squad-validation-10069", "mrqa_squad-validation-10074", "mrqa_squad-validation-10086", "mrqa_squad-validation-10216", "mrqa_squad-validation-10228", "mrqa_squad-validation-10254", "mrqa_squad-validation-10310", "mrqa_squad-validation-10324", "mrqa_squad-validation-10338", "mrqa_squad-validation-10353", "mrqa_squad-validation-1036", "mrqa_squad-validation-10378", "mrqa_squad-validation-10477", "mrqa_squad-validation-1090", "mrqa_squad-validation-1320", "mrqa_squad-validation-1450", "mrqa_squad-validation-1603", "mrqa_squad-validation-1636", "mrqa_squad-validation-1672", "mrqa_squad-validation-1694", "mrqa_squad-validation-178", "mrqa_squad-validation-1802", "mrqa_squad-validation-1852", "mrqa_squad-validation-1855", "mrqa_squad-validation-1857", "mrqa_squad-validation-1938", "mrqa_squad-validation-1967", "mrqa_squad-validation-2040", "mrqa_squad-validation-2126", "mrqa_squad-validation-2153", "mrqa_squad-validation-2216", "mrqa_squad-validation-2289", "mrqa_squad-validation-2384", "mrqa_squad-validation-2400", "mrqa_squad-validation-2436", "mrqa_squad-validation-2460", "mrqa_squad-validation-2477", "mrqa_squad-validation-255", "mrqa_squad-validation-2577", "mrqa_squad-validation-2602", "mrqa_squad-validation-2619", "mrqa_squad-validation-268", "mrqa_squad-validation-2693", "mrqa_squad-validation-2773", "mrqa_squad-validation-2782", "mrqa_squad-validation-2798", "mrqa_squad-validation-282", "mrqa_squad-validation-2824", "mrqa_squad-validation-285", "mrqa_squad-validation-2929", "mrqa_squad-validation-3019", "mrqa_squad-validation-3041", "mrqa_squad-validation-3135", "mrqa_squad-validation-3185", "mrqa_squad-validation-320", "mrqa_squad-validation-3337", "mrqa_squad-validation-3476", "mrqa_squad-validation-353", "mrqa_squad-validation-3589", "mrqa_squad-validation-3709", "mrqa_squad-validation-383", "mrqa_squad-validation-3931", "mrqa_squad-validation-3948", "mrqa_squad-validation-3955", "mrqa_squad-validation-397", "mrqa_squad-validation-3993", "mrqa_squad-validation-4005", "mrqa_squad-validation-4079", "mrqa_squad-validation-4140", "mrqa_squad-validation-415", "mrqa_squad-validation-4181", "mrqa_squad-validation-427", "mrqa_squad-validation-4291", "mrqa_squad-validation-4305", "mrqa_squad-validation-4333", "mrqa_squad-validation-4338", "mrqa_squad-validation-4472", "mrqa_squad-validation-462", "mrqa_squad-validation-4686", "mrqa_squad-validation-4704", "mrqa_squad-validation-4835", "mrqa_squad-validation-4856", "mrqa_squad-validation-4870", "mrqa_squad-validation-5054", "mrqa_squad-validation-5088", "mrqa_squad-validation-5096", "mrqa_squad-validation-5154", "mrqa_squad-validation-5176", "mrqa_squad-validation-5238", "mrqa_squad-validation-5302", "mrqa_squad-validation-5326", "mrqa_squad-validation-5376", "mrqa_squad-validation-550", "mrqa_squad-validation-5537", "mrqa_squad-validation-5541", "mrqa_squad-validation-5588", "mrqa_squad-validation-5616", "mrqa_squad-validation-5672", "mrqa_squad-validation-5703", "mrqa_squad-validation-5767", "mrqa_squad-validation-5777", "mrqa_squad-validation-5913", "mrqa_squad-validation-60", "mrqa_squad-validation-60", "mrqa_squad-validation-607", "mrqa_squad-validation-6099", "mrqa_squad-validation-6126", "mrqa_squad-validation-6143", "mrqa_squad-validation-6178", "mrqa_squad-validation-6220", "mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6362", "mrqa_squad-validation-6395", "mrqa_squad-validation-6414", "mrqa_squad-validation-6564", "mrqa_squad-validation-660", "mrqa_squad-validation-6641", "mrqa_squad-validation-6737", "mrqa_squad-validation-6754", "mrqa_squad-validation-6782", "mrqa_squad-validation-68", "mrqa_squad-validation-6817", "mrqa_squad-validation-6915", "mrqa_squad-validation-696", "mrqa_squad-validation-7018", "mrqa_squad-validation-703", "mrqa_squad-validation-7069", "mrqa_squad-validation-707", "mrqa_squad-validation-7150", "mrqa_squad-validation-7161", "mrqa_squad-validation-7180", "mrqa_squad-validation-7198", "mrqa_squad-validation-7260", "mrqa_squad-validation-7399", "mrqa_squad-validation-754", "mrqa_squad-validation-7552", "mrqa_squad-validation-7597", "mrqa_squad-validation-7640", "mrqa_squad-validation-765", "mrqa_squad-validation-7678", "mrqa_squad-validation-7770", "mrqa_squad-validation-7782", "mrqa_squad-validation-7814", "mrqa_squad-validation-7856", "mrqa_squad-validation-7882", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-804", "mrqa_squad-validation-8056", "mrqa_squad-validation-8104", "mrqa_squad-validation-8115", "mrqa_squad-validation-8189", "mrqa_squad-validation-8226", "mrqa_squad-validation-8226", "mrqa_squad-validation-8285", "mrqa_squad-validation-8406", "mrqa_squad-validation-8480", "mrqa_squad-validation-8527", "mrqa_squad-validation-8629", "mrqa_squad-validation-8735", "mrqa_squad-validation-8760", "mrqa_squad-validation-8765", "mrqa_squad-validation-8832", "mrqa_squad-validation-884", "mrqa_squad-validation-8867", "mrqa_squad-validation-890", "mrqa_squad-validation-8957", "mrqa_squad-validation-898", "mrqa_squad-validation-9031", "mrqa_squad-validation-9066", "mrqa_squad-validation-9135", "mrqa_squad-validation-9186", "mrqa_squad-validation-9227", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9391", "mrqa_squad-validation-9392", "mrqa_squad-validation-9465", "mrqa_squad-validation-9504", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9652", "mrqa_squad-validation-9658", "mrqa_squad-validation-9771", "mrqa_squad-validation-979", "mrqa_squad-validation-9818", "mrqa_squad-validation-987", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-3051", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-469", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5568", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-6909", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-776"], "OKR": 0.857421875, "KG": 0.375, "before_eval_results": {"predictions": ["A progressive tax", "Jacksonville", "monophyletic", "Orthogonal components", "Fox Network", "Anhaltisches Theater in Dessau", "Anna Clyne", "Terence Winter", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "DS Virgin Racing Formula E Team", "Eastern College Athletic Conference", "Kim Jong-hyun", "Peter Chelsom", "The Ninth Gate", "heavy metal", "Cinderella", "Los Angeles", "Sharyn McCrumb", "Acid house", "at the end of the 18th century", "Capture of the Five Boroughs", "Miranda Leigh Lambert", "Shenandoah National Park", "the Royal Automobile Club's Tourist Trophy", "10 Years", "Honolulu", "Armin Meiwes", "1886", "Rockhill Furnace, Pennsylvania", "northeastern part", "coca wine", "Entrepreneur", "the lead roles", "PBS stations nationwide,", "second largest", "CARoccan clementine", "in 1911", "Johnnie Ray", "The Five", "Walt Disney Feature Animation", "The 2017\u201318 Premier League", "new, small and more expensive vessels", "1972", "Geographical Indication tag", "Ringo Starr", "Celtics", "World Championship Wrestling", "in New York", "TD Garden", "the Chechen Republic", "Chrysler", "Princeton University", "in 2005", "Tenochtitlan", "in 1986", "mike atherton", "Mexico", "Thundercats", "he fears a desperate country with a potential power vacuum that could lash out.", "the Catholic League", "the Los Angeles County Coroner's Office,", "mike atherton", "\"Death, be not\"", "green"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6069320436507937}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.8, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-2056", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-4298", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9487", "mrqa_triviaqa-validation-7575", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-364", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4240"], "SR": 0.53125, "CSR": 0.5366586538461539, "EFR": 1.0, "Overall": 0.6979567307692307}, {"timecode": 26, "before_eval_results": {"predictions": ["bacteriophage T4", "1606", "Xingu", "The Ruhr", "Dar es Salaam", "Heinkel Flugzeugwerke", "Victor Garber", "Pope John X", "Harold Edward Holt", "aged between 11 or 13 and 18", "\"Histoires ou contes du temps pass\u00e9\"", "Orchard Central", "Chris Hemsworth", "late eighteenth century", "\"The Snowman\"", "1979", "Premier League club", "port city of Aden, on the southern coast", "northeastern Algonquian tribes", "Louis Mountbatten", "2013", "Archie Andrews", "2 May 2015", "Hess 15", "Crystal Dynamics", "Cleveland Cavaliers", "goalkeeper", "Debbie Harry", "\"media for the 65.8 million,\"", "John Travolta", "Hall & Oates", "Mazatl\u00e1n", "striker", "Las Vegas, Nevada", "1919", "Kevin Spacey", "Love Streams", "Michael Edwards", "The Rite of Spring", "Lake Wallace", "England", "1993", "Boston Celtics", "The Eisenhower Executive Office Building", "6,396", "Australian coast, primary products, consumer cargoes and extensive passenger services", "The Saturdays", "Attack the Block", "Leonarda Cianciulli", "Alfond Stadium", "Tudor music and English folk-song", "CMYK", "1600 BC", "The centuries - old Jedi Grand Master", "1963", "a palla", "The Herald of Free Enterprise,", "Nutshell", "361", "10 to 15 percent", "\"It has never been the policy of this president or this administration to torture.\"", "Tarzan of the Apes", "Kong Gumi", "a kite"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5771834935897435}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3339", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-1352", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-1677", "mrqa_newsqa-validation-4143", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-3515", "mrqa_searchqa-validation-13669"], "SR": 0.515625, "CSR": 0.5358796296296297, "EFR": 1.0, "Overall": 0.6978009259259259}, {"timecode": 27, "before_eval_results": {"predictions": ["immediately north of Canaveral at Merritt Island", "pedagogy", "Catholic", "Extension", "Cinderella", "Dan Tyminski", "Guthred", "Fall 2017", "Kolkata", "Dumb and Dumber", "Boeing EA-18G Growler", "IT products and services", "Paper", "Sir Matthew Alistair Grant", "most awarded female act of all-time.", "pneumatic tyres", "Bonkyll Castle", "F.C. Kossou Bib\u00f3", "Algernod Lanier Washington", "Josh Hartnett", "due to a leg injury", "Antonio Salieri", "American", "Europe", "What You Will", "Brooklyn, New York", "Thriller", "Jesper Myrfors", "The Supremes", "Cersei", "Kamehameha I", "his father", "Don Bluth", "2008", "Chief of the Operations Staff of the Armed Forces High Command", "Hong Kong Disneyland", "London", "I Won't Let You Down", "September 8, 2017", "FBI", "Christine MacIntyre", "July 16, 1911", "Wildhorn, Bricusse and Cuden", "Hotch kiss M1914 machine gun", "Elena Verdugo", "Germany's position in a Europe", "January 2004", "co-founder and lead guitarist", "ten", "seven species", "October 25, 1881", "J. Robert Oppenheimer", "Rukmini", "Mark Jackson", "Road / Track", "the Colossus", "Equatorial Guinea", "c3H8O3", "identity documents belonging to Miguel Mejia Munera", "Chris Robinson and girlfriend Allison Bridges", "Somalia's coast.", "corsets", "tip on why you may be experiencing breathing or sinus problems.", "cQR - CQ Press Library"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5762152777777778}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.4000000000000001, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3840", "mrqa_squad-validation-1916", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-873", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3346", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-3400", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-2957", "mrqa_naturalquestions-validation-3536", "mrqa_triviaqa-validation-2051", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-6398"], "SR": 0.484375, "CSR": 0.5340401785714286, "EFR": 1.0, "Overall": 0.6974330357142857}, {"timecode": 28, "before_eval_results": {"predictions": ["public (government)", "boarding schools", "The National Science Foundation Network", "London", "Henry Mancini", "jap", "Barbata Hotel Casino & Spa", "Gorbachev", "Stripes", "a rose", "Rameses II (Yul Brynner)", "London's Notting Hill", "a horse", "a balsamea", "Paddy Doherty", "smallpox", "colossus of rhodes", "the Central African Republic", "The Twist", "F\u00fcr Elise", "Saudi Arabia", "Who's Who in David Copperfield", "wry, compassionate, and brimm[ing]", "a Musketeer", "April", "Eric Morley", "atenlol and lisinopril", "The Garrick Club", "Belle", "Tim Finney", "Manhattan", "Stoppard", "The Greatest: Remembering Muhammad Ali's Rumble in the Jungle", "colossus", "off the coast of Northumberland", "a Scotch bonnet", "pianist", "Everett", "Llanfairpwllgwyngyll, Llan fairfechan", "Cardiff", "Louisiana", "a carburetors", "Tahrir Square", "Romanian", "bathtub curve", "Michael Caine", "Snooty", "Edvard Grieg", "James", "Meerkat", "Greek", "passion fruit", "Thomas Lennon", "Haikou on the Hainan Island", "Thomas Hobbes", "Denmark and Norway", "1966", "North America", "Florida's Everglades.", "Garth Brooks", "the sport of kings.", "drive", "a set of steak knives", "Florida"], "metric_results": {"EM": 0.375, "QA-F1": 0.47080853174603177}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6891", "mrqa_squad-validation-6918", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3726", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-6165", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3586", "mrqa_triviaqa-validation-7182", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-9024", "mrqa_hotpotqa-validation-2910", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1004", "mrqa_searchqa-validation-12186", "mrqa_searchqa-validation-15919"], "SR": 0.375, "CSR": 0.5285560344827587, "EFR": 1.0, "Overall": 0.6963362068965517}, {"timecode": 29, "before_eval_results": {"predictions": ["Chicago Theological Seminary", "ABC", "$100,000", "Super Bowl LII", "starch", "Taylor Michel Momsen", "Kennedy Space Center ( KSC ) in Florida", "Tim Duncan", "James W. Marshall", "Blue laws", "Randy VanWarmer", "the U.S. Senate", "Gugu Mbatha - Raw, Ian McKellen", "between 8.7 and 9.1", "between January 21 and February 20", "if the occurrence of one does not affect the probability of occurrence of the other", "Jason Flemyng", "Chesapeake Bay, south of Annapolis in Maryland", "northern China", "T.J. Miller", "Pyeongchang County, Gangwon Province, South Korea", "the status line", "retina", "Tim Rooney", "the Triple Alliance of Germany", "Andrew Lloyd Webber", "1955", "Nick Sager", "Buffalo Lookout", "Humpty Dumpty", "Charlene Holt", "1 US dollar", "Leonard Nimoy", "1960", "Primes", "the tax rate paid by a small business", "beneath the liver", "Andy Serkis", "West Norse sailors", "Kristy Swanson", "Bonnie Plunkett", "1995", "Fleetwood Mac", "technological advances in printing", "Cairo, Illinois", "1970s and'80s", "in the books of Exodus and Deuteronomy", "Lula", "Psychomachia", "January 2, 1971", "The Miracles", "three - domain system of taxonomy", "elbow", "Big Mama", "Charlie heston", "\"Twice in a Lifetime\"", "George Orwell", "Bardot", "teenager", "Long Island convenience store", "former Massachusetts governor", "rock", "heart", "Dancing with the Stars"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5939674588934458}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.21052631578947367, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-6865", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-3968", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-1958", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-4017", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7426"], "SR": 0.546875, "CSR": 0.5291666666666667, "EFR": 0.9655172413793104, "Overall": 0.6895617816091955}, {"timecode": 30, "before_eval_results": {"predictions": ["San Jose", "the number of quality rental units", "cultural tourism", "Eccentricity", "( Sequoia sempervirens)", "The Fairly Odd Parents", "Spanish Republic", "taxicab", "coyote", "(PVM)", "Harry Reid", "Ray", "(or point)", "forge", "(St.) Thomas Edison", "(Why did he kill them?)", "Flowerbomb", "Blackbird", "Footprints", "charlie", "LA Kings", "(DR)", "The Fugitive", "(Luke 19:4)", "The Memory Keeper's daughter", "George Eliot", "hubris", "Yahtzee", "( Tony Danza)", "(St.) Berners-Lee", "(overlapping circles)", "74.3", "William S. Hart", "(St.) Muhammad", "Pride and Prejudice", "(St.) Peter", "Kosher Wines", "Munich", "Michael Jordan", "(St.) Andrew's", "charlie", "(George Takei, Mr. Sulu", "(Pulsatrix perspicillata)", "(pte brise)", "(Aso 1)", "honey", "Boston", "(St.) Leonidis", "Arctic Ocean", "pizza", "butternut squash", "Spain", "Thomas Chisholm", "May 2002", "1936", "Newfoundland and Labrador", "The Fortune cookie", "Spamalot", "1911", "(Bryan Phillips)", "(Principia) containing basic laws of physics.", "(Kuntar) Goldwasser and Eldad Regev.", "adidas", "anti-trust laws."], "metric_results": {"EM": 0.390625, "QA-F1": 0.4502604166666666}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true], "QA-F1": [0.0, 0.7499999999999999, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-315", "mrqa_squad-validation-7576", "mrqa_squad-validation-2965", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-2374", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-11167", "mrqa_searchqa-validation-768", "mrqa_searchqa-validation-6792", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-16307", "mrqa_searchqa-validation-12408", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-5162", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-12415", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-13549", "mrqa_searchqa-validation-9773", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-9379", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-6412", "mrqa_naturalquestions-validation-10656", "mrqa_triviaqa-validation-28", "mrqa_hotpotqa-validation-4718", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-1406"], "SR": 0.390625, "CSR": 0.5246975806451613, "EFR": 1.0, "Overall": 0.6955645161290323}, {"timecode": 31, "before_eval_results": {"predictions": ["Rev. Paul T. Stallsworth", "blue", "stand-up", "satirical erotic romantic comedy", "Ferengi Quark", "Federal Minister for Transport, Innovation and Technology", "June 26, 1970", "The Worcester Cold Storage and Warehouse Co. fire", "Elena Stefanik", "Fleetwood Mac", "Odense Boldklub", "President of the Supreme Court, Yitzhak Kahan", "Bangkok, Thailand", "The Sooners", "Merrimack", "Charlie Wilson", "The Late Late Show", "Mark Anthony \"Baz\" Luhrmann", "two", "The Indianapolis Motor Speedway", "Ravenna", "Anita Dobson", "a family member", "January 19, 1943", "The Worm", "Eliot Cutler", "Mercury Records", "1970s and 1980s", "C. J. Cherryh", "Pablo Escobar", "Asbury Park, New Jersey", "Rockland", "\"Slaughterhouse-Five\"", "The Adventures of Huckleberry Finn", "wineries", "Frank Sinatra", "Robert L. Stone", "goalkeeper", "Philadelphia", "New York", "Massapequa", "Sinngedichte", "The Highwayman", "Madrid", "Kevin Spacey", "Arizona State University.", "Blue Grass Airport", "Lawton Chiles", "1952", "the Nebula Award, the Philip K. Dick Award", "I'm Shipping Up to Boston", "the Royal Air Force", "Lenny Jacobson", "Hathi Jr.", "1935", "a lion", "\"foreigner,\"", "The Miracles", "Winter Park at Union Station in Denver,", "anti-trust laws.", "Friday,", "the High Plains", "The Atlantic", "Ukraine"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6484107905982905}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.4444444444444444, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-2607", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-434", "mrqa_hotpotqa-validation-2554", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-5034", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-6414", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-6055"], "SR": 0.546875, "CSR": 0.525390625, "EFR": 0.9655172413793104, "Overall": 0.6888065732758621}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh L. Dryden", "2004", "Kenya", "The Rocky Horror Picture Show", "Trainspotting", "Argentina", "Apollo 4", "jellyfish", "March", "a clogs", "Fauntleroy", "the World Health Organization", "i second that emotion", "Kofi Annan", "oxygen", "first daily newspaper in London", "Taggart", "Che Amanwe and Chi Eekway", "the Gulf of Mexico", "i second that emotion", "Sven Goran Eriksson", "Barry Taylor", "Route 66", "Brussels", "left the country aboard the French frigate L'Heureux", "John Poulson", "Orly", "the euro", "i second that emotion", "the Precambrian Shield", "Laurent Planchon", "the Solent", "vomiting", "i second that emotion", "Bristol Aeroplane Company", "Spinach", "Stephen Hendry", "i second that emotion", "i second that emotion", "Surrey", "1971", "chippenham", "London", "the i second that\u2019s envied by the temperature climate not dissimilar to the climate of California", "i second that emotion", "borax", "stadtschnellbahn", "Jamaica", "Peter Nichols", "Jason David", "Kent", "Vickers-Armstrong's", "Ray Charles", "very important", "customary", "Miller Brewing", "northwestern Italian coast", "Melbourne", "revelry", "her decades-long portrayal of Alice Horton", "\"I loved the convenience [of the train],' she says. and when you leave Denver through the northern plains and into the mountains,\"", "Peter Bogdanovich", "a dove", "the Federal Republic of Germany"], "metric_results": {"EM": 0.421875, "QA-F1": 0.47815795595533495}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.12903225806451615, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-5360", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5129", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-7262", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-6989", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-5817", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-3368", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-5611"], "SR": 0.421875, "CSR": 0.5222537878787878, "EFR": 1.0, "Overall": 0.6950757575757576}, {"timecode": 33, "before_eval_results": {"predictions": ["New York and Virginia", "1887", "Lana Del Rey", "1,228 km / h ( 763 mph )", "New England Patriots", "Doc '' Brown", "Antarctica", "Mitch Murray", "blue", "Gunpei Yokoi", "John Bull", "775 rooms", "eusebeia", "waiting tables at the Moondance Diner", "a sweet alcoholic drink made with rum, fruit juice, and maple or dorsadine", "a snood", "Jesus'birth", "a habitat", "bristol", "Central Germany", "Andrew Johnson", "dennis de Mestre", "Menelaus", "electors", "Julia Ormond", "Sauron's", "1961", "Mike Mushok", "2013", "March 1", "book", "redox", "Spain", "Taylor Hayes", "Paul Lynde", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Jocelyn Flores", "abdicated in November 1918", "one of the most recognisable structures in the world", "erosion", "March 2, 2016", "stuffing", "1996", "Ray Charles", "18", "Ramones", "completed in 1800", "Anglo - Norman French waleis", "Frank Theodore", "New Jersey", "May 2010", "built in France", "Heath Ledger", "dennis taylor", "a centaur", "a young getaway driver", "cricket fighting", "Luis Resto", "drama that pulls in the crowds", "\"Ivan the Terrible,\" a guard at the Sobibor death camp", "atlantic", "Tunisia", "dennis taylor", "bios & Profiles"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49299358265296295}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.06451612903225806, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.5263157894736842, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8205128205128205, 0.0, 0.4210526315789474, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3127", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-187", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-3396", "mrqa_triviaqa-validation-5607", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-1997", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2243", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-9333"], "SR": 0.40625, "CSR": 0.5188419117647058, "EFR": 1.0, "Overall": 0.6943933823529411}, {"timecode": 34, "before_eval_results": {"predictions": ["Venus", "Beyonc\u00e9 and Bruno Mars", "Zeebo", "Brahmasputha Siddhanta ( 7th century )", "2018", "her abusive husband", "premiered on CBS on September 29, 2017", "interstellar medium ( ISM ) and giant molecular clouds ( GMC )", "transmission and final drive", "Universal Pictures", "Vijay Prakash", "March 14, 1942", "Nick Sager", "London boroughs, Metropolitan boroughs, unitary authorities, and district councils", "a biblical title of respect applied to prophets and beloved religious leaders", "the state legislators of Assam", "Gastric acid, gastric juice or stomach acid", "Renishaw Hall, Derbyshire, England, UK", "accomplish the objectives of the organization", "sport utility vehicles", "Isabella Palmieri", "atm ( 101.325 kPa )", "Mind your Ps and Qs is an English expression meaning `` mind your manners ''", "Germany \ufffds failure to destroy Britain \ufffds air defences to force an armistice ( or even outright surrender )", "20 November 1989", "Tom\u00e1s de Torquemada", "Bob Gaudio", "1975", "Mel Gibson", "Procol Harum", "Erica Rivera", "lithium", "first published in the United States by Melvil Dewey in 1876", "2003", "Sebastian Lund", "Sunday, 2 September to Wednesday, 5 September 1666", "Pebble Beach", "The management team", "gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Flex SDK", "Steveston Outdoor pool in Richmond", "Phillip Schofield and Christine Bleakley", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Ukraine", "Lula", "1850", "activated by the footbrake brake being in use when the car comes to a halt", "early Christians of Mesopotamia", "Derrick Henry", "ice cap climate ( K\u00f6ppen EF )", "`` Mirror Image ''", "engraved on a bronze plaque and mounted inside the pedestal's lower level", "a chocolate cereal", "dennis taylor", "Brian Close", "a hard rock/blues rock band", "Galleria Vittorio Emanuele II", "totalitarian and oppressive world of World War II-era Japan", "\"At this moment we are trying to reach a consensus so we can reconstruct democracy,\"", "last week", "Henry Ford", "Toyota", "Abraham Lincoln", "a pituitary gland"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5727935022052668}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.6, 0.4615384615384615, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 0.42857142857142855, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.11764705882352941, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7567567567567568, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.08333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-10576", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-1562", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-904", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-15641"], "SR": 0.453125, "CSR": 0.5169642857142858, "EFR": 0.9428571428571428, "Overall": 0.6825892857142858}, {"timecode": 35, "before_eval_results": {"predictions": ["domestic Islamists who attacked it", "a supposed mild euphoric", "James McConkey", "Venezuela", "Kansas", "The Ring", "Peter Pan", "dennis tMLipp", "the Arctic Ocean", "the air going out causes a vacuum effect, so the egg is sucked in by the differential in this between the inside & outside of the bottle", "a weir", "Lafayette", "Elijah Muhammad", "a tropical depression", "the Royal Marines Band", "Alexander Pushkin", "Australia", "The War of the Worlds", "puebla", "a night shift", "pope", "Arkansas", "Subclue 2", "Pierre-August Renoir", "a manque", "gioachino", "Innsbruck", "Lance Ito", "Microsoft", "petticoat", "Sony", "Viking Museum Ship, Sea Stallion, Roskilde, Denmark", "Atlantic City's", "Blackwater USA", "elephants", "American Airlines", "the Markhor", "Odysseus", "Geronimo", "Kensington Palace", "Charles Bassett", "Netherlands", "Captain John Smith", "The Creator of Narnia", "John Galt", "the amygdala", "Chicago Mercantile Exchange", "Las Vegas", "danskins", "to the stars", "Pablo Casals", "an ostrich or common ostrich", "1943", "The City of San Antonio", "beneath the liver,", "James I", "penrhyn", "a psychological  horror film", "John Morgan", "Hungarian Rhapsody No. 2", "Henry II", "Sen. Jon Tester", "63", "\"We are resetting, and because we are resetted, the minister and I have an overload of work.\""], "metric_results": {"EM": 0.40625, "QA-F1": 0.49055735930735933}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.09090909090909091, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9699", "mrqa_searchqa-validation-11665", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-265", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-4473", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-11675", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-5646", "mrqa_searchqa-validation-3975", "mrqa_searchqa-validation-8694", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-15775", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-5219", "mrqa_searchqa-validation-5473", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-3348", "mrqa_hotpotqa-validation-3745", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352"], "SR": 0.40625, "CSR": 0.5138888888888888, "EFR": 1.0, "Overall": 0.6934027777777778}, {"timecode": 36, "before_eval_results": {"predictions": ["electric lighting", "James W. Marshall", "Terrell Suggs", "north of the Equator", "Lucknow", "in the eighth episode of Arrow's second season", "National Industrial Recovery Act ( NIRA )", "The User State Migration Tool ( USMT )", "the Battle of Antietam", "William DeVaughn", "the National September 11 Memorial plaza", "Southend Pier", "Santa Monica", "layered systems of sovereignty", "Aiko Watanabe", "31 January 1934", "Filipino", "1773", "RAM", "September 15, 2012", "April 1917", "Bart Cummings", "October 27, 1904", "Harishchandra", "Olivia Olson", "1990", "The pretentious, materialistic desires of a wannabe rockstar, who craves money, cars and women", "Bill Pullman", "BC Jean", "in IMAX 3D", "Daryl ( Bryan Cranston )", "stratum lucidum", "60", "Hasmukh Adhia", "four", "as few as 5 photoreceptor cells", "1980s", "in soils", "card verification value", "`` rebuke with all authority '' ( Titus 2 : 15 )", "bohrium", "Britain", "Escherichia coli", "Archduke Franz Ferdinand of Austria", "June 1991", "2010", "he lost the support of the army, abdicated in November 1918, and fled to exile in the Netherlands", "in De Inventione by Marcus Tullius Cicero", "Mike Czerwien", "103", "Vienna", "English", "Mexico", "Stalin", "$10.5 million", "Al Horford", "Andrew Johnson", "$30.6 million", "leftist Workers' Party", "his mother, Katherine Jackson,", "cotton", "Dennis Haysbert", "Quinn", "Towcester"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6769503066378066}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.7142857142857143, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-4351", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1953", "mrqa_searchqa-validation-13161"], "SR": 0.546875, "CSR": 0.5147804054054055, "EFR": 0.9310344827586207, "Overall": 0.6797879776328053}, {"timecode": 37, "before_eval_results": {"predictions": ["Joseph Swan", "brazil", "South Africa", "minorities", "brazilian", "a cappella", "albinism", "cavalry", "an aglet", "NBC\u2019s Saturday Night Live", "FC Bayern M\u00fcnchen", "brazil", "Bonnie and Clyde", "brazil", "copper", "Dawn French", "Blackstar", "b Benedict", "Doris Lessing", "adults", "swaziland", "a building for the Royal College of Art (where Casson was Professor of Interior Design)", "Kent", "brazil", "a points based scoring system", "brazil", "Kent", "Rodgers & Hammerstein", "Culture Club", "Galileo Galilei", "Zelle", "Scotland Yard", "Marilyn Manson", "Medellin", "The Tempest", "spark plugs", "brazilts", "Boulder Dam", "painkillers", "Saudi Arabia", "sex blogger", "Gateshead", "brazil", "rain", "blue, yellow, and red", "the scientists of Laputa", "France", "Smokey", "kunsky", "dying, death and how human beings respond to the inevitability of their mortality and the reality of loss", "Lady Penelope", "the forces of Andrew Moray and William Wallace", "142,907", "mid November", "YouTube", "Theo James Walcott", "Sir Charles Benedict Ainslie, CBE (born 5 February 1977)", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "heavy turbulence", "different women coping with breast cancer in", "blankets", "a sunflower", "Madonna", "January,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4902704203691046}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333336, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.7368421052631579, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-5175", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5058", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7074", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-5596", "mrqa_triviaqa-validation-6503", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2982", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-687", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-8884", "mrqa_hotpotqa-validation-2580", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-6291", "mrqa_newsqa-validation-3847"], "SR": 0.421875, "CSR": 0.5123355263157895, "EFR": 0.972972972972973, "Overall": 0.6876866998577525}, {"timecode": 38, "before_eval_results": {"predictions": ["the Rip", "tyne", "liver", "40", "table salt", "light", "cuba", "le", "Phil Redmond", "Stevie Wonder", "head", "dogs", "hanover", "an astronomy", "Charles I of England", "workless", "scales", "Dirty Dancing", "henryes", "Diana Ross", "Montezuma", "a 1934 Austin seven box saloon", "Richie Unterberger", "Carthage", "cuba", "lewis", "Blade Runner", "Jay-Z", "leopards", "a cymbal", "santa gently paws", "saturday", "lebgate", "henry seidman", "South Africa", "Marie Trepanier", "a Blackop/Salopia", "a toothed whale", "Georgia", "France", "raspberries", "pilgrimage", "Cyprus", "speed camera", "duke", "lizard", "nuclear missile", "frauds", "a sea horse", "even numbers", "Tony Blair", "quartz or feldspar", "54 Mbit / s, plus error correction code", "Manley", "Stacey Kent", "\"Traumnovelle\" (\"Dream Story\"),", "Anthony Ray Lynn", "piano", "paid tribute to pop legend Michael", "the United States", "French Guiana", "albion", "arms", "Tiger Woods"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5281994047619047}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-6161", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3351", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6603", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-5730", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-9940", "mrqa_searchqa-validation-4817", "mrqa_newsqa-validation-3899"], "SR": 0.4375, "CSR": 0.5104166666666667, "EFR": 1.0, "Overall": 0.6927083333333334}, {"timecode": 39, "before_eval_results": {"predictions": ["\"No, that's no good\"", "Aldi", "Midnight Cowboy", "charles", "seborrheic dermatitis", "norman bachrie", "a hoy", "Niger", "central Stockholm", "Tangled", "dog", "jimmy davis", "Bulls Eye", "Adam Smith", "norman planchon", "Martin Clunes", "Jane Austen", "pembrokeshire", "Kevin macdonald", "peppers", "pannotia", "john Mellencamp", "Isambard Kingdom Brunel", "gaul", "1957", "bristol", "charles", "a sauce of lemon juice, parsley, and drawn butter", "micelles", "Ralph Vaughan Williams", "musical scale", "cats", "flannel", "charles pchaikovsky", "Shanghai", "Spain", "grow", "Tuesday", "Guru Nanak", "bleak house", "Inigo Montoya", "phosphorus", "charles charles", "Indianapolis", "norman humbert", "cuckoo", "detective", "Dodge Ram", "Alice Cooper", "mallorca", "blood", "Royal Bengal Tiger", "a circular orbit", "peter", "was an American newspaper based in New York City", "1999", "Sela Ann Ward", "The Cycle of Life", "forgery and flying without a valid license,", "137", "a log cabin", "St. Patrick's Day", "defensive backs", "Sondheim"], "metric_results": {"EM": 0.40625, "QA-F1": 0.46800595238095233}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-7408", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6153", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5688", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-2711", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8554", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2100", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-807", "mrqa_naturalquestions-validation-9755"], "SR": 0.40625, "CSR": 0.5078125, "EFR": 0.9473684210526315, "Overall": 0.6816611842105262}, {"timecode": 40, "before_eval_results": {"predictions": ["19th Century", "Famous Players-Lasky Corporation", "norway", "Thomas De Quincey", "yersinia pestis", "horse", "buffalo", "Octavian", "a pigeon", "Sarajevo", "the Bill of Rights", "Fine", "Neighbours", "shirley boyne", "a trumpet", "Westminster Abbey", "origami", "electrical resistance", "the Arabian Gulf", "stockholm", "jane jane pembroke", "mater", "jack Nicholson", "\u201cTonight Is Another Day\u201d", "diesel", "Tomorrow Never Dies", "Sudan", "a Great Dane", "norway", "norway", "New Hampshire", "jimmy i", "jimmy broadbent", "japan", "purple", "everyHit.com", "charles taylor", "a wave", "Venice", "5", "Southwest Airlines", "a person born within hearing distance of the sound of Bow bells", "jane deaver", "The Comedy of Errors", "chile", "glyn Jones", "President Ford and first lady Betty Ford", "a crossword clue", "norway", "al-Qaeda", "rodins", "Humpty Dumpty and Kitty Softpaws", "August 18, 1998", "Vijay Prakash", "the EN World web site", "the 100th anniversary of the first \" Tour de France\" bicycle race", "a dimensionless quantity", "Janet and La Toya", "more than 2.5 million", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter", "Thomas A. Anderson", "Curb Your Enthusiasm", "irena", "Inequality of opportunity was higher in the transition economies of Central and Eastern Europe and Central Asia"], "metric_results": {"EM": 0.3125, "QA-F1": 0.39823778195488724}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.3157894736842105]}}, "before_error_ids": ["mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-2275", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-4716", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4538", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-863", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-4337", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-7346", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-864", "mrqa_newsqa-validation-2372", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-11519", "mrqa_naturalquestions-validation-3969"], "SR": 0.3125, "CSR": 0.5030487804878049, "EFR": 0.9772727272727273, "Overall": 0.6866893015521065}, {"timecode": 41, "before_eval_results": {"predictions": ["1220", "chile", "rhea", "nixon", "henry", "roddy Doyle", "an abacus", "Robin Hood", "Aeolus", "lazquez", "Dutch", "caracas", "oregon", "henry spencer", "oregon", "Scotland", "true stories", "dennis Bowie", "Neil Armstrong", "george Steiner", "spencer", "jack Johnson", "rust", "spencer aniston", "william county", "tbilisi", "william spencer", "othello", "fabric", "spencer brazil", "Lacock Abbey", "spencer", "domestic cat", "anita Brookner", "james", "richard meir", "Black Sea", "bagram", "Susie Dent", "a power outage", "london", "The Archers", "Launcelot", "james phonograph Company", "chicago", "james boyd", "marcella", "the Marx Brothers", "henry", "henry", "Dry Ice", "Pat McCormick", "April 2018", "2001", "from 1993 to 1996", "james Gandolfini", "March 23, 2017", "he and the others were trained for more than a year in Pakistan by Lashkar-e-Tayyiba,", "June 6, 1944", "sniff out cell phones.", "a bassoon", "o.K. Corral", "butternut squash", "usistocles"], "metric_results": {"EM": 0.4375, "QA-F1": 0.49416666666666675}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-7425", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-3306", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-5625", "mrqa_triviaqa-validation-6097", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-2641", "mrqa_triviaqa-validation-7225", "mrqa_naturalquestions-validation-1163", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-9161", "mrqa_searchqa-validation-233"], "SR": 0.4375, "CSR": 0.5014880952380952, "EFR": 0.9722222222222222, "Overall": 0.6853670634920634}, {"timecode": 42, "before_eval_results": {"predictions": ["lack of reliable statistics from this period", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.", "Eintracht Frankfurt", "south of Beirut", "\"revolution of values\"", "Jeddah, Saudi Arabia", "40", "his chest", "\"I'm just getting started.\"", "Manny Pacquiao", "$250,000", "27,", "Michelle Obama", "Alberto Fujimori", "Salt Lake City, Utah", "our showroom", "the \" Michaoacan Family,\"", "64", "New Delhi, India", "fastest", "Department of Homeland Security Secretary Janet Napolitano", "iran's parliament speaker", "ended his playing", "on the set at \"E! News\"", "Haiti's capital, Port-au-Prince", "Madeleine K. Albright", "a dike", "breast cancer", "Benazir Bhutto", "July", "U.S. senators", "south africa", "Larry Ellison", "our local political representative", "her fianc\u00e9,", "cal Ripken Jr.", "Johannesburg", "cancer", "acid attack", "Vernon Forrest", "take a more active role in countering the spread of the", "one", "play piano lessons", "the company says it recycles 100% of its byproducts which supplies 80% of the operation energy at the plant.", "about 5:20 p.m. at Terminal C", "Herman Thomas", "\"a gift\" from U.S. President-elect Barack Obama.", "a man's lifeless, naked body", "\"release\" civilians,", "jennifer fayed", "we can use solar and renewable energy at home everyday,\"", "when a population temporarily exceeds the long term carrying capacity of its environment", "Real Madrid", "emperor Cuauhtemoc", "phnom Penh", "our mutual friend", "johnson purdy", "Antonio Lippi", "Thorgan ganael Francis", "River Clyde", "brazil", "johnny weissmuller", "Cy Young", "Reese Witherspoon"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5722533132072607}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7272727272727273, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8421052631578948, 1.0, 0.0, 0.4, 0.0, 0.8, 0.06666666666666667, 1.0, 1.0, 0.1818181818181818, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3979", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-1641", "mrqa_triviaqa-validation-4313", "mrqa_hotpotqa-validation-727", "mrqa_searchqa-validation-6881"], "SR": 0.4375, "CSR": 0.5, "EFR": 1.0, "Overall": 0.690625}, {"timecode": 43, "before_eval_results": {"predictions": ["25,033", "House of Borromeo", "Washington, D.C.,", "1943", "the Volvo 850", "the Mountain West Conference", "the National Basketball Association (NBA)", "Western Europe", "political thriller", "Juergen M. Geissinger", "English football", "1989", "Distinguished Service Cross", "\"50 best cities to live in.\"", "Bridgetown,", "Lollywood and Pollywood films", "Emmanuel ofosu Yeboah", "Ant-Man", "Bhushan Patel", "1986", "1916", "Reginald Engelbach", "Vince Staples", "Archbishop of Canterbury", "Galway", "ZZ Top, Lynyrd Skynyrd, Cinderella, Queensr\u00ffche, Heart, Ted Nugent, Charley Pride, and Ricky Skaggs", "1988", "coaxial", "Northern Lights", "three different covers", "Malayalam cinema", "\"Regno di Dalmazia\"", "August 11, 1946", "Vincent Landay", "May 26, 2010", "\"Estadio de L\u00f3pez Cort\u00e1zar\"", "Brian A. Miller", "Christian Dug Uruguay", "1985", "Gal Gadot", "Meghan Markle", "Boeing B-17 Flying Fortress", "Erika Girardi", "Joe Scarborough", "British", "76,416", "Bonkyll Castle", "second cousin once removed", "2012 Summer Olympics", "Sony Studio Liverpool", "Brig Gen Augustine Warner Robins,", "the United Nations Convention on the Rights of the Child", "Lewis Carroll", "two occasions", "a trademark", "blue", "tennis elbow", "Citizens are picking members of the lower house of parliament,", "the Employee Free Choice act in Lafayette Square in Washington", "the release of the four men -- Jesus Ortiz, 19; Stalin Felipe, 19, Kevin Taveras, 20; and Rondell Bedward, 21;", "a rake", "Jack the Ripper", "a carriage", "teak"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6843543458353241}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.888888888888889, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.08695652173913042, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6153846153846153, 0.6153846153846153, 0.38095238095238093, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7278", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-1464", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7240", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2345", "mrqa_newsqa-validation-3804"], "SR": 0.5625, "CSR": 0.5014204545454546, "EFR": 1.0, "Overall": 0.6909090909090909}, {"timecode": 44, "before_eval_results": {"predictions": ["British Prime Minister Edward Heath", "Sean Yseult", "Washington, D.C.", "over 12 million", "Luchadora", "Conservatorio Verdi in Milan", "President of the United States", "the backside", "\"the Docile Don\"", "The Future", "the Knight Company", "Andrew Joseph", "Denmark", "the 2015 Orange Bowl", "Margarine Unie", "death", "Fort Valley, Georgia", "Bill Paxton", "Vladimir Valentinovich Menshov", "Kramer Guitars", "the Dominican Republic", "Humberside Airport", "June 12, 2017", "Douglas Jackson", "the figure-eight variety", "Blackpool Football Club", "William Lyon Mackenzie King", "Ted", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Chrysler", "Bruce Grobbelaar", "Honda Ballade", "Ascona", "Boston Celtics", "Austrian", "Australian Electoral Division", "Sun Tzu, Socrates, Napoleon, Malcolm X, and James Baldwin", "American singer Toni Braxton", "Hindi", "Richard Masur", "Brian Patrick Friel", "Bad Religion", "\"Dr. Gr\u00e4sler, Badearzt\"", "Alexandre Dimitri Song Billong", "The Arizona Health Care Cost Containment System", "Mineola", "Gian Carlo Menotti", "bobsledder", "Mazda", "102,984", "Roscoe Lee Browne", "the second team ( the first AFL / AFC team )", "Charles Gibbs", "216", "The Spectator", "Easter Parade", "The first performance of Elgar\u2019s \u2018Enigma\u2019 Variations", "last summer.", "almost 100", "into the Southeast,", "a piece of the pie", "Great Balls of Fire", "the advance, a rapid cavalry", "One Direction"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6789806547619048}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.33333333333333337, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3087", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-12050"], "SR": 0.578125, "CSR": 0.503125, "EFR": 1.0, "Overall": 0.6912499999999999}, {"timecode": 45, "before_eval_results": {"predictions": ["to take charge of Methodist activities there", "quod erat demonstrandum", "Elizabeth II", "Gaius Julius Caesar", "Northern Exposure", "cocoa butter", "Kokomo", "Esther", "Stanley D. Winck", "Maurice Halperin", "miniature golf", "CNN", "Punxsutawney,", "Bratislava", "yellow fever", "sea otters", "M&M's", "\"submarine\" or \"sub\"", "rod", "Nixon's 'dirty tricks' man,", "horse training", "moon Observers", "Mickey Mouse", "anthers", "associate professor", "fruit snacks", "Medusa", "a helix", "a domestic cat", "musical notes", "Voyager 1", "Farsi (Persian)", "glucose", "capuchins", "China", "Helen of Sparta,", "animal products", "the peace sign", "Morrie: An Old Man, a Young Man", "books", "Rajasthan", "joined a loose-knit underworld gang that commits a robbery about once every five to ten years", "joined by slender, square columns", "NHL", "(over) one's head", "How shall he cut it", "joined", "William Wordsworth", "brushes", "A dwarf planet", "Arabian Nights", "Vincent Price", "Rugrats in Paris : The Movie", "Middle Eastern alchemy", "London", "Isle of Wight", "The Peppercorn Pioneer", "\"Queen In-hyun's Man\"", "Oneida Limited", "James Worthy", "Libreville, Gabon.", "two tickets", "Christopher Gregory Roussie.", "Tuscaloosa"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4958096590909091}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false], "QA-F1": [0.18181818181818182, 0.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9923", "mrqa_searchqa-validation-2593", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-3796", "mrqa_searchqa-validation-7347", "mrqa_searchqa-validation-13022", "mrqa_searchqa-validation-4671", "mrqa_searchqa-validation-13469", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-8467", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-5006", "mrqa_searchqa-validation-11964", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-3322", "mrqa_triviaqa-validation-6557", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-3949", "mrqa_triviaqa-validation-888"], "SR": 0.40625, "CSR": 0.5010190217391304, "EFR": 0.9736842105263158, "Overall": 0.6855656464530893}, {"timecode": 46, "before_eval_results": {"predictions": ["the position of people within the four-class system", "Jorge Lorenzo", "Frank McCourt", "Indiana Jones", "fungi", "Venus flytrap", "Abraham", "orwell", "Faggot", "the black geese", "California Chrome", "Pluto", "Route 66", "the Taklamakan Desert", "South Asia", "Gemini", "Great Victoria Desert", "Germany", "the British pop band Go West", "December 18, 1958", "1749", "Portugal", "Operation Overlord", "Birmingham", "a genie", "Sedgefield", "Coral Sea", "Saddam Hussein", "Nadia Comaneci", "tank Museum", "South Korea", "the pig", "\"Oasis'", "chile", "Kenya", "Stephen Potter", "Verona", "Anwar Sadat", "a hundred", "Susquehanna River", "Argentina", "Vader", "Frankfurt", "a chipmunk", "Goldie Hawn", "a pulsar", "Belgium", "a horse", "a little extra juice and zest", "Chelsea", "Sunny Leone", "Games played", "the girlfriend of her brother", "reproductive cloning", "in the 13th century", "1 January 1788", "Radcliffe College", "11", "\"Twilight\"", "the Louvre", "Speed Racer", "H. G. Wells", "Queen Elizabeth", "Sir Walter Scott"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6103174603174604}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true], "QA-F1": [0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8093", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3840", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3654", "mrqa_triviaqa-validation-4088", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5636", "mrqa_triviaqa-validation-7773", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5109", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3234", "mrqa_newsqa-validation-2953", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5788"], "SR": 0.53125, "CSR": 0.5016622340425532, "EFR": 1.0, "Overall": 0.6909574468085106}, {"timecode": 47, "before_eval_results": {"predictions": ["Arabah", "toadstool", "Sinclair Lewis", "bear", "Christmas Jones", "egg tempera", "Jonathan Demme", "Vaclav Havel", "Dick Van Dyke", "sahara", "Tina Turner", "2010", "benjamin", "a lens", "perfume", "Duke Orsino", "hematite", "carmenborg", "The Apprentice", "plimsoll", "Cubism", "sahara", "sahara", "eucharist", "zuckerman", "james", "Brick Lane", "Charles Foster Kane", "Lorne Greene", "archery", "carin", "Call My Bluff", "A", "andes", "frank McCourt", "oats", "sahara", "sealion", "starch", "pears", "Donna Summer", "Pillar", "nottingham", "Poland", "Welcome Stranger", "taggart", "decem", "Chechnya", "Spot", "A- Team", "football", "801,200", "Sir Ronald Ross", "Sun Tzu", "bioelectromagnetics", "Foxborough, Massachusetts", "Speedway World Championship", "golden city", "36", "Michelle Obama", "kbenhavn", "Proletariat", "sara", "Floxin"], "metric_results": {"EM": 0.453125, "QA-F1": 0.49242424242424243}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-3740", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-3179", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6892", "mrqa_triviaqa-validation-4665", "mrqa_triviaqa-validation-1730", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5726", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-334", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-7020"], "SR": 0.453125, "CSR": 0.5006510416666667, "EFR": 1.0, "Overall": 0.6907552083333334}, {"timecode": 48, "before_eval_results": {"predictions": ["Firth of Forth", "Caesars Entertainment Corporation", "Supergirl", "\u00c6lfgifu of York", "Creature Comforts", "Stephen Mangan", "William McKinley", "1905", "Vanilla air Inc.", "Mineola, New York", "dziga Vertov", "Strange Interlude", "Julia Compton Moore", "mash-Up", "1986", "early Romantic period", "The Gettysburg Address", "randmore, New South Wales", "Washington and Essex Streets", "Mathew Sacks", "Babylon", "Ford Falcon", "New York State Route 907E", "The Company", "177026", "Kim Bauer", "United States Food and Drug Administration", "Edward James Olmos", "Bury St Edmunds, Suffolk, England", "Prussian", "o", "1909 Cuban-American Major League Clubs Series", "86 ft", "American", "January 2004", "sulfur mustard H or HD blister gas", "The 45th Infantry Division", "2009", "5 Grammy Award", "Anita Dobson", "City of Westminster, London", "Boyd Gaming", "February 14, 1859", "Texas Tech University", "John McClane", "Larry Wayne Gatlin", "924", "The International Space Station", "piedmont", "Selinsgrove,", "her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "cake", "Najmeddin Al Hadad", "Space Shuttle Challenger", "basil", "Clio Awards", "The Rosie Show", "North Korea", "over 1,000 pounds).", "octavian", "a volcano", "Library of Congress", "the stroma"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6297575914423741}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false], "QA-F1": [0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8695652173913043, 0.19999999999999998, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-659", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-1352", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1115", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-4119", "mrqa_hotpotqa-validation-5714", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-6806", "mrqa_triviaqa-validation-6785", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-1028", "mrqa_naturalquestions-validation-4685"], "SR": 0.546875, "CSR": 0.5015943877551021, "EFR": 0.9655172413793104, "Overall": 0.6840473258268825}, {"timecode": 49, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-2508", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4095", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4589", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5192", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-564", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5755", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-1887", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2722", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3569", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-938", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-13836", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-15433", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-15641", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-16122", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-165", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1954", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4937", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6398", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-8410", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-975", "mrqa_squad-validation-10069", "mrqa_squad-validation-10086", "mrqa_squad-validation-1019", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-10397", "mrqa_squad-validation-10444", "mrqa_squad-validation-10449", "mrqa_squad-validation-1052", "mrqa_squad-validation-1129", "mrqa_squad-validation-1211", "mrqa_squad-validation-1265", "mrqa_squad-validation-1311", "mrqa_squad-validation-139", "mrqa_squad-validation-164", "mrqa_squad-validation-1672", "mrqa_squad-validation-1712", "mrqa_squad-validation-1916", "mrqa_squad-validation-2132", "mrqa_squad-validation-2155", "mrqa_squad-validation-2176", "mrqa_squad-validation-2326", "mrqa_squad-validation-2436", "mrqa_squad-validation-2467", "mrqa_squad-validation-264", "mrqa_squad-validation-2798", "mrqa_squad-validation-2824", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-2906", "mrqa_squad-validation-2914", "mrqa_squad-validation-294", "mrqa_squad-validation-2999", "mrqa_squad-validation-305", "mrqa_squad-validation-3337", "mrqa_squad-validation-3650", "mrqa_squad-validation-3742", "mrqa_squad-validation-3948", "mrqa_squad-validation-4025", "mrqa_squad-validation-4066", "mrqa_squad-validation-4135", "mrqa_squad-validation-4258", "mrqa_squad-validation-4338", "mrqa_squad-validation-4349", "mrqa_squad-validation-44", "mrqa_squad-validation-4472", "mrqa_squad-validation-4480", "mrqa_squad-validation-4605", "mrqa_squad-validation-4607", "mrqa_squad-validation-4686", "mrqa_squad-validation-4835", "mrqa_squad-validation-487", "mrqa_squad-validation-4897", "mrqa_squad-validation-4947", "mrqa_squad-validation-5088", "mrqa_squad-validation-5136", "mrqa_squad-validation-5238", "mrqa_squad-validation-5330", "mrqa_squad-validation-5672", "mrqa_squad-validation-594", "mrqa_squad-validation-60", "mrqa_squad-validation-6362", "mrqa_squad-validation-6562", "mrqa_squad-validation-6737", "mrqa_squad-validation-6737", "mrqa_squad-validation-6811", "mrqa_squad-validation-6918", "mrqa_squad-validation-696", "mrqa_squad-validation-703", "mrqa_squad-validation-7173", "mrqa_squad-validation-7435", "mrqa_squad-validation-754", "mrqa_squad-validation-7576", "mrqa_squad-validation-7598", "mrqa_squad-validation-7814", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-8285", "mrqa_squad-validation-8402", "mrqa_squad-validation-8406", "mrqa_squad-validation-8483", "mrqa_squad-validation-8607", "mrqa_squad-validation-8636", "mrqa_squad-validation-8715", "mrqa_squad-validation-8747", "mrqa_squad-validation-8760", "mrqa_squad-validation-879", "mrqa_squad-validation-8846", "mrqa_squad-validation-9015", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9691", "mrqa_squad-validation-9757", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1297", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2068", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2572", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2970", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3886", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-4904", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-5118", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-5505", "mrqa_triviaqa-validation-5607", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6785", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-92"], "OKR": 0.80078125, "KG": 0.440625, "before_eval_results": {"predictions": ["a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Jena Malone", "U.S. Royalty", "the utopian Ascona community", "John W. Henry", "zenyin Bey", "his father, Robert Mitchum", "4 April 1963", "1995", "Steve Carell", "Wendell Berry", "Keitar\u014d Arima", "eastern", "novelty songs, comedy, and strange or unusual recordings", "OutKast", "44", "Alain Robbe-Grillet", "the Seasiders", "musical research", "Dragon TV", "the Appalachian Mountains", "Bay Ridge, Brooklyn", "Jean- Marc Vall\u00e9e", "over 1.6 million", "1928", "November 20, 1942", "September 26, 2010", "North Greenwich Arena", "1614", "Lucy Maud Montgomery", "Eminem, Bad Meets Evil, Akon, Christina Aguilera and Taio Cruz", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Saint Michael, Barbados", "Sleepy Hollow", "more than 26,000", "the EN World web site", "Charles Russell", "KB", "Colonel Patrick John Mercer, OBE (born 26 June 1956)", "three Golden Globe Awards", "southwest Denver, Colorado near Bear Creek", "Port Clinton", "Art of Dying", "Dallas", "Harvard", "fennec fox", "Dutch", "Terry Malloy", "Golden Calf", "tomorrow May Never Come", "Thorgan Ganael Francis Hazard", "the moderately paced dance cut dead in its tracks", "Everywhere", "JNCASR", "honda CBR1000RR", "S. W. Turner", "the Republic of Upper Volta", "56,", "Nkepile Mabuse", "Eintracht Frankfurt", "U.S. Marines", "Hephaestus", "Amherst College", "two"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6620404411764707}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [0.26666666666666666, 1.0, 0.0, 0.8571428571428571, 0.5714285714285715, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-4583", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-1498", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-928", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-616", "mrqa_searchqa-validation-9636", "mrqa_searchqa-validation-14102"], "SR": 0.53125, "CSR": 0.5021875, "EFR": 1.0, "Overall": 0.6819218749999999}]}