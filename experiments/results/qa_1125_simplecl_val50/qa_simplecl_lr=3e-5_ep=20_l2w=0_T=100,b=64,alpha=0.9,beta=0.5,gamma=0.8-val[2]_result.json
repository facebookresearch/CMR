{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4220, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Ed Asner", "arrows", "1st century BC", "Marburg Colloquy", "Brookhaven", "ca. 2 million", "the Hungarians", "Mercury", "19th Century", "Art Deco style in painting and art", "The ability to make probabilistic decisions", "impact process effects", "1999", "phagosome", "the mass of the attracting body", "the Association of American Universities", "three", "allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "freight services", "up to four minutes", "the Little Horn", "Muslim and Chinese", "intracellular pathogenesis", "Santa Clara, California", "1784", "George Low", "Annual Conference Cabinet", "three", "Students", "Atlantic", "2001", "1887", "Chicago Bears", "John Harvard", "increase its bulk and decrease its density", "literacy and numeracy", "Christmas Eve", "the state", "Paris", "gender roles and customs", "outdated or only approproriate", "soy farmers", "United States", "Albert Einstein", "the number of social services that people can access wherever they move", "Tesco", "ABC Inc.", "1776", "wireless", "an electric current", "Warszowa", "the courts of member states", "supervisory church body", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church", "Manakin Episcopal Church", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Westminster", "Von Miller", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "Khwarezmia", "Queen Elizabeth II", "CBS", "Pittsburgh Steelers", "The chloroplast peripheral reticulum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8875363542546205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212122, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1826", "mrqa_squad-validation-4874", "mrqa_squad-validation-4283", "mrqa_squad-validation-1802", "mrqa_squad-validation-6210", "mrqa_squad-validation-3650", "mrqa_squad-validation-7430", "mrqa_squad-validation-6136"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["the Inland Empire", "New Zealand", "Jacksonville", "Newton's First Law", "the ability to pursue valued goals", "May 1888", "lecture theatre", "more than 28 days", "elliptical", "Boston", "Wednesdays", "Orange Counties", "three", "Lampea", "San Jose State", "March 29, 1883", "between AD 0\u20131250", "Pleurobrachia", "eleven", "punts", "the Solim\u00f5es Basin", "1474", "Arizona Cardinals", "Julia Butterfly Hill", "Orange County", "Doctor in Bible", "left Graz", "waldzither", "over $40 million", "14th century", "6.7+", "end of the 19th century", "peace", "$40,000", "the Cloth of St Gereon", "time and space", "7,000", "elementary particles", "indigenous", "3.5 billion", "New York City O&O WABC-TV and Philadelphia O&o WPVI-TV", "John Fox", "architectural", "Prime ideals", "Normant", "Leonardo da Vinci", "2003", "modern buildings", "Charles River", "KOA", "a disaster", "no contest", "Latin", "Manakin Town", "40,000", "After liberation", "\"winds up\" the debate", "10%", "The Daily Mail is mentioned in The Beatles\u2019 hit single Paperback Writer  The Yorkshire Post was the first British newspaper to report on The Abdication Crisis on 2nd December 1936", "Uncle Tom\u2019s Cabin", "The liver", "No man", "Martina Hingis", "Ukraine does not have real established and ratified borders with Russia"], "metric_results": {"EM": 0.875, "QA-F1": 0.8916666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669]}}, "before_error_ids": ["mrqa_squad-validation-4458", "mrqa_squad-validation-1775", "mrqa_squad-validation-1001", "mrqa_squad-validation-696", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-7750", "mrqa_naturalquestions-validation-646"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 2, "before_eval_results": {"predictions": ["$155 million", "CBS", "San Jose State", "Half", "the evolution of the German language and literature", "the Qur'an", "the Brotherhood", "high wages", "Tolui", "human law", "the object's weight", "over half", "1960s", "two months", "his friends", "1805", "Elders", "30\u201375%", "45,000 pounds", "self molecules", "Taishi", "1960", "Captain America: Civil War", "political divisions", "D loop mechanism", "Monterey", "The Book of Common Prayer", "14", "Charleston", "fear of their lives", "warmest", "intracellular pathogenesis", "Safari Rally", "10,006,721", "Philip Segal", "the decision problem in Presburger arithmetic", "1965", "a coherent theory", "German Te Deum", "Stanford Stadium", "the Jin dynasty", "Trevathan", "Doctor Who", "1206", "clinical services", "CRISPR sequences", "Queen Elizabeth II", "zero", "1992", "food security", "plasmas", "their low ratio of organic matter to salt and water", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "guardian", "brown", "guardian", "guardian", "black", "brown"], "metric_results": {"EM": 0.625, "QA-F1": 0.6742458062770562}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6099", "mrqa_squad-validation-6641", "mrqa_squad-validation-2547", "mrqa_squad-validation-8360", "mrqa_squad-validation-2577", "mrqa_squad-validation-8747", "mrqa_squad-validation-5893", "mrqa_squad-validation-2906", "mrqa_squad-validation-1860", "mrqa_squad-validation-10427", "mrqa_squad-validation-6405", "mrqa_squad-validation-1435", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11930", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-12889", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3857"], "SR": 0.625, "CSR": 0.7916666666666666, "EFR": 1.0, "Overall": 0.8958333333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["fewer than 10 employees", "1624", "Hangzhou", "committee", "the 19th century", "1962", "dealing with patients' prescriptions and patient safety issues", "England", "Vistula River", "1290", "21 October 1512", "427,652", "double membrane", "August 1967", "German", "27-30%", "four", "the 50 fund", "Arizona Cardinals", "Peanuts", "comb rows", "calcitriol", "Warsaw", "time", "since at least the mid-14th century", "the mitochondrial double membrane", "Mike Figgis", "in an adult plant's apical meristems", "isopentenyl pyrophosphate synthesis", "Associating forces with vectors", "Prime ideals", "The Three Doctors", "Malik Jackson", "four", "the Koori", "1910\u20131940", "pressure swing adsorption", "Johann Tetzel", "English", "allocution", "gauge bosons", "the A1 (Gateshead Newcastle Western Bypass", "Sun Life Stadium", "the Duchy of Prussia, the Channel Islands, and Ireland", "John Houghton", "February 2015", "draftsman", "Mollusca", "Orestes", "the Galapagos Islands", "the U.S. government", "a liquid", "the Travel Detective", "before", "denver", "the Mycenaean civilization", "a biological clock", "the Belasco Theatre", "the Normandy Landings", "John Dillinger", "the fibula", "Il Trovatore", "the South Pole", "the North British Railway"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7132711038961039}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6345", "mrqa_squad-validation-3347", "mrqa_squad-validation-4730", "mrqa_squad-validation-1036", "mrqa_squad-validation-3673", "mrqa_squad-validation-6737", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-8509", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-1372", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-7003"], "SR": 0.6875, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 4, "before_eval_results": {"predictions": ["Germany and Austria", "Centrum", "blue police box", "to spearhead the regeneration of the North-East", "Rhenus", "gambling", "secular powers", "applied mathematics to the construction of calendars", "Zhongtong", "11.1%", "1538", "Deacons", "New Testament", "experience, ideology, and weapons", "25", "May 2013", "K-9 and Company", "capturing prey", "a four-carbon compound", "livestock pasture", "Victorian", "1,300,000", "passing between carbon dioxide and oxygen, so at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "two tumen (20,000 soldiers)", "eight", "computational complexity theory", "WZM and WOTV", "Orange", "tentilla", "gender roles and customs", "social unrest and violence", "Woodward Park", "1745", "Battle of Olustee", "observer status", "50-yard line", "3D printing technology", "The Malkin Athletic Center", "24\u201310", "6.7+", "empire", "domestic legislation of the Scottish Parliament", "a placebo", "New York City Mayor Michael Bloomberg", "the oceans are growing crowded, and governments are increasingly trying to plan their use", "innovative, exciting skyscrapers", "a cancerous tumor", "the war years", "Microsoft", "Matt Kuchar and Bubba Watson", "fastest circumnavigation of the globe in a powerboat", "video", "the foyer of the BBC building", "buddhism", "Manchester City", "Noriko Savoie", "three", "change course", "Morgan Tsvangirai", "\"Wicked\"", "January 2, 1971", "a 100% pure and natural sweetener", "Chelsea Lately", "Luxembourg"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7217491010333752}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.7741935483870968, 0.4, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.2666666666666667, 1.0, 1.0, 0.2, 0.0, 0.8333333333333333, 0.2857142857142857, 0.0, 0.7272727272727273, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9250", "mrqa_squad-validation-4258", "mrqa_squad-validation-2984", "mrqa_squad-validation-8832", "mrqa_squad-validation-6046", "mrqa_squad-validation-1643", "mrqa_squad-validation-6092", "mrqa_squad-validation-4572", "mrqa_squad-validation-7094", "mrqa_squad-validation-10045", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-862", "mrqa_triviaqa-validation-1366", "mrqa_hotpotqa-validation-547"], "SR": 0.609375, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 5, "before_eval_results": {"predictions": ["money from foreign Islamist banking systems, especially those linked with Saudi Arabia", "Anheuser-Busch InBev", "4000", "$37.6 billion", "Anglo-Saxons", "seven", "the Golden Gate Bridge", "Southwest Fresno", "divergent boundaries", "chloroplasts are surrounded by a double membrane", "QuickBooks", "surface condensers", "clinical pharmacists", "a seal", "Philip Howard", "King Ethelred II of England", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "three", "French", "Golden Super Bowl", "the constitutional traditions common to the member states", "pharmacological effect", "Huguenots", "10\u20137", "Polish Academy of Sciences", "spherical", "Nurses", "New England Patriots", "Time", "Class II MHC", "two", "Westinghouse", "by disrupting their plasma membrane", "internal combustion engines", "indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons,", "Religious and spiritual teachers", "B cells", "property damage", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops", "Anjuna beach in Goa", "How I Met Your Mother", "the Louvre", "Leo Frank", "Thessaloniki and Athens", "Graziano Transmissioni", "opposition parties", "1.2 million", "United's", "the release of the four men", "first lady Michelle Obama is weighing in on the issue by focusing on how health care can affect families", "Ed McMahon", "The Magna Carta is expected to fetch at least $20 million to $30 million", "the Democratic VP candidate", "the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "Obama", "paper ballots", "Sodra nongovernmental organization", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium", "promotes fuel economy and safety while boosted the economy", "heart rate that exceeds the normal resting rate", "heavy breeds", "Denmark", "Cincinnati", "Donald Sutherland"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6693770515582629}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8181818181818182, 0.4, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.2666666666666667, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.11764705882352941, 0.09523809523809525, 0.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9691", "mrqa_squad-validation-754", "mrqa_squad-validation-8715", "mrqa_squad-validation-1090", "mrqa_squad-validation-3087", "mrqa_squad-validation-3610", "mrqa_squad-validation-3075", "mrqa_squad-validation-827", "mrqa_squad-validation-6644", "mrqa_squad-validation-10444", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-3935", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-4043", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-4171", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-2465"], "SR": 0.546875, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 6, "before_eval_results": {"predictions": ["18 February 1546", "11", "neither conscientious nor of social benefit", "the University of Chicago Press", "$2 million", "2015", "1762", "biased against Genghis Khan", "Warsaw Stock Exchange", "they are often branched and entangled with the endoplasmic reticulum", "computational", "to denote unknown or unexplored territory", "John Michael Rysbrack", "early Lutheran hymnals", "world line", "1991", "William Smith", "William Pitt", "geochemical component called KREEP", "Theory of the Earth to the Royal Society of Edinburgh", "Japan", "Super Bowl Opening Night", "the Working Group chairs", "laws of physics", "Denver's Executive Vice President of Football Operations and General Manager", "noisiest", "independent of each other", "issues under their jurisdiction", "unsuccessful", "human", "more convenient and private method", "multiples of 1", "nerves", "1689", "Christo Vladimirov Javacheff", "a double sugar or a disaccharide (di = two)", "Democratic National Committee (DNC)", "heart, blood, and blood vessels", "Troggs", "He could have created the whole universe, the earth and all it contains in no time at all.", "Slovakia", "Diana the Princess", "monopolies", "vena cava", "Chesapecten jeffersonius", "bull's-eye", "Tartarus", "hor\u017fe", "New York", "Achaemenid", "LAP", "German Count Ferdinand von Zeppelin", "San Francisco", "Duke of Clarence", "Datson, H., Birch,... plus assorted small iron and slag particles.", "\"Popeye\" Doyle\"", "Judas", "Dr. Jack Shephard, Kate Austen, Sayid Jarrah, Hugo \" Hurley\"", "TV series", "Love Is All Around", "Los Angeles Dance Theater", "United States", "unapproved pain-relief drugs.", "boy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5786412646198831}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6230", "mrqa_squad-validation-8765", "mrqa_squad-validation-5588", "mrqa_squad-validation-10477", "mrqa_squad-validation-4005", "mrqa_squad-validation-5054", "mrqa_squad-validation-383", "mrqa_squad-validation-10398", "mrqa_squad-validation-9064", "mrqa_squad-validation-3113", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4830", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-1637", "mrqa_searchqa-validation-12770", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-10057", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-759"], "SR": 0.515625, "CSR": 0.6763392857142857, "EFR": 1.0, "Overall": 0.8381696428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["extinction of the dinosaurs", "oxygen", "reduce growth", "Torchwood", "9.1 million", "U.S. economy consistently affords a lower level of economic mobility than all the continental European countries", "individual countries", "cattle", "Mongol", "a maze of semantical problems and grammatical niceties", "five", "the \"gold standard\" of religion in minds of some or many Muslims.", "British", "Finsteraarhorn", "Abilene", "white", "Yosemite Freeway", "Thanksgiving", "874.3 square miles (2,264 km2)", "Two thirds", "the Privy Council", "well into the nineteenth century", "\u201ccapability deprivation\u201d", "Daily Mail", "San Mateo", "Spanish", "around 300,000", "cryptomonads", "Swahili", "hymn-writer", "starch accumulation in plants that contain them", "Bryant", "Jupiter", "Utica, Illinois", "Rod Steiger", "hog", "Barack Obama", "Kenny G", "a small, half size cup", "Chazz Michael Michaels", "Alberta", "Spring", "klammeraffe", "bark beetles", "Allah", "bones", "Boa Constrictor Python", "Cecil B. DeMille", "Ada", "Faith Hill", "Ben Affleck", "U.S.", "V", "time", "Charles Parker, Jr.", "Sweden", "Vietnam", "1", "Alexandria", "Perfume", "New Jersey Economic Development Authority's 20% tax credit", "Georgetown", "Essex Eagles", "Alzheimer's disease"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5710227272727273}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_squad-validation-7399", "mrqa_squad-validation-1938", "mrqa_squad-validation-6220", "mrqa_squad-validation-9588", "mrqa_squad-validation-4562", "mrqa_squad-validation-7473", "mrqa_squad-validation-8189", "mrqa_squad-validation-8828", "mrqa_searchqa-validation-47", "mrqa_searchqa-validation-7048", "mrqa_searchqa-validation-7869", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-943", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-8990", "mrqa_searchqa-validation-4050", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10916", "mrqa_searchqa-validation-5178", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-7551", "mrqa_naturalquestions-validation-10073", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3468"], "SR": 0.515625, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 8, "before_eval_results": {"predictions": ["shocked", "lymphocytes or an antibody-based humoral response", "producers", "BSkyB", "Kawann Short", "Daidu", "silent", "22", "the park", "1965", "tidal currents", "an exlposion", "Ma Jianlong", "Demaryius Thomas", "Lake Constance", "the Orange Democratic Movement", "Bannow Bay", "the Red Army", "20th century", "ITT", "1966", "masses", "Linebacker", "high art and folk music", "four", "with six series of theses", "the midseason forensic investigation drama Body of Proof", "seven-eighths", "Cardinal Richelieu", "the Atlas Mountains", "Madrid", "the Danube", "an an exclamation to Yahweh", "leather", "George Pullman", "a red-purple plum", "the Messiah", "Sappho", "an expression meaning that ownership is easier to maintain if one has possession of something, or difficult to enforce if one does not.", "the tonka bean", "the divisor", "Hypnos", "Texas", "the Rooty Tooty Fresh 'N Fruity", "a black breed", "the Bill of Rights", "an SAT", "Brasilia", "Henry David Thoreau", "Southern California", "Dick Cheney", "the Earth", "William Donald Scherzer", "Edward Hopper", "the CIA", "d'Artagnan", "an e", "1985", "apple", "its air-cushioned sole", "13", "Fort Worth", "Agent 99", "private"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6174611262077294}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6559", "mrqa_squad-validation-7803", "mrqa_squad-validation-3478", "mrqa_squad-validation-1169", "mrqa_squad-validation-2474", "mrqa_squad-validation-5926", "mrqa_searchqa-validation-15994", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-15182", "mrqa_searchqa-validation-523", "mrqa_searchqa-validation-15584", "mrqa_searchqa-validation-9386", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-10315", "mrqa_searchqa-validation-14478", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-9179", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4461"], "SR": 0.53125, "CSR": 0.6423611111111112, "EFR": 1.0, "Overall": 0.8211805555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["\"Provisional Registration\"", "August 15, 1971", "Levi's Stadium", "Framework Convention on Climate Change", "Inflammation", "Brown v. Board of Education of Topeka", "15 May 1525", "The Walt Disney Company", "Dundee", "Over 61 per cent", "During the Second World War", "the integer factorization problem", "Scottish independence", "Exploration is still continuing to determine if there are more reserves", "\"prep schools\", boarding schools and day schools\"", "its soft power", "strong Islamist outlook", "lengthening rubbing surfaces of the valve", "$32 billion", "keyed Northumbrian smallpipes", "the Dutch Republic", "Alex Haley", "three", "the helmeted honeyeater", "4:51", "Khrushchev", "Hera", "the vine", "Elton John", "Cuba", "the Battle of Thermopylae", "the Khazars", "Kroc", "cricket", "white", "Washington", "Carmen", "Genoa", "12", "tarn", "972; 451; 100; or 25", "a bison", "Ann Widdecombe", "Isosceles", "the Old Kent Road", "Tuesday", "anhydrous copper sulfate", "Ab Fab", "Massachusetts", "Barrow", "California", "the Susquehanna River", "80\u2019s", "the maqui berry", "Singapore", "Wigan Warriors", "Davos", "eight", "Hoffa", "Home Rule Party", "Secretary Janet Napolitano", "J. Crew", "Deval Patrick", "Orvon Grover"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6215277777777777}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.16666666666666669, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 0.6, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8784", "mrqa_squad-validation-2920", "mrqa_squad-validation-8273", "mrqa_squad-validation-6918", "mrqa_squad-validation-9870", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2533", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-1203", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2672", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1553", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-7509"], "SR": 0.546875, "CSR": 0.6328125, "EFR": 0.9655172413793104, "Overall": 0.7991648706896552}, {"timecode": 10, "before_eval_results": {"predictions": ["tenggis", "environmental determinism", "4 August 2010", "King George III", "radio network", "the Dongshan Dafo Dian", "League of Augsburg", "Duarte Barbosa", "the People's Republic of China", "Roman Catholic", "Amazonia: Man and Culture in a Counterfeit Paradise", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "Sydney", "five", "January 18, 1974", "Spanish", "Professional", "extremely difficult, if not impossible", "student populations", "Catholic", "the Parliament of the United Kingdom", "296", "Ghent-Terneuzen Canal", "mulberry", "vitis", "stone", "Ken Russell", "Dan Dare", "pompey", "Smiths", "Mike Tyson", "Turkey", "Pesach", "Brian Deane", "kaleidoscope", "Uranus", "Apollon", "George Carlin", "Ukraine", "Sydney", "Los Angeles", "Underground", "pouk", "\"beyond violet\"", "passion fruit", "Portugal", "cricket", "Serena Williams", "63 to 144 inches", "Titanic", "William Tell", "Christian Dior", "snail", "Mendip", "Wichita", "eukharisti\u0101", "New Croton Reservoir", "Big Boi and Sleepy Brown", "Leucippus", "Stephen King", "Venus Williams", "firefighter", "Ponty Mython", "Mia Farrow"], "metric_results": {"EM": 0.625, "QA-F1": 0.6747429653679653}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5454545454545454, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6263", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-2265", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-7138", "mrqa_newsqa-validation-2710", "mrqa_searchqa-validation-3397", "mrqa_searchqa-validation-8589"], "SR": 0.625, "CSR": 0.6321022727272727, "EFR": 1.0, "Overall": 0.8160511363636364}, {"timecode": 11, "before_eval_results": {"predictions": ["the method by which the medications are requested and received", "salvation", "stoves", "they produce secretions (ink) that luminesce at much the same wavelengths as their bodies", "zaju", "administration", "Chivas USA", "Edinburgh", "The Pink Triangle", "the dot", "Magdalen Tower", "a large public network that supported dial-up users and a private network business that allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "public service", "Guy de Lusignan", "tiger team", "sub-group of T cells", "The European Commission", "completed (or local) fields", "a force is required to maintain motion, even at a constant velocity", "Mongol and Turkic tribes", "hez-bah-lah", "1966 to 1969", "Whist", "Burundi", "Toscana (Tuscany) Region", "achromatopsia", "black", "Pluto", "chromium", "copper, zinc or silver", "The Hague", "Vancouver Island", "Ironside", "John le Carr\u00e9", "Nizhny Novgorod", "brown trout", "Beyonce", "Wordsworth", "Man V Food", "Queen Elizabeth II", "Samuel Johnson", "Conrad Murray", "Mary Poppins", "Sid Ziff", "green leaf", "Hudsyn", "sea of Azov", "Shrek", "Oslo", "lions", "Rhododendron", "Chicago", "Franklin D. Roosevelt", "Shanghai", "julie Forbush and Rade Serbedzija", "pass the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "Snake River Valley", "17 October 2006", "beer", "Kingman Regional Medical Center", "Saturday's Hungarian Grand Prix", "The Ass of Viterbo", "Edgar Allan Poe", "puck"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6435732886904761}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5460", "mrqa_squad-validation-4870", "mrqa_squad-validation-6530", "mrqa_squad-validation-10338", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-3550", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-2204", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-3846", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6189", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-890", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-8473"], "SR": 0.578125, "CSR": 0.6276041666666667, "EFR": 1.0, "Overall": 0.8138020833333334}, {"timecode": 12, "before_eval_results": {"predictions": ["a gift from God", "Greenland", "1724 to 1725", "placing them on prophetic faith.", "1.25 million", "1080i HD", "five", "Maria Goeppert-Mayer", "the International Association of Methodist-related Schools, Colleges, and Universities", "three", "majority in Parliament, a minority in the Council, and a majority in the Commission", "President Mahmoud Ahmadinejad", "Newcastle Eagles", "cholera", "General Pharmaceutical Council (GPhC)", "relative units of force", "AD 14", "orogenic wedges", "his exploration and settlement of what is now Kentucky", "The Handmaid's Tale", "The bonobo", "The Fault in Our Stars", "Vigor, Prelude, CR-X, and Quint", "a puzzle video game", "1898", "400 MW", "Total Nonstop Action Wrestling", "galt\u00fcr avalanche", "Archbishop of Canterbury", "1861", "Disneyland theme park in Anaheim, California", "David Villa", "Red and Assiniboine Rivers", "New Jersey", "Continental Army", "Jack Kilby", "Ryan Babel", "Ramzan Kadyrov", "July 16, 1971", "1933", "What's Up", "Baudot code", "1959", "1995", "Mark Dayton", "Marvel Comics", "Chris Brown", "Nick Cassavetes", "Lamar Hunt", "Sarah Winnemucca", "Jean Baptiste Point DuSable", "England", "Paul W. S. Anderson", "a basilica", "1994", "Ryan", "Wakanda and the Savage Land", "mercury", "phobias", "drug trade", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "glenn johnson", "a spider", "pre-Columbian times"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6375}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 0.04761904761904762, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4958", "mrqa_squad-validation-937", "mrqa_squad-validation-4079", "mrqa_squad-validation-6320", "mrqa_squad-validation-10428", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5658", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-383", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-6015", "mrqa_triviaqa-validation-2685", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-7025", "mrqa_naturalquestions-validation-8227"], "SR": 0.59375, "CSR": 0.625, "EFR": 1.0, "Overall": 0.8125}, {"timecode": 13, "before_eval_results": {"predictions": ["\u00a341,004", "The Rocket", "Tolui", "lower lake", "Gospi\u0107, Austrian Empire", "since 2001", "a maze of semantical problems and grammatical niceties", "Southwest Fresno", "5,000", "Huguenot", "ABC News Now", "sold Wardenclyffe for $20,000 ($472,500 in today's dollars)", "\u00c9mile Girardeau", "Brownlee", "partial funding", "a certain number of teacher's salaries are paid by the State", "NCAA Division II", "Adrian Lyne", "his most brilliant student", "Las Vegas", "Ranulf de Gernon", "2017", "Dallas", "Rudolf Schenker", "Shrek", "Lucille Ball", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "16\u201321", "Vince Guaraldi", "Anthony Stephen Burke", "Imperial War Museum", "8th", "Highlands Course", "Hawaii", "Liquidambar styracif", "Marie-Joseph Paul Yves Roch Gilbert du Motier", "Gujarat", "three", "Winter Haven", "four", "The Process", "Jane Hollander", "Surrey", "Claudio Javier L\u00f3pez", "\"My Beautiful Dark Twisted Fantasy\" (2010)", "FCI Danbury", "a few", "US Naval Submarine Base New London submarine school", "Las Vegas", "Pope John X", "2008", "Arlo Looking Cloud", "Rwandan genocide of 1994", "Johnny Cash", "Secretary of State", "2009", "1982", "272.25 square feet", "Ryder Russell", "gossip Girl", "Compressible flow", "the coalitions threatening its core terrain", "a b.B.W. (Big Blue Woman)", "Richie Unterberger"], "metric_results": {"EM": 0.453125, "QA-F1": 0.552666083916084}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.4, 0.4, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.8, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3176", "mrqa_squad-validation-3287", "mrqa_squad-validation-1488", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-3556", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-5454", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-16181", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-469"], "SR": 0.453125, "CSR": 0.6127232142857143, "EFR": 1.0, "Overall": 0.8063616071428572}, {"timecode": 14, "before_eval_results": {"predictions": ["over 10,000 British", "perpendicular to the velocity vector", "Inherited wealth", "December 1963", "1970", "religious freedom in the Polish\u2013Lithuanian Commonwealth", "Spreading throughout the Mediterranean and Europe", "kilogram-force", "ten times their own weight", "Quaternary", "1887", "other ctenophores", "symbiotic relationship", "mathematical models of computation", "Vistula River", "100 to 150 troops", "Apple's new iOS5 operating system", "palesa and her granddaughter", "March 8", "Democrats and Republicans", "the Catholic League", "1,000 pounds", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "Friday", "Movahedi", "different women coping with breast cancer in five vignettes", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "peter", "$40 and a quarter of bread", "Lance Cpl. Maria Lauterbach", "Hyundai Steel", "London", "more than 4,000 commercial farmers", "Val d'Isere, France", "the results by a chaplain about 1:45 p.m.", "two soldiers and two civilians", "$14.1 million", "1616", "Saturn", "Chile", "some of the most gigantic pumpkins in the world", "Buddhism", "J.Crew", "Japanese Foreign Minister Hirofumi Nakasone", "\"I always kind of admired him, oddly.\"", "boyhood experience in a World War II internment camp", "suppress the memories and to live as normal a life as possible", "4 meters (13 feet) high", "the Irish capital", "Republican", "a Utah jail", "Allred", "Islamabad", "three hours", "March 26, 1973", "Indian Ocean", "argument", "peter", "Sevens", "England", "Yemen", "Domenikos Theotokopoulos", "peter", "mercury"], "metric_results": {"EM": 0.34375, "QA-F1": 0.48783434447496954}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [0.8, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 0.8, 0.3, 1.0, 0.6666666666666666, 0.8750000000000001, 0.7555555555555554, 0.0, 0.8000000000000002, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.0, 0.25, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5517", "mrqa_squad-validation-10400", "mrqa_squad-validation-7770", "mrqa_squad-validation-4856", "mrqa_squad-validation-10458", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-3798", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-1329", "mrqa_naturalquestions-validation-6733", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-12191", "mrqa_triviaqa-validation-3839"], "SR": 0.34375, "CSR": 0.5947916666666666, "EFR": 1.0, "Overall": 0.7973958333333333}, {"timecode": 15, "before_eval_results": {"predictions": ["Prospect Park", "Khanbaliq", "Quaternary", "1870", "water", "a prime number", "50 fund", "Camisards", "over $40 million", "GTE", "1,100", "\"caveat\"", "Oligocene", "Melodie Rydalch", "Christopher Columbus", "a Little Rock military recruiting center", "March 24,", "the Beatles", "Robert Park", "Adriano", "Eleven", "2007", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "\"She had a smile on her face, like she always does when she comes in here,\"", "56,", "National Football League", "\"The Lost Symbol,\"", "one of its diplomats", "These intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan", "at least 18 federal agents and two soldiers", "Atlanta", "resources", "the highest ranking former member of Saddam Hussein's regime still at large,", "two Emmys", "\"I have a very, very good family that I love back home in America, and are 100 percent behind you, and we thank God every day that you have our back.\"", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "Rwanda", "75.", "eradication of the Zetas cartel from the state of Veracruz, Mexico", "closing these racial gaps.", "\"He was held in federal custody over the weekend.", "President Bush", "Amstetten,", "African National Congress Deputy President Kgalema Motlanthe", "spiral into economic disaster.", "\"The Kirchners have been weakened by this latest economic crisis,\"", "Ralph Cifaretto", "a strict interpretation of the law,", "saying Chaudhary's death was warning to management.", "Iran", "20% tax credit", "July 23.", "70,000 or so are estimated to be there now.", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "Robert Remak", "Tim McGraw", "Prussian 2nd Army", "cabbage", "a homebrew campaign setting", "Beno\u00eet Jacquot", "blue", "United States Capitol", "The Left Book Club", "holography"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5514601604921727}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.888888888888889, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0975609756097561, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.09523809523809525, 0.0, 0.29629629629629634, 0.0, 1.0, 1.0, 1.0, 0.6923076923076924, 0.2962962962962963, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9016", "mrqa_squad-validation-10449", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2727", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-1335", "mrqa_triviaqa-validation-6296"], "SR": 0.46875, "CSR": 0.5869140625, "EFR": 1.0, "Overall": 0.79345703125}, {"timecode": 16, "before_eval_results": {"predictions": ["two thousand", "a connection identifier", "the high risk of a conflict of interest and/or the avoidance of absolute powers.", "to look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system.", "Thames", "Lt Col Paul von Lettow-Vorbeck", "several hundred thousand, some 30% of the city \u2013 herded into the Warsaw Ghetto.", "the Tower District", "Ted Ginn Jr.", "Catch Me Who Can", "John Fox", "the housing bubble", "137", "Adam Lambert and Kris Allen,", "Brian Smith", "\"Hillbilly Handfishin'\"", "Charles Lock", "a charge of voluntary manslaughter after witnesses identified him and he was interviewed by police.", "his enjoyment of sex and how he lost his virginity at age 14.", "his injuries,", "30 years ago.", "the armed robbery and kidnapping of another victim,", "next year", "\"The administration appeared to have \"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions, \"China is a different matter,\"", "Christopher Savoie", "Anil Kapoor", "Afghanistan and India", "Dr. Albert Reiter,", "\"theoretically\" Iran could develop a nuclear bomb within the next year.", "to step up attacks against innocent civilians", "Matthew Fisher", "cancer,", "Courtney Love,", "us to step up.", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "your own environmental videos", "two women", "the \"A knighthood is the highest honor,", "Monday,", "male veterans", "Yusuf Saad Kamel", "hand-painted Swedish wooden clogs", "11 healthy eggs", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Fullerton, California,", "around 1918 and 1919,", "in the 1950s,", "the U.S. Air Force, operating an unmanned aerial vehicle, fired a Hellfire missile at three men spotted setting roadside bombs, killing all three.", "Brazil has saved $1 billion alone by producing its own generic versions of HIV and AIDS medicines and negotiating discounts for imported drugs.", "vegan bake sales", "\"The Rosie Show,\"", "al Fayed's security team", "Oxbow,", "gastrocnemius", "Ed Sheeran", "20", "Australia", "three-part", "2001", "the Bailiwick of Guernsey", "U.S.", "George Blake", "Bogota"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5801035916270292}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.9743589743589743, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.27272727272727276, 0.22222222222222224, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.08333333333333333, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0625, 0.1081081081081081, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-8570", "mrqa_squad-validation-7159", "mrqa_squad-validation-8339", "mrqa_squad-validation-914", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-837", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-2957", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-4647", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-10239", "mrqa_triviaqa-validation-6739"], "SR": 0.46875, "CSR": 0.5799632352941176, "EFR": 1.0, "Overall": 0.7899816176470589}, {"timecode": 17, "before_eval_results": {"predictions": ["relatively equal distributions of wealth", "a pharmacy practice residency", "questions and answers in the catechism", "wakes (sed vigilat) and experiences visions", "Captain Francis Fowke,", "12 January 1943,", "60,000", "an audio drama titled Zagreus", "CBS", "17", "temperate", "Rod Blagojevich", "\"Sesame Street's\"", "Windsor, Ontario,", "$50 less,", "Afghanistan's restive provinces", "fled Zimbabwe and found his qualifications mean little as a refugee.", "\"executioners\"", "Iran", "expressed concerns about the missile defense system.", "Sharon Bialek", "Matthew Fisher,", "remains unknown,", "in the north and west of the country,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "introduce legislation Thursday to improve the military's suicide-prevention programs.", "$250,000", "first or second week in April.", "Derek Mears", "braving elements ranging from rain to wind and even one speeding ticket", "Gary Player", "Nieb\u00fcll", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "kite boards", "Virgin America", "International Polo Club Palm Beach in Florida.", "Daniel Wozniak,", "22-year-old", "bin Laden", "how health care can affect families.", "Appathurai", "U.S. Food and Drug Administration", "Casa de Campo International Airport", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "2002", "at checkposts and military camps in the Mohmand agency,", "all the attackers were Pakistanis,", "Friday,", "\"Taxman,\"", "crocodile eggs", "from Geraldine Ferraro to Bill Clinton.", "hid his money,", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "senators", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "Messenger", "Arlene Phillips", "23 July 1989", "Ry\u016bkyuan people", "surrealism", "C. S. Lewis", "9", "sake", "Halifax"], "metric_results": {"EM": 0.5, "QA-F1": 0.6069619221713212}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.11764705882352942, 1.0, 1.0, 1.0, 0.3529411764705882, 0.5217391304347826, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.7692307692307693, 0.15384615384615388, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2338", "mrqa_squad-validation-2408", "mrqa_squad-validation-1570", "mrqa_squad-validation-7678", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-157", "mrqa_naturalquestions-validation-132", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-8695"], "SR": 0.5, "CSR": 0.5755208333333333, "EFR": 1.0, "Overall": 0.7877604166666666}, {"timecode": 18, "before_eval_results": {"predictions": ["melatonin", "constant factors and smaller terms", "Shi Bingzhi", "New France's governor, the Marquis de Vaudreuil", "linear", "Advanced Steam movement", "Defensive ends", "the dot", "chastity", "European Court of Justice", "a bronze medal in the women's figure skating final,", "that U.S. Ambassador to Zimbabwe James McGee would be expelled from the country if he \"persisted in meddling in Zimbabwe's electoral process,\"", "UK", "\" Teen Patti\"", "Argentina", "Congress", "28", "Frank Ricci,", "the project, which is designed to promote private sector investment in a variety of gas-related industries,", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "Bill & Melinda Gates Foundation", "$106.5 million", "because everybody around me likes Obama,\"", "not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "\"political and religious\"", "$163 million (180 million Swiss francs)", "Afghan lawmakers", "North Korea", "because of the inauguration of Barack Obama as the 44th president of the United States.", "the mammoth's fossil", "recognizes the importance of Turkey and wants to engage with it from the start.", "because the federal government is asleep at the switch,", "Molotov cocktails, rocks and glass.", "because they were near their goal of being offered half the jobs involved in one of the latest subcontracts connected to the construction project", "Ben Roethlisberger", "Dr. Christina Romete,", "Ewan McGregor", "Brazil", "Meira Kumar", "next week.", "Hong Kong from other parts of Asia, such as India and mainland China, and sold on the streets illegally,", "Lindsey Vonn", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Amnesty International.", "President Obama's race", "Brazil", "Malmo", "trying to detonate an explosive device in his underwear aboard a Christmas 2009 flight to Detroit,", "two people were", "40-year-old", "outside the Iranian consulate in Peshawar", "Casey Anthony,", "the iconic Hollywood headquarters of Capitol Records,", "Emma Watson and Dan Stevens", "2002", "a leak in a Dike", "\"Low Budget\"", "Che Guevara", "Pearl Brewing Company", "Elizabeth I", "John Fogerty", "Garonne", "giraffe", "cheese"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5742898245241995}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.12121212121212123, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 0.8571428571428571, 0.0, 0.0, 0.9696969696969697, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.07142857142857142, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16, 0.8, 1.0, 0.5, 1.0, 1.0, 0.2702702702702703, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-1603", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9104", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-254", "mrqa_hotpotqa-validation-596"], "SR": 0.484375, "CSR": 0.5707236842105263, "EFR": 0.9696969696969697, "Overall": 0.770210326953748}, {"timecode": 19, "before_eval_results": {"predictions": ["1876", "1507,", "defensive end Kony Ealy", "11", "would be killed through overwork", "Japanese", "Muqali,", "2011 and 2012,", "Pittsburgh Steelers", "apartment building in Cologne, Germany,", "Aung San Suu Kyi", "Hank Moody", "the 3rd District of Utah.", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Stephen Tyrone Johns", "three", "procedures", "acid attack by a spurned suitor.", "most of those who managed to survive the incident hid in a boiler room and storage closets", "that the Bainbridge would be getting backup shortly.\"", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Courtney Love,", "33-year-old", "cell phones", "a book", "he was in good health, contrary to media reports he was diagnosed with skin cancer.", "stand down.", "Ashley \"A.J. Jewell,", "at least 17", "home in Satsuma, Florida,", "to the southern city of Naples", "Hugo Chavez", "London's", "home in rural California,", "off the front pages for the first time in days.", "Old Trafford", "the area of the 11th century Preah Vihear temple", "steam-driven, paddlewheeled overnight passenger boat", "clothes", "about 3,000 kilometers (1,900 miles),", "homicide", "The Ski Train", "Aniston, Demi Moore and Alicia Keys", "Lillo Brancato Jr.", "intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "CBS, CNN, Fox and The Associated Press.", "$60 billion on America's infrastructure.", "protective shoes", "three major public-sector labor", "U.S. President-elect Barack Obama", "Burhanuddin Rabbani,", "was depressed over a recent breakup, grabbed the gun and  took her own life.", "Too many glass shards", "Sedimentary rock", "around 2.45 billion years ago", "London", "Colorado", "Bangor International", "GZA, \"Grandmasters\"", "Suffragist", "Canterbury", "Tunisia", "Silver", "Bonnie and Clyde"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6981826561596298}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.6666666666666665, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-816", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-413", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-6758", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2805"], "SR": 0.546875, "CSR": 0.56953125, "EFR": 0.9655172413793104, "Overall": 0.7675242456896552}, {"timecode": 20, "before_eval_results": {"predictions": ["the Hostmen", "Greg Brady", "Fort Caroline", "Hungarians", "middle eastern scientists", "Marshall Field", "four", "mistreatment from government officials.", "Beijing, China,", "Virgil Tibbs", "Thaddeus Rowe Luckinbill", "up to 100,000", "United States, its NATO allies and others", "Virginia Dare", "JackScanlon", "Kylie Minogue", "94 by 50", "Lalo Schifrin", "MGM Resorts International", "16 August 1975", "seawater pearls", "1962,", "Buddhist", "1978", "1927, 1934, 1938, 1956 ) and four since the advent of the Super Bowl ( Super Bowls XXI ( 1986 ), XXV ( 1990 ), XLII ( 2007 ), and XLVI ( 2011 ) )", "1969", "New England Patriots", "Joseph Heller", "90 \u00b0 N 0 \u00b0 W", "1,350", "Leonard Bernstein", "25 September 2007", "Howard Caine", "Yale University", "62", "the team", "September 2014 and PlayStation 3 and Xbox 360 in November 2014", "Gavrilo Princip", "Koine Greek : apokalypsis", "October 1941", "peace between two entities ( especially between man and God or between two countries ), or to the well - being, welfare or safety of an individual or a group of individuals", "mughal gardens", "Cee - Lo", "before they kill him", "lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Labour", "three times", "November 25, 2002,", "December 27, 2015", "Peter Greene", "31 March 1909", "Ed Sheeran", "two", "Alberto Salazar", "live animals", "American", "Hoosick,", "CEO of an engineering and construction company with a vast personal fortune.", "more than 1.2 million", "The Three Little Pigs", "Robert Louis Stevenson", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Russia and China", "south-central Washington,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6838021706120962}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.08695652173913045, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8421052631578948, 0.0, 0.0, 1.0, 0.5945945945945945, 0.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-8934", "mrqa_triviaqa-validation-3886", "mrqa_hotpotqa-validation-2298", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3167", "mrqa_searchqa-validation-13486", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-2446"], "SR": 0.546875, "CSR": 0.5684523809523809, "EFR": 0.9655172413793104, "Overall": 0.7669848111658457}, {"timecode": 21, "before_eval_results": {"predictions": ["about 515 million years ago", "Spanish", "an attack on New France's capital, Quebec", "the Fresno Traction Company", "Westminster", "the ancestors of chloroplasts", "24 of the 32 songs", "Sauron", "Washington metropolitan area", "the negative of the base 10 logarithm of the molar concentration", "the breast or lower chest of beef or veal", "the ruling city of the Northern Kingdom of Israel, Samaria", "in either Tagalog or English", "around 1600 BC", "By mid-1988", "Michael Phelps", "Rajendra Prasad", "Ren\u00e9 Georges Hermann - Paul", "Donna", "Keith Hernandez and Willie Stargell", "Orangeville, Ontario, Canada", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Janie Crawford", "by the early 3rd century", "in the pancreas", "Kanawha Rivers", "1961", "Macintosh High Sierra", "in rocks and minerals", "Michael Schumacher", "a foreign exchange option", "1957", "1776", "1963", "President Friedrich Ebert", "2018", "Ireland", "Kit Harington", "the teenage porcupine punk rocker", "no embryo", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Baker, California, USA", "a Raja Dhilu", "the 1979 -- 80 season", "Guy Berryman", "Russian figure skater Evgeni Plushenko", "Sophocles", "Tim Allen as Luther Krank", "thick skin", "biscuit - sized cake", "India", "Brazil, Turkey and Uzbekistan", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "cricket", "the Major General of the Navy", "Marktown", "14,000", "Iran could be secretly working on a nuclear weapon", "Honduran", "pardoning Richard Nixon", "Ellen DeGeneres", "1961", "punk rock", "Westfield Old Orchard"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5783827232816203}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.30769230769230765, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 1.0, 0.30769230769230765, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 0.8, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.7777777777777778, 1.0, 0.0, 1.0, 0.6666666666666666, 0.08333333333333333, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4472", "mrqa_squad-validation-8780", "mrqa_squad-validation-2387", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-4641", "mrqa_hotpotqa-validation-1675", "mrqa_newsqa-validation-727", "mrqa_searchqa-validation-843", "mrqa_hotpotqa-validation-3984"], "SR": 0.421875, "CSR": 0.5617897727272727, "EFR": 1.0, "Overall": 0.7808948863636364}, {"timecode": 22, "before_eval_results": {"predictions": ["the mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania)", "Stanford University", "linebacker", "Mongol and Turkic tribes", "between 1859 and 1865", "Danny Lane", "in the New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13 )", "The Fixx", "Andrew Johnson", "Hellenism", "Mark Jackson", "Long Island", "The Chainsmoker", "in the United States, the poverty threshold for a single person under 65 was an annual income of US $11,770", "al - khimar", "week 4", "L.K. Advani", "the President", "Zachary John Quinto", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash in Hindi, Urdu and Punjabi", "Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "two", "John Adams", "Lituya Bay in Alaska", "Manhattan", "Burbank, California", "Ricardo Chavira", "2014", "Yuzuru Hanyu", "Glenn Close", "Elk", "flawed democracy", "China", "Craig Belden", "Jodie Foster", "September 6, 2007", "Neil Patrick Harris", "8ft", "Owen Vaccaro", "bacteria", "on the lateral side", "Lynda Carter", "erosion", "90 \u00b0 N 0 \u00b0 W \ufeff / \ufffdrous 90 \u00b0N - 0 \u00b0 E", "London to Canterbury", "in the bloodstream or surrounding tissue", "2017", "February 29", "1840s", "9.7 m ( 31.82 ft )", "Alabama", "Juliet", "Hotel California", "Brown Beauty", "\u00ef\u00bf\u00bd", "Eddie Vedder", "2005", "Dan Tyminski", "Elisabeth", "southern port city of Karachi", "at least nine", "Bashar al-Assad", "the New Revised Standard Version", "biathlon"], "metric_results": {"EM": 0.53125, "QA-F1": 0.616893433911097}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.34782608695652173, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5714285714285715, 0.14814814814814814, 1.0, 1.0, 0.2857142857142857, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8148148148148148, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2079", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-7486", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-4139", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-2101", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1295", "mrqa_searchqa-validation-15510"], "SR": 0.53125, "CSR": 0.5604619565217391, "EFR": 0.9666666666666667, "Overall": 0.7635643115942029}, {"timecode": 23, "before_eval_results": {"predictions": ["internal strife", "a new stage in the architectural history of the regions they subdued", "Fresno", "its many castles and vineyards", "0 \u00b0C (32 \u00b0F)", "Von Miller", "Kansas", "Thaddeus Rowe Luckinbill", "December 25", "2002", "the Mandate of Heaven", "Geoffrey Zakarian", "Christopher Allen Lloyd", "prenatal development", "Ali", "Sukhvinder Singh", "Article 1, Section 2, Clause 3", "Republic Day honours the date on which the Constitution of India came into effect on 26 January 1950", "Thomas Marvolo Riddle", "Dick Rutan and Jeana Yeager", "the closing of the atrioventricular valves and semilunar valves", "Ren\u00e9 Descartes", "James P. Flynn", "detritus", "September 27, 2017", "Johnny Logan", "1981 TV series, a 1984 video game, and 2005 feature film", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Telma Hopkins", "September 28, 2017", "Alex Skuby", "May 4, 1607", "March 2016", "1922 to 1991", "Richard Roxburgh", "Bacon", "Dr Thomas Chamberlain ( Richard Lintern ), is introduced at the start of the seventeenth series", "Heather Stebbins", "Redenbacher family", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "`` 0 '' trunk code", "April 1, 2016", "Friedman Billings Ramsey", "New York City", "cutting surfaces of a milling cutter", "Massachusetts Compromise", "Justin Timberlake", "Andrew Moray and William Wallace", "Alamodome and city of San Antonio", "female - only species that reproduces by producing an egg through parthenogenesis", "Al Schmid", "1871 A.D. Pt. Buddhiballav Pant", "eyes", "The History Boys", "sturgeon", "the White Knights of the Ku Klux Klan", "five", "Darkthrone", "U.S. 93 in White Hills, Arizona", "Phillip A. Myers", "Osama bin Laden", "Antarctica", "gaius caesar augustus gross", "gourmet Mushrooms"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5912614295426795}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.7499999999999999, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 0.0, 0.5714285714285715, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.8333333333333333, 0.2857142857142857, 0.18181818181818182, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1129", "mrqa_squad-validation-8990", "mrqa_squad-validation-2914", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-1445", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-1207", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-3651", "mrqa_hotpotqa-validation-395", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-505", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-12624"], "SR": 0.421875, "CSR": 0.5546875, "EFR": 0.972972972972973, "Overall": 0.7638302364864865}, {"timecode": 24, "before_eval_results": {"predictions": ["research, exhibitions and other shows", "no damage", "Stadtholder William III of Orange", "1933\u20131953", "faith alone, whether fiduciary or dogmatic, cannot justify man; justification rather depends only on such faith as is active in charity and good works", "Jim Thorpe", "1996 PGA Championship", "the onset and progression of Alzheimer's disease.", "Disco", "Kingdom of Dalmatia", "McKinsey's offices in Silicon Valley and India", "New York City", "Charles Whitman", "C. H. Greenblatt", "Fear and Loathing in Las Vegas", "A55", "Corendon Dutch Airlines", "86 ft", "Capella University", "sea loch", "Fatih Ozmen", "U.S.", "Pacific Place", "published February 5, 1959", "Flamingo Las Vegas", "Westminster, London", "2016 United States elections", "Wildhorn", "New York University School of Law", "The Crips", "\"Harper's Bazaar\"", "dementia", "Ferrara", "Guadalcanal Campaign", "Bishop's Stortford", "The Hungry Hustlerz: Starvation Is Motivation", "Barbara Niven", "Black Friday", "Archbishop of Canterbury", "TD Garden", "Vic Chesnutt", "the Teutonic Knights", "Australia", "Julie Taymor", "Easy", "World War I", "79 AD", "Musicology", "Portland, OR", "Yoruba", "No Surprises", "Charles Otto Puth Jr.", "2007 and 2008", "2001", "1966", "the Stig", "Ryan Harris", "giselda Blanco", "Joe Jackson", "Lawmakers Eyeall one of several alternative-energy vehicles parked this week", "2004.", "genes", "olive", "Stockholm"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6534750839438339}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.8571428571428571, 1.0, 0.7499999999999999, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7092", "mrqa_squad-validation-2153", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-1697", "mrqa_hotpotqa-validation-431", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-5855", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2930"], "SR": 0.546875, "CSR": 0.5543750000000001, "EFR": 0.9655172413793104, "Overall": 0.7599461206896552}, {"timecode": 25, "UKR": 0.669921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-1123", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-4562", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3206", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-3654", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-11385", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11900", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12624", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-1335", "mrqa_searchqa-validation-13486", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14883", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-16181", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3783", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-4857", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8589", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-9756", "mrqa_squad-validation-10045", "mrqa_squad-validation-10069", "mrqa_squad-validation-10074", "mrqa_squad-validation-10086", "mrqa_squad-validation-10216", "mrqa_squad-validation-10228", "mrqa_squad-validation-10254", "mrqa_squad-validation-10310", "mrqa_squad-validation-10324", "mrqa_squad-validation-10338", "mrqa_squad-validation-10353", "mrqa_squad-validation-1036", "mrqa_squad-validation-10378", "mrqa_squad-validation-10477", "mrqa_squad-validation-1090", "mrqa_squad-validation-1320", "mrqa_squad-validation-1450", "mrqa_squad-validation-1603", "mrqa_squad-validation-1636", "mrqa_squad-validation-1672", "mrqa_squad-validation-1694", "mrqa_squad-validation-178", "mrqa_squad-validation-1802", "mrqa_squad-validation-1852", "mrqa_squad-validation-1855", "mrqa_squad-validation-1857", "mrqa_squad-validation-1938", "mrqa_squad-validation-1967", "mrqa_squad-validation-2040", "mrqa_squad-validation-2126", "mrqa_squad-validation-2153", "mrqa_squad-validation-2216", "mrqa_squad-validation-2289", "mrqa_squad-validation-2384", "mrqa_squad-validation-2400", "mrqa_squad-validation-2436", "mrqa_squad-validation-2460", "mrqa_squad-validation-2477", "mrqa_squad-validation-255", "mrqa_squad-validation-2577", "mrqa_squad-validation-2602", "mrqa_squad-validation-2619", "mrqa_squad-validation-268", "mrqa_squad-validation-2693", "mrqa_squad-validation-2773", "mrqa_squad-validation-2782", "mrqa_squad-validation-2798", "mrqa_squad-validation-282", "mrqa_squad-validation-2824", "mrqa_squad-validation-285", "mrqa_squad-validation-2929", "mrqa_squad-validation-3019", "mrqa_squad-validation-3041", "mrqa_squad-validation-3135", "mrqa_squad-validation-3185", "mrqa_squad-validation-320", "mrqa_squad-validation-3337", "mrqa_squad-validation-3476", "mrqa_squad-validation-353", "mrqa_squad-validation-3589", "mrqa_squad-validation-3709", "mrqa_squad-validation-383", "mrqa_squad-validation-3931", "mrqa_squad-validation-3948", "mrqa_squad-validation-3955", "mrqa_squad-validation-397", "mrqa_squad-validation-3993", "mrqa_squad-validation-4005", "mrqa_squad-validation-4079", "mrqa_squad-validation-4140", "mrqa_squad-validation-415", "mrqa_squad-validation-4181", "mrqa_squad-validation-427", "mrqa_squad-validation-4291", "mrqa_squad-validation-4305", "mrqa_squad-validation-4333", "mrqa_squad-validation-4338", "mrqa_squad-validation-4472", "mrqa_squad-validation-462", "mrqa_squad-validation-4686", "mrqa_squad-validation-4704", "mrqa_squad-validation-4835", "mrqa_squad-validation-4856", "mrqa_squad-validation-4870", "mrqa_squad-validation-5054", "mrqa_squad-validation-5088", "mrqa_squad-validation-5096", "mrqa_squad-validation-5154", "mrqa_squad-validation-5176", "mrqa_squad-validation-5238", "mrqa_squad-validation-5302", "mrqa_squad-validation-5326", "mrqa_squad-validation-5376", "mrqa_squad-validation-550", "mrqa_squad-validation-5537", "mrqa_squad-validation-5541", "mrqa_squad-validation-5588", "mrqa_squad-validation-5616", "mrqa_squad-validation-5672", "mrqa_squad-validation-5703", "mrqa_squad-validation-5767", "mrqa_squad-validation-5777", "mrqa_squad-validation-5913", "mrqa_squad-validation-60", "mrqa_squad-validation-60", "mrqa_squad-validation-607", "mrqa_squad-validation-6099", "mrqa_squad-validation-6126", "mrqa_squad-validation-6143", "mrqa_squad-validation-6178", "mrqa_squad-validation-6220", "mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6362", "mrqa_squad-validation-6395", "mrqa_squad-validation-6414", "mrqa_squad-validation-6564", "mrqa_squad-validation-660", "mrqa_squad-validation-6641", "mrqa_squad-validation-6737", "mrqa_squad-validation-6754", "mrqa_squad-validation-6782", "mrqa_squad-validation-68", "mrqa_squad-validation-6817", "mrqa_squad-validation-6915", "mrqa_squad-validation-696", "mrqa_squad-validation-7018", "mrqa_squad-validation-703", "mrqa_squad-validation-7069", "mrqa_squad-validation-707", "mrqa_squad-validation-7150", "mrqa_squad-validation-7161", "mrqa_squad-validation-7180", "mrqa_squad-validation-7198", "mrqa_squad-validation-7260", "mrqa_squad-validation-7399", "mrqa_squad-validation-754", "mrqa_squad-validation-7552", "mrqa_squad-validation-7597", "mrqa_squad-validation-7640", "mrqa_squad-validation-765", "mrqa_squad-validation-7678", "mrqa_squad-validation-7770", "mrqa_squad-validation-7782", "mrqa_squad-validation-7814", "mrqa_squad-validation-7856", "mrqa_squad-validation-7882", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-804", "mrqa_squad-validation-8056", "mrqa_squad-validation-8104", "mrqa_squad-validation-8115", "mrqa_squad-validation-8189", "mrqa_squad-validation-8226", "mrqa_squad-validation-8226", "mrqa_squad-validation-8285", "mrqa_squad-validation-8406", "mrqa_squad-validation-8480", "mrqa_squad-validation-8527", "mrqa_squad-validation-8629", "mrqa_squad-validation-8735", "mrqa_squad-validation-8760", "mrqa_squad-validation-8765", "mrqa_squad-validation-8832", "mrqa_squad-validation-884", "mrqa_squad-validation-8867", "mrqa_squad-validation-890", "mrqa_squad-validation-8957", "mrqa_squad-validation-898", "mrqa_squad-validation-9031", "mrqa_squad-validation-9066", "mrqa_squad-validation-9135", "mrqa_squad-validation-9186", "mrqa_squad-validation-9227", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9391", "mrqa_squad-validation-9392", "mrqa_squad-validation-9465", "mrqa_squad-validation-9504", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9652", "mrqa_squad-validation-9658", "mrqa_squad-validation-9771", "mrqa_squad-validation-979", "mrqa_squad-validation-9818", "mrqa_squad-validation-987", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-3051", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-469", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5568", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-6909", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-776"], "OKR": 0.8203125, "KG": 0.37578125, "before_eval_results": {"predictions": ["progressive tax", "Jacksonville", "monophyletic", "Orthogonal components", "Fox Network", "La clemenza di Tito", "Anna Clyne", "Terence Winter, based on the memoir of the same name by Jordan Belfort", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Formula E", "Eastern College Athletic Conference", "Kim Jong-hyun", "Peter Chelsom,", "The Ninth Gate", "heavy metal", "Cinderella", "Los Angeles", "Sharyn McCrumb", "acid house", "Austria, south Germany, German Switzerland, and Slovenia at the end of the 18th century", "Capture of the Five Boroughs", "Miranda Leigh Lambert", "Shenandoah National Park", "BBC Formula One coverage", "10 Years", "Haleiwa Ali'i Beach Park", "Rotenburg Cannibal", "1886", "Rockhill Furnace, Pennsylvania", "northeastern", "coca wine", "Entrepreneur", "the lead roles", "PBS stations nationwide,", "second largest", "Citric acid", "1911", "Johnnie Ray", "the group known as \"The Five\"", "Walt Disney Feature Animation", "on 13 May 2018", "torpedo boats", "1972", "Geographical Indication tag", "Buck Owens and the Buckaroos", "Celtics", "National Wrestling Alliance (NWA)-affiliated \"territory\" promotion \u2013 Jim Crockett Promotions", "1994", "TD Garden", "the Chechen Republic", "Chrysler", "The Institute for Advanced Study (IAS)", "in 2005", "Tenochtitlan", "over the age of 5", "Henry Louis", "Mexico", "ThunderCats", "he fears a desperate country with a potential power vacuum that could lash out.", "Catholic League", "Krishna Rajaram", "michael and forgery", "to become more sustainable at a critical point in its development", "green"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6410695207570207}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-2056", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4737", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-2658", "mrqa_hotpotqa-validation-4298", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9487", "mrqa_triviaqa-validation-7575", "mrqa_newsqa-validation-2772", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4240"], "SR": 0.515625, "CSR": 0.5528846153846154, "EFR": 0.967741935483871, "Overall": 0.6773284351736973}, {"timecode": 26, "before_eval_results": {"predictions": ["bacteriophage T4", "1698", "Xingu", "The Ruhr", "Dar es Salaam", "Heinkel Flugzeugwerke", "Victor Garber", "Pope John X", "Stanmore, New South Wales", "aged between 11 or 13 and 18", "\"Histoires ou contes du temps pass\u00e9\"", "Orchard Central", "Tom Holland", "late eighteenth century", "\"The Snowman\"", "1979", "Premier League club Liverpool", "port city of Aden", "British", "second cousin", "2013", "Archie Andrews", "2 May 2015", "1827", "Crystal Dynamics", "Cleveland Cavaliers", "goalkeeper", "Debbie Harry", "\"media for the 65.8 million,\"", "John Joseph Travolta", "Hall & Oates", "Mazatl\u00e1n", "Michael Owen", "Las Vegas", "1919", "Kevin Spacey", "\"Love Streams\"", "Michael Edwards", "Daphnis et Chlo\u00e9", "Lake Wallace", "England", "1993", "Boston Celtics", "The Eisenhower Executive Office Building", "6,396", "Adelaide Steamship Company", "The Saturdays", "Attack the Block", "Leonarda Cianciulli", "Morse Field", "Tudor music and English folk-song", "CMYKOG", "1600 BC ( possibly a fragmentary copy of a text from 2500 BC )", "The centuries - old Wars Grand Master", "1963", "a peplos", "The Herald of Free Enterprise", "Lyrical Ballads", "15,000", "10 to 15 percent", "\"It has never been the policy of this president or this administration to torture.\"", "Tarzan", "Japan", "potp0urri"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6524439102564102}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-1677", "mrqa_newsqa-validation-4143", "mrqa_searchqa-validation-13669"], "SR": 0.578125, "CSR": 0.5538194444444444, "EFR": 1.0, "Overall": 0.6839670138888889}, {"timecode": 27, "before_eval_results": {"predictions": ["immediately north of Canaveral at Merritt Island", "pedagogic diversity", "Christian", "Extension", "Cinderella", "Dan Tyminski", "Guthred", "October 17, 2017", "Kolkata", "\"Dumb and Dumber\"", "Boeing EA-18G Growler", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Paper", "Alistair Grant", "whoopi Goldberg, and Houston", "Dunlop Tyres", "Bonkyll Castle", "UTA Arad's Leoh Digbeu", "Algernod Lanier Washington", "erotic romantic comedy", "a leg injury", "Antonio Salieri", "American", "Europe", "What You Will", "Brooklyn, New York", "Thriller", "Jesper Myrfors", "The Primettes", "Cersei Tyrister", "Elizabeth Kekaaniau La\u02bbanui Pratt", "Indonesian National Revolution", "Don Bluth", "December 8, 1970", "Chief of the Operations Staff of the Armed Forces High Command", "Hong Kong Disneyland", "London", "I Should Have Known Better", "September 8, 2017", "FBI", "Christine MacIntyre", "Virginia Katherine McMath", "Wildhorn, Bricusse and Cuden", "Hotch Kiss M1914", "David Victor and David J. O'Connell", "Prussian army", "January 2004", "co-founder and lead guitarist", "ten", "seven", "October 25, 1881", "Sam Waterston", "Pradyumna", "Mark Jackson", "R / T badging", "The Colossus of Rhodes", "Equatorial Guinea", "glycerol", "Victor Mejia Munera", "Chris Robinson and girlfriend Allison Bridges", "off east  Africa", "bromide", "nasal septum", "infrared cameras"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6498329156223893}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false], "QA-F1": [0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5333333333333333, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3840", "mrqa_squad-validation-1916", "mrqa_squad-validation-1075", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3346", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-3400", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-697", "mrqa_naturalquestions-validation-4039", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-6398"], "SR": 0.546875, "CSR": 0.5535714285714286, "EFR": 1.0, "Overall": 0.6839174107142858}, {"timecode": 28, "before_eval_results": {"predictions": ["public (government) funding", "prep schools", "a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF)", "Hanover", "Henry Mancini", "tenno", "Gordon Ramsay", "Gorbachev", "a sort of one-of-a-kind comedy", "sub rosa", "Charlton Heston", "Anna (Julia Roberts)", "Binky", "orchid", "Paddy Doherty", "a milder disease", "the Pyramid of Cheops", "Libya", "U.S.", "F\u00fcrabeth'', WoO 59", "Saudi Arabia", "Who's Who in David Copperfield", "The Soul of Indiscretion: Tom Driberg, Poet, Philanderer, financiers, explorers", "Milady de Winter", "April", "Eric Morley", "ADHD and hypertension", "the Garrick Club", "Belle", "Fabio Capello", "New York", "Tom Stoppard", "The Greatest", "the manager of Elvis Presley", "off the coast of Northumberland in Northern England", "a habanero", "jazz tenor saxophonist", "Tacoma", "The Union Inn", "Cardiff", "Baton Rouge", "a carburetors", "Tahrir Square", "Romanian", "the \"useful life period\"", "Michael Caine", "Lord Snooty", "Borodin", "Jesse James", "dog", "Greek", "passion fruit", "Thomas Lennon", "Haikou on the Hainan Island", "Leviathan", "Denmark and Norway", "1966", "North America", "Florida's Everglades", "Trisha Yearwood", "ties", "a street or", "Glengarry Glen Ross", "shark"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5342757936507936}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.39999999999999997, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6891", "mrqa_squad-validation-4846", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-4619", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-2313", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-7182", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-9024", "mrqa_hotpotqa-validation-2910", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3674", "mrqa_newsqa-validation-1004", "mrqa_searchqa-validation-8934"], "SR": 0.484375, "CSR": 0.5511853448275862, "EFR": 1.0, "Overall": 0.6834401939655173}, {"timecode": 29, "before_eval_results": {"predictions": ["Chicago Theological Seminary", "CBS and NBC", "$100,000", "Super Bowl LII", "starch", "Taylor Michel Momsen", "Kennedy Space Center ( KSC ) in Florida", "Tim Duncan", "James W. Marshall", "Blue laws", "Randy VanWarmer", "negotiates treaties with foreign nations", "Emma Watson and Dan Stevens", "between 8.7 % and 9.1 %", "2018", "if the occurrence of one does not affect the probability of occurrence of the other", "Jason Flemyng", "Chesapeake Bay, south of Annapolis in Maryland", "northern China", "T.J. Miller", "in Pyeongchang County, Gangwon Province, South Korea", "the status line", "the retina", "Tom Brady", "Triple Entente", "Andrew Lloyd Webber", "1955", "Charles Frederickson ( Nick Sager )", "Buffalo Lookout", "Humpty Dumpty", "Charlene Holt", "1 US dollar", "Leonard Nimoy", "1960", "Bumblebee", "the tax rate paid by a small business", "beneath the liver", "Andy Serkis", "West Norse sailors", "Kristy Swanson", "Anna Faris", "early to mid-2000s", "Fleetwood Mac", "improved economics of distribution", "Cairo, Illinois", "in the 1970s and'80s", "in the books of Exodus and Deuteronomy", "Earl ( John Doe )", "Psychomachia", "January 2, 1971", "The Miracles", "a domain ( Latin : regio )", "elbow", "Big Wiesy", "Charlie Sheen", "Love Child", "George Orwell", "Bardot", "27", "Long Island convenience store", "Mitt Romney", "island", "high fever", "dancing with the stars"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6268677183562051}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5454545454545454, 0.21052631578947367, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5764", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-6865", "mrqa_triviaqa-validation-6508", "mrqa_hotpotqa-validation-4870", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-1979", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7426"], "SR": 0.5625, "CSR": 0.5515625, "EFR": 0.9642857142857143, "Overall": 0.6763727678571428}, {"timecode": 30, "before_eval_results": {"predictions": ["San Jose", "quality rental units", "cultural tourism and sports tourism", "a circle", "( Sequoia sempervirens)", "corey", "Spanish Republic", "taxicab", "jaguar", "corey pobdio ekonomin veikl. PVM, kaip", "Harry Reid", "Ray", "axis", "forge", "boxing", "(Why did he kill them?)", "(M) Fleurs", "Blackbird", "Footprints", "(1.)2.360)", "corey prapavessis", "U.S. Census Bureau", "Tommy Lee Jones", "(St.) Jose Maria Escriva", "The Memory Keeper's", "(1874-1876)", "hubris", "Yahtzee", "Tony Danza", "HTML", "hives", "74.3", "William S. Hart", "(Sahih al-Bukhari, Volume 1, Book 8, Number 367)", "Pride and Prejudice", "corey", "kosher wine", "Munich", "Michael Jordan", "February 2nd", "Prospero", "Hikaru Sulu", "parrots", "dough", "honshu", "honey", "Boston", "Fisher- Price", "Arctic Ocean", "pizze Napoletane", "squash", "Spain", "Thomas Chisholm", "May 2002", "1936", "Newfoundland", "The Fortune cookie", "Monty Python's Spamalot", "1911", "Harlow Cuadra and Joseph Kerekes", "Newtonian mechanics", "Israel", "top designers", "anti-trust laws."], "metric_results": {"EM": 0.453125, "QA-F1": 0.49479166666666663}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-315", "mrqa_squad-validation-2965", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-2374", "mrqa_searchqa-validation-11167", "mrqa_searchqa-validation-768", "mrqa_searchqa-validation-6792", "mrqa_searchqa-validation-5093", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-12408", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-15027", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-5162", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-12415", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-13549", "mrqa_searchqa-validation-9773", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-9379", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-9820", "mrqa_naturalquestions-validation-10656", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3030", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-3054"], "SR": 0.453125, "CSR": 0.5483870967741935, "EFR": 1.0, "Overall": 0.6828805443548387}, {"timecode": 31, "before_eval_results": {"predictions": ["Rev. Paul T. Stallsworth", "white", "stand-up", "satirical erotic romantic comedy", "the \"Ratchet & Clank\"", "Chancellor Christian Kern", "June 26, 1970", "the Brown Square Station", "elis Stefanik", "Fleetwood Mac", "Odense Boldklub", "the Supreme Court", "Bangkok", "Oklahoma Sooners", "Merrimack", "Gust Avrakotos", "\"The Late Late Show\"", "Mark Anthony \"Baz\" Luhrmann", "two", "Indianapolis Motor Speedway", "Romagnol", "Anita Dobson", "a family member", "January 19, 1943", "The Worm", "Eliot Cutler", "Blackheart Records", "1970s and 1980s", "C. J. Cherryh", "Pablo Escobar", "16,116", "Rockland", "\"Slaughterhouse-Five\"", "The Adventures of Huckleberry Finn", "wineries", "Frank Sinatra", "Robert L. Stone", "goalkeeper", "Philadelphia", "New York", "Town of Oyster Bay", "Sinngedichte", "The Highwaymen", "Madrid", "Kevin Spacey", "Arizona State University", "Blue Grass Airport", "Lawton Chiles", "1952", "the Nebula Award, the Philip K. Dick Award, and the Hugo Award", "I'm Shipping Up to Boston", "the Royal Air Force", "Lenny Jacobson", "Hathi Jr", "1935", "a serpent", "slow", "\"Just to See Her\"", "Winter Park.", "the anti-trust unit", "Friday", "Burgundy", "Franklin D. Roosevelt", "Ukraine"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6971955128205127}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-2607", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-2554", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-5034", "mrqa_triviaqa-validation-6414", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-6055"], "SR": 0.59375, "CSR": 0.5498046875, "EFR": 1.0, "Overall": 0.6831640625000001}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh L. Dryden", "2004", "Kenya", "The Rocky Horror Picture Show", "Trainspotting", "Argentina", "Apollo 11 mission", "jellyfish", "March", "a clogs", "Fauntleroy", "the World Health Organization", "Eat porridge", "Kofi Annan", "oxygen", "the first newspaper in London", "Taggart", "Che Amanwe and Chi Eekway", "the Gulf of Mexico", "Ladee-Lo", "Sven Goran Eriksson", "the BBC", "Jackson Street", "i second that emotion", "Charles Edward Stuart", "John Poulson", "i second largest airport, Orly", "the Treaty on European Union", "Jack Frost", "Saskatchewan (province)", "Laurent Planchon", "the Solent", "vomiting", "Basketball", "Bristol Aeroplane Company", "Spinach", "Steve Davis", "\u201cArgo\u201d", "Gemini", "Surrey", "1969", "chippenham", "London", "The Coquimbo Region", "William Shakespeare", "borax", "a \"surburbano\" (in Milano)", "Jamaica", "Peter Nichols", "Diana Dors", "Kent", "Vickers-Armstrong's", "Ray Charles", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "USCS or USC", "Miller Brewing", "northwestern Italian coast", "Sydney Opera House", "died. So did members of four bands who were practicing inside a studio that collapsed. Other musicians lost legs, arms and hands.", "her decades-long portrayal of Alice Horton", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Peter Bogdanovich", "i second that emotion", "the Federal Republic of Germany"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5372247544122544}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.12121212121212123, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-5360", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-3435", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5129", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-5817", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-3368", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-5611"], "SR": 0.46875, "CSR": 0.5473484848484849, "EFR": 1.0, "Overall": 0.682672821969697}, {"timecode": 33, "before_eval_results": {"predictions": ["New York", "1887", "Lana Del Rey", "1,228 km / h ( 763 mph )", "New England Patriots", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the Arctic Ocean", "Mitch Murray", "blue", "Gunpei Yokoi", "John Bull", "775 rooms", "eusebeia", "waiting tables at the Moondance Diner", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "long line", "Jesus'birth", "a habitat", "Kirsten Simone Vangsness", "Central Germany", "Andrew Johnson", "Etienne de Mestre", "Aegisthus", "electors", "Julia Ormond", "Sauron's assistance", "1961", "ste\u026and / STAYND", "2013", "March 1", "novelization", "a usually red oxide formed by the redox reaction", "Spain", "MacKenzie Mauzy", "Paul Lynde", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Jocelyn Flores", "abdicated in November 1918", "visited paid monument", "erosion", "March 2, 2016", "stuffing", "1996", "Ray Charles", "16", "the Ramones", "1800", "Anglo - Norman French waleis", "Frank Theodore `` Ted '' Levine", "New Jersey", "July 31, 2010", "France", "Heath Ledger", "dennis taylor", "a centaur", "DJ", "cricket fighting", "Luis Edgardo Resto", "drama that pulls in the crowds", "a Nazi German death camp", "Islamabad", "Tunisia", "RAND Corporation", "alberta"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5229225612654889}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.125, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.7368421052631579, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8205128205128205, 0.0, 0.4210526315789474, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3127", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-9972", "mrqa_triviaqa-validation-5607", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-1997", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2118", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-9333"], "SR": 0.4375, "CSR": 0.5441176470588236, "EFR": 1.0, "Overall": 0.6820266544117647}, {"timecode": 34, "before_eval_results": {"predictions": ["Venus", "Beyonc\u00e9 and Bruno Mars", "Cal", "7th century", "2018", "her abusive husband", "premiered on CBS on September 29, 2017", "interstellar space", "transmission and final drive", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions", "Sukhvinder Singh", "March 14, 1942", "Nick Sager", "the Parliament of the United Kingdom", "a biblical title of respect applied to prophets and beloved religious leaders", "state legislators of Assam", "digestion of proteins", "Renishaw Hall, Derbyshire, England", "accomplish the objectives of the organization", "pickup trucks", "Isabella Palmieri", "temperature at which the phase transition occurs", "Mind your Ps and Qs", "Germany", "20 November 1989", "Spanish Dominican Tom\u00e1s de Torquemada", "Bob Gaudio", "16 August 1975", "Mel Gibson", "Procol Harum", "Erica Rivera", "zinc", "a four - page pamphlet in 1876", "2003", "Sebastian Lund", "Wednesday, 5 September 1666", "California State Route 1", "The management team", "various submucosal membrane sites", "modern programming practices", "Richmond, BC", "Phillip Schofield and Christine Bleakley", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Ukraine", "Lula", "1850", "when the car comes to a halt", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "Derrick Henry", "lowest air temperature", "`` Mirror Image ''", "on a bronze plaque and mounted inside the pedestal's lower level", "krave,", "wilson taylor", "Brian Close", "hard rock/blues rock band", "Galleria Vittorio Emanuele II", "every aspect of public and private life", "reached an agreement late Thursday to form a government of national reconciliation.", "this week", "Henry Ford", "Toyota", "a vice president", "a brain tumour"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5252284169175333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.6, 0.5714285714285715, 0.0, 0.16666666666666669, 1.0, 0.5, 1.0, 0.0, 0.42857142857142855, 0.3333333333333333, 0.19999999999999998, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.29629629629629634, 0.375, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.20000000000000004, 0.2758620689655173, 0.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-10576", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-1562", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-15641"], "SR": 0.40625, "CSR": 0.5401785714285714, "EFR": 0.9473684210526315, "Overall": 0.6707125234962407}, {"timecode": 35, "before_eval_results": {"predictions": ["domestic Islamists who attacked it", "mild euphoric", "hanostratus", "Venezuela", "Kansas City", "ring", "\"All children, except one, grow up\"", "david taylor", "the Arctic Ocean", "air pressure", "fish", "Lafayette", "Malcolm X", "a belt of calms and light baffling winds", "the Royal Marines Band", "Alexander Pushkin", "Australia", "war of the Worlds", "puebla", "a night shift", "bishops", "Arkansas", "gcpmaternity", "Pierre-August Renoir", "muh-syuh, mah-dahm, mahd-mwah-zellMister, Misses,", "libretti", "Innsbruck", "Lance Ito", "Microsoft", "a fern", "Sony", "a Viking chieftain's longhouse", "Atlantic City", "Blackwater USA", "elephants", "American Airlines", "a gerenuk", "Odysseus", "Brian Sanders", "Royal Palaces", "William Carver", "Netherlands", "George Smith", "c.S. Lewis", "Dagny Taggart", "casping on a chalkboard", "the Chicago Mercantile Exchange", "Las Vegas", "\"Tights: not just for dancing\"", "wheat", "Madrid Symphony Orchestra", "a ostrich", "1943", "Payaya Indians", "beneath the liver", "James I", "penrhyn", "psychological horror", "John Morgan", "Hungarian Rhapsody No. 2", "Henry II", "Sen. Debbie Stabenow", "63", "\"perezagruzka,\""], "metric_results": {"EM": 0.390625, "QA-F1": 0.4568452380952381}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9699", "mrqa_searchqa-validation-11665", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-265", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-4473", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-11675", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-15317", "mrqa_searchqa-validation-5646", "mrqa_searchqa-validation-3975", "mrqa_searchqa-validation-8694", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-15775", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-16094", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-3348", "mrqa_hotpotqa-validation-3745", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352"], "SR": 0.390625, "CSR": 0.5360243055555556, "EFR": 1.0, "Overall": 0.6804079861111111}, {"timecode": 36, "before_eval_results": {"predictions": ["electric lighting", "James W. Marshall", "Terrell Suggs", "north of the Equator", "at Lucknow in September 1906", "in the eighth episode of Arrow's second season", "National Industrial Recovery Act ( NIRA )", "The User State Migration Tool", "the Battle of Antietam", "William DeVaughn", "the World Trade Center Transportation Hub", "the Thames Estuary", "Santa Monica", "sovereign states", "Raza Jaffrey", "31 January 1934", "Filipino", "1773", "modern random - access memory ( RAM )", "May 31, 2012", "April 1917", "Bart Cummings", "October 27, 1904", "Harishchandra", "Olivia Olson", "1990", "Billy Gibbons", "Bill Pullman", "BC Jean", "2016", "Frankie Muniz", "stratum lucidum", "60", "Hasmukh Adhia", "the Four Seas", "retinal", "the 1980s", "in soils", "card verification value ( CVV )", "ordain presbyters / bishops", "bohrium", "Britain", "Escherichia coli", "Archduke Franz Ferdinand", "June 1991", "2010", "he lost the support of the army, abdicated in November 1918, and fled to exile in the Netherlands", "in the basic curriculum -- the enkuklios paideia or `` education in a circle ''", "Mike Czerwien", "103", "Vienna", "English", "Mexico", "Stalin", "$10.5 million", "Al Horford", "Andrew Johnson", "$22 million", "leftist Workers' Party", "his mother, Katherine Jackson, his three children and undisclosed charities", "cotton", "Dennis Haysbert", "Quinn", "Towcester"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6595503021284271}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5454545454545454, 0.33333333333333337, 0.7142857142857143, 0.888888888888889, 0.0, 1.0, 1.0, 0.37499999999999994, 0.0, 0.5714285714285715, 0.08333333333333334, 0.0, 0.5, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.09523809523809523, 0.5454545454545454, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-2833", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-4351", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1953", "mrqa_searchqa-validation-13161"], "SR": 0.515625, "CSR": 0.535472972972973, "EFR": 0.9354838709677419, "Overall": 0.667394493788143}, {"timecode": 37, "before_eval_results": {"predictions": ["Joseph Swan", "the United States", "South Africa", "first among equals", "Shade", "a cappella", "albinism", "boston", "aglet", "Saturday Night Live", "FC Bayern M\u00fcnchen", "winter", "Bonnie and Clyde", "Spanish", "copper", "Dawn French", "Blackstar", "brazil", "Doris Lessing", "Scooby-Doo", "Swaziland", "brazilia", "Kent", "the Humber", "points based scoring", "a seat", "Kent", "Rodgers & Hammerstein", "Boy George", "Galileo Galilei", "Zelle", "Scotland Yard", "Marilyn Manson", "Medellin", "The Tempest", "a carburetor", "Celts", "Boulder Dam", "long-term effects of using drugs", "brazil", "Belle de Jour", "Morecambe", "an abba", "rain", "blue", "the scientists of Laputa", "France", "Snowbell", "Kunsky", "death", "Lady Penelope", "the forces of Andrew Moray and William Wallace", "142,907", "mid November", "YouTube", "Theo James Walcott", "Ben Ainslie", "at least three bodies were trapped in a \"very compressed area.\"", "thunderstorms", "women and breast cancer", "Blaine", "a sunflower", "Madonna", "March 24"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6269903273809524}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.375, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7074", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-2982", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-330", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-8884", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-6291"], "SR": 0.59375, "CSR": 0.5370065789473684, "EFR": 1.0, "Overall": 0.6806044407894737}, {"timecode": 38, "before_eval_results": {"predictions": ["the Rip", "tyne", "the liver", "40", "table salt", "Bears", "cuba", "lewis", "Phil Redmond", "Stevie Wonder", "large", "a hound", "hanover", "an astronomy", "Charles I", "workless", "scales", "Dirty Dancing", "goddess of Revenge", "Diana Ross", "a man of gold named Montezuma", "a 1934 Austin seven box saloon", "Paul Anka", "cuba", "cuba", "the king Duncan", "Blade Runner", "Jay-Z", "leopons", "cymbals", "albion", "San Diego", "Tory MP Andrew Mitchell", "Ticket Sarasota", "South Africa", "Christian Dior", "scrobbesbyrig", "a killer whale", "Ukrainian", "France", "raspberries", "pilgrimage", "Cyprus", "speed camera", "duke", "lizard", "fandango Groovers", "frauds", "a dolphin", "even numbers", "Tony Blair", "quartz or feldspar", "54 Mbit / s", "Manley", "Stacey Kent", "Eyes Wide Shut", "Anthony Lynn", "piano", "paid tribute to pop legend Michael Jackson, who died Thursday afternoon in Los Angeles.", "the United States", "a knuckle on northern South America's fist,", "New Network Tech News podcasts", "a second son", "Tiger Woods"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4933035714285714}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-1112", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3351", "mrqa_naturalquestions-validation-655", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-12737", "mrqa_searchqa-validation-9940", "mrqa_searchqa-validation-4817", "mrqa_newsqa-validation-3899"], "SR": 0.453125, "CSR": 0.5348557692307692, "EFR": 1.0, "Overall": 0.6801742788461539}, {"timecode": 39, "before_eval_results": {"predictions": ["\"No, that's no good\"", "albion", "Midnight Cowboy", "georgia", "seborrheic dermatitis", "Amanda Barrie", "a hoy was a ship working in the Thames Estuary and southern North Sea in the manner of the Thames sailing barge", "Niger", "central Stockholm", "Tangled", "dogs", "georgia davis", "Bulls Eye", "georgia", "norman leben", "Martin Clunes", "Jane Austen", "pembrokeshire Coast National Park", "Kevin macdonald", "peppers", "geologic era", "jimmy Boyd", "isambard Kingdom Brunel", "georgia", "1957", "watlinghouse", "la Marne", "leonangere", "moctes", "Ralph Vaughan Williams", "musical scale", "animal sanctuaries", "flannel", "E. T. A. Hoffmann", "Shanghai", "Spain", "grows in St. James Parish", "Tuesday", "Guru Nanak", "bleak house", "Inigo Montoya", "hexagonal", "Little Jack Horner", "bristol", "louis haze", "cuckoo", "murder", "Ford", "Alice Cooper", "mallorca", "albion", "Royal Bengal Tiger", "an inward spiral", "grouchy", "television personality, sports and entertainment reporter", "1999", "Sela Ann Ward", "The Cycle of Life", "forgery and flying without a valid license,", "137", "a log cabin", "St. Patrick's Day", "blitz", "Sondheim"], "metric_results": {"EM": 0.40625, "QA-F1": 0.44717261904761907}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6497", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-7408", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6153", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5688", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2738", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-2711", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8554", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2100", "mrqa_searchqa-validation-7546", "mrqa_naturalquestions-validation-9755"], "SR": 0.40625, "CSR": 0.531640625, "EFR": 0.9736842105263158, "Overall": 0.6742680921052632}, {"timecode": 40, "before_eval_results": {"predictions": ["the 19th Century", "Famous Players", "upper devon", "jane devincey", "the black death", "horses", "buffalo", "rome", "a raven", "Sarajevo", "the Bill of Rights", "fine", "Neighbours", "devon", "trumpet", "abbey", "origami", "resistance", "the Arabian Gulf", "secretary", "devon", "avunculicide", "jack Nicholson", "\u201cGone With the Wind\u201d", "rail tour", "Tomorrow Never Dies", "indian", "a Great Dane", "Washington", "indianapolis", "New Hampshire", "jimmy tresham", "charlie fenton", "Japanese", "purple", "fezzik", "devon", "cliffs", "Venice", "10", "Southwest Airlines", "jimmy", "Jeffery Deaver", "The Comedy of Errors", "brazil", "Alex Turner", "President Ford", "crossword clue", "upper devon", "radicalization", "rodinsons", "Humpty Dumpty", "1998", "Tanvi Shah", "the ENnies", "the first \"Tour de France\" bicycle race", "the Mach number (M or Ma) ( ;] )", "Janet and La Toya", "2.5 million", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter", "The Matrix", "Curb Your Enthusiasm", "nibelung", "Inequality of opportunity"], "metric_results": {"EM": 0.375, "QA-F1": 0.41145833333333326}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2680", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-4716", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4538", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-863", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-7516", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-2372", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-11519"], "SR": 0.375, "CSR": 0.5278201219512195, "EFR": 1.0, "Overall": 0.678767149390244}, {"timecode": 41, "before_eval_results": {"predictions": ["1220", "philippine", "hula hoops", "nixon", "red fleece", "roddy dixon", "a abacus", "henry vii", "sisyphus", "Velazquez", "european", "caracas", "henie", "tchaikovsky", "henry vii", "angola", "private Eye", "david Bowie", "Buzz Aldrin", "james paul sartre", "james city", "james thomas", "rust", "jane niston", "pembroke County, Wales", "tbilisi", "james georgia", "othello", "sewing up of a small hole or tear in a piece of material", "henry de Becque", "lacock abbey", "henry twat", "domestic cat", "Anita Brookner", "vii vii", "Golda Meir", "Black Sea", "bagram airfield", "Susie Dent", "a power outage", "london", "The Archers", "henry vii", "james james sousa", "henry gee", "james b Boyd", "marcella city", "the Marx Brothers", "avon", "vii vii", "Dry Ice", "Pat McCormick", "19 June 2018", "18 - season", "from 1993 to 1996", "Michael Rispoli", "September 29, 2017", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "June 6, 1944", "sniff out cell phones.", "a bassoon", "o.K. Corral", "butternut juice", "phoenicia"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49097222222222225}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5582", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-7425", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-3306", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-5625", "mrqa_triviaqa-validation-6097", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-2641", "mrqa_triviaqa-validation-7225", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-3866", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-9161", "mrqa_searchqa-validation-233"], "SR": 0.40625, "CSR": 0.5249255952380952, "EFR": 1.0, "Overall": 0.6781882440476191}, {"timecode": 42, "before_eval_results": {"predictions": ["lack of reliable statistics from this period", "\"For the life of me I can't believe the TWU walked away from that offer,\"", "Eintracht Frankfurt", "Na'ameh", "\"revolution of values\"", "Jeddah, Saudi Arabia", "40", "chest", "\"Big change is hard,\"", "Manny Pacquiao", "$250,000", "Wednesday,", "Michelle Obama", "Alberto Fujimori", "a facility in Salt Lake City, Utah", "Marty Abrams", "a drug cartel", "64", "her home", "fastest", "Department of Homeland Security Secretary Janet Napolitano", "Iran's parliament speaker", "ended his playing career", "\"E! News\"", "off Haiti's coast", "Madeleine K. Albright", "an ice jam", "toxic smoke from burn pits in Iraq and contaminated water.", "Benazir Bhutto", "July as part of the State Department's Foreign Relations of the United States series.", "U.S. senators", "south african", "Larry Ellison", "Alan Graham", "Ashley \"A.J.\" Jewell", "Cal Ripken Jr.", "Johannesburg", "cancer", "acid attack", "Vernon Forrest", "urged NATO to take a more active role in countering the spread of the", "one", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "recycles 100% of its byproducts which supplies 80% of the operation energy at the plant.", "about 5:20 p.m. at Terminal C", "Herman Thomas", "\"A salute to the martyrs of the massacre, and our condolences to their families.\"", "a man's lifeless, naked body", "\"release\" civilians", "Trevor Rees", "the catamaran", "when a population temporarily exceeds the long term carrying capacity of its environment", "Real Madrid", "emperor Cuauhtemoc", "city of wilson", "Misery", "Ken purdy", "Antonio Lippi", "Thorgan ganael Francis Hazard", "River Clyde", "chile", "jennifer jim", "Cy Young", "Reese Witherspoon"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5504035765790797}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.14285714285714285, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.47058823529411764, 0.0, 0.8, 0.05714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3979", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-4313", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-5649"], "SR": 0.46875, "CSR": 0.5236191860465116, "EFR": 1.0, "Overall": 0.6779269622093024}, {"timecode": 43, "before_eval_results": {"predictions": ["25,033", "House of Borromeo", "Washington, D.C.,", "1943", "a facelifted 850 saloon", "the Mountain West Conference", "Dominique Wilkins", "Western Europe", "political thriller", "Continental AG", "English football", "1989 until 1994", "the Distinguished Service Cross", "\"50 best cities to live in.\"", "Bridgetown", "Pakistani cinema", "Emmanuel ofosu Yeboah", "Ant-Man", "Bhushan Patel", "1986", "1916", "Reginald Engelbach", "Vince Staples", "Archbishop of Canterbury", "Galway", "My Backyard", "15 October 1988", "coaxial", "\"Northern Lights\"", "three different covers", "Malayalam fantasy comedy", "Regno di Dalmazia", "August 11, 1946", "Vincent Landay", "May 26, 2010", "\"Estadio de L\u00f3pez Cort\u00e1zar\"", "Brian A. Miller", "Christian Duguay", "1985", "Gal Gadot", "Meghan Markle", "Boeing B-17 Flying Fortress", "Erika Girardi", "Joe Scarborough", "Crimean", "76,416", "Bonkyll Castle", "second cousin once removed", "2012 Summer Olympics", "Sony Computer Entertainment", "Brig Gen Augustine Warner Robins,", "United Nations", "Lewis Carroll", "two", "under the UK\u2019s Trade Mark Registration Act 1875,", "blue", "elbow", "Citizens are picking members of the lower house of parliament,", "the Employee Free Choice act in Lafayette Square in Washington on Monday.", "the release of the four men", "a rake", "Jack the Ripper", "a carriage", "a Royal Tree"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6634294374104157}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.888888888888889, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.08695652173913042, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6153846153846153, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7278", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-1464", "mrqa_hotpotqa-validation-1776", "mrqa_naturalquestions-validation-4007", "mrqa_triviaqa-validation-2264", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2345", "mrqa_searchqa-validation-10888"], "SR": 0.5625, "CSR": 0.5245028409090908, "EFR": 1.0, "Overall": 0.6781036931818182}, {"timecode": 44, "before_eval_results": {"predictions": ["British Prime Minister Edward Heath", "Sean Yseult", "Washington, D.C.", "over 12 million", "Dulce Maria Garc\u00eda Rivas", "Conservatorio Verdi", "George Herbert Walker Bush", "\"the backside.\"", "Angelo Bruno", "The Future", "Newton Knight", "Andrew Joseph", "Denmark", "January 11, 2016", "Margarine Unie", "death", "Fort Valley, Georgia", "Tom Hanks", "Vladimir Valentinovich Menshov", "Kramer", "the Dominican Republic", "Humberside Airport", "June 12, 2017", "Douglas Jackson", "wooden roller ride", "Blackpool Football Club", "tenth", "Seth MacFarlane", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Chrysler", "Bruce Grobbelaar", "Honda Ballade", "ascona", "Boston Celtics", "Austrian", "Australian Electoral Division", "Socrates", "Caesars Entertainment Corporation", "Hindi", "Richard Masur", "Brian Patrick Friel", "311", "\"Dr. Gr\u00e4sler, Badearzt\"", "Alexandre Dimitri Song Billong", "Arizona Health Care Cost Containment System", "Mineola", "Gian Carlo Menotti", "bobsledder", "Mazda", "102,984", "Roscoe Lee Browne", "1972", "Mary Gibbs", "over 38 million", "Spectator", "Easter Parade", "Elgar", "last summer.", "almost 100", "into the Southeast,", "a piece of the pie", "Great Balls of Fire", "rapid cavalry", "One Direction"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6453125}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-2525", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3087", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-12050"], "SR": 0.546875, "CSR": 0.525, "EFR": 1.0, "Overall": 0.678203125}, {"timecode": 45, "before_eval_results": {"predictions": ["John Wesley was compelled in 1784 to break with standard practice and ordain two of his lay preachers as presbyters, Thomas Vasey and Richard Whatcoat", "crossword", "a crown", "Belgae", "Northern Exposure", "cocoa butter", "Kokomo", "Esther", "Warren Harding", "Monty Hall", "minigolf", "Wolf Blitzer", "Punxsutawney, Pennsylvania", "bratislava", "yellow fever", "a sea otter", "MMs", "Starbucks", "rod", "Nixon's 'dirty tricks' man,", "dressage", "astronomy", "Mickey Mouse", "stigma", "professor of Practice", "fruit", "Medusa", "a staircase", "tabby", "stave", "Voyager 1", "Farsi (Persian)", "glucose", "objects", "China", "Helen of Sparta", "meat", "the peace sign", "Morrie Schwartz", "John, Harry Three", "India", "Ben Kingsley", "Observatory", "NFL", "a samt ar-ras road", "white bread and butter", "a nomination", "Wordsworth", "brushes", "A dwarf planet", "Arabian Nights", "Vincent Price", "Rugrats in Paris", "Middle Eastern alchemy", "London", "Isle of Wight", "The Peppercorn Pioneer", "\"Queen In-hyun's Man\"", "Oneida Limited", "James Ager Wworthy", "Libreville, Gabon.", "tickets", "the station", "tuscaloosa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5769345238095238}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false], "QA-F1": [0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9923", "mrqa_searchqa-validation-2593", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-13022", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-13469", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-8467", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-975", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-11964", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3322", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6557", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-824", "mrqa_triviaqa-validation-888"], "SR": 0.5, "CSR": 0.5244565217391304, "EFR": 1.0, "Overall": 0.6780944293478262}, {"timecode": 46, "before_eval_results": {"predictions": ["\"degrees of privilege\"", "Jorge Lorenzo", "Malachy McCourt", "Indiana Jones", "fungi", "Venus flytrap", "Abraham", "george orwell", "faggot", "a plump", "California Chrome", "Pluto", "route 66", "Zagros", "Sindh", "Astro-Santa Morph", "Great Victoria Desert", "German", "Bill's Batman", "December 18, 1958", "Benjamin Franklin", "Portugal", "Operation Neptune", "Birmingham", "snakes", "Sedgefield", "Coral Sea", "Saddam Hussein", "Nadia Comaneci", "a trenches exhibition", "South Korea", "a pig", "sequel, maybe", "chile", "Kenya", "Stephen Potter", "Casa di Giulietta", "Anwar Sadat", "1", "potomac", "Argentina", "Luke Kenobi", "Frankfurt", "chipmunk", "Goldie Hawn", "a pulsar", "Belgium", "horse stories", "honey", "Benfica", "Sun Lust Pictures", "Games played", "her arranged marriage to Chino", "somatic cell nuclear transfer", "in the 13th century", "1 January 1788", "Radcliffe College", "11", "\"Twilight\"", "the Carrousel du Louvre", "Speed Racer", "H.G. Wells", "Queen Elizabeth", "Sir Walter Scott"], "metric_results": {"EM": 0.5625, "QA-F1": 0.628125}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-736", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-445", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3654", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-4088", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-7773", "mrqa_naturalquestions-validation-5241", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3234", "mrqa_searchqa-validation-4652"], "SR": 0.5625, "CSR": 0.5252659574468085, "EFR": 1.0, "Overall": 0.6782563164893618}, {"timecode": 47, "before_eval_results": {"predictions": ["Arabah", "toadstool", "Sinclair Lewis", "pig", "The World is Not Enough", "pigments", "Jonathan Demme", "vyrozum\u011bn\u00ed", "Dick Van Dyke", "millais", "Tina Turner", "1789", "derry", "glasses", "perfume", "duke orsino", "magnetite", "city of carousels", "show", "toadstool", "cubism", "sahara desert", "The Commonwealth Scientific and Industrial Research Organisation", "eucharist", "The Real Miracle of Charlotte's Web", "Octopussy", "silks", "William Randolph Hearst", "Lorne Greene", "rowing", "Michael Redgrave", "call My Bluff", "a", "andes", "frank mccourt", "milk or water", "Caroline Aherne", "germany", "oxygen", "Pears soap", "Donna Summer", "a pole", "Nottingham", "gdansk", "the Welcome Stranger", "taggart", "decem", "Chechnya", "a police janitor", "the A- Team", "football", "801,200", "Sir Ronald Ross", "Sun Tzu", "bioelectromagnetics", "Foxborough, Massachusetts", "Speedway World Championship", "beautiful", "11 people", "British Prime Minister Gordon Brown's wife, Sarah,", "kbenhavn", "the Proletariat", "sahara", "fluoroquinolone"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4715909090909091}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-1666", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-6892", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-4665", "mrqa_triviaqa-validation-1730", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5726", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3780", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-7020", "mrqa_searchqa-validation-15651", "mrqa_newsqa-validation-1804"], "SR": 0.453125, "CSR": 0.5237630208333333, "EFR": 1.0, "Overall": 0.6779557291666667}, {"timecode": 48, "before_eval_results": {"predictions": ["East Lothian", "Caesars Entertainment Corporation", "Supergirl", "thored, Earl of southern Northumbria", "creatures have been interviewed about their living conditions", "British Airways", "William McKinley", "1905", "Vanilla Air", "Mineola, New York", "dovzhenko", "Strange Interlude", "Julia Compton Moore", "Olivia Newton-John", "1988", "early Romantic period", "Gettysburg Address", "Harold Edward Holt", "Washington Street", "Mathew Sacks", "Babylon", "Ford Falcon", "Southern State Parkway", "The Company", "1827", "Kim Bauer", "approximately $700 million)", "Lieutenant Martin", "Suffolk", "Prussian", "O", "1909 Cuban-American Major League Clubs Series", "86 ft long", "American", "January 2004", "Sulfur mustard", "The 45th Infantry Division", "2009", "5 Grammy Award nominations", "Angie Watts", "City of Westminster, London", "Boyd Gaming", "August 14, 1848", "Texas Tech University", "John McClane", "Larry Gatlin & the Gatlin Brothers", "924", "576.6 days", "North Carolina", "Harrisburg", "Augusta Ada King-Noel, Countess of Lovelace (\"n\u00e9e\" Byron; 10 December 1815 \u2013 27 November 1852)", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "cake", "Salman Khan", "Challenger", "basil", "Clio Awards", "\"The Rosie Show,\"", "North Korea", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "Julius Caesar", "desert", "the Library of Congress", "the stroma"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6300800120772947}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false], "QA-F1": [0.22222222222222224, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.16666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8695652173913043, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-4008", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5348", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1115", "mrqa_hotpotqa-validation-2614", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5714", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-6806", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-1028", "mrqa_naturalquestions-validation-4685"], "SR": 0.5625, "CSR": 0.5245535714285714, "EFR": 0.9642857142857143, "Overall": 0.6709709821428571}, {"timecode": 49, "UKR": 0.662109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-2508", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4095", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4589", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5192", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-564", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5755", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-1887", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2722", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3569", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-938", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-13836", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-15433", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-15641", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-16122", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-165", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1954", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4937", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6398", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-8410", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-975", "mrqa_squad-validation-10069", "mrqa_squad-validation-10086", "mrqa_squad-validation-1019", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-10397", "mrqa_squad-validation-10444", "mrqa_squad-validation-10449", "mrqa_squad-validation-1052", "mrqa_squad-validation-1129", "mrqa_squad-validation-1211", "mrqa_squad-validation-1265", "mrqa_squad-validation-1311", "mrqa_squad-validation-139", "mrqa_squad-validation-164", "mrqa_squad-validation-1672", "mrqa_squad-validation-1712", "mrqa_squad-validation-1916", "mrqa_squad-validation-2132", "mrqa_squad-validation-2155", "mrqa_squad-validation-2176", "mrqa_squad-validation-2326", "mrqa_squad-validation-2436", "mrqa_squad-validation-2467", "mrqa_squad-validation-264", "mrqa_squad-validation-2798", "mrqa_squad-validation-2824", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-2906", "mrqa_squad-validation-2914", "mrqa_squad-validation-294", "mrqa_squad-validation-2999", "mrqa_squad-validation-305", "mrqa_squad-validation-3337", "mrqa_squad-validation-3650", "mrqa_squad-validation-3742", "mrqa_squad-validation-3948", "mrqa_squad-validation-4025", "mrqa_squad-validation-4066", "mrqa_squad-validation-4135", "mrqa_squad-validation-4258", "mrqa_squad-validation-4338", "mrqa_squad-validation-4349", "mrqa_squad-validation-44", "mrqa_squad-validation-4472", "mrqa_squad-validation-4480", "mrqa_squad-validation-4605", "mrqa_squad-validation-4607", "mrqa_squad-validation-4686", "mrqa_squad-validation-4835", "mrqa_squad-validation-487", "mrqa_squad-validation-4897", "mrqa_squad-validation-4947", "mrqa_squad-validation-5088", "mrqa_squad-validation-5136", "mrqa_squad-validation-5238", "mrqa_squad-validation-5330", "mrqa_squad-validation-5672", "mrqa_squad-validation-594", "mrqa_squad-validation-60", "mrqa_squad-validation-6362", "mrqa_squad-validation-6562", "mrqa_squad-validation-6737", "mrqa_squad-validation-6737", "mrqa_squad-validation-6811", "mrqa_squad-validation-6918", "mrqa_squad-validation-696", "mrqa_squad-validation-703", "mrqa_squad-validation-7173", "mrqa_squad-validation-7435", "mrqa_squad-validation-754", "mrqa_squad-validation-7576", "mrqa_squad-validation-7598", "mrqa_squad-validation-7814", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-8285", "mrqa_squad-validation-8402", "mrqa_squad-validation-8406", "mrqa_squad-validation-8483", "mrqa_squad-validation-8607", "mrqa_squad-validation-8636", "mrqa_squad-validation-8715", "mrqa_squad-validation-8747", "mrqa_squad-validation-8760", "mrqa_squad-validation-879", "mrqa_squad-validation-8846", "mrqa_squad-validation-9015", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9691", "mrqa_squad-validation-9757", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1297", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2068", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2572", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2970", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3886", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-4904", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-5118", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-5505", "mrqa_triviaqa-validation-5607", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6785", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-92"], "OKR": 0.8515625, "KG": 0.46796875, "before_eval_results": {"predictions": ["baseball, oin\u0103, ( Italy) and pes\u00e4pallo", "Jena Malone", "Washington, D.C.", "the utopian Ascona community", "John William Henry", "James Woods", "James Mitchum", "1963", "1995", "Steve Carell", "Wendell Berry", "Love Hina", "eastern", "novelty songs, comedy, and strange or unusual recordings", "OutKast", "five aerial victories", "Alain Robbe-Grillet", "the Seasiders", "musical research", "Dragon TV", "the New York-New Jersey Highlands region of the Appalachian Mountains", "Bay Ridge, Brooklyn", "Jean- Marc Vall\u00e9e", "over 1.6 million", "1928", "November 20, 1942", "September 26, 2010", "North Greenwich Arena", "1614", "central character", "Royce da 5'9\" (Bad) and Eminem", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Saint Michael, Barbados", "Sleepy Hollow", "more than 26,000", "the EN World web site", "Charles Russell", "Kj\u00f8benhavns Boldklub", "the Conservative Party", "Best Performance by an Actress in a Mini Series award", "southwest Denver, Colorado", "Port Clinton", "Art of Dying", "Dallas", "Harvard", "fennec fox", "Dutch", "Terry Malloy", "Golden Calf", "Kal Ho Naa Ho", "Thorgan Ganael Francis Hazard", "the closing scene of the final episode of the first season", "Everywhere", "the birth centenary of Pandit", "Honda", "J. M. W. Turner", "the Republic of Upper Volta", "56", "Nkepile M abuse", "Eintracht Frankfurt", "the fort", "Hephaestus", "Amherst College", "six nice golf courses."], "metric_results": {"EM": 0.53125, "QA-F1": 0.659935116966367}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [0.25, 1.0, 0.0, 0.8571428571428571, 0.8571428571428571, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-1344", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-1476", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-3430", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-616", "mrqa_searchqa-validation-9636", "mrqa_searchqa-validation-14102", "mrqa_newsqa-validation-492"], "SR": 0.53125, "CSR": 0.5246875, "EFR": 1.0, "Overall": 0.7012656249999999}]}