{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=20_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=10.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=20_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4020, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "Pacific", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "the infected corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "two integers", "a Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "British Gas plc", "More than 1 million", "2011", "by the market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Taoism", "Matthew 16:18", "U.S.-flagged Maersk Alabama", "Rwanda", "revelry", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8228365384615385}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-9655", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.765625, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "the coast of Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel and the establishment of an Islamic state in Palestine", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "Arizona Cardinals", "1991", "Chaffee", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "to become more integral within the health care system", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ronnie Wood and Brandon Block", "NLP Stand For - Documents", "the company's factory in Waterford City, Ireland", "nitrogen", "Christopher Nolan", "Agulhas Current Flow Rates", "six", "It always begins with the music", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.796875, "QA-F1": 0.8357567065287653}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9600", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-235", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.796875, "CSR": 0.796875, "EFR": 0.9230769230769231, "Overall": 0.8599759615384616}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "monumental size", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "beautiful voice", "Upper Lake", "northern China", "giving her brother Polynices a proper burial", "political figures", "President", "2000 guests", "oxygen", "increase local producer prices by 20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "Edict of Nantes", "reserved to, and dealt with at, Westminster", "multiple revisions", "philanthropic initiative", "integer factorization problem", "economic inequality", "Isel", "adapted quickly and often married outside their immediate French communities", "U.S. Ambassador to the European Union", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "energy stored in an H+ or hydrogen ion gradient", "late 19th century", "Channel Islands", "honor the separate spheres of knowledge that each applies to", "Alberich", "9", "ireil Lagasse", "Churchill Downs", "Ghent-Terneuzen Canal", "tetrahedron", "ireland", "Brazil", "study insects and their relationship to humans, other organisms, and the environment", "limbic system", "Allan Border", "George Fox", "Virginia", "Great Expectations", "24 hours a day and 7 days a week", "Sponsorship scandal", "\"Krabby Road\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6464409722222222}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-4293", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_squad-validation-2463", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_hotpotqa-validation-3821"], "SR": 0.609375, "CSR": 0.75, "EFR": 0.88, "Overall": 0.815}, {"timecode": 4, "before_eval_results": {"predictions": ["in higher plants", "Parliament of Victoria", "Zaha Hadid", "the Marquis de Vaudreuil", "Science and Discovery", "the Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "Battle of Olustee", "Sicily and the south of Europe", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "British troops", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "Italian government", "22", "terror groups that they say were planning numerous suicide attacks", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "a way of getting into that Lexus, Lincoln, Infiniti ororsche you always wanted", "Muslim", "will be the first time any version of the Magna Carta has ever gone up for auction", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "CNN's Campbell Brown", "a woman who may have been contacted through a Craigslist ad", "one", "celebrity-inspired names", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "gABRIEL ROSSETTI"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7121231280973928}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.15999999999999998, 1.0, 0.1, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-7094", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.6875, "CSR": 0.7375, "EFR": 0.95, "Overall": 0.84375}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a high-level marketing manager,", "roughly 500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "reminding their countrymen of injustice", "June 1978", "Milton Latham", "1914", "the Philippines", "the Broncos", "the 1970s", "the characteristics of the conquering peoples", "German Te Deum", "1795", "Bermuda 419 turf", "evaporated to cool oxygen gas", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "rudimentary", "1957", "mother-of-pearl", "Gene Barry", "negotiates treaties with foreign nations", "It is mainly for the purpose of changing display or audio settings quickly", "The term monkey's uncle, most notably seen in the idiom", "Woodrow Wilson", "radius R of the turntable", "Panning", "Justin Timberlake", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom", "military experts. European nations contribute nearly 6,000 units to this total. Pakistan, India, and Bangladesh are among the largest individual contributors", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "Lowe's opened its first three stores in Canada", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman", "David Gahan", "it includes a restaurant, spa, and bed - and - breakfast", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the day before Ash Wednesday", "Jaipur", "Johan Persson and Martin Schibbye", "torpedo boats", "Newport"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7204456955690508}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38888888888888895, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5833333333333334, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-436", "mrqa_squad-validation-3473", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.640625, "CSR": 0.7213541666666667, "EFR": 0.9565217391304348, "Overall": 0.8389379528985508}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "a Serbian Orthodox priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "social and political action", "1936", "the New Birth", "gold", "a deficit", "Vivienne Westwood", "reduction", "disease", "\"TFIF\"", "Confucian propriety and ancestor veneration", "the rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples.", "end of the season", "10", "Jacob", "African-Americans", "will not support the Stop Online Piracy Act", "Don Draper", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Stuttgart on Sunday.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "North Korea", "first five Potter films", "you love the environment and hate using fuel", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Her husband and attorney, James Whitehouse,", "we want to ensure we have all the capacity that may be needed over the course of the coming days.", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "James Lillywhite, Alfred Shaw and Arthur Shrewsbury", "Colgate University", "Church of Christ, Scientist", "unsaturated fats are comprised of lipids that contain?", "gospel does not present any elaboration of who these twelve actually were"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7193837412587413}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2, 0.5, 1.0, 0.5, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.2666666666666667, 1.0, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6001", "mrqa_squad-validation-486", "mrqa_squad-validation-3390", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.65625, "CSR": 0.7120535714285714, "EFR": 1.0, "Overall": 0.8560267857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "return to his side", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism", "Jim Gray", "unequal", "July 1969", "that Hitler's secret police demanded to know if they were hiding a Jew in their house.", "a yellow chlorophyll precursor", "spontaneous", "the courts of member states", "gold", "TARDIS", "Buckland Valley near Bright", "Scottish rivers", "ricks for Warsaw", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "their \"Freshman Year\" experience", "India", "Benazir Bhutto", "at the Lindsey oil refinery in eastern England", "April 24 through May 2", "Krishna Rajaram", "early detection", "250,000", "Timothy Masters", "homicide", "permitted under Spanish Football Federation (RFEF) rules", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jared Polis", "William S. Cohen", "\"Dance Your Ass Off.\"", "leniency", "Matthew Fisher", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Japan", "condition", "\"Empire of the Sun,\"", "Norman given name Robert", "performance enhancing drugs", "Matt Winer", "Wyatt Earp", "opposite R\u00fcgen island", "royalty", "green"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6119090544871795}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8749999999999999, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6115", "mrqa_squad-validation-7533", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-3938", "mrqa_squad-validation-872", "mrqa_squad-validation-1556", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_hotpotqa-validation-4367", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-4305"], "SR": 0.53125, "CSR": 0.689453125, "EFR": 0.9666666666666667, "Overall": 0.8280598958333334}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers and French regular troops were returned to France aboard British ships", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday", "Journey's End", "immediate", "Levi's Stadium", "Wesleyan", "art posters", "Elbegdorj", "Chinggis Khaan", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "Islamic", "tweener love", "Fernando Gonzalez", "Graeme Smith", "strong work ethic", "finance", "terminal brain cancer.", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice act", "separated", "Animal Planet", "fake his own death", "there were no radar outages and said it had not lost contact with any planes", "54 bodies", "early detection and helping other women cope with the disease", "Diversity", "$250,000", "fill sandbags", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Carl Froch", "Everglades", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans, Louisiana", "Tulip mania", "MIBs", "olympics", "olympics"], "metric_results": {"EM": 0.640625, "QA-F1": 0.712732233044733}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.12121212121212123, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.640625, "CSR": 0.6840277777777778, "EFR": 0.9565217391304348, "Overall": 0.8202747584541064}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "Book of Discipline", "Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "hoos", "30%\u201350% O2 by volume", "very badly disposed towards the French, and are entirely devoted to the English", "the top 15 most populous", "CRISPR sequences", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "issues related to the substance of the statement", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "helping the United States frame the challenges", "25", "a treadmill", "the couple's surrogate lost the pregnancy.", "environmental and political events", "he fears a desperate country with a potential power vacuum that could lash out.", "at least two and a half hours.", "Elin Nordegren", "Europe, Asia, Africa and the Middle East.", "6,000", "cortisone", "President Clinton.", "delivered three machine guns and two silencers", "Morgan Tsvangirai.", "policing the world and Africa", "future relations between the Middle East and Washington", "in a canyon", "Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school", "strife in Somalia", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "they did not know how many people were onboard.", "London", "after Shawn's kidnapping", "an individual", "William Tell", "OutKast", "Groundhog Day", "he really didn't mean t", "a singer"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6196180555555555}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-2429", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-11812"], "SR": 0.578125, "CSR": 0.6734375, "EFR": 0.9259259259259259, "Overall": 0.799681712962963}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley, New York", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever", "DC traction", "Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "within the premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "July 18, 1994", "pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "precipitation will briefly transition back to light snow or flurries", "Willem Dafoe", "Maude", "Phillip A. Myers.", "Korea", "two weeks after Black History Month", "58 people", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Touma", "Dangjin", "e-mails", "Chinese President Hu Jintao", "magazine, GospelToday,", "it pulls the scab and it cracks, and it starts to bleed.\"", "October 3,", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next.", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "bankruptcy", "16 August 1975", "Bonnie Aarons", "one", "kabinett", "Lionsgate.", "James Lofton", "mystic", "short, hair-like structures"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6884469696969697}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-7230", "mrqa_squad-validation-1299", "mrqa_squad-validation-8655", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.59375, "CSR": 0.6661931818181819, "EFR": 0.9615384615384616, "Overall": 0.8138658216783217}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company under the Office of Alien Property (OAP) seal.", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.", "Elway", "Philo of Byzantium", "36 acres", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "Euler's totient function", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "Agnes Wickfield", "pink mice", "antelope", "nipples", "the Precambrian period", "'helpful'", "Anastasia Dobromyslova", "Lady Gaga", "9", "Space Jam 2", "radishes", "Robert Ludlum", "a magical creatures", "a Space vehicle", "the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew", "London Underground Piccadilly Line", "Wisconsin", "orangutan", "Manet", "The Magic Finger", "Massachusetts", "2005", "1969", "DodgeDodge", "dolt", "Venice", "a peplos", "Enrico Caruso", "Elizabeth Arden", "lightweight baby buggy with a collapsible support assembly", "Sir Hardy Amies", "Liechtenstein", "the 14th most common surname in Wales and 21st most common in England", "Rob Davis", "Cody Miller", "Bloomingdale Firehouse", "acquire nuclear weapons", "Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Jamaica", "Buddhism"], "metric_results": {"EM": 0.5625, "QA-F1": 0.651383551950157}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.2608695652173913, 1.0, 0.8, 1.0, 0.7234042553191489, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.4444444444444444, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983"], "SR": 0.5625, "CSR": 0.6575520833333333, "EFR": 1.0, "Overall": 0.8287760416666666}, {"timecode": 12, "before_eval_results": {"predictions": ["Southern Border Region", "70-50's", "Panini", "Bills", "anti-colonial movements", "the Rhine Valley", "protein A", "to test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "if the EU does not comply with its basic constitutional rights and principles (particularly democracy, the rule of law and the social state principles)", "1788", "2006", "Roman Catholic", "Henry of Navarre", "John Wesley", "because the nationalisation law was from 1962, and the treaty was in force from 1958,", "Eternal Heaven", "Ness Point", "John Mayer", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Eric Pickles", "Sir Hugo Drax", "Vladivostok", "sheryl Crow", "Telstar", "Camellia sinensis", "AFC Wimbledon", "Charles Hawtrey", "Malaysia's capital Kuala Lumpur", "cosmology", "gin", "George Clooney", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "champagne", "an extended period of abundant rainfall lasting many thousands of years", "United States", "Brigit Forsyth", "Lord Melbourne", "state of Japan", "The History of Troilus and Cressida", "Thomas Edward Lawrence,", "Kent", "Renoir\u00b4s", "Vanguard", "Dannebrog", "Switzerland", "gin", "people of France to the people of the United States", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "bremen", "David in the Bargello", "\"Stagecoach\" (John Ford, 1939)"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6410309750733139}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8387096774193548, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-4590", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-609", "mrqa_naturalquestions-validation-594", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_searchqa-validation-9647", "mrqa_newsqa-validation-3860"], "SR": 0.5625, "CSR": 0.6502403846153846, "EFR": 0.9642857142857143, "Overall": 0.8072630494505495}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "had their own militia", "after the end of the Mexican War", "Over 61", "quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron", "HYMENAEUS", "Zeus", "albinism", "Straits of Tiran", "Brigit Forsyth", "call My Bluff", "March 10, 1997", "cuddly new pet", "The Battle of the Three Emperors", "Velazquez", "Arthur Ashe", "lizards", "strong cold southwest wind", "table tennis", "jAMA", "penhaligon", "reed", "Edgar Allen Poe", "Jinnah International Airport", "Monday", "Caracas", "a crucifix", "soap", "liquor", "Avro Lancaster", "gove wuthering", "covey Brooker", "camomile", "harrods", "2007", "Christina Ricci", "Scarface", "pale yellow", "reed melbourne farmer Ted Moult", "bubba", "June 12, 2018", "Filipino American History Month", "London's West End", "Lambic", "Nook", "Steven Green", "commas", "fortune", "renoir", "Synchronicity"], "metric_results": {"EM": 0.5, "QA-F1": 0.5826388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_squad-validation-2875", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-6547", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-4981", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_hotpotqa-validation-5340", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-6628"], "SR": 0.5, "CSR": 0.6395089285714286, "EFR": 1.0, "Overall": 0.8197544642857143}, {"timecode": 14, "before_eval_results": {"predictions": ["seven", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep after it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "a declining state of mind", "1898", "The Deadly Assassin and Mawdryn undead", "for scientific observation", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1936", "Authority", "senior enlisted sailor", "Spanish moss", "Chinese cooking", "Vienna", "World Trade Center", "Kevin Spacey", "1 November", "78", "in lymph", "Bangladesh -- India border", "President", "minor key", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shetty", "metamorphic rock", "January 12, 2017", "the United States", "claims adjusters", "The neck", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column", "three", "spring", "long", "Kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "a revolver", "BBC building in Glasgow, Scotland", "they would get tested to see if their kidney could be donated."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6652165203455964}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false], "QA-F1": [0.5, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 1.0, 0.5, 0.2857142857142857, 0.8, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4615384615384615, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8333333333333333, 0.14285714285714285]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.515625, "CSR": 0.63125, "EFR": 0.967741935483871, "Overall": 0.7994959677419355}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "a special episode of The Late Show with Stephen Colbert", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "ESPN Deportes", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "note number 60", "Seattle, Washington", "Battle of Antietam", "Dimitar Berbatov and Carlos Tevez", "In Time", "the 2nd century", "Glenn Close", "four times", "Agostino Bassi", "five", "Malibu, California", "the church at Philippi", "the Netherlands", "September 2017", "Professor Kantorek", "1546", "Jane Fonda", "Bhupendranath Dutt", "a warrior", "Dr. Lexie Grey", "Majandra Delfino", "September 1972", "Uruguay", "Alex Skuby", "Matt Jones", "The National Legal Aid & Defender Association", "Monk's Caf\u00e9", "domesticated sheep", "1970s", "Director of National Intelligence", "D.A.D. a.", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr.", "Thespis", "Portugal", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "a garage beetle", "Code 02PrettyPretty", "musician", "opposition group, also known as the \"red shirts,\"", "the abduction of minors", "$6.2 trillion", "Gabriel Garcia", "Stage Stores", "1881"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6324576465201466}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.42857142857142855, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_searchqa-validation-13473", "mrqa_searchqa-validation-5103"], "SR": 0.53125, "CSR": 0.625, "EFR": 0.9333333333333333, "Overall": 0.7791666666666667}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "Arizona Cardinals", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "Western Xia", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland", "Microsoft Office", "SAVE", "Scandinavian Airlines System", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award Best Actor", "Jello Biafra drew on Nardwuar's face with a marker pen", "the 1745 rebellion of Charles Edward Stuart", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements", "Sir William McMahon", "North Sea", "7.63\u00d725mm Mauser", "Academy Award for Best Animated Feature", "the CAC/PAC JF-17 Thunder", "Delacorte Press", "Neighbourhoods", "Secretariat", "Wake Island", "Hydrogen vehicle", "Fort Valley, Georgia", "King of France", "\"Southern Living\" Reader's Choice Awards", "William Shakespeare", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Alticor", "Parlophone Records", "South Africa", "Surrey", "The Girl", "Charles Russell", "Boyd Gaming", "Anthony Davis of the New Orleans Pelicans", "1991, 1992, 1993, 1994, 1998, 2009, 2010", "Glenn Close", "Mary Welch", "Neighbours", "Ewan McGregor", "2011", "a priest's cowl", "a son of an African-born slave mother in Southampton, Virginia", "power-sharing talks", "Shelley Moore Capito"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6839353354978355}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3521"], "SR": 0.59375, "CSR": 0.6231617647058824, "EFR": 0.9615384615384616, "Overall": 0.792350113122172}, {"timecode": 17, "before_eval_results": {"predictions": ["forces", "theology and philosophy", "ITV", "the University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter", "1830", "nonfunctional pseudogenes", "the inner chloroplast membrane", "Charlie Harper", "steveland Hardaway Morris", "beaver", "La Boh\u00e8me Giacomo Puccini", "formic acid", "Puente del Arzobispo", "Zimbabwe", "Mr. Boddy", "Ted Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "hound", "Xenophon", "Fuller's", "a reference mark", "Nick Hornby", "The Comedy of Errors", "Charles V", "England", "Lagertha", "weight plates", "\"big house\"", "Hadrian", "US", "human flea", "Moonee Ponds, a suburb in Melbourne, Victoria", "Hamburg", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "Leicester City", "Sergei Prokofiev", "Jessica Simpson", "British public", "Corsican Republic", "3000m", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "Confederate", "New Jewel Movement", "sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "du'y van", "two", "jeopardy/1870_Qs.txt at master  jedoublen/jeopardy", "Nightlife in Mallorca"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6338541666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-3198", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.5625, "CSR": 0.6197916666666667, "EFR": 1.0, "Overall": 0.8098958333333334}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "extremely high", "Manakin Town", "northwest", "fewer than 10 employees", "Middle Miocene", "new magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "in a balance sheet as an asset", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "2010", "Coppolas and, technically, the Farrow / Previn / Allens", "Allison Janney", "the Isthmus of Corinth", "inability to comprehend and formulate language", "Splodgenessabounds", "ser Gregor Clegane", "electron donors", "Meredith Quill", "1993", "19 state rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Coton in the Elms", "55 -- 69 %", "Zoe Badwi", "1995", "Identification of alternative plans / policies", "16 August 1975", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "`` Killer Within ''", "Western Australia", "the aorta", "July 21, 1861", "Dr. Addison Montgomery", "capital and financial markets", "An empty line", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "universal significance", "September 2017", "moral", "`` Rising Sun Blues ''", "Part 2", "Disney's 1941", "the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "mentor", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.625, "QA-F1": 0.7108110182478788}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 0.0, 0.2857142857142857, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.28571428571428575, 1.0, 1.0, 0.08695652173913042, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-4227", "mrqa_hotpotqa-validation-4735"], "SR": 0.625, "CSR": 0.6200657894736843, "EFR": 0.9583333333333334, "Overall": 0.7891995614035088}, {"timecode": 19, "before_eval_results": {"predictions": ["the law as the Holy Spirit's tool to work sorrow over sin in man's heart, thus preparing him for Christ's fulfillment of the law offered in the gospel", "black", "Louisiana, Biloxi, Mississippi, Mobile, Alabama", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "Long troop deployments", "Joe Pantoliano", "the girl's stepmother", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "Nearly eight in 10", "natural resources around the islands should be protected, and Britain must accept international resolutions labeling the Falklands a disputed area", "Tuesday", "forgery and flying without a valid license", "Anil Kapoor", "55", "President Obama", "unwanted baggage from the 80s", "The Louvre", "snowstorm", "exotic sports cars", "hatchlings", "Mutassim", "Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18,", "\"Steamboat Bill, Jr.\"", "Russia", "Al alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "\u00a341.1 million", "Kingman Regional Medical Center", "charlie Moore", "Manmohan Singh", "Michael Jackson", "to go the White Palace and show him my poems, show him what is happening and ask him to come to Pakistan and control it because he is a super power", "40 militants and six Pakistan soldiers dead", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Lousiana", "the Southeast", "Misty Croslin", "Nancy Sutley", "\" Michael Phelps, who won a record eight gold medals in Beijing, is the author of a new memoir, \"A Mother For All Seasons.\"", "Moe and Sana Maraachli", "back at work", "necropsy", "27", "Derek Hough", "John Adams, a leader in pushing for independence, had persuaded the committee to select Thomas Jefferson to compose the original draft of the document, which Congress edited to produce the final version", "borsht", "Zager & Evans", "Robert Matthew Hurley", "At-large", "\" Adult Theatre - You must be 21 and able to prove it\"", "(Oliver) Cromwell", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.421875, "QA-F1": 0.54001574933687}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.923076923076923, 0.6666666666666666, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.13793103448275862, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-7208", "mrqa_hotpotqa-validation-2013", "mrqa_searchqa-validation-328", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.421875, "CSR": 0.61015625, "EFR": 0.972972972972973, "Overall": 0.7915646114864865}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "diphthong", "1331", "Death wish Coffee", "L", "Cameroon", "1994", "policing the world and Africa", "fabric", "three empty vodka bottles,", "deputy director for strategy, plans and policy on the Army staff.", "Bobby Darin,", "Nico Rosberg", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "acknowledged the procedures should be changed.", "the composer of \"Phantom of the Opera\" and \"Cats\"", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony, 2,", "Amanda Knox's aunt", "Half Moon Bay", "Iran's nuclear program.", "through 12 shades of violet, including a welcoming, bright blue-purple during the day, a softer violet hue after dusk", "allegedly faking a doctor's note", "ceo Herbert Hainer", "Brett Cummins,", "a nearby day care center whose children are predominantly African-American.", "inmates", "Col. Elspeth Cameron-Ritchie,", "\"E! News\"", "three French journalists, a seven-member Spanish flight crew and one Belgian", "jobs up and down the auto supply chain", "saying Tuesday the reality he has seen is \"terrifying.\"", "The controversial technique that simulates drowning -- and which President Obama calls torture -- was used at least 83 times in August 2002 on suspected al Qaeda leader Abu Zubaydah,", "any group that tries to take justice into its own hands.", "Republicans", "start developing a youth ballpark in his hometown of Aberdeen, Maryland, financed in part by a $75,000 gift from the Major League Baseball Players Association.", "An undated photo of Alexandros Grigoropoulos,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "a 57-year old male", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Gary Brooker", "outlaws", "boogeyman Jason Voorhees", "The United States had been seeking the death penalty against al-Qahtani and five other men in connection with the 9/11 attacks.", "Sea World in San Antonio", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "about 50", "the Ku Klux Klan", "The Wizard of Oz ( 1939 )", "Branford College", "bury", "stamens", "Malayalam", "August 17, 2017", "a jacket", "mouse followed, in the '80s?", "Hodel", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "Coldplay"], "metric_results": {"EM": 0.359375, "QA-F1": 0.47486108190086884}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.125, 0.0, 0.0, 1.0, 0.9523809523809523, 0.0, 0.25, 0.23076923076923078, 0.8, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615383, 0.8571428571428571, 0.0851063829787234, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225808, 0.0, 0.19047619047619047, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5263157894736842, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.359375, "CSR": 0.5982142857142857, "EFR": 0.8292682926829268, "Overall": 0.7137412891986062}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "Splash", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "both shoulders", "Madonna's", "Glasgow", "satellites", "Australia", "bird's food,", "Pearson PLC.", "Irish Setter", "American Civil War,", "Loch Lomond", "rochner", "Brisbane", "medium", "largest city of the country", "Harrisburg", "mustela erminea,", "glockenspiel", "Dr John Sentamu", "rochoon", "Pongo", "Anne Boleyn", "EMI", "Freddie Mercury,", "Emma Chambers", "Charlemagne", "the community", "Russell Crowe", "Theodore Roosevelt", "rochian", "Robin Goodfellow", "Samuel Butler", "chamomile tea", "Ireland", "tarn", "Model Ship Master", "Albert Square", "Newbury", "a book of the Old Testament", "70 million people, at that time 21 % of the world's entire population", "Target Corporation", "Sister, Sister (1982 film)", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "international NGO", "John Jackson Dickison", "better conditions for inmates, like Amnesty International.", "talk show queen Oprah Winfrey.", "his mother."], "metric_results": {"EM": 0.53125, "QA-F1": 0.5805765415140416}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6908", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.53125, "CSR": 0.5951704545454546, "EFR": 1.0, "Overall": 0.7975852272727273}, {"timecode": 22, "before_eval_results": {"predictions": ["The flushing action of tears and urine", "1765", "primarily along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin", "jesuit", "Robert Peary", "pearls", "jesuit", "Carrie Underwood", "liqueur liquor", "he made his horse a consul", "jesuit", "Langston Hughes", "Self-torture", "frantiek Drtikol", "mambo", "rope", "unFINISHED", "USS LST 325", "prey drive", "David Beckham", "Arturo Toscanini", "economics", "Miracle in the Andes", "triumphal arch", "Montenegro", "discus", "jedoublen", "basidiomycota", "james", "Courtney thorne-Smith", "Idi Amin", "Riding mower", "a body, or a personal item associated with a saint", "terracotta", "plutarch", "Rudy Giuliani", "masa", "50", "the Vikings.", "mulberry Street", "jesuit", "New York City", "fjord", "baviere-quebec.org", "jesuit", "telegraph", "University of tualatin", "a chemical reaction to speed up but is not used up", "jen Knox", "the internal reproductive anatomy", "$657.4 million in North America and $1.528 billion in other countries", "epidemiology", "jockey", "Tesco", "Mallard", "Graham Hill", "the Battelle Energy Alliance.", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment", "debris", "$10 billion", "Bailey, Colorado,"], "metric_results": {"EM": 0.375, "QA-F1": 0.41427067621920566}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false], "QA-F1": [0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.5, 0.5, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6437", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-5091", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-16854", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-1997"], "SR": 0.375, "CSR": 0.5855978260869565, "EFR": 1.0, "Overall": 0.7927989130434783}, {"timecode": 23, "before_eval_results": {"predictions": ["New Year's Day 2007", "1493\u20131500", "the Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "the Gulf of Mexico", "domestic cat in America,", "the daughter of Tony Richardson and Vanessa Redgrave", "Europ\u00e9ennes", "the Argo", "prometheus", "The Altamont Speedway Free Festival", "John F Kennedy,", "Tim Gudgin", "Rosslyn Chapel", "conducting", "a MUD (multi-user dungeon),", "Libya", "Khaki", "a sedimentary", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "Barack Obama,", "the Earth", "Nafea Faa Ipoipo?", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "the colony of Suriname", "Justin Trudeau", "Air traffic control, Air-defense systems, and Antimissile systems", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "Influenza", "Fife", "Money Saving", "Adidas", "the Snark", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "all pieces capture opponent's pieces by moving to the square that the opponent's piece occupies.", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "The Wall Street Journal Europe", "the Arctic fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.625, "QA-F1": 0.6734834558823529}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7871", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-3049", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4882", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_searchqa-validation-6736"], "SR": 0.625, "CSR": 0.5872395833333333, "EFR": 0.9583333333333334, "Overall": 0.7727864583333333}, {"timecode": 24, "before_eval_results": {"predictions": ["limited coercion", "the chosen machine model", "Fox", "1997", "a suite of network protocols", "Noriko Savoie", "15", "nine-wicket", "between Pyongyang and Seoul", "he was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver on February 14, 2002.", "11", "fly to Australia", "Alwin Landry's supply vessel Damon Bankston", "Chaffetz", "money or other discreet aid", "Sarah Brown", "it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "environmental", "switzerland", "coalition troops", "Saturday", "38", "70,000 or so", "Climatecare", "E! News", "former Boca Juniors teammate and national coach", "Steve Williams", "McDonald's", "poetry", "Pastor Paula White", "2008", "Diego Maradona", "Dog patch Labs", "The drama of the action in-and-around the golf course", "two", "Itawamba County School District", "Romney", "EU naval force", "Plymouth Rock", "Liza Murphy, 42,", "The nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "police", "former U.S. secretary of state", "At least 33", "five", "improve health and beauty.", "Texas", "students often know ahead of time when and where violence will flare up on campus.", "Damon Bankston", "Krishna Rajaram,", "Sunday,", "killing", "an Irish feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "Northern Ireland", "radar", "art", "the National Basketball Association", "23", "Guarani", "freestyle", "Florence Nightingale", "Belief"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5900543587044212}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 0.0, 0.8571428571428571, 0.3076923076923077, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.9655172413793104, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.967741935483871, 0.0, 1.0, 0.25, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4673", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-3826"], "SR": 0.40625, "CSR": 0.5800000000000001, "EFR": 0.9736842105263158, "Overall": 0.776842105263158}, {"timecode": 25, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.869140625, "KG": 0.4828125, "before_eval_results": {"predictions": ["its 50th anniversary special", "Thomas Savery", "Vicodin", "Eastern crops such as carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago onward", "violent separatist campaign", "Eleven", "269,000", "theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "38 feet", "Eintracht Frankfurt", "150", "a man had been stoned to death by an angry mob.", "Russian bombers", "41", "Los Alamitos Joint Forces Training Base", "Wally", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "the Russian air force", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "\"several pieces of aircraft equipment were at fault or had broken down.\"", "ensuring that all prescription drugs on the market are FDA approved", "patients hoping to keep their skin looking young.", "Tom Baer", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "a two-piece bathing suit", "Brian Mabry", "iTunes", "Sunday.", "60 euros -- $89 --", "American Civil Liberties Union", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "Some truly mind-blowing structures are being planned for the Middle East.", "a passenger's name", "he was one of 10 gunmen who attacked several targets in Mumbai", "2006", "San Diego", "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "The Taliban has threatened to kill Bergdahl if foreign troops continue targeting civilians in the name of search operations in Ghazni and Paktika provinces,", "@", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "Haitian National Police (HNP)", "heart", "Hyderabad", "Sinai ( / \u02c8sa\u026ana\u026a / ; Arabic : \u0633\u064a\u0646\u0627\u0621\u200e S\u012bn\u0101\u02bc", "to stay, abide", "Las Vegas", "Jackson Pollock", "Lyrical", "Mississippi", "October 4, 1970", "King Duncan", "Georgia", "thimble", "a stride"], "metric_results": {"EM": 0.4375, "QA-F1": 0.534459560754791}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.8750000000000001, 0.8571428571428571, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.3157894736842105, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.07407407407407408, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.1111111111111111, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_squad-validation-9432", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-11832"], "SR": 0.4375, "CSR": 0.5745192307692308, "EFR": 0.9722222222222222, "Overall": 0.7348170405982907}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "lost their phycobilisomes", "Off-Off Campus", "reckless", "Six people were killed and at least 300 were injured", "Krishna Rajaram,", "25", "Booches Billiard Hall,", "finance", "Ross Perot.", "Hong Kong's Victoria Harbor", "2002", "six", "legitimacy of that race.", "think about saving the rainforests", "three", "Monday", "Scarlett Keeling", "two years,", "84-year-", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "in July", "Akshay Kumar", "Graham's wife", "the guerrillas", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "David Bowie", "the death of Prince George's County police Cpl. Richard Findley,", "Phil Spector", "Kim Il Sung", "1994", "numerous suicide attacks,", "Friday,", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "dependable Camry", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "in his 60s,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.", "the administration's progress,", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "6 - 6", "Matt Monro", "Jack Frost", "the innermost digit of the forelimb", "1974", "25 million", "Peoria, Illinois", "Honolulu", "'hardheads", "incense", "Ottoman Empire"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6839359642094016}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.06250000000000001, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_hotpotqa-validation-5856", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.640625, "CSR": 0.5769675925925926, "EFR": 1.0, "Overall": 0.7408622685185186}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "John Dillinger,", "what caused the collapse of the building which contained the city's historical archives,", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen", "Brazil's response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "Lashkar-e-Tayyiba (LeT)", "$1.5 million", "2006", "Rev. Alberto Cutie", "Angels", "eight Indian army troopers, including one officer, and 17 militants,", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "in a Starbucks", "BADBUL", "98", "2008", "near the Somali coast", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Swat Valley", "South Dakota State Penitentiary", "Iran", "November 26", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "in July", "Zoe's Ark", "Four Americans", "Josef Fritzl,", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "at Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41)", "broken pelvis,", "Wednesday", "abduction of minors", "gun", "Aniston, Demi Moore and Alicia Keys", "Dick Cheney", "19 June 2018", "Flag Day in 1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester", "Drowning Pool", "Missouri", "Burlington", "beta blockers"], "metric_results": {"EM": 0.6875, "QA-F1": 0.810294991167836}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.3636363636363636, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2402", "mrqa_searchqa-validation-14535"], "SR": 0.6875, "CSR": 0.5809151785714286, "EFR": 1.0, "Overall": 0.7416517857142858}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author", "rule", "1981", "forgery and flying without a valid license,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force.", "shoot down the satellite", "European Commission", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "seven", "the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations -- embraces this technology the same way the public has", "Anil Kapoor.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "The Rosie Show", "Form Design Center.", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "Christianity and Judaism", "the Dalai Lama's", "Russia", "8 p.m. local time Thursday", "Passers-by", "one day,", "executive director of the Americas Division of Human Rights Watch,", "750", "300", "Matthew Fisher", "The Ski Train", "Big Brother.", "Ozzy Osbourne", "AbdulMutallab,", "some U.S. senators", "inconclusive", "5:20 p.m. at Terminal C", "environmental and political events", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600", "a million", "Sen. Arlen Specter", "Deutschneudorf,", "a bill that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "a deceased organ donor,", "bragging about his sex life on television", "a vertebral column ( spine )", "January to May 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the South Bank", "Douglas Hofstadter", "Stephen King", "American", "Marmee", "Castle Rock", "a tomato, mozzarella, anchovy"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6743322633697348}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.6, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.631578947368421, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9743589743589743, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_hotpotqa-validation-5376", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.5625, "CSR": 0.5802801724137931, "EFR": 1.0, "Overall": 0.7415247844827586}, {"timecode": 29, "before_eval_results": {"predictions": ["downward pressure on wages", "poison", "438,000", "Marty Ingels", "coaxial", "Pakistan A", "Everbank Field", "7 members appointed by the chief executive", "the German Campaign of 1813", "Arabella Churchill,", "1965", "Paris", "fifth", "Culiac\u00e1n, Sinaloa", "seven", "Province of Syracuse", "1963", "non-alcoholic", "puzzle", "Knoxville, Tennessee", "Washington, D.C.", "Gal\u00e1pagos giant rat", "Tom Kartsotis", "2017", "Wayman Tisdale", "Mexico", "Kolkata", "Northern Ireland", "late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell", "2002", "Gregg Harper", "Shohola Falls", "small forward", "ARY Films", "Erinsborough", "United States Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey", "Floridians", "Kentucky", "NBA Slam Dunk Contest", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "a field in Somerset County, Pennsylvania,", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "Richard Parker", "off the southernmost tip of the South American mainland", "Rameses II (Yul Brynner)", "allergic reaction", "Peter Townsend,", "3,000 kilometers (1,900 miles)", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "Russia", "peel and devein shrimp", "Australia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6450622248730802}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true], "QA-F1": [0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.25, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.6666666666666666, 1.0, 0.15384615384615383, 0.3157894736842105, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-1982", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-4033", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-6793"], "SR": 0.546875, "CSR": 0.5791666666666666, "EFR": 1.0, "Overall": 0.7413020833333334}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a farmers' co-op", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations.", "ten years of probation", "John Joseph \" Jay\" Joyce", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Dirk Werner Nowitzki", "Cecil B. DeMille Award", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "the United States and Canada", "British comedian", "Apatosaurus", "1885", "American", "Frank Thomas' Big Hurt", "\"Gliding Dance of the Maidens\"", "Margarine Unie", "Winecoff Hotel fire", "mentalfloss.com", "The Seduction of Hillary Rodham", "1919", "Lambic", "Massive Entertainment", "Argentina", "Larry Alphonso Johnson Jr.", "Mike Mills", "veto power", "Joseph E. Grosberg", "Chelsea Lately", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "the Californian coast at The Inn at Newport Ranch", "New York", "discus thrower", "Aston Villa Football Club", "2005", "228", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6814157196969697}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.5833333333333334, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-670", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.5625, "CSR": 0.5786290322580645, "EFR": 0.9642857142857143, "Overall": 0.7340516993087558}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "Silent Snow, Secret Snow", "Log Ride", "vote", "Take Me", "Nassau", "a gemstone formed by the nacreous inner shell", "Dr. Anthony Gallo", "Martin Van Buren", "a network of rail lines", "Rigoletto", "aardwolf", "Beijing", "Sir Roger Gilbert Bannister", "Rich Clune", "Death Valley", "Yves Saint Laurent", "reindeer", "Fortinbras", "the War of 1812", "Anna Mary Robertson", "Sailor Moon", "char-Broil", "Harry Bosch", "spectacled bear", "a whirlwind", "Gilson Lavis", "Monty Python and the Holy Grail", "a... gun that uses this type of negative electrode", "(Milton) Berle", "George Herbert Walker Bush", "Patrice Lumumba", "lunar module", "Almagro", "Dan Marino", "Mars", "clownfish", "E = mc2", "Guru Pitka", "Las Vegas", "millet", "a butterfly", "Camelot", "orangutan", "Baja California", "Soothsayer", "Yitzhak Rabin", "Saul", "Gettysburg", "Jack Gleeson", "a board of wood", "Buddhism", "Carl John", "Portugal", "John Mayall, Cyril Davies, Long John Baldry and Alexis Korner", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "\"has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\"", "knocking the World Cup off the front pages for the first time in days.", "12.3 million people worldwide"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5578497023809523}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-1768", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_naturalquestions-validation-5896", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.515625, "CSR": 0.57666015625, "EFR": 0.967741935483871, "Overall": 0.7343491683467742}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "62", "Herbert Henry Asquith", "40", "Libya", "Shania Twain", "Sheffield Wednesday", "insulin and glucagon", "The New York Yankees", "rapid eye movement", "green", "Stanley", "al-Bakr", "French", "Jim Branning", "Ohio", "Francis Matthews", "photographic", "magnetite", "Noah", "London", "New Years Day", "Prince Andrew and Sarah Ferguson", "Mercury", "The wattage of an electrical component", "Peter Butterworth", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "Rio de Janeiro", "his faith that \"something will turn up\"", "aged 75 or older", "Jennifer Lopez", "1664", "Annie Lennox", "Fred Perry", "Downton Abbey", "Martina Hingis", "a painter", "cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "Gulliver's Travels", "Pomona", "Lombardy", "Mike Skinner", "Appalachian Trail", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "Campbell's", "Kirkcudbright", "the soldiers", "banned substance cortisone", "providing the basic securities", "Jennifer Aronofsky", "a typeface", "lung"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6245535714285715}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.33333333333333337, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-6656", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.546875, "CSR": 0.5757575757575757, "EFR": 1.0, "Overall": 0.7406202651515151}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "allergic rhinitis, or a combination of both.", "sarah Churchill", "getafix", "Brighton", "Belfast", "wind", "fire insurance", "a Holy Grail", "West Point", "Andy Warhol", "it has to come from La Mancha", "John T. Cable", "rykjavik", "the solar system", "potatoes", "Moldovan", "Mitsubishi A6M Zero Fighter", "the Dartford Warblers", "Franz Liszt", "Estimate", "a rider that makes a valiant last stand", "iron", "Pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "italy", "beaver", "Mel Blanc", "a dog", "Bobby Riggs", "Ellen Morgan", "Phil Woolas", "5000 meters", "racing", "casein", "Newfoundland and Labrador", "crow", "Yellowstone National Park", "St. Thomas", "Cebu", "Hugh Laurie", "Buddhism", "Guy Berryman", "Ohio", "Port Melbourne", "\u00c6thelwald Moll", "Scarface", "forgery and flying without a valid license,", "It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory over FK Ventspils.", "Liza Murphy", "Spock", "Kazakhstan", "Andorra, Belgium, Germany, Italy, Luxembourg"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5791666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-11382"], "SR": 0.546875, "CSR": 0.5749080882352942, "EFR": 0.9655172413793104, "Overall": 0.7335538159229209}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business", "tundras tundra", "bologna, Italy", "George Santayana", "opossum", "Alice Cooper", "angiotensin II", "trumpet", "peter Kay", "The Cry", "shildon", "appalachian mountain range", "MS Herald of Free Enterprise", "ballet", "philippines", "george george", "lizard", "manhattan", "mule train", "The Mystery of Edwin Drood", "pommel", "a scarlet tanager", "Dick Van Dyke", "egremont", "manhunt", "george conasso", "Medea", "Basil Feldman, Baron Feldman", "Canada", "ink", "Pears soap", "Some Like It Hot", "m Mull", "ireland", "Mike Meyers", "a sea horse", "plutonium", "magma", "Passepartout", "How are you?", "george", "spain", "shrek", "26.22", "Cleveland Brown", "heston Blumenthal", "One Direction", "spain", "Jupiter", "george Christie", "Charles Lindbergh", "September 2001", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "stoneware", "Pittsburgh", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "tchaikovsky", "george Carter"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5654513888888889}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-1724", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-508", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-78", "mrqa_searchqa-validation-7443"], "SR": 0.46875, "CSR": 0.571875, "EFR": 0.9411764705882353, "Overall": 0.7280790441176471}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Leonardo da Vinci", "Matlock", "American Civil War", "amhara", "ticks and mites", "Arafura Sea", "passe pasiphae\u2019s Wooden Cow", "the Euphrates River", "b Bavarian Forest", "to make wrinkles in one's face", "Spain", "carousel", "bullfighting", "charlie martin", "tessitura", "mouse", "flore", "Guys and Dolls", "jean Fellowes", "Denmark", "Another Day in paradise", "The Last King of Scotland", "Ghana", "pembrokeshire", "L. Pasteur", "jean feldsworth", "rachmaninoff", "Finland", "massive stars", "Mille Miglia", "caves", "Billaley & His comets", "peter rabbit", "Muriel Spark", "happy birthday to you", "seven", "opossum", "pickwick", "presliced bread", "argen", "raven", "jean", "bPA", "jean buckle", "Etruscan", "Ken Burns", "grosvenor crescent", "helene glover", "e. T. A. Hoffmann", "Mujib,", "libras", "Donna", "season four", "the atrioventricular node", "yeeun", "tomato", "Senate", "problems with the way Britain implements European Union employment directives.", "L'Aquila earthquake", "March 24,", "peter paulounced his Greek royal title in 1947 and became a naturalised British subject", "chuseok", "p Pocahontas"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5148809523809523}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_naturalquestions-validation-1091", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-1247", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-108"], "SR": 0.4375, "CSR": 0.5681423611111112, "EFR": 0.9166666666666666, "Overall": 0.7224305555555556}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "Kim", "city of acacias", "Branson,", "Gordon Ramsay", "Denis Law", "jean jean smith", "sulfur dioxide and nitrogen oxides", "Margot Betti", "Manchester Airport", "Portuguese", "travelocity", "The Avengers", "knaresborough", "comets", "sounds of Silence", "a ghost does not have flesh and bones,", "canola", "Tina Turner", "Benjamin Barker", "duttlenheim", "Bolivia", "John Donne", "Uranus", "Rio Grande", "Percheron", "The Graduate", "Greek", "Ginger Rogers", "King James I of England", "One Foot in the Grave", "Bronx Mowgli", "maurice deller", "George Santayana", "Finger Tab", "borowdale", "Wee Jimmy Krankie and his father", "Tomas De Torquemada", "julie du Pr\u00e9", "Canada", "rum and cola", "houseboat", "ghee", "George III", "ed Sheeran", "Hyperbole", "a litter of pipes", "June", "David Graham", "Ceylon", "screwdrivers", "the Kansas City Chiefs", "G minor", "My Summer Story", "2004", "The Outsiders", "Amberley Village", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "U.S. troops to the ongoing war in Afghanistan,", "her father, Josef Fritzl", "cixi", "Brigham Young", "May", "chalk quarry"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5023437500000001}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.25, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_hotpotqa-validation-2330", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-2849"], "SR": 0.4375, "CSR": 0.5646114864864865, "EFR": 0.9722222222222222, "Overall": 0.7328354917417418}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen )", "the roofs of the choir side - aisles", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist James Hutton", "dakar", "joy of living", "420", "George Strait", "sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "British Columbia, Canada", "Los Angeles", "February 10, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "By 1770 BC", "Niveditha, Diwakar, Shruti", "freehold", "Tamora Jean Calhoun", "DNA", "Anakin and Mace Windu", "Travis Tritt and Marty Stuart", "1983", "the Bee Gees", "Bruce Greenwood", "Pradyumna", "compulsory registration of births with the United Kingdom government", "A line joining Isle Vierge", "Psychomachia", "the New Jersey Devils", "two", "7.6 mm", "Cress", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Lisa Stelly", "the Canadian Rockies", "The Maginot Line", "France", "dumbo", "purple", "Charles Guiteau", "Gettysburg Address", "iTunes, iTunes Radio, and iTunes Music", "$273 million", "India", "near Garacad, Somalia", "Desperate Housewives", "The Flying Dutchman", "morelos", "Tuesday"], "metric_results": {"EM": 0.53125, "QA-F1": 0.636448434884147}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.5454545454545454, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.787878787878788, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14035087719298245, 0.25, 0.5454545454545454, 1.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-5119", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.53125, "CSR": 0.563733552631579, "EFR": 0.9666666666666667, "Overall": 0.7315487938596491}, {"timecode": 38, "before_eval_results": {"predictions": ["James Watt", "25 years after the release of their first record", "the Taft -- Katsura Agreement", "Kim Basinger", "fall of 2015", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in the pancreas", "Charles Crozat Converse", "Lady Gaga", "Chicago metropolitan area", "The president", "Domhnall Gleeson", "eusebeia", "horticulture", "Notts County", "nobiliary particle", "Stephen A. Douglas", "1984", "a loanword of the Visigothic word guma `` man ''", "Pakistan", "21 February", "Tagalog", "Bryan Cranston", "thylakoid membranes", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "almost entirely in Wake County", "December 1922", "18 Divisional Round", "602", "stable, non-radioactive rubidium", "between $10,000 and $30,000", "R2E Micral CCMC", "1931", "University of Oxford", "Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "The `` Southern Cause ''", "Randy", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "a divergent tectonic plate boundary", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "mrs Sir John Major", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "starting a dialogue while maintaining sanctions,", "10 below", "Henry Ford", "David McCullough", "Rendezvous with Rama", "CERN", "l Lisbon"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5925412475596299}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.09523809523809525, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 0.4, 1.0, 0.4, 0.32, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.515625, "CSR": 0.5625, "EFR": 0.967741935483871, "Overall": 0.7315171370967742}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale : Sammy's Adventures", "Jenny Slate", "Active absorption", "Philippe Petit", "September 1980", "January 2004", "provinces along the Yangtze River and in provinces in the south", "Toby Keith", "development of electronic computers in the 1950s", "17 - year - old", "heavy metal", "March 12, 2013", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "XXXX", "Gestaltism", "53", "lying between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Skylar Astin", "Richard Crispin Armitage", "Brooks & Dunn", "Lieutenant Templeton `` Faceman '' Peck", "Bonnie Aarons", "late 2018 or early 2019", "clouds, or diffuse nebulae", "effectively overturned the Plessy v. Ferguson decision of 1896", "McKim Marriott", "Secretary of Homeland Security", "Charles Sherrington", "1886", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "1960s", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "1940", "Ariel Winter", "Mark Jackson", "Michael Buffer", "`` There is one body and one Spirit just as you were called to the one hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all", "on location", "the federal government", "New England", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "4.5", "Juan Manuel de Ayala", "Prophet Joseph Smith, Jr.", "funny Folks", "1909", "Lawn Dogs", "179", "Princess Diana", "Mikkel", "curfew", "Pearl", "shark", "Fast Food Nation", "Hep Stars"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5944091254812962}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.4347826086956522, 0.5333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.47058823529411764, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_triviaqa-validation-3500"], "SR": 0.46875, "CSR": 0.56015625, "EFR": 1.0, "Overall": 0.7375}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "eight Indian army troopers, including one officer, and 17 militants,", "Joan Rivers", "education about rainforests", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "\"Z Zimbabwe cannot be British, it cannot be American. Yes, it is African,\"", "two weeks ago", "NATO", "Switzerland", "after takeoff from Lebanon early Monday", "second", "\"Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "\"it is impossible to turn back the tide of globalization.\"", "Clifford Harris,", "oaxaca, Mexico", "Robert Barnett,", "a class A traffic violation", "41", "Nick Adenhart", "a strict interpretation of the law,", "Derek Mears", "sylt", "a 2,700-acre sanctuary in rural Tennessee", "Tuesday afternoon", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "on the headstones to show that a visitor had been to the grave.", "Ali Bongo", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Jason Voorhees", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "St. Louis, Missouri.", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2007", "P.V. Sindhu", "Mexico", "Snickers candy bars", "narwhal", "abbotshire", "Anaheim, California", "uncle", "Bergen", "embalming", "bistro.com", "a graphical user", "German and UK Kennel Clubs"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6495852262448266}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.2857142857142857, 0.4, 0.3870967741935484, 0.07142857142857142, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 0.3076923076923077, 0.4, 0.0, 1.0, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-793", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.46875, "CSR": 0.5579268292682926, "EFR": 1.0, "Overall": 0.7370541158536585}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "Directed distance is a positive, zero, or negative scalar quantity", "sovereign states", "Megan Park", "The euro", "Kate Walsh", "September 14, 2008", "Trace Adkins", "Mars Hill", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "one of the largest financial inflows to developing countries", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "collect menstrual flow", "pigs", "General George Washington", "Spanish", "Howard Ellsworth Rollins Jr.", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Intertropical Convergence Zone ( ITCZ )", "Missouri River", "the right to vote", "frontal lobe", "10 June 1940", "Tandi", "Alberich", "ear", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "\"Kurdistan Gas City.\"", "Denver, Colorado.", "\"Mr. Farley was a member of our embedded Provincial Reconstruction Team for the Sadr City and Adhamiya districts of Baghdad City,\"", "President Charles Logan", "King Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7032935049019607}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.47058823529411764, 1.0, 0.0, 0.0, 0.08333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 1.0, 0.7000000000000001, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518"], "SR": 0.609375, "CSR": 0.5591517857142857, "EFR": 0.92, "Overall": 0.7212991071428572}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water", "Middlesex County, Province of Massachusetts Bay", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "its population", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea tothe south", "to bring", "Hermann M\u00fcller and colonial minister Johannes Bell", "Ceramic art", "Russia", "Covington, Kentucky", "New Mexico", "reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption", "December 15, 2017", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "L.K. Advani", "differential erosion", "Glenn Close", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "compasses", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "Joan Baez", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Daya Jethalal Gada", "2,140 kilometres ( 1,330 mi )", "asexually", "1926", "East Asia", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Alfheim", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "won two", "prostate cancer,", "wyvern", "Little Lord Fauntleroy", "a waistcoat", "yellow"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6330197147275636}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.4799999999999999, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 0.9, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.09090909090909093, 1.0, 0.8181818181818182, 0.14814814814814814, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.9767441860465117, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.484375, "CSR": 0.5574127906976745, "EFR": 0.8484848484848485, "Overall": 0.7066482778365046}, {"timecode": 43, "before_eval_results": {"predictions": ["2003", "February 27, 2007", "\" pick yourself up and dust yourself off and keep going ', female - empowerment song ''", "to provide jobs for young men", "Lynne", "2013", "as the arms of the king of Ireland", "Miami Heat", "1981", "After World War I", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Paul Signac", "Virgil Ogletree", "a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "rootlets", "Alex Ryan", "a habitat", "2018", "Advanced Systems Format ( ASF )", "100", "Toledo", "embryo", "the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "Paradise, Nevada", "Dominic West, Walton Goggins, Daniel Wu, and Kristin Scott Thomas", "annually in late January or early February", "Ashoka", "the name of a work gang", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Devastator", "into the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "AMX - 30", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "during the winter of the 2017 -- 18 network television season on CBS", "Florida", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Caparra", "considered to be unfair", "wintertime", "Pangaea", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\" Peschong said.", "\"We are resetting, and because we are resetts, the minister and I have an overload of work.\"", "Flint, Michigan", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.484375, "QA-F1": 0.637886848097665}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3333333333333333, 0.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 1.0, 0.9189189189189189, 0.0, 0.32, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 0.08695652173913043, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-8027", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977"], "SR": 0.484375, "CSR": 0.5557528409090908, "EFR": 0.9696969696969697, "Overall": 0.7305587121212122}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "lightweight aluminum foil", "Laurel, Mississippi", "about the outdoors, especially mountain-climbing", "Indianola, Mississippi", "Escorts Limited, an engineering company that manufacture agricultural machinery, machine construction and material handling equipment and railway equipment", "Jean Baptiste Point DuSable", "1992", "Cher", "Alabama", "James Harrison", "Toronto", "Tomorrowland", "fennec", "the United States Army", "stop motion animation", "Jean Acker", "4,530", "Democritus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Reba McEntire", "Maria Brink", "Manitowoc County, Wisconsin", "the Northrop P-61 Black widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "Andr\u00e9 3000", "Richard Street", "Zaire", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "pioneering New Zealand food writer", "South America", "2006", "four months in jail", "Operation Neptune", "Mary Elizabeth Hartman", "over 9,000 employees", "James Bolam as Jack Ford", "potential of hydrogen", "Alamodome in San Antonio, Texas", "horror fiction", "a Finger Tab", "Kent", "almost 9 million", "South African", "2008", "terrorism", "baby Moses", "Chapter 5", "Jeff Barry and Andy Kim"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5816907051282052}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-2020", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.484375, "CSR": 0.5541666666666667, "EFR": 0.9393939393939394, "Overall": 0.7241808712121213}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "1858", "Australian", "1903", "the power to regulate interstate commerce", "Naomi Wallace", "McLaren-Honda", "Tufts University", "People's Republic of China", "Azeroth", "Squam Lake", "Philip Livingston", "Tayeb Salih.", "King James II of England", "God Save the Queen", "203", "Scotland", "\"rock and roll\"", "Gesellschaft mit beschr\u00e4nkter Haftung", "Mick Jackson", "Lalit", "Hindustani classical vocalist of the Patiala Gharana", "Tampa Bay Lightning", "Steven Selling", "Sullenberger III", "Manhattan Project", "Asia-Pacific War", "Romantic", "Hugh Dowding,", "AMC Entertainment Holdings, Inc.", "New York Islanders", "Fennec fox", "1978", "six different constructors taking the first six positions", "French", "Pacific Place", "the Matildas", "\"Bad Blood\"", "Rudebox", "the E22", "Andrea Maffei", "Engirundho Vandhaal", "Sacramento Kings", "Walldorf, Baden-W\u00fcrttemberg", "Fife", "Fyvie Castle", "Aamina Sheikh, Hasan Ahmed, Neelam Muneer, Zaheen Tahira, Ismat Zaidi, Sami Khan", "the British Army", "exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Quadricycle", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Behati Prinsloo", "Saudi Arabia", "Geraldine A. Ferraro", "two"], "metric_results": {"EM": 0.53125, "QA-F1": 0.607422683747412}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-3592", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327"], "SR": 0.53125, "CSR": 0.5536684782608696, "EFR": 1.0, "Overall": 0.736202445652174}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican", "1996", "5", "Greenland sharks", "The Word", "President Abraham Lincoln's", "St Jude Thaddeus", "Anthoonij van Diemenslandt", "death penalty", "xerophyte", "Jack Roosevelt Robinson", "Manhattan", "Dian Fossey", "MI5", "Harrow", "creme anglaise", "onions", "pork", "curling", "Victoria Coren Mitchell", "Gettysburg", "Chile", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "President Barack Obama", "Canada's Liberal Party", "Bologna Song Lyrics - Daniel Bedingfield", "Dominican Republic", "Iggy Pop", "Stephen King", "Hinduism", "caryatid", "feet", "Great Britain returned Manila and Havana to Spain while Spain granted Florida to Great Britain. Great Britain also returned Guadaloupe, Saint Lucia, Goree, Martinique, and Indian trading posts to France.", "Mary Poppins", "Alex Turner", "Port Moresby Harbour, P.N.G.  Milt and Joan Mann/cameraMann International", "Connecticut", "Quentin Blake", "whooping cough", "The Daily Herald", "(1939\u20131945)", "food that is permissible according to Islamic law", "beginning in 2016", "the courts", "2017", "Chief of Protocol", "Diamond White", "1944", "Daniel Nestor, from Canada,", "Jeddah, Saudi Arabia,", "death", "Beatrix Potter", "Dan Eggen and Elizabeth Williamson", "How to Keep Young Mentally", "\"Divide the living child in two, and give half to the king"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6075579382535028}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-4157", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.53125, "CSR": 0.5531914893617021, "EFR": 1.0, "Overall": 0.7361070478723405}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "david Beckham", "victoria", "Runic", "florida", "cricket", "Planck", "rotherham United", "heat transfer", "Misery", "Styal", "stately", "Blind Beggar", "Mr Brainwash", "Leroy Burrell", "parlophone", "Wild Atlantic Way", "John Denver", "Ankh-Morpork", "noddy", "Lackawanna six", "Brazil", "Backyard Kerplunk", "muezzin", "window", "a ship", "realist", "Apollo 11", "flit", "Stanford White", "h Henderson", "Evita", "an albino sperm whale", "shrewd business man", "east fife", "St Pancras International Station", "social environment", "pre sliced bread", "Dilbert", "mayor of casterbridge", "nunc dimittis", "French", "Medea", "Burgundy", "cribbage", "Westlife", "Johannesburg", "French", "Muffin Man", "Seoul", "The Province", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Veracruz, Mexico", "if the airline doesn't perform, the credit card company still has your money and can give it right back to you.", "Robert Frost", "King Henry VIII", "Tucker", "1995 Mitsubishi Eclipse"], "metric_results": {"EM": 0.625, "QA-F1": 0.6453125}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618"], "SR": 0.625, "CSR": 0.5546875, "EFR": 0.9583333333333334, "Overall": 0.7280729166666667}, {"timecode": 48, "before_eval_results": {"predictions": ["Route 66", "sesame Street", "marcella Hazan,", "cabbage", "south Australia", "Mr. MagooQuincy Magoo", "fleece", "Ash tree", "marsupials", "New Zealand", "the Distin family", "60", "Auric Goldfinger", "1984", "small pikeb", "Mongol Empire", "1875", "tax collector", "penny", "maria maria", "Wars of the Roses", "bagram", "Tommy Lee Jones", "Chrysler", "a winter fur hat", "dandy", "civil law", "the United States", "Brazil", "tientsin", "biathlon", "lodiston", "Charlie Chan", "Vienna", "white", "jaws", "Paul Rudd", "gnomino", "Scotland", "good life", "Orson Welles", "Hindu Wisdom", "menorah", "post-impressionist", "Texas", "Super Bowl Sunday", "quant pole", "Little Tommy Stout", "marie", "azalea", "Ireland", "Chuck Noland", "the Colony of Virginia", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park", "2010", "in Vancouver, British Columbia,", "10 below", "100 to 150", "coins", "the American Kennel Club", "Omaha", "jimmy jesuit"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5885416666666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-263", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-726", "mrqa_naturalquestions-validation-4803", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.53125, "CSR": 0.5542091836734694, "EFR": 1.0, "Overall": 0.736310586734694}, {"timecode": 49, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.75390625, "KG": 0.42265625, "before_eval_results": {"predictions": ["qu Quin Ivy", "shahcheh-e Namak", "alcohol", "frenchman", "jockey", "Daniel Boone", "Thames Street", "jennifer roosevelt", "satyrs", "crabs", "boheme", "IBM", "wishbone", "Garrick club", "Lackawanna 6", "stewaby", "britten", "American Civil War", "dark", "dyan Cannon", "Bobby Tambling", "Florence", "Basil", "veruca salt", "severn", "doris pilkington Garimara", "south Africa", "bunch grasses", "guinea", "Churchill", "duke of Suffolk", "chemnitz", "statistical", "trout", "an ap\u00e9ro", "wali Muhammad", "boise", "dutch", "hair loss", "sprint", "charlie Drake", "robin hood", "Chris Martin", "flinstone", "sergeant", "rugby", "honda", "dane", "11", "tobacco", "cow", "free floating", "Tom Selleck", "New Orleans", "superhuman abilities", "Texas Tech University", "Loughborough Technical Institute", "Herman Cain", "the United States", "air support", "the typical... son", "Oklahoma", "Company", "four"], "metric_results": {"EM": 0.296875, "QA-F1": 0.36041666666666666}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2309", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-5556", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-3615"], "SR": 0.296875, "CSR": 0.5490625, "EFR": 0.9555555555555556, "Overall": 0.6831111111111111}]}