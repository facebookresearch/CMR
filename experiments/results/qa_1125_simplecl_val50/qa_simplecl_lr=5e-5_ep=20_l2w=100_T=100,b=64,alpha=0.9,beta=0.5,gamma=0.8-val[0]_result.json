{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=100.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4100, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection", "Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "Atlantic", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "the infected corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "a squared integer", "a Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "Swedex GmbH & Co KG", "More than 1 million", "2011", "by the market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Buddhism", "Matthew 16:18", "U.S.-flagged Maersk Alabama", "Rwanda", "revelry", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7591212606837607}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-4289", "mrqa_squad-validation-9655", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.71875, "CSR": 0.7734375, "EFR": 0.9444444444444444, "Overall": 0.8589409722222222}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "the coast of Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel and the establishment of an Islamic state in Palestine", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "Arizona Cardinals", "1991", "Mercury/Gemini", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "compensated for their patient care skills", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "The Spice Girls", "Documents", "the company's factory in Waterford City, Ireland", "nitrogen", "Christopher Nolan", "the Agulhas current", "six", "It always begins with the music", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.78125, "QA-F1": 0.8158424908424908}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9600", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-235", "mrqa_squad-validation-3967", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.78125, "CSR": 0.7760416666666666, "EFR": 0.8571428571428571, "Overall": 0.8165922619047619}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "woodcuts", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "beautiful voice", "Upper Lake", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President", "2000 guests", "oxygen", "increase local producer prices by 20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "reserved to, and dealt with at, Westminster", "multiple revisions", "philanthropic initiative", "integer factorization problem", "economic inequality", "Isel", "adapted quickly and often married outside their immediate French communities", "U. S. Secretary of Housing and Urban Development", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "energy stored in an H+ or hydrogen ion gradient", "the late 19th century", "the Channel Islands", "honoring their different epistemological spheres", "Alberich", "119", "\u201cBAM!\u201d came to wake them son-of-a-guns up.", "Churchill Downs", "The port of Terneuzen is the third largest in the Netherlands, after those of Rotterdam and Amsterdam", "charlagoreans", "will apparently \u201cshake Ambridge [the village where The Archers is set] to the core\u201d", "Germany", "study insects and their relationship to humans", "the limbic system", "Allan Border", "George Fox", "Virginia", "Great Expectations", "24 hours a day and 7 days a week", "Sponsorship scandal", "\"Krabby Road\""], "metric_results": {"EM": 0.625, "QA-F1": 0.6425347222222222}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_squad-validation-2463", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_hotpotqa-validation-3821"], "SR": 0.625, "CSR": 0.73828125, "EFR": 0.7916666666666666, "Overall": 0.7649739583333333}, {"timecode": 4, "before_eval_results": {"predictions": ["in higher plants", "Parliament of Victoria", "Zaha Hadid", "the French", "Science and Discovery", "the Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "The Skirmish of the Brick Church", "Sicily and the south of Europe", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related diseases", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "2010", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "Sandro Bondi", "22 felony counts", "seven alleged militants.At least 13 suspects were arrested Sunday and Monday, including three people carrying suicide jackets and explosives inside a bus station", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "a way of getting into that Lexus, Lincoln, Infiniti ororsche you always wanted", "Muslim", "This will be the first time any version of the Magna Carta has ever gone up for auction", "a unit of Time Warner", "15", "militants operating from havens in the lawless tribal regions", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "CNN's Campbell Brown", "murder", "one", "celebrity-inspired names", "$1,500", "set up codes to reduce unfair competition, raise wages and prices", "Travis", "Humberside Airport", "gABRIEL ROSSETTI"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7093014705882353}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.08, 0.15999999999999998, 1.0, 0.1, 0.0, 0.32, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_naturalquestions-validation-2908", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.6875, "CSR": 0.728125, "EFR": 0.85, "Overall": 0.7890625}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a high-level marketing manager", "roughly 500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches", "2011", "algae", "part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "the Philippines", "the Broncos", "the 1970s", "the spoils of the war", "German Te Deum", "1795", "Bermuda 419", "compressing and cooling", "Infinity Broadcasting Corporation", "semi-legal", "1972", "rudimentary", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "negotiates treaties with foreign nations", "used in a compact layout to combine keys which are usually kept separate", "from an Ohio newspaper on 8 February 1925", "Woodrow Wilson", "radius R of the turntable", "Panning", "Justin Timberlake", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom", "member states on a voluntary basis. As of 31 December 2013, the total size of the peacekeeping force is 98,200 police", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "Lowe's opened its first three stores in Canada", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman reprising his role as detective Alex Cross", "David Gahan", "it includes a restaurant, spa, and bed - and - breakfast and provides guided tours which feature the history and alleged paranormal activity of the site", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the day before Ash Wednesday", "Jaipur", "Johan Persson and Martin Schibbye", "torpedo boat", "Newport"], "metric_results": {"EM": 0.625, "QA-F1": 0.7022330922001975}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 0.3636363636363636, 0.5, 0.0, 0.5833333333333334, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-9908", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.625, "CSR": 0.7109375, "EFR": 0.7916666666666666, "Overall": 0.7513020833333333}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "social and political action", "1936", "the New Birth", "gold", "a deficit", "Costiff", "reduction", "disease", "TGIF", "Confucian propriety and ancestor veneration", "the rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "complaints involving members", "end of the season", "10", "Jacob", "murdering African-Americans", "will not support the Stop Online Piracy Act", "Chuck Bass", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Sunday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "the United States, Japan, Russia, South Korea and China", "first five Potter films", "know what's important", "3 to 17", "two suicide bombers", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Her husband and attorney, James Whitehouse", "we want to ensure we have all the capacity that may be needed over the course of the coming days", "a series of monthly meals", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "Mike Gatting", "Colgate University", "New Ice Age ahead", "A fatty acid chain is monounsaturated if it contains one double bond", "New Testament"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7632626488095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 0.5, 0.2666666666666667, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.37499999999999994, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-486", "mrqa_squad-validation-3390", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.671875, "CSR": 0.7053571428571428, "EFR": 0.7142857142857143, "Overall": 0.7098214285714286}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "unequal", "July 1969", "Hitler's secret police demanded to know if they were hiding a Jew in their house.", "yellow chlorophyll precursor", "spontaneous", "the courts of member states and the Court of Justice of the European Union", "gold", "TARDIS", "Buckland Valley near Bright", "Scottish rivers", "\"Bricks for Warsaw\"", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "\"Freshman Year\" experience", "India", "Zulfikar Ali Bhutto", "at the country's third-largest oil refinery", "April 24 through May 2", "Krishna Rajaram, a Fulbright Scholar and honor student at UCLA.", "early detection and helping other women cope with the disease", "250,000", "Timothy Masters", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jared Polis", "William S. Cohen", "\"Dance Your Ass Off.\"", "lenay evidence will be limited.", "Gary Brooker", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Japan", "condition", "boyhood experience in a World War II internment camp", "Norman given name Robert", "performance enhancing drugs", "Matt Winer", "$400, 25", "opposite R\u00fcgen island", "Les Jolies Eaux", "green"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6186320749324782}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.9032258064516129, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 0.3636363636363636, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.10810810810810811, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-1556", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-417", "mrqa_newsqa-validation-3281", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_hotpotqa-validation-4367", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-4305"], "SR": 0.546875, "CSR": 0.685546875, "EFR": 0.6896551724137931, "Overall": 0.6876010237068966}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe.", "Enric Miralles", "25-foot (7.6 m)", "eight", "Monday", "\"Journey's End\"", "immediate", "Levi's Stadium", "Wesleyan", "art posters", "Elbegdorj", "Chinggis Khaan", "Einstein", "fast forwarding", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "militants that operate from havens in Pakistan's lawless tribal regions near the porous 1,500-mile border", "tweener love", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie", "finance", "terminal brain cancer", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice", "The child wasn't heard", "Animal Planet", "fake his own death", "there were no radar outages and said it had not lost contact with any planes during the computer glitches.", "54 bodies", "early detection and helping other women cope with the disease", "Diversity", "$250,000", "fill sandbags", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "2050", "Alfredo Astiz, a former Navy captain whose boyish looks and deceitful ways", "Abdullah Gul", "Carl Froch", "The Everglades, known as the River of Grass,", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans, Louisiana", "many investors paying huge sums for individual bulbs", "Will helped protect the Earth from the scum of the universe", "ommoublen/jeopardy", "get his NHL rights and expect him to play for them this season despite rumors to the contrary coming from Russia."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6776319166944167}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0909090909090909, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.8571428571428571, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.3076923076923077, 1.0, 0.0, 0.2857142857142857, 0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2344", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.59375, "CSR": 0.6753472222222222, "EFR": 0.6538461538461539, "Overall": 0.664596688034188}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "His wife Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "hoos", "30%\u201350%", "very badly disposed towards the French, and are entirely devoted to the English.", "the top 15 most populous", "CRISPR sequences", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "issues related to the substance of the statement.", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "engaging with the Taliban in Pakistan and Afghanistan.", "an average of 25 percent", "a gym", "couple's surrogate lost the pregnancy. Klein had to contact his wife, a neonatologist at Texas Children's Hospital, while she was at work", "environmental and political events", "he fears a desperate country with a potential power vacuum that could lash out.", "two and a half hours", "Elin Nordegren", "Europe, Asia, Africa and the Middle East", "6,000", "an anaphylactic shock", "President Clinton", "delivered three machine guns and two silencers to the hip-hop star,", "Morgan Tsvangirai.", "policing the world and Africa", "future relations between the Middle East and Washington.", "in a canyon in the path of the blaze", "President Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "strife in Somalia", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia Police Department", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "the area was sealed off, so they did not know casualty figures.", "London", "after Shawn's kidnapping", "the immediate physical and social setting in which people live or in which something happens or develops.", "William Tell", "OutKast", "Groundhog Day", "he really didn't mean t", "a singer who takes a job working with a struggling carnival."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5894069442622831}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.17391304347826086, 0.4, 0.0, 0.07407407407407408, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.923076923076923, 0.2, 0.0, 0.0, 0.1111111111111111, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-2429", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-11812"], "SR": 0.515625, "CSR": 0.659375, "EFR": 0.7741935483870968, "Overall": 0.7167842741935484}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Cathedral of Saint John the Divine.", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever", "DC traction motor", "Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO.", "Brazil", "Friday", "pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "precipitation will briefly transition back to light snow or flurries Saturday before ending Saturday afternoon", "Willem Dafoe", "Maude", "Phillip A. Myers.", "Korean military", "two weeks after Black History Month", "58", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Touma", "Dangjin", "e-mails", "Chinese President Hu Jintao", "magazine", "burns over about two-thirds of his body, according to the hospital's associate director, Dr. Carl Schulman.", "October 3,", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next.", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "American Airlines", "16 August 1975", "Bonnie Aarons", "one", "kabinett - Wines made with fully ripened grapes.", "Lionsgate.", "James Lofton", "Mystic", "hair-like structures that help paramecium move around."], "metric_results": {"EM": 0.671875, "QA-F1": 0.7324100378787879}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7230", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.671875, "CSR": 0.6605113636363636, "EFR": 0.9047619047619048, "Overall": 0.7826366341991342}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "hermaphroditism and early reproduction", "Victoria Department of Education", "seized", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.", "Elway", "Philo", "36 acres", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "sum of divisors", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "Babley, Richard", "The Soup Dragon", "antelope", "nipples", "Triassic Periods", "'helpful'", "Anastasia Dobromyslova", "Lady Gaga", "9", "The Female Brain", "radish", "Robert Ludlum", "Acromantula", "(.mov)", "The Donington Grand Prix Collection is, quite simply, the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew", "London Underground", "Wisconsin", "orangutan", "Manet", "The Magic Finger", "Massachusetts", "2005", "1969", "Chrysler", "dolt", "Rome", "petticoat", "Enrico Caruso", "Elizabeth Arden", "collapsible support assembly", "Sir Hardy Amies", "Liechtenstein", "patronymic surname", "Kylie Minogue", "Cody Miller", "Brown Square Station", "Israel's vice prime minister compared Iran to Nazi Germany", "Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Jamaica", "Buddhism"], "metric_results": {"EM": 0.5625, "QA-F1": 0.61549465974333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.7234042553191489, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983"], "SR": 0.5625, "CSR": 0.65234375, "EFR": 0.7857142857142857, "Overall": 0.7190290178571428}, {"timecode": 12, "before_eval_results": {"predictions": ["Southern Border Region", "70-50's", "Panini", "new laws or amendments to existing laws as a bill", "anti-colonial movements", "Rhine Valley", "protein A", "by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship.", "cases of an express wish of the people to withdraw from the EU", "1788", "2006", "Roman Catholic", "Henry of Navarre", "John Wesley", "because the nationalisation law was from 1962, and the treaty was in force from 1958, Costa had no claim.", "Eternal Heaven", "Ness Point", "John Mayer", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Nick Boles", "Sir Hugo Drax", "Vladivostok", "Sheryl Crow", "Telstar", "Camellia sinensis", "AFC Wimbledon", "Bob Monkhouse and Kenneth Connor", "Malaysia", "cosmology", "gin", "George Clooney", "Eric Coates", "james chadwick neutron experiment", "\"No one was saved\"", "Monopoly", "champagne", "abundant rainfall lasting many thousands of years.", "the United States", "Brigit Forsyth", "William Lamb, 2nd Viscount Melbourne", "state of Japan", "What's Up With the Title?", "Thomas Edward Lawrence", "Kent", "Renoir\u00b4s", "Vanguard", "white", "Switzerland", "soda water", "people of the United States", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "bremen", "David", "\"Stagecoach\" (John Ford, 1939)"], "metric_results": {"EM": 0.609375, "QA-F1": 0.64324553135037}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8387096774193548, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9452", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-3503", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_newsqa-validation-3860"], "SR": 0.609375, "CSR": 0.6490384615384616, "EFR": 0.72, "Overall": 0.6845192307692307}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes)", "Georgia", "articles 1 to 7", "\"it would appear to be some form of the ordinary Eastern or bubonic plague\"", "their own militia", "weeks and months", "Over 61", "quality of a country's institutions and high levels of education", "cilia", "friction", "Sky Digital", "2005", "force", "business", "John Connally", "saffron", "HYmenaeus", "Zeus", "a chemical", "The Straits of Tiran", "Ingrid Chambers", "call My Bluff", "March 10, 1997", "crazed Holiday", "The Battle of the Three Emperors", "Velazquez", "Arthur Ashe", "ground lizards", "strong cold southwest wind", "table tennis", "medical journal", "penhaligon", "Gandalf", "Edgar Allen Poe", "Jinnah International Airport", "Monday", "capital of Venezuela", "a crucifix", "soap", "highball", "Avro Lancaster", "Genesis", "covey Brooker", "tea that can calm, soothe and relax has great value in today\u2019s often hectic world.", "harrods", "2007", "Christina Ricci", "Scarface", "pale yellow", "aluminium", "bubba", "June 12, 2018", "Filipino American", "London", "Lambic", "Nook", "Steven Green", "commas", "cunY-CoLAG - The City University of New York  Dr. Cullen Schaffer,", "emperor", "Synchronicity"], "metric_results": {"EM": 0.5, "QA-F1": 0.6097747432306255}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-6463", "mrqa_squad-validation-4908", "mrqa_squad-validation-6933", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-4981", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628"], "SR": 0.5, "CSR": 0.6383928571428572, "EFR": 0.84375, "Overall": 0.7410714285714286}, {"timecode": 14, "before_eval_results": {"predictions": ["seven", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep after it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "declining state of mind", "13 May 1899", "The Deadly Assassin and Mawdryn undead", "radioisotope thermoelectric generator", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1936", "Authority", "senior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "2 World Trade Center", "Kevin Spacey", "1 November", "2.5", "white blood cell", "Bangladesh -- India border", "President", "minor key", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shetty", "Sedimentary rock", "January 12, 2017", "the United States", "policyholders", "The neck", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column ( spine)", "third", "consolida", "long", "Kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "a revolver", "the foyer of the BBC building in Glasgow, Scotland", "that they would get tested to see if their kidney could be donated.King handed a thick stack of paper to Cole.\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.6482525287212787}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5333333333333333, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.09090909090909091]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-2339", "mrqa_squad-validation-1454", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-1279"], "SR": 0.5625, "CSR": 0.6333333333333333, "EFR": 0.8571428571428571, "Overall": 0.7452380952380953}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "late night talk shows", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "CBS", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "60", "Seattle, Washington", "Battle of Antietam", "Andy Cole and Shearer", "In Time", "by the early 3rd century", "Glenn Close", "four times", "Agostino Bassi", "The first five seasons of Prison Break have been released on DVD and Blu - ray in Regions 1, 2, and 4", "Malibu, California", "the church at Philippi", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Jane Fonda", "Bhupendranath Dutt", "Grey Wardens", "Dr. Lexie Grey", "Majandra Delfino", "December 19, 1971", "Uruguay defeated Argentina 4 -- 2 in front of a crowd of 68,346 people", "Ray '' Barone ( seasons 1, 2 & 8 ; 5 episodes ; played by Ray Romano )", "Thomas Middleditch", "National Legal Aid & Defender Association", "Monk's Caf\u00e9", "domestication of the domesticated sheep goes back to between 11000 and 9000 BC", "1970s", "Director of National Intelligence", "Defence Against the Dark Arts", "Isaiah Amir Mustafa", "Abigail Rokison", "Saphira hatches from the stone", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Thespis", "Portugal. The Man", "John Coffey", "Rachel Kelly Tucker", "Bohemia, now Czech Republic", "earwigs", "Code 02PrettyPretty", "musician", "opposition group, also known as the \"red shirts,\"", "abduction of minors", "Nevada", "Chilean writer, Gabriela Mistral & Pablo Neruda", "Stage Stores,", "1881"], "metric_results": {"EM": 0.5, "QA-F1": 0.5936226533882784}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.2, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.375, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-551", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5271", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_searchqa-validation-5103"], "SR": 0.5, "CSR": 0.625, "EFR": 0.78125, "Overall": 0.703125}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "Arizona Cardinals", "Robert Watson", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "Western Xia", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland", "Office", "SAVE", "Scandinavian Airlines", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award", "Jello Biafra drew on Nardwuar's face with a marker pen", "1745 rebellion of Charles Edward Stuart", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements", "Sir William McMahon", "North Sea", "7.63\u00d725mm Mauser", "Academy Award for Best Animated Feature", "Pakistan Aeronautical Complex (PAC)", "Delacorte Press", "Neighbourhoods", "Secretariat", "Wake Atoll", "Flex-fuel", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth from 1573 to 1575", "\"Southern Living\"", "William Shakespeare", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Amway", "Parlophone Records", "South Africa", "Surrey", "\"That Bizarre Girl\"", "Charles Russell", "Boyd Gaming", "three different covers", "1991", "Glenn Close", "Mary Welch", "Neighbours", "Ewan McGregor", "2011", "Alice Lloyd College", "an enslaved African American", "power-sharing talks", "Republicans Rep. Shelley Moore Capito"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6651481331168831}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3636363636363636, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8509", "mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3521"], "SR": 0.578125, "CSR": 0.6222426470588236, "EFR": 0.9259259259259259, "Overall": 0.7740842864923747}, {"timecode": 17, "before_eval_results": {"predictions": ["forces", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter", "1830", "nonfunctional pseudogenes", "inner chloroplast membrane", "Charlie Sheen", "steveland Hardaway Morris", "beaver", "La Boh\u00e8me", "formic acid", "Talavera de la Reina", "Zimbabwe", "Mr. Boddy", "Edward \"Ted\" Hankey", "Richard Walter Jenkins", "Democratic Party of Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "a type of dog called sighthounds, hunters who use keen eyesight and explosive speed to spot, chase, and capture prey animals.", "Xenophon", "Fuller's", "a reference mark located on a ship\u2019s hull that indicates the maximum depth to which the vessel may be safely immersed", "Nick Hornby", "The Comedy of Errors", "Charles V", "England", "Lagertha", "weight plates", "big house", "Hadrian", "France", "flea", "Moonee Ponds, a suburb in Melbourne, Victoria", "Essen", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "In 2014/15, only six have won the title", "Sergei Prokofiev", "Jessica Simpson", "Boy George", "In 1906, Finland became the first country in the world to grant women full political rights.", "3000m", "Scotland", "Russia", "Travis Tritt and Marty Stuart", "The Union's forces were slow in positioning themselves, allowing Confederate reinforcements time to arrive by rail", "New Jewel Movement", "in sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "Oshkosh", "two", "jeopardy/1870_Qs.txt at master  jedoublen/jeopardy", "A newspaper extra, extra edition, or special edition"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6298255404135338}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 0.4, 1.0, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.125, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-2812", "mrqa_naturalquestions-validation-767", "mrqa_hotpotqa-validation-1658", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.546875, "CSR": 0.6180555555555556, "EFR": 0.7931034482758621, "Overall": 0.7055795019157088}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "extremely high", "Manakintown", "northwest", "fewer than 10 employees", "Middle Miocene", "magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "1994", "The Hustons", "Anna Faris", "peninsular mainland", "ability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Laura Jane Haddock", "1985", "19 state rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Alice", "20 November 1989", "Coton in the Elms", "55 -- 69 %", "The Vamps", "1995", "Identification of alternative plans / policies", "16 August 1975", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "`` Killer Within ''", "Western Australia", "coronary arteries", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "empty line", "lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "violation of nature", "September 2017", "moral", "Rising Sun Blues", "Part 2", "Dumbo", "the failure of the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "mentor", "Dan Brown", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.625, "QA-F1": 0.7185297035480859}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3193", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-3232", "mrqa_hotpotqa-validation-4735"], "SR": 0.625, "CSR": 0.618421052631579, "EFR": 0.875, "Overall": 0.7467105263157895}, {"timecode": 19, "before_eval_results": {"predictions": ["Luther reviews and reaffirms, on the one hand, what has been called the \"second use of the law,\"", "black", "Illinois Country, hugging the east side of the Mississippi River and its tributaries", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "Long troop deployments in Iraq, above, and Afghanistan have been cited in the rise in military suicides.\"We must find ways to relieve some of this stress,\" said Gen. Peter W. Chiarelli,", "CNN.com", "a Florida girl who disappeared in February", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "32 percent", "sovereignty over them", "Tuesday", "forgery and flying without a valid license", "Anil Kapoor", "55", "President Obama", "unwanted baggage from the 80s and has grown beyond a resort town into something more substantial.", "The Louvre", "snowstorm", "exotic sports cars", "they laid 11 healthy eggs and, this week, all 11 of them hatched -- the last one on Wednesday.\"Eleven out of eleven,\"", "Mutassim", "two Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18, the publicist said.The Glasgow, Scotland concert has been shifted from this Sunday to May 1", "\"Steamboat Bill, Jr.\" (Charles Reisner, 1928)", "Russian defense ministry said Wednesday.Russia's Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "\u00a341.1 million", "Kingman Regional Medical Center", "CNN", "Manmohan Singh", "Michael Jackson", "silent", "40 militants and six Pakistan soldiers dead", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Louisiana", "Southeast", "a friend in Orlando, 50 miles south, Shoemaker", "Carol Browner", "tells Larry King her son has strong values.", "a tracheotomy, a surgical procedure in which an opening is made into the airway through an incision in the neck to allow for suction of fluid out of the lungs.", "back at work", "necropsy or animal autopsy", "27", "Amber Riley and her partner Derek Hough", "John Adams, a leader in pushing for independence, had persuaded the committee to select Thomas Jefferson to compose the original draft of the document, which Congress edited to produce the final version", "parsley", "Zager and Evans", "Robert Matthew Hurley", "At-large", "obscenity", "(Oliver) Cromwell", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5763615307873509}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true], "QA-F1": [0.14814814814814814, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.11764705882352941, 1.0, 1.0, 0.8, 0.2608695652173913, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.923076923076923, 0.6666666666666666, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.13793103448275862, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-2013", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.453125, "CSR": 0.61015625, "EFR": 0.8857142857142857, "Overall": 0.7479352678571428}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rh-", "1331", "Death wish Coffee", "L", "Cameroon", "1994", "ballots", "clothes that are consistent and accessible", "three empty vodka bottles,", "secretary of defense on China, Taiwan, Hong Kong and Mongolia, and was deputy director for strategy, plans and policy on the Army staff.", "23", "Nico Rosberg", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "humiliated", "composer", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony, 2,", "Amanda Knox's aunt", "well over 1,000 pounds", "Iran's nuclear program.", "12 shades of violet, including a welcoming, bright blue-purple during the day, a softer violet hue after dusk", "for pulling on the top-knot of an opponent,", "ceo Herbert Hainer", "Brett Cummins,", "a nearby day care center whose children are predominantly African-American.", "inmates", "Col. Elspeth Cameron-Ritchie", "E! News", "Most of the 103 children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans", "jobs up and down the auto supply chain", "saying Tuesday the reality he has seen is \"terrifying.\"", "the frequency of waterboarding was among the operational details that had not been declassified.", "Mexican's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "developing a youth ballpark in his hometown of Aberdeen, Maryland, financed in part by a $75,000 gift from the Major League Baseball Players Association.", "An undated photo of Alexandros Grigoropoulos, whose death", "signed a power-sharing deal with the opposition party's breakaway faction, his party said Tuesday, though Mugabe's opponents denied the claim.", "know what's important in life", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Matthew Fisher", "jund Ansar Allah", "boogeyman Jason Voorhees", "refused to refer the case of Mohammed al-Qahtani to prosecutors", "Sea World in San Antonio", "guard", "about 50", "the Ku Klux Klan", "The Wizard of Oz ( 1939 )", "Branford College", "Bolton, Bury, Oldham, Rochdale, Stockport, Tameside, Trafford, Wigan, and the cities of Manchester and Salford.", "husbands", "Malayalam", "August 17, 2017", "a jacket, gloves or a briefcase", "By the 1950s, scientists were able to do this to frogs; mice followed, in the '80s", "Hodel", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "Coldplay"], "metric_results": {"EM": 0.359375, "QA-F1": 0.49790256724878434}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.9523809523809523, 0.5, 1.0, 0.23076923076923078, 0.8, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07692307692307693, 0.15384615384615383, 0.8571428571428571, 0.125, 0.14814814814814814, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8181818181818181, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.13333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5263157894736842, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.359375, "CSR": 0.5982142857142857, "EFR": 0.6097560975609756, "Overall": 0.6039851916376306}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "Splash", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "undersirts", "Madonna's", "Glasgow", "satellite", "Australia", "Gizzard", "Pearson PLC.", "Irish Setter", "American Civil War,", "Loch Lomond", "Jes", "Queensland", "medium-sized cat, fine-boned, long, and firmly muscled.", "capital city of a nation", "Harrisburg", "Mustela erminea,", "glockenspiel", "Dr John Sentamu", "Cameroon", "Cruella de Vil", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "Charles V", "hometown", "Russell Crowe", "Theodore Roosevelt", "ACC", "Puck", "Samuel Butler", "chamomile tea", "Ireland", "tarn", "Atlantic", "Albert Square", "Newbury", "the Old Testament", "70 million people, at that time 21 % of the world's entire population", "Target Corporation", "Sister, Sister", "Michelle Rounds", "that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "international NGO", "Swamp Fox", "better conditions for inmates, like Amnesty International.", "talk show queen Oprah Winfrey.", "his mother."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6270704439530659}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.975609756097561, 0.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_newsqa-validation-2971", "mrqa_searchqa-validation-11802", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.5625, "CSR": 0.5965909090909092, "EFR": 0.8214285714285714, "Overall": 0.7090097402597403}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin, generically known as hydrocodone", "nones", "Robert Peary", "Pearls", "blackbird Hundreds.", "Carrie Underwood", "liqueur liquor", "Nero", "Google", "Langston Hughes", "Self-torture", "madge Larabee", "mambo", "riata", "black and honey all away", "USS LST 325", "prey drive", "David Beckham", "Arturo Toscanini", "economics", "Nando Parrado", "triumphal arch", "Montenegro", "discus", "jedoublen", "basidiomycota", "James Gandolfini", "Ally McBeal", "Idi Amin Dada", "agricultural green & yellow", "a body", "glazed terracotta", "Gaius Cassius Longinus", "lawyer, businessman, former politician", "masa", "two-minute", "the Vikings.", "fairfield Street", "Champs Elysees", "typhoid fever", "fjord", "baviere-quebec.org", "james", "The telegraph", "University of Missouri-St. Louis", "mixing hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off in this type of 10-letter chemical reaction", "John Knox", "the internal reproductive anatomy", "$657.4 million in North America and $1.528 billion in other countries", "the distribution and determinants of health and disease conditions in defined populations", "jape", "Tesco", "Mallard", "Graham Hill", "Battelle Energy Alliance", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services.", "debris", "$10 billion", "out in the woods"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4441879965273823}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08695652173913042, 1.0, 0.5, 0.15384615384615385, 0.625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-16257", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-5091", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-68"], "SR": 0.34375, "CSR": 0.5855978260869565, "EFR": 0.7380952380952381, "Overall": 0.6618465320910973}, {"timecode": 23, "before_eval_results": {"predictions": ["New Year's Day 2007", "1493\u20131500", "Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "Atlantic", "domestic cat", "daughter of Tony Richardson and Vanessa Redgrave", "UEFA", "The Argonauts", "prometheus", "Altamont Speedway Free Festival", "John F Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conductor", "a MUD", "Italy", "khaki", "Metamorphic", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "Mendip", "Barack Obama, then junior United States Senator from Illinois,", "the Earth", "Nafea Faa Ipoipo?", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "the colony of Suriname", "Justin Trudeau", "aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations", "Denis Law", "Love Is All Around", "Mr. Golding", "Sally Ride", "Cyclone", "Fife", "Money Saving", "Adidas", "The Hunting of the Snark", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "all pieces capture opponent's pieces by moving to the square that the opponent's piece occupies.", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alexandre Dimitri Song Billong", "86", "Musharraf", "The Wall Street Journal Europe", "red fox", "60 Minutes", "Earth"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6969209558823529}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7871", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-5135", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_newsqa-validation-848", "mrqa_searchqa-validation-6736", "mrqa_searchqa-validation-1586"], "SR": 0.640625, "CSR": 0.587890625, "EFR": 0.9565217391304348, "Overall": 0.7722061820652174}, {"timecode": 24, "before_eval_results": {"predictions": ["illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "the chosen machine model", "Fox", "1997", "a suite of network protocols", "Noriko Savoie", "15", "nine-wickets", "Pyongyang and Seoul", "fatally shooting a limo driver", "11", "change course", "Damon Bankston", "Jason Chaffetz", "money or other discreet aid", "Sarah", "normal maritime traffic", "environmental", "Hungary", "Afghan security forces", "Saturday", "38", "70,000 or so", "Climatecare", "E! News", "coach", "Steve Williams", "McDonald's", "poetry", "five female pastors", "2008", "Diego Maradona", "Dog patch Labs", "The drama of the action in-and-around the golf course", "two", "Itawamba County School District", "Romney", "EU naval force", "Plymouth Rock", "Liza Murphy", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "Samson D'Souza, 29", "former U.S. secretary of state", "At least 33", "five", "improve health and beauty.", "state governments going after them.", "it is primarily students, the reputed problem, who can best prevent acts of violence on campus.", "Damon Bankston", "Krishna Rajaram", "December 1", "killing", "a feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "Northern Ireland", "radar", "art", "16th season", "23", "South America", "freestyle", "Florence Nightingale", "Belief"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6266169140925815}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9655172413793104, 0.0, 1.0, 0.5, 1.0, 1.0, 0.33333333333333337, 0.3225806451612903, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-4673", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_searchqa-validation-3826"], "SR": 0.484375, "CSR": 0.58375, "EFR": 0.5454545454545454, "Overall": 0.5646022727272727}, {"timecode": 25, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.7578125, "KG": 0.453125, "before_eval_results": {"predictions": ["its 50th anniversary special", "Thomas Savery", "Vicodin, generically known as hydrocodone", "Eastern crops such as carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago onward", "violent separatist campaign", "Eleven", "269,000", "The Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "38 feet", "Eintracht Frankfurt", "150", "a man had been stoned to death by an angry mob.", "Russian bombers have landed at a Venezuelan airfield where they will carry out training flights for several days, the Russian defense ministry said Wednesday.", "41", "Los Alamitos Joint Forces Training Base", "super-yacht designers Wally", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "north coast of Puerto Rico.", "Russian air force", "34", "With a handful of signatures to executive orders, President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "\"several pieces of aircraft equipment were at fault or had broken down.\"", "ensuring that all prescription drugs on the market are FDA approved", "customers", "Tom Baer", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "a two-piece bathing suit", "Brian Mabry", "iTunes, which completely changed the business of music, to offering the world its first completely full-length computer-generated animated film", "Sunday.Maurice Clemmons, 37, was shot and killed early Tuesday by Seattle police.", "60 euros -- $89 -- for shoes that are also worn by dogs who walk on ice in Alaska.", "American Civil Liberties Union", "\"Our treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "Some truly mind-blowing structures are being planned for the Middle East.", "his wife's name", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "2006", "San Diego", "five", "A family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "@", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "Haitian National Police (HNP)", "heart", "Hyderabad", "Sinai Peninsula ( / \u02c8sa\u026ana\u026a / ; Arabic : \u0633\u064a\u0646\u0627\u0621\u200e S\u012bn\u0101\u02bc", "The person who has existence in two parallel worlds", "Las Vegas", "Jackson Pollock", "Lyrical", "Mississippi", "October 4, 1970", "King Duncan", "Brasstown Bald", "a thimble", "timeshare resale"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5014094876553759}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.8750000000000001, 0.8571428571428571, 0.8, 0.0, 0.0, 0.3157894736842105, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2631578947368421, 1.0, 1.0, 0.0, 0.07407407407407408, 0.6666666666666666, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.21052631578947367, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.1818181818181818, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_squad-validation-9432", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-1982", "mrqa_searchqa-validation-11832", "mrqa_searchqa-validation-9476"], "SR": 0.40625, "CSR": 0.5769230769230769, "EFR": 0.6842105263157895, "Overall": 0.6463673456477733}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "pro-democracy activists clashed Friday with Egyptian security forces", "Krishna Rajaram", "at least 25", "Booches Billiard Hall", "finance", "Ross Perot", "Hong Kong's Victoria Harbor", "2002", "seven", "the legitimacy of that race.", "the sight of celebrity pontificating about the plight of the environment", "three", "Monday", "Scarlett Keeling", "two years", "nearly 28 years", "inspectors in the agency's Colorado office", "Military commission judges also will be able to establish the jurisdiction of their own courts.", "July", "Akshay Kumar", "Graham's wife", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "David Bowie", "the death of Prince George's County police Cpl. Richard Findley,", "Phil Spector", "Kim Il Sung", "1994", "planned attacks in the southern port city of Karachi,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "dependable", "sexual assault", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "in his 60s,", "overthrow the socialist government of Salvador Allende in Chile", "Miguel Cotto", "9 a.m.", "the repeal of the military's \"don't ask, don't tell\" policy", "military veterans", "bartering", "semi-autonomous organisational units", "one", "Matt Monro", "Superintendent Norman Mullet", "the innermost digit of the forelimb", "1960", "25 million", "Peoria, Illinois", "Hawaii", "large water near the bottom, as deep as 477 m but typically 75-150 m", "Lear", "Ottoman Empire"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6590035643704122}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6086956521739131, 0.6666666666666666, 1.0, 1.0, 0.06666666666666668, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.59375, "CSR": 0.5775462962962963, "EFR": 0.8461538461538461, "Overall": 0.6788806534900285}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped buildings, a grass-roofed branch merging into adjacent parkland and gabion walls formed from the stones of previous buildings.", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "John Dillinger,", "what caused the collapse of the building which contained the city's historical archives, bringing down parts of the two nearby structures.", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen", "Brazil's response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "Lashkar-e-Tayyiba (LeT)", "$1.5 million", "2006", "Rev. Alberto Cutie", "Angels", "eight Indian army troopers, including one officer, and 17 militants,", "\"There's no chance of it being open on time.", "Karen Floyd", "14", "in a Starbucks this summer.", "BADBUL", "98", "2008", "near the Somali coast", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Pakistan's combustible Swat Valley,", "South Dakota State Penitentiary", "Iran", "last month's Mumbai terror attacks", "people have chosen their rides based on what their cars say about them.", "in July", "Most of the 103 children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "Four Americans", "Josef Fritzl,", "Glasgow, Scotland", "At least 38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "Scardia", "fractured pelvis and sacrum", "Wednesday at the age of 95", "abduction of minors.", "gun", "Aniston, Demi Moore and Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "Flag Day in 1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "apteka", "a mall", "metoprolol"], "metric_results": {"EM": 0.625, "QA-F1": 0.7428483013739475}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38095238095238093, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.3636363636363636, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9307", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-2108", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-12398"], "SR": 0.625, "CSR": 0.5792410714285714, "EFR": 0.9583333333333334, "Overall": 0.7016555059523809}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author of the Fifth Assessment Report", "ownership of private industries", "1981", "forgery and flying without a valid license,", "comments he made after his new boss, golffer Adam Scott, defeated Woods at the Bridgestone Invitational in Ohio in August.", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "Genocide Prevention Task Force.", "shoot down the satellite", "semiconductors", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "13", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "misdemeanor assault charges", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations -- embraces this technology the same way the public has", "Anil Kapoor.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico", "\"The Rosie Show,\"", "Form Design Center.", "killing at least 27 Awa Indians in southwest Colombia in the past two weeks.", "Christianity and Judaism,", "the Dalai Lama's", "Russia", "8 p.m. local time Thursday", "Passers-by", "UPS delivery box", "executive director of the Americas Division of Human Rights Watch,", "750", "nearly 100 people", "Matthew Fisher", "The Ski Train", "Boys And Girls alone", "Ozzy Osbourne", "AbdulMutallab,", "U.S. senators who couldn't resist taking the vehicles for a spin.", "inconclusive", "5:20 p.m. at Terminal C", "environmental and political events.", "$250,000", "100% of its byproducts", "School-age girls", "5,600 people every year,", "a million", "Sen. Arlen Specter", "Deutschneudorf,", "new legislation that would let prisons jam cell-phone signals within their walls.", "a deceased organ donor,", "bragging about his sex life", "vertebral column ( spine ) ; invertebrates don't", "January to May 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the South Bank", "Douglas Hofstadter", "\"The Dark Tower\" series", "American", "Marmee", "Castle Rock", "anchovy"], "metric_results": {"EM": 0.5625, "QA-F1": 0.691321519790569}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.08, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.1935483870967742, 1.0, 0.1111111111111111, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_hotpotqa-validation-5376", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.5625, "CSR": 0.5786637931034483, "EFR": 0.6785714285714286, "Overall": 0.6455876693349754}, {"timecode": 29, "before_eval_results": {"predictions": ["stagnant wages for the working class amidst rising levels of property income for the capitalist class.", "poison", "estimated 438,000", "Marty Ingels", "coaxial", "Pakistan A", "Everbank Field.", "7 members appointed by the chief executive.", "the German Campaign of 1813", "John Churchill,", "1965", "Paris at Charles de Gaulle Airport.", "Championnat National 3", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven", "Province of Syracuse", "1963", "non-alcoholic recipe", "puzzle video", "Knoxville, Tennessee", "Washington, D.C.", "Oryzomyini", "Tom Kartsotis", "November 6, 2009", "Wayman Tisdale", "Mexico", "Srinagar", "Northern Ireland", "late 19th and early 20th centuries", "political thriller", "22,500 acres", "the Harpe brothers", "Eric Liddell", "2002", "Gregg Harper", "Shohola Falls", "small forward", "ARY Digital Network", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey.", "Seminole and Miccosukee tribes.", "Virginia", "1996 NBA Slam Dunk Contest.", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "Somerset County, Pennsylvania", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "Bengal tiger", "the southernmost tip of the South American mainland", "Charlotte Heston", "allergic reaction", "Peter Townsend.", "3,000 kilometers (1,900 miles)", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg,", "Russia", "peel and devein shrimp", "Queensland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6184863701722254}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false], "QA-F1": [0.25, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.5, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.15384615384615383, 0.3157894736842105, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_squad-validation-4347", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-4716", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-2701", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-4033", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-6793", "mrqa_searchqa-validation-5030"], "SR": 0.53125, "CSR": 0.5770833333333334, "EFR": 0.8333333333333334, "Overall": 0.6762239583333334}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "effects of deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a farmers' co-op", "Danish", "1903", "attack on Pearl Harbor", "other individuals, teams, or entire organizations.", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Werner Nowitzki", "Cecil B. DeMille Award honoree", "Alex Song", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "United States and Canada", "British comedian", "Apatosaurus", "1885", "American", "Frank Thomas' Big Hurt", "\"Polovetskie plyaski\"", "Margarine Unie", "Winecoff Hotel fire", "New York City.", "The Seduction of Hillary Rodham", "2005", "Lambic", "Tom Clancy's The Division", "Venancio Flores", "Larry Alphonso Johnson Jr.", "Mike Mills", "nuclear weapons", "Joseph E. Grosberg", "Chelsea Does", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "New York", "discus thrower", "Aston Lower Grounds", "2005", "228 people", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6770928030303031}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6399999999999999, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4446", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-672", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.546875, "CSR": 0.576108870967742, "EFR": 0.896551724137931, "Overall": 0.6886727440211347}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "a huge terrestrial globe", "log flume", "Iowa Demonstration Construction Grant Program", "a broken casserole", "Nassau", "a gemstone", "Dr. Robert Gallo", "Thomas Beekman", "a network of rail lines", "Rigoletto", "aardvark", "Beijing", "Roger Bannister", "jordan tootoo", "state", "Yves Saint Laurent", "Unlike horns,", "Fortinbras", "a schooner & 1 sloop", "Anna Mary Robertson", "a lunar", "Nevilles Superette", "georgia", "a bear", "a charleston", "Gilson Lavis", "John Cleese", "a polarized electron source consisting of a 3-electrode photocathode gun and a flashlamp-", "(Milton) Berle", "george bush", "patrice Lumumba", "lunar module", "a Spanish conquistador", "Dan Marino", "red", "clownfish", "e/c^2", "Guru Pitka", "Las Vegas", "millet", "butterflies", "heavy drinking", "orangutan", "New Mexico's", "soothsayer", "Yitzhak Rabin", "defeated the philistines", "Gettysburg", "Jack Gleeson", "thirteen if Plank, a board of wood who acts as one character's imaginary friend, is included", "Buddhism", "Jean de Saint-Vincent", "Portugal", "John Mayall, Cyril Davies, Long John Baldry and Alexis Korner", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "\"has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\" leaving them \"vulnerable to disruption,\"", "knocking the World Cup off the front pages for the first time in days.", "12.3 million people worldwide"], "metric_results": {"EM": 0.375, "QA-F1": 0.42855154454022987}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-1768", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-3856", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12657", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-11275", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-2051", "mrqa_naturalquestions-validation-5896", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.375, "CSR": 0.56982421875, "EFR": 0.75, "Overall": 0.65810546875}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "62", "Herbert Henry Asquith", "40", "Libya", "Shania Twain", "Hillsborough", "glucagon", "The New York Yankees", "rapid eye movement", "green, red, white", "Ann Dunham", "Saddam Hussein", "French", "conman Charlie Cotton (John Altman)", "Ohio", "Francis Matthews", "art", "magnetite", "Noah", "Royal Albert Hall, London", "New", "Prince Andrew and Sarah Ferguson", "Mercury", "watt", "Jack Douglas", "Subway's", "Madagascar", "Swansea City", "Gatcombe Park", "S\u00e3o Paulo", "optimism", "75", "Jennifer Lopez", "1664", "Eurythmics", "Fred Perry", "Downton Abbey", "Martina Hingis", "a prominent Derry sept.", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "Gulliver's Travels", "Pomona", "Milan", "Mike Skinner", "Appalachian Mountains", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "Campbell's", "Kirkcudbright", "the soldiers", "the banned substance cortisone.", "providing safety and physical security, but also could further assist with the reconstruction projects", "(Juno)", "a typeface", "lungs"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5386160714285715}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.33333333333333337, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-7490", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-6656", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.46875, "CSR": 0.5667613636363636, "EFR": 0.7352941176470589, "Overall": 0.6545517212566845}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "pollen and mold spores.", "james i, great-grandson of James I.", "getafix", "Brighton", "Belfast", "wind", "fire", "Robin Hood's A Holy Grail", "West Point", "Andy Warhol", "Spain", "robo cop", "japan", "the solar system", "potato", "Moldova", "mitsubishi", "warblers", "George Sand.", "Estimate", "a group of poor", "clon", "pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "Manila", "beaver", "Mel Blanc", "leopard", "moffitt", "Ellen Morgan", "phil Woolas", "5000 meters", "racing", "micelles", "Newfoundland and Labrador", "crow", "Yellowstone National Park", "St. Francis Xavier", "luzon", "Hugh Laurie", "Buddhism", "Chris Martin", "Ohio", "Port Melbourne", "\u00c6thelwald Moll", "Scarface", "forgery and flying without a valid license,", "In Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.", "Liza Murphy, 42,", "Spock", "Kazakhstan", "France"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6721354166666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-525", "mrqa_searchqa-validation-11382"], "SR": 0.609375, "CSR": 0.5680147058823529, "EFR": 0.88, "Overall": 0.6837435661764706}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business", "prairies", "Bologna, Italy", "George Santayana", "opossum", "Alice Cooper", "diastolic", "trumpet", "Peter Kay.", "sky", "the Stockton & Darlington Railway", "philippines", "Herald of Free Enterprise", "ballet", "philippine", "philippines", "lizard", "Blackburn Lancashire", "Frankie Laine", "The Mystery of Edwin Drood", "pommel horse", "insect", "Dick Van Dyke", "Egremont", "manhunt", "Diego Velazquez", "phrixus", "Basil Feldman,", "Canada", "ink", "pears soap", "Some Like It Hot", "Mull", "Ireland", "Mike Meyers", "sea horse", "stainless steel", "magma", "jules Verne", "welcome", "philippine", "Denmark", "shrek", "26 miles", "Cleveland Brown", "Heston Blumenthal", "One Direction", "Flint", "Jupiter", "Mr. Stringer", "Charles Lindbergh", "September 2001", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "stoneware", "Pittsburgh", "Pakistan's High Commission in India", "shock, quickly followed by speculation about what was going to happen next,\"", "Hunter S. Thompson", "tchaikovsky", "pharaoh"], "metric_results": {"EM": 0.5, "QA-F1": 0.5743303571428571}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-78", "mrqa_searchqa-validation-7443"], "SR": 0.5, "CSR": 0.5660714285714286, "EFR": 0.84375, "Overall": 0.6761049107142857}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Leonardo da Vinci's", "Matlock", "American Civil War", "jane", "cetaceans", "Arafura Sea", "a man of many  skills and arts,", "the Euphrates", "Bavarian Forest", "to make wrinkles in one's face", "Spain", "carousel", "bullfighting", "Mike Brady", "The Countertenor", "ale mix", "Ticket Sarasota", "Guys and Dolls", "Julian Fellowes", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "Ghana", "eddisbury Way", "L. Pasteur", "jean feldsworth", "Ravel", "Finland", "massive stars", "Mille Miglia", "cave bug", "Billaley & His comets", "spruce goat", "Muriel Spark", "happy birthday to You", "seven", "opossum", "Pickwick Papers", "presliced bread", "Saga Noren", "raven", "jordan", "genetically engineered", "armageddon", "the Etruscan army", "Ken Burns", "grosvenor crescent", "team GB", "e. T. A. Hoffmann", "Mujib,", "lyra", "Donna", "season four", "the sinoatrial node", "Yubin, Yeeun", "tomato", "senate election in Minnesota, 2002", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "L'Aquila earthquake", "March 24,", "duke nedir, ne demek, duke anlam", "fall", "Pocahontas"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5697244623655914}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.12903225806451613, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_hotpotqa-validation-1247", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.46875, "CSR": 0.5633680555555556, "EFR": 0.7352941176470589, "Overall": 0.6538730596405229}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "kim", "city of acacias", "branson", "Gordon Ramsay", "luton Town", "Robert Kennedy", "sulfur dioxide", "Margot Betti Frank", "ringway", "Portuguese", "travelocity", "The Avengers", "spipton", "criminally Insane", "Simon and Jerry", "disciples", "rapeseed", "bartertown", "Benjamin Barker", "duttlenheim", "Bolivia", "John Donne", "Uranus", "Rio Grande", "Percheron", "The Graduate", "black", "Ginger Rogers", "James I of England", "One Foot in the Grave", "Bronx Mowgli", "Bob Anderson", "George Santayana", "The Finger Tab", "branenthwaite", "Wee Jimmy Krankie and his father", "Tomas De Torquemada", "dane farenboim", "Canada", "rum and cola with a slice of optional lime", "lake Union", "ghee", "George III", "ed Sheeran", "Hyperbole", "black clay pipe", "June", "lady Penelope", "Ceylon", "screwdrivers", "Kansas City Chiefs", "G minor", "My Summer Story", "2004", "The Outsiders", "Amberley Village", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "Canadian Prime Minister Stephen Harper", "four of her brothers, ages 18 and 5,", "cixi", "Brigham Young", "month of June", "chalk quarry"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5328993055555555}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 0.5, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_hotpotqa-validation-2330", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-2849"], "SR": 0.46875, "CSR": 0.5608108108108107, "EFR": 0.7058823529411765, "Overall": 0.6474792577503974}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "neutral", "roofs of the choir side - aisles", "baby Charlotte", "Sakshi Malik", "Columbia River Gorge", "secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist James Hutton", "N \u00b0 26 \u2032 34 ''", "joy of living", "420", "George Strait", "sovereignty over some or all of the current territory of the U.S. state of Texas", "1990", "Shawn", "Kiss", "January to May 2014", "Los Angeles", "September 28, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "By 1770 BC", "Niveditha, Diwakar, Shruti", "freehold", "John C. Reilly", "cell nucleus", "Mace Windu", "Travis Tritt and Marty Stuart", "1976", "the Bee Gees", "Matt Czuchry", "Pradyumna", "compulsory registration of births with the United Kingdom government is a practice that originated at least as far back as 1853", "Lands End", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates men's basketball", "two", "7.6 mm", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria", "Hudson Bay", "The Maginot Line", "France", "Dumbo", "purple", "Charles Guiteau", "Gettysburg Address", "iTunes", "$273 million", "India", "near Garacad, Somalia", "Desperate Housewives", "Cannonball Run", "puebla", "Tuesday"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6587826439864484}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 0.14285714285714285, 0.0, 1.0, 1.0, 0.6363636363636364, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 1.0, 0.787878787878788, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07272727272727272, 0.56, 0.7272727272727273, 1.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-2335"], "SR": 0.546875, "CSR": 0.5604440789473684, "EFR": 0.7931034482758621, "Overall": 0.664850130444646}, {"timecode": 38, "before_eval_results": {"predictions": ["Isaac Newton", "25 years after the release of their first record", "the Taft -- Katsura Agreement", "Kim Basinger", "fall of 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in the pancreas", "Joseph M. Scriven", "Lady Gaga", "Chicago metropolitan area in 1982", "President of the United States", "Domhnall Gleeson", "eusebeia", "Pastoral farming", "Southampton ( 1902, then in the Southern League ) being the last finalist from outside the top two tiers", "nobiliary particle", "Stephen A. Douglas", "1984", "a loanword of the Visigothic word guma", "Pakistan", "2018 Winter Olympics was held at the Gangneung Ice Arena", "Tagalog or English", "Bryan Cranston", "thylakoid membranes", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "located almost entirely in Wake County, it lies just north of the state capital, Raleigh", "1922", "2017 / 18 Divisional Round game", "520 - Southern Arizona", "99.57 %", "Membership is believed to cost between $10,000 and $30,000", "the studies and developments department of the French firm R2E Micral in 1980", "1931", "the University of Oxford", "Cherbourg in France and Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "The `` Southern Cause ''", "Randy", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "a divergent tectonic plate boundary", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "Sir John Major", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "starting a dialogue while maintaining sanctions,", "10 below", "Henry Ford", "David McCullough", "Rendezvous with Rama", "CERN", "Portugal"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6208043097289421}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.09523809523809525, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.962962962962963, 0.0, 0.5714285714285715, 0.0, 0.0, 0.4666666666666666, 0.28571428571428575, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-3219"], "SR": 0.53125, "CSR": 0.5596955128205128, "EFR": 0.8, "Overall": 0.6660797275641025}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "Diary of a Wimpy Kid : The Long Haul", "Jenny Slate", "ATP, generated by the root respiration", "Philippe Petit", "September 1980", "January 2004", "southwest and along the Yangtze", "Toby Keith", "development of electronic computers", "17 - year - old", "heavy metal", "March 12, 2013", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "XXXX", "the Berlin School of experimental psychology", "53 volumes per month", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Skylar Astin", "Richard Crispin Armitage", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "either late 2018 or early 2019", "diffuse interstellar medium", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "McKim Marriott", "Secretary of Homeland Security is Kirstjen Nielsen", "Santiago Ram\u00f3n y Cajal", "1886", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "1960s", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "1940", "Kerry Shale as Tadheus", "Mark Jackson", "Michael Buffer", "one faith, one baptism, one God and Father of all, who is over all and through all and in all", "on location", "the federal government", "1958", "Cody Fern", "the name of the war, the tariff", "prophets and beloved religious leaders", "4.5", "Juan Manuel de Ayala", "Prophet Joseph Smith, Jr.", "funny Folks", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew", "Me and Bobby McGee", "bluefin", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6373601001373179}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.42857142857142855, 0.967741935483871, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.17142857142857143, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.7499999999999999, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-10341", "mrqa_searchqa-validation-8323"], "SR": 0.484375, "CSR": 0.5578125, "EFR": 0.7575757575757576, "Overall": 0.6572182765151515}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"His treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "eight Indian army troopers, including one officer, and 17 militants", "Donald Trump.", "breast self-examination", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "his supporters were responsible for election-related violence.", "two weeks ago", "NATO", "Switzerland", "Monday", "second", "digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "he wants to spend billions to revitalize the nation's economy, a plan the campaign of his likely Republican opponent said would slow economic growth with higher taxes.", "T.I.", "ensenada, Mexico", "Michael Holmes", "a class A traffic violation that can command a fine of $627,", "41", "Nick Adenhart", "strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "Derek Mears", "290 miles", "rural Tennessee", "Tuesday afternoon", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "Section 60", "The son of Gabon's former president", "The Transportation Security Administration", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Jason Voorhees", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "St. Louis, Missouri", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2019", "P.V. Sindhu", "Mexico", "Snickers bar", "monoceros", "abon", "Anaheim, California", "uncle", "Bergen", "embalming", "Cartagena", "a graphical user interface", "the American Kennel Club"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6043406472047674}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.47058823529411764, 0.0, 0.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.32, 0.13953488372093023, 0.0, 0.25, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 0.9523809523809523, 0.3076923076923077, 0.4, 0.0, 1.0, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.421875, "CSR": 0.5544969512195121, "EFR": 0.8648648648648649, "Overall": 0.6780129882168755}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "in the United States", "Directed distance is a positive, zero, or negative scalar quantity", "sovereign states", "Megan Park", "It is the currency used by the institutions of the European Union", "Kate Walsh", "September 14, 2008", "Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point of a liquid", "increases the life of the pump, allows a smaller and lighter device to be used, and reduces electrical load", "one of the largest financial inflows to developing countries", "Aamir Khan's performance for Dangal", "Shirley Mae Jones", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "the Dutch figure of Sinterklaas", "Javier Fern\u00e1ndez", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "collect menstrual flow", "pigs", "General George Washington", "Spanish", "Howard Ellsworth Rollins Jr.", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six 50 minute ( one - hour with advertisements )", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Jos Plateau", "Missouri River", "the right to be served in facilities which are open to the public", "the frontal lobe", "10 June 1940", "at Tandi, in Lahaul", "Alberich", "ear canal", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "the project, which is designed to promote private sector investment in a variety of gas-related industries, on September 21.", "Denver, Colorado.", "the Sadr City and Adhamiya districts of Baghdad City", "the President of the United States", "King Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6619089133078861}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.47058823529411764, 1.0, 0.0, 0.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 0.6666666666666666, 0.17391304347826086, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_triviaqa-validation-2114", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518"], "SR": 0.546875, "CSR": 0.5543154761904762, "EFR": 0.7586206896551724, "Overall": 0.6567278581691297}, {"timecode": 42, "before_eval_results": {"predictions": ["Egypt", "Water extinguishment is thus a combination of `` asphyxia '' ( cutting off the oxygen supply ) and cooling", "Middlesex County, Province of Massachusetts Bay", "chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "its population", "The technologically superior machine", "During Hanna's recovery masquerade celebration, she suddenly regains her memory, revealing that Mona is A.", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "to bring", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "Ceramic art", "Norway", "Covington, Kentucky", "New Mexico", "reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption", "December 15, 2017", "Paradise, Nevada", "L.K. Advani", "differential erosion", "Glenn Close", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "compasses", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Jethalal Gada", "2,140 kilometres ( 1,330 mi )", "by producing an egg through parthenogenesis", "1926", "Durban, South Africa", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Alfheim", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "two", "prostate cancer", "wyvern", "Lord Fauntleroy", "watch", "yellow"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6242985747564236}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [0.6666666666666666, 0.14285714285714288, 0.4799999999999999, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.9, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.09090909090909093, 1.0, 0.8, 0.14814814814814814, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.9767441860465117, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5611", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-10562", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636"], "SR": 0.46875, "CSR": 0.5523255813953488, "EFR": 0.7058823529411765, "Overall": 0.645782211867305}, {"timecode": 43, "before_eval_results": {"predictions": ["1985", "February 27, 2007", "she was `` sick of keeping all these feelings inside and not speaking up for myself ''", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "Blue with a harp of gold", "Miami Heat", "1981", "After World War I,", "mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Camille Pissarro", "Virgil Ogletree", "a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "rootlets", "Alex Ryan", "a habitat", "2018", "WMA", "100 members, two from each of the 50 states", "Toledo", "embryo", "the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "Paradise, Nevada", "Alicia Vikander as Tomb Croft,", "annually in late January or early February", "Ashoka", "the name of a work gang", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority ( PREPA )", "Devastator", "the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "armored fighting vehicle", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "during the winter of the 2017 -- 18 network television season on CBS", "Florida", "The Ranch is an American comedy web television series", "Caparra", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "winter", "Pangaea", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site, once the center of the country's Cold War plutonium production.", "\"peregruzka\"", "America's infrastructure.", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6074955426517925}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.1818181818181818, 0.8, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 0.0, 0.8000000000000002, 0.4444444444444444, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 0.25641025641025644, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352"], "SR": 0.4375, "CSR": 0.5497159090909092, "EFR": 0.6666666666666666, "Overall": 0.6374171401515152}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "aluminum foil", "Laurel, Mississippi", "his writings about the outdoors, especially mountain-climbing.", "Indianola", "Escorts Limited", "the British military", "1992", "Cher", "Appalachian Mountains", "Jim Harrison", "Toronto", "Tomorrowland", "fennec", "United States Army", "stop motion animation", "Jean Acker", "Bracebridge Heath", "Leucippus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "the F-15", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony awards", "1991\u201392", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "Andr\u00e9 3000", "Richard Street", "Zaire", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "a pioneering New Zealand food writer", "South America", "2007", "perjury and obstruction of justice", "Operation Overlord", "Elizabeth Hartman", "over 9,000 employees", "Malcolm Terris", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base", "Alamodome in San Antonio, Texas", "Carrie", "The Finger Tab", "Kent", "almost 9 million", "Bahrain", "2008", "terrorism", "Moses", "Finding Haiti", "Wilson Pickett"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5616758241758242}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-993", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-8652", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.46875, "CSR": 0.5479166666666666, "EFR": 0.7941176470588235, "Overall": 0.662547487745098}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "early 20th century", "Australian", "1996", "the power to regulate interstate commerce", "Donna Wallace", "McLaren-Honda", "Tufts College", "People's Republic of China", "Azeroth", "Squam Lake", "Philip Livingston", "Tayeb Salih", "King James II of England", "God Save the Queen", "526", "Scotland", "AC/DC", "GmbH", "Mick Jackson", "Lalit", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "Sullenberger III", "Los Alamos National Laboratory", "Pacific War", "Romantic", "Hugh Dowding", "AMC Entertainment Holdings, Inc.", "New York Islanders", "Fennec fox", "1978", "John Surtees", "Canadian", "Pacific Place", "The Australian women's national soccer team", "\"Bad Blood\"", "\"SexyBack\"", "5320 km", "Giuseppe Verdi", "Chief Minister of Tamil Nadu", "Sacramento Kings", "Walldorf", "Fife", "Fyvie Castle", "Faisal Qureshi", "the British Army", "exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Model A", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Maroon 5", "Saudi Arabia", "Geraldine A. Ferraro", "two"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6412768504140787}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-3592", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-8327"], "SR": 0.5625, "CSR": 0.5482336956521738, "EFR": 0.8928571428571429, "Overall": 0.6823587927018633}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "getting to Know You", "Republican National Committee", "1996", "five", "Greenland shark", "The Word", "Abraham Lincoln", "St Jude Thaddeus", "Anthoonij van Diemenslandt", "the death penalty", "xerophyte", "Jackie Robinson", "Staten Island", "Dian Fossey", "MI5", "harrow", "creme anglaise", "onions, white wine, and a meat glaze", "pork", "curling", "Only Connect", "Gettysburg", "The Coquimbo Region", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "Barack Obama", "Canada", "It's O-s-c-a-are", "Dominican Republic", "Iggy Pop", "Stephen King", "Hinduism", "caryatid", "feet", "most of its land in North America", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Sun", "numerous", "kosher", "2016", "the courts", "2017", "ambassador to Ghana and to Czechoslovakia", "Diamond White", "1944", "a fan", "Jeddah, Saudi Arabia", "death", "Beatrix Potter", "George Stephanopoulos", "Reader's Digest Association", "\"Divide the living child in two, and give half to the one"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5607142857142857}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_hotpotqa-validation-0", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.484375, "CSR": 0.546875, "EFR": 0.6666666666666666, "Overall": 0.6368489583333333}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "David Beckham", "on Scottish soil took place on a nearby moor at Culloden", "Runic", "estoril, Portugal", "robert", "Planck", "rotherham United", "heat transfer", "Misery", "Styal", "stately tempo", "Blind Beggar", "Brainwash", "l Leroy Burrell", "parlophone", "Wild Atlantic Way", "John Denver", "Ankh-Morpork", "noddy Goes To Toyland", "Lackawanna Six", "Brazil", "Backyard Kerplunk", "muezzin", "window", "a ship", "Madame Bovary", "Apollo 7", "flit gun", "Nikola Tesla", "Nicky Henderson", "Evita", "sperm whale", "ring man", "east fife", "st Pancras International Station", "social environment", "presliced bread", "Dilbert", "mayor of casterbridge", "dimittis", "French", "Medea", "Burgundy", "cribbage", "The Beatles", "Johannesburg", "French", "Muffin Man", "Seoul", "Prince James, Duke of York and of Albany ( later King James II & VII )", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Veracruz, Mexico", "Kuwait-based Wataniya Airways", "Robert Frost", "King Henry VIII", "Pillsbury", "Mitsubishi Lancer OZ Rally"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6219381313131314}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-5130", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.5625, "CSR": 0.5472005208333333, "EFR": 0.8571428571428571, "Overall": 0.675009300595238}, {"timecode": 48, "before_eval_results": {"predictions": ["Route sixty-six", "sesame Street", "minced tomatoes", "cabbage", "australia", "Mr. Magoo", "eagle", "Ash", "marsupials", "New Zealand", "slide whistles", "60", "goldfinger", "1983", "a fish", "Mongol Empire", "1875", "tax collector", "penny", "santain", "Wars of the Roses", "Bagram", "maggie Gilkeson", "Chrysler", "a round, slightly tapered,ergusonless fur hat", "dandy", "arts", "United States", "Brazil", "pei Tang", "biathlon", "Idaho", "Charlie Chan", "Vienna", "white", "jaws", "Paul Rudd", "rabbit", "Scottish", "jodhpurs", "Orson Welles", "hindu Wisdom", "menorah", "post-impressionist", "Texas", "Super Bowl Sunday", "quant pole", "Little Tommy Stout", "go West", "Rhododendron", "Ireland", "Chuck Noland", "the Colony of Virginia", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park", "2010", "ancient Greek site of Olympia", "10 below", "100 to 150", "coins", "the American Kennel Club", "Omaha", "George Jones"], "metric_results": {"EM": 0.5, "QA-F1": 0.5610119047619048}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-263", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-6306", "mrqa_naturalquestions-validation-4803", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.5, "CSR": 0.5462372448979591, "EFR": 0.78125, "Overall": 0.6596380739795918}, {"timecode": 49, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.671875, "KG": 0.46015625, "before_eval_results": {"predictions": ["sledge", "shahcheh-e Namak", "alcohol", "Francis of Assisi", "dutch Francis", "Daniel Boone", "Thames Street", "Theodore Roosevelt", "satyrs", "crabs", "lite boheme", "amorance", "wishbone", "Garrick club", "Lackawanna six", "barnaby rudge", "britten's", "American Civil War", "a succession of bad events, repeated bad luck", "jean conchita Alonso", "Jimmy Robertson", "Florence", "Basil", "willy Wonka", "severn", "Western Australia", "south", "bunch grasses", "Nicaragua", "Clement Attlee", "Wars of the Roses", "Chemnitz", "Fan Graphs and Baseball Prospectus", "chubs, whitefish, squawfish, rainbows, cutthroats, and bull trout", "Joseph Dubonnet", "ken Norton", "belize", "archivist, writer, scholar, political activist, oral historian, and film-maker", "hair loss", "sprint", "Charlie Drake", "Robin Hood's A Holy Grail", "Chris Martin", "flintstone", "sergeant", "rugby", "honda", "sterning Dan", "11", "tobacco", "cows", "free floating", "Tom Selleck", "New Orleans", "abilities after being bitten by radioactive/genetically-altered spiders.", "Texas Tech University", "Loughborough Technical Institute", "Sharon Bialek", "the United States", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "George Babbitt", "Oklahoma", "company", "four"], "metric_results": {"EM": 0.375, "QA-F1": 0.4803215579710145}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.17391304347826086, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2309", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.375, "CSR": 0.5428124999999999, "EFR": 0.8, "Overall": 0.649265625}]}