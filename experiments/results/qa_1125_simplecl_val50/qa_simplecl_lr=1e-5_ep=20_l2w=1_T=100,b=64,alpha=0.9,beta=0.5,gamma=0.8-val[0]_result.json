{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=1e-5_ep=20_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=1e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=1e-5_ep=20_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "Atlantic", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "the infected corpses", "the United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "two integers", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "British Gas plc", "More than 1 million", "2011", "by the market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Buddhism", "Matthew 16:18", "U.S. ship that was hijacked off Somalia's coast", "Rwanda", "revelry", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.75, "QA-F1": 0.805317599067599}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-4289", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.75, "CSR": 0.7890625, "EFR": 1.0, "Overall": 0.89453125}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "the coast of Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel and the establishment of an Islamic state in Palestine", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "a multi-party system", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "Arizona Cardinals", "1991", "Mercury/Gemini veteran Wally Schirra, Eisele, and rookie Walter Cunningham", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "to become more integral within the health care system", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "The Spice Girls", "the ten stages of corporate life cycle", "the company's factory in Waterford City, Ireland", "nitrogen", "Annemarie Moody", "Water Current", "six", "It always begins with the music", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.75, "QA-F1": 0.7888817065287653}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9600", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-8316", "mrqa_squad-validation-235", "mrqa_squad-validation-3967", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.75, "CSR": 0.7760416666666666, "EFR": 0.9375, "Overall": 0.8567708333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "woodcuts", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "the best-known legend", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President", "2000 guests", "oxygen", "increase local producer prices by 20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "multiple revisions", "philanthropic initiative", "integer factorization problem", "economic inequality", "Isel", "adapted quickly and often married outside their immediate French communities", "former Pakistani Prime Minister Benazir Bhutto", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "energy stored in an H+ or hydrogen ion gradient", "the late 19th century", "the Channel Islands", "in no way contributes to faith", "Alberich", "charleston", "How Emeril Really Feels About the Word", "Churchill Downs", "The port of Terneuzen is the third largest in the Netherlands, after those of Rotterdam and Amsterdam", "charleston", "charl", "christopher", "study insects and their relationship to humans, other organisms, and the environment", "the limbic system", "trahan Mubarak", "George Fox", "Maryland", "Great Expectations", "24 hours a day and 7 days a week", "Sponsorship scandal", "\"Krabby Road\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.6773809523809523}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-9286", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_squad-validation-2463", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_hotpotqa-validation-3821"], "SR": 0.640625, "CSR": 0.7421875, "EFR": 0.8695652173913043, "Overall": 0.8058763586956521}, {"timecode": 4, "before_eval_results": {"predictions": ["in plants that contain them", "Parliament of Victoria", "Zaha Hadid", "the French", "Science and Discovery", "the Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "The Skirmish of the Brick Church", "the port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "the Italian government", "22", "terror groups that they say were planning numerous suicide attacks", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "because a new model is simply out of their reach", "Muslim", "will be the first time any version of the Magna Carta has ever gone up for auction", "\"a fantastic five episodes.\"", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "CNN's Campbell Brown", "a woman who may have been contacted through a Craigslist ad", "one", "most popular baby names", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "in the second section, which transcribes his letters to Dante Gabriel Rossetti"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7080181338269573}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.15999999999999998, 1.0, 0.9411764705882353, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.671875, "CSR": 0.728125, "EFR": 0.9523809523809523, "Overall": 0.8402529761904762}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a high-level marketing manager, was given the job of turning the business around", "roughly 500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches", "2011", "algae", "part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "the Philippines", "the Broncos", "the 1970s", "the characteristics of the conquering peoples", "German Te Deum", "1795", "Bermuda 419", "evaporated to cool oxygen gas", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "rudimentary", "1957", "mother-of-pearl", "Gene Barry", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "used in a compact layout to combine keys which are usually kept separate", "from an Ohio newspaper on 8 February 1925", "Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress", "cannonball", "Panning", "Justin Timberlake", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom", "troops are contributed by member states on a voluntary basis. As of 31 December 2013, the total size of the peacekeeping force is 98,200 police", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "Lowe's opened its first three stores in Canada", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman reprising his role as detective Alex Cross", "David Gahan", "the Overlook Hotel in his 1977 bestseller The Shining and its 1980 film adaption of the same name, as well as the location for the 1997 miniseries", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the day before the long fast for the Lenten fast", "Jaipur", "Johan Persson and Martin Schibbye", "\"torpedo boat destroyers\"", "Newport Gwen Dragons"], "metric_results": {"EM": 0.625, "QA-F1": 0.6817594207566984}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.1111111111111111, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 0.3636363636363636, 0.5, 0.08333333333333334, 0.5833333333333334, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-3473", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.625, "CSR": 0.7109375, "EFR": 0.9583333333333334, "Overall": 0.8346354166666667}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "social and political action", "1936", "New Birth", "gold", "a 3\u20130 lead", "Vivienne Westwood", "reduction gears", "disease", "TGIF", "Confucian propriety and ancestor veneration", "Luther's rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "members in good standing with the college, and private schools may also require their teachers to be college peoples", "end of the season", "10", "Jonas", "murdering African-Americans", "will not support the Stop Online Piracy Act", "Vince Chase", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Stuttgart", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "North Korea", "first five Potter films", "know what's important in life", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Her husband and attorney, James Whitehouse,", "we want to ensure we have all the capacity that may be needed over the course of the coming days", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "Mike Gatting", "Colgate University", "Church of Christ, Scientist", "unsaturated fats are comprised of lipids that contain? A fatty acid in which there is at least one double bond within the fatty acid chain", "Luke 6 : 67 -- 71"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7020695724231909}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.2, 0.0, 1.0, 0.5, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.2666666666666667, 1.0, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.3571428571428571, 0.75]}}, "before_error_ids": ["mrqa_squad-validation-800", "mrqa_squad-validation-3370", "mrqa_squad-validation-2133", "mrqa_squad-validation-486", "mrqa_squad-validation-3390", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.59375, "CSR": 0.6941964285714286, "EFR": 1.0, "Overall": 0.8470982142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism", "Jim Gray", "P = PSPACE", "July 1969", "Hitler's secret police demanded to know if they were hiding a Jew in their house.", "a yellow chlorophyll precursor", "spontaneous", "the courts of member states", "gold", "TARDIS", "Buckland Valley near Bright", "Scottish rivers", "\"Bricks for Warsaw\"", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "up to \u00a339,942", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "\"Freshman Year\" experience", "India", "Zulfikar Ali Bhutto, former president and prime minister of Pakistan", "at the Lindsey oil refinery in eastern England.", "April 24 through May 2", "Krishna Rajaram, a Fulbright Scholar", "early detection", "250,000", "Timothy Masters", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jared Polis", "William S. Cohen", "\"Dance Your Ass Off\"", "military trials for some Guant Bay detainees.", "Matthew Fisher", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "seeking help", "Japan", "too many glass shards", "\"Empire of the Sun,\"", "Norman given name Robert", "stronger, stronger", "Matthew Ward Winer", "5498, aired 2008-07-02", "on the Strelasund, a strait of the Baltic Sea", "Mustique", "green"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6551919630548664}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.9032258064516129, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.10256410256410256, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5333333333333333, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-3938", "mrqa_squad-validation-1556", "mrqa_squad-validation-2091", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858"], "SR": 0.5625, "CSR": 0.677734375, "EFR": 0.9642857142857143, "Overall": 0.8210100446428572}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday", "Journey's End", "immediate", "Levi's Stadium", "decidedly Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: \u6210\u5409\u601d\u6c57", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers,", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "Islamic", "$12.9 million", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie business.", "finance", "terminal brain cancer", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice act", "separated", "Animal Planet", "fake his own death", "there were no radar outages and said it had not lost contact with any planes during the computer glitches.", "54 bodies", "early detection and helping other women cope with the disease", "Diversity", "$250,000", "breaking up ice jams", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president was declared the winner of the country's presidential elections on Thursday,", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Mikkel Kessler", "The Everglades, known as the River of Grass,", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans, Louisiana", "investors", "MIBs", "How to help your child (and yourself) through cold season", "it has to be the most difficult sport to be a play-by-play announcer."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6850411741036742}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.13333333333333333, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-6300", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.59375, "CSR": 0.6684027777777778, "EFR": 1.0, "Overall": 0.8342013888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "\"hoos\"", "30%\u201350% O2 by volume", "very badly disposed towards the French, and are entirely devoted to the English.", "United States", "CRISPR sequences", "six", "about 300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "issues related to the substance of the statement.", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "providing the basic securities that Turkey can be a great partner.", "an average of 25", "a trainer", "the couple's surrogate lost the pregnancy.", "environmental and political events", "he fears a desperate country with a potential power vacuum that could lash out.", "at least two and a half hours.", "Elin Nordegren", "Europe, Asia, Africa and the Middle East.", "6,000", "cortisone", "President Clinton.", "delivered three machine guns and two silencers to the hip-hop star,", "Morgan Tsvangirai.", "policing the world and Africa", "liquidity", "a canyon", "President Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school,", "strife in Somalia,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "the area was sealed off, so they did not know casualty figures.", "London", "after Shawn's kidnapping", "the immediate physical and social setting in which people live or in which something happens or develops.", "William Tell", "OutKast", "Groundhog Day", "t", "a singer"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6161157459778783}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.25, 0.2, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-11812"], "SR": 0.5625, "CSR": 0.6578125, "EFR": 0.9285714285714286, "Overall": 0.7931919642857144}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley, New York", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever", "DC traction", "Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "Friday", "pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "precipitation will briefly transition back to light snow or flurries Saturday before ending Saturday afternoon", "Gainsbourg", "\"Maude\"", "Phillip A. Myers.", "Koreans", "two weeks after Black History Month", "58 people", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Touma", "Dangjin", "\"novel that you would embarrassed to buy,\"", "Chinese President Hu Jintao", "magazine", "The teen faces a lifelong recovery from his injuries,", "October 3,", "Adriano", "Larry Zeiger", "shock,", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "bankruptcy", "16 August 1975", "Bonnie Aarons", "one", "kabinett", "Lionsgate.", "James Lofton", "Sanskrit", "hair-like structures that help paramecium move around."], "metric_results": {"EM": 0.609375, "QA-F1": 0.7063041125541125}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.18181818181818182, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-7230", "mrqa_squad-validation-1299", "mrqa_squad-validation-8655", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.609375, "CSR": 0.6534090909090908, "EFR": 1.0, "Overall": 0.8267045454545454}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company under the Office of Alien Property (OAP) seal", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks,", "Elway", "Philo of Byzantium", "36", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "Euler's totient function", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "The Soup Dragon", "antelope", "nipples", "the Precambrian period", "'helpful'", "Anastasia Dobromyslova", "Gagapedia", "9", "Who Does Michael Jordan Want as the Star of Space Jam 2?", "radishes", "Robert Ludlum", "a great power", "(.mov)", "the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew", "London Underground Piccadilly Line", "Wisconsin", "orangutan", "Manet", "The Magic Finger", "Massachusetts", "2005", "1971", "Dodge", "dolt", "Venice", "a palla", "Enrico Caruso", "Elizabeth Arden", "collapsible support assembly", "Sir Hardy Amies", "Liechtenstein", "the 14th most common surname in Wales and 21st most common in England", "Rob Davis", "Cody Miller", "Bloomingdale Firehouse", "Israel's vice prime minister compared Iran to Nazi Germany", "Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Jamaica", "Buddhism"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6415684556988905}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.2608695652173913, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444444, 0.5714285714285715, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983"], "SR": 0.5625, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.8229166666666666}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "70-50's", "Panini", "new laws or amendments to existing laws as a bill", "anti-colonial movements", "glacial alpine valley", "surface proteins", "test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "basic constitutional rights and principles (particularly democracy, the rule of law and the social state principles)", "1788", "2006", "Roman Catholic", "Navarre", "John Wesley", "because the nationalisation law was from 1962, and the treaty was in force from 1958,", "Eternal Heaven", "Ness Point", "Simpson,", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Eric Pickles", "chess", "Vladivostok", "Sheryl Crow", "alfred", "Camellia sinensis", "AFC Wimbledon", "Bob Monkhouse", "Malaysia", "cosmology", "gin", "Andrea Waguespack", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "champagne", "an extended period of abundant rainfall lasting many thousands of years", "the United States", "Brigit Forsyth", "Lord Melbourne", "state of Japan", "The Tragedy of Troilus and Cressida", "David Lean", "Kent", "Edgar Degas", "Standard Motor Company", "white", "Switzerland", "soda water", "people of France", "73", "ITV", "Scottish national team", "the death of a pregnant soldier", "Dereks", "alsace-Lorraine and Bavaria", "The Goat Amalthea", "\"Stagecoach\" (John Ford, 1939)"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5347840969514357}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 0.8387096774193548, 0.3846153846153846, 1.0, 1.0, 1.0, 0.5, 1.0, 0.9600000000000001, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9452", "mrqa_squad-validation-9126", "mrqa_squad-validation-6655", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-3161", "mrqa_squad-validation-4590", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-3503", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-2159", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_newsqa-validation-3207", "mrqa_searchqa-validation-8450", "mrqa_searchqa-validation-9647", "mrqa_newsqa-validation-3860"], "SR": 0.4375, "CSR": 0.6298076923076923, "EFR": 1.0, "Overall": 0.8149038461538461}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "their own militia", "days, weeks and months", "Over 61", "quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron  Saffron", "HYMENAEUS", "gods", "albinism", "Straits of Tiran", "Brigit Forsyth", "Call My Bluff", "March 10, 1997", "auguste", "The Battle of the Three Emperors", "Velazquez", "althea Gibson", "lizard", "strong cold southwest wind", "table tennis", "medical journal", "penhaligon", "Gandalf", "thaddeus sholto", "Jinnah International", "Monday", "capital of Venezuela", "auguste", "soap", "liquor", "Avro", "Genesis", "Charlie Brooker", "camaemelum nobile", "harrods", "2007", "Christina Ricci", "Scarface", "pale yellow", "Everest", "bubba", "June 12, 2018", "Filipino", "London", "Lambic", "Nook", "Steven Green", "commas", "auguste fortune", "baltic", "Synchronicity"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6185267857142858}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_squad-validation-6933", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-5320", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628"], "SR": 0.53125, "CSR": 0.6227678571428572, "EFR": 1.0, "Overall": 0.8113839285714286}, {"timecode": 14, "before_eval_results": {"predictions": ["seven", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep after it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "a declining state of mind", "1898", "The Deadly Assassin and Mawdryn undead", "for scientific observation", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1936", "Authority", "junior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "between 2 World Trade center and 3 World Trade Center", "Kevin Spacey", "All Hallows'Day", "78", "in lymph", "Bangladesh -- India border", "the President", "minor key", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shetty", "Sedimentary rock", "January 12, 2017", "United States", "claims adjuster", "the nucleus", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column ( spine)", "three", "perennial", "a penis", "Kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "NYPD Shield", "the foyer of the BBC building in Glasgow, Scotland", "\"Larry King Live\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.7034977703455965}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false], "QA-F1": [0.5, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9523809523809523, 1.0, 0.28571428571428575, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 1.0, 0.5, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-1279"], "SR": 0.578125, "CSR": 0.6197916666666667, "EFR": 0.8888888888888888, "Overall": 0.7543402777777778}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "The Late Late Show", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "CBS", "Jean Ribault", "Tetzel", "visitation of the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "note number 60", "Seattle, Washington", "Battle of Antietam", "Nicolas Anelka", "In Time", "by the early 3rd century", "Glenn Close", "four times", "Agostino Bassi", "five seasons", "Malibu, California", "the church at Philippi", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "trane thonda", "Bhupendranath Dutt", "Grey Wardens", "Dr. Lexie Grey ( Chyler Leigh )", "Majandra Delfino", "December 19, 1971", "Uruguay", "Alex Skuby", "Thomas Middleditch", "The National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "domesticated sheep goes back to between 11000 and 9000 BC", "1970s", "Director of National Intelligence", "Defence Against the Dark Arts", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Thespis", "Portugal", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "dermaptera", "Code 02PrettyPretty", "game designer", "opposition group, also known as the \"red shirts,\"", "the abduction of minors", "$6.2 trillion", "Isabel Allende", "Stage Stores,", "1881"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5854456741175491}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.375, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-551", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_searchqa-validation-13473", "mrqa_searchqa-validation-5103"], "SR": 0.46875, "CSR": 0.6103515625, "EFR": 0.9411764705882353, "Overall": 0.7757640165441176}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "Arizona Cardinals", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "Khitan", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Vancouver", "Microsoft Office", "SAVE", "SAS AB", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award Best Actor", "Jello Biafra drew on Nardwuar's face with a marker pen", "the 1745 rebellion of Charles Edward Stuart", "Burny Mattinson", "Sir William McMahon", "the North Sea coast", "7.63\u00d725mm Mauser", "Academy Award for Best Animated Feature", "the Chengdu Aircraft Corporation (CAC) of China", "Delacorte Press.", "Neighbourhoods", "Secretariat", "Wake Atoll", "Hydrogen vehicle", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "\"Southern Living\" Reader's Choice Awards", "William Shakespeare", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Amway", "Parlophone Records", "South Africa", "Surrey", "The Girl", "Charles Russell", "Boyd Gaming", "three different covers", "1979", "Glenn Close", "Mary Welch", "Neighbours", "Ewan McGregor", "2011", "Alice Lloyd College", "son of an African-born slave mother in Southampton, Virginia", "power-sharing talks", "Brown-Waite"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6873207521645022}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3636363636363636, 0.2857142857142857, 0.4, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655"], "SR": 0.578125, "CSR": 0.6084558823529411, "EFR": 1.0, "Overall": 0.8042279411764706}, {"timecode": 17, "before_eval_results": {"predictions": ["forces", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter status", "1830", "nonfunctional pseudogenes", "the inner chloroplast membrane", "Charlie welch", "steveland Hardaway Morris", "beaver", "La Boh\u00e8me Giacomo Puccini", "red itchy swelling, a burning or stinging sensation, itchy white bruises, and, in some cases, a severe allergic reaction", "Talavera de la Reina", "Zimbabwe", "Mr. Boddy", "Ted Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "hound", "Xenophon", "Fuller's", "a reference mark", "Nick Hornby", "The Comedy of Errors", "Charles V", "England", "Lagertha", "weight plates", "\"big house\"", "Hadrian", "California", "human flea", "Moonee Ponds, a suburb in Melbourne, Victoria", "Hamburg", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "Manchester United (13), Chelsea (4), Arsenal (3), Manchester City (2), Blackburn Rovers and Leicester City (1)", "Prokofiev", "Jessica Simpson", "Boy George", "In 1906, Finland became the first country in the world to grant women full political rights.", "3000m", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "It was a Confederate victory", "New Jewel Movement", "sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "Oshkosh", "two", "jeopardy/1870_Qs.txt at master  jedoublen/jeopardy", "\"The Sunday Thing\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6279513888888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7089", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.546875, "CSR": 0.6050347222222222, "EFR": 0.9655172413793104, "Overall": 0.7852759818007663}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "extremely high", "Manakin Town", "northwest", "fewer than 10 employees", "Middle Miocene", "magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "2010", "Coppolas and, technically, the Farrow / Previn / Allens", "Anna Faris", "peninsular mainland", "inability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Laura Jane Haddock", "( 1985 -- 1993", "775 rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Alice", "20 November 1989", "Coton in the Elms", "55 -- 69 %", "Ella Eyre", "1995", "Identification of alternative plans / policies", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "the aorta", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "an optional message body", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "a violation of nature", "September 2017", "moral", "\" Rising Sun Blues ''", "Part 2", "Dumbo", "the failure of the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "jegna", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.640625, "QA-F1": 0.714139822595705}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 0.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.8, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735"], "SR": 0.640625, "CSR": 0.606907894736842, "EFR": 0.9565217391304348, "Overall": 0.7817148169336384}, {"timecode": 19, "before_eval_results": {"predictions": ["the law as the Holy Spirit's tool to work sorrow over sin in man's heart, thus preparing him for Christ's fulfillment of the law offered in the gospel", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "Long troop deployments", "Joe Pantoliano", "a Florida girl who disappeared in February, plans to file for divorce from the girl's stepmother", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "Nearly eight in 10", "sovereignty over them", "Tuesday", "forgery and flying without a valid license", "Anil Kapoor", "55", "President Obama", "unwanted baggage from the 80s", "The Louvre", "snowstorm", "sports cars", "11 healthy eggs", "Saadi", "Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18, the publicist said.The Glasgow, Scotland concert has been shifted from this Sunday to May 1,", "\"Steamboat Bill, Jr.\" (Charles Reisner, 1928)", "Russia", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "\u00a320 million ($41.1 million) fortune", "Kingman Regional Medical Center", "Laura Ling and Euna Lee", "Manmohan Singh", "Michael Jackson", "be silent", "40 militants and six Pakistan soldiers dead", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Louisiana", "Southeast", "the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home.", "Nancy Sutley", "\" Michael Phelps, who won a record eight gold medals in Beijing, is the author of a new memoir, \"A Mother For All Seasons.\"", "Moe and Sana Maraachli", "back at work", "the initial necropsy or animal autopsy", "27", "Derek Hough", "Thomas Jefferson", "parsley", "Zager and Evans", "Bob Hurley", "fourth term", "\"adult Theatre - You must be 21 and able to prove it\"", "(Oliver) Cromwell", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5918412316849817}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.6666666666666666, 1.0, 0.923076923076923, 0.6666666666666666, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-328", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.484375, "CSR": 0.60078125, "EFR": 1.0, "Overall": 0.800390625}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death wish Coffee", "L", "Cameroon,", "1994", "ballots", "clothes that are consistent and accessible", "traces of tablets in Winehouse's stomach", "secretary of defense on China, Taiwan, Hong Kong and Mongolia, and was deputy director for strategy, plans and policy on the Army staff.", "Bobby Darin,", "compatriot Nico Rosberg", "16", "his former Boca Juniors teammate and national coach Diego Maradona,", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "the composer of \"Phantom of the Opera\" and \"Cats\" and one of Britain's richest men,", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony, 2,", "Amanda Knox's aunt", "well over 1,000 pounds", "Iran's nuclear program.", "a welcoming, bright blue-purple during the day, a softer violet hue after dusk, and a deep, soothing near-black on red-eyes when it's time to sleep.", "allegedly faking a doctor's note and was restricted from leaving his house in Tokyo,", "ceo Herbert Hainer", "Brett Cummins,", "a nearby day care center whose children are predominantly African-American.", "inmates", "Col. Elspeth Cameron-Ritchie,", "\"E! News\"", "Several of the 103 children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "jobs", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "Mexican's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "\"They left without me,' which is what I thought I would do.\"", "An undated photo of Alexandros Grigoropoulos,", "signed a power-sharing deal with the opposition party's breakaway faction, his party said Tuesday, though Mugabe's opponents denied the claim.", "a 57-year old male", "Kim Jong Il seems to be \"testing the new administration.\"", "Angola", "Gary Brooker", "\"outlaws\"", "\"A remake of the original 1980 film, the new movie aims to breathe life into the former franchise about a pathological killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The techniques they used were all authorized, but the manner in which they applied them was overly aggressive and too persistent,", "Sea World in San Antonio,", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "about 50", "the Ku Klux Klan", "1939", "Branford College", "Bolton, Bury, Oldham, Rochdale, Stockport, Tameside, Trafford, Wigan, and the cities of Manchester and Salford.", "husbands", "Malayalam", "August 17, 2017", "a jacket, gloves or a briefcase", "clone. right: Dave.", "Hodel", "access to US courts", "Coldplay"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4801367137383375}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.9523809523809523, 0.0, 0.14285714285714288, 0.23076923076923078, 0.8, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07692307692307693, 0.0, 0.8571428571428571, 0.0, 0.14814814814814814, 1.0, 0.1, 0.22222222222222224, 0.0, 0.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0851063829787234, 0.0, 0.0, 0.19047619047619047, 0.6666666666666666, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.22222222222222224, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3070", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.359375, "CSR": 0.5892857142857143, "EFR": 0.9512195121951219, "Overall": 0.7702526132404182}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "Splash", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "on both shoulders", "Madonna's", "Glasgow", "satellite-based navigational system that can tell users exactly where they are on Earth.", "Australia", "Giblet", "Pearson PLC", "Irish Setter", "American Civil War,", "Loch Lomond", "Jesuit", "Tasmania", "medium-sized cat, fine-boned, long, and firmly muscled.", "capital of Taiwan", "Harrisburg", "mink mink,", "glockenspiel", "Dr John Sentamu", "national women's soccer team", "Cruella de Vil", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "Charles V", "the community", "Russell Crowe", "Warren G. Harding", "ACC", "Puck", "Samuel Butler", "chamomile tea", "Ireland", "tarn", "SS United States", "Albert Square", "Newbury", "a book of the Old Testament", "70 million people, at that time 21 % of the world's entire population", "Target Corporation", "Sister, Sister (1982 film)", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "international NGO", "Swamp Fox", "funds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "talk show queen Oprah Winfrey.", "his mother"], "metric_results": {"EM": 0.625, "QA-F1": 0.682709478021978}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_searchqa-validation-11802", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.625, "CSR": 0.5909090909090908, "EFR": 1.0, "Overall": 0.7954545454545454}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "primarily along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin,", "Rome", "Robert Peary", "pearls", "Utah", "Carrie Underwood", "Drambuie", "he made his horse a consul, his palace a brothel, and his", "Google", "Langston Hughes", "Jimmy", "harding", "Tito Puente", "rope", "UNFINISHED", "USS LST 325", "rhodesian ridgebacks", "David Beckham", "Arturo Toscanini", "biology", "Miracle", "triumphal arch", "Montenegro", "discus", "thick", "basidiomycota", "James Gandolfini", "Georgia Thomas", "Idi Amin", "blacksmith", "physical", "terracotta", "Plutarch", "Rudy Giuliani", "masa harina", "four quarters", "the Vikings.", "fairfield", "Champs Elysees", "Typhoid Mary", "a valley that has been partially submerged by water near a coastline.", "Munich", "Williamsburg", "The telegraph", "University of Missouri-St. Louis", "heat", "John Knox", "the internal reproductive anatomy", "$657.4 million in North America and $1.528 billion in other countries", "the distribution and determinants of health and disease conditions in defined populations", "jockey", "Tesco", "Mallard", "Graham Hill", "the Battelle Energy Alliance", "IT", "debris", "$10 billion", "Bailey, Colorado,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5142528044871795}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.15384615384615385, 0.625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-1997"], "SR": 0.46875, "CSR": 0.5855978260869565, "EFR": 1.0, "Overall": 0.7927989130434783}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "the Gulf of Mexico", "cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Switzerland", "the Argo", "prometheus", "Altamont", "John F Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "a multi-user real-time virtual world", "Italy", "khaki", "an igneous rock", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "Barack Obama", "the Earth", "Nafea Faa Ipoipo?", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "Influenza", "Fife", "Money Saving", "Adidas", "the Snark", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "the two queen, one queen, two bishops, two knights, and eight pawns", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "The Wall Street Journal Europe", "a fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7396292892156863}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3049", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848"], "SR": 0.6875, "CSR": 0.58984375, "EFR": 0.95, "Overall": 0.769921875}, {"timecode": 24, "before_eval_results": {"predictions": ["illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "the chosen machine model", "Fox", "1997", "a suite of network protocols", "Noriko Savoie", "15", "South Africa inflicted the first home series defeat on Australia in almost 16 years as", "between Pyongyang and Seoul", "killed a man, the latter cheated on his wife.", "11", "fly to Australia.", "Alwin Landry's supply vessel Damon Bankston", "Chaffetz is a conservative Republican married father of three who is sleeping on a cot in his congressional office to save money.", "money or other discreet aid", "Sarah Brown", "it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "environmental", "Italy in the quarterfinals", "Afghan security forces", "Saturday", "38", "70,000 or so", "Climatecare,", "E! News", "former Boca Juniors teammate and national coach", "Steve Williams", "McDonald's", "is a slight girl of 11, living in a simple home in a suburb of Islamabad.", "Pastor Paula White", "2008", "Diego Maradona", "Dog patch Labs Europe", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"  - the central attraction of golf remains at all the film's core.", "two", "Itawamba County School District", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "EU naval force", "Plymouth Rock", "Liza Murphy", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "police", "former U.S. secretary of state.", "At least 33", "five", "improve health and beauty.", "contraband", "that students often know ahead of time when and where violence will flare up on campus.", "Damon Bankston", "Krishna Rajaram,", "Sunday,", "death and destruction", "an Irish feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "london", "radar", "art", "16th season", "23", "Paraguayan Guarani", "freestyle", "the Nightingale Training School", "the Kingdom of the Crystal Skull"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5390556228972176}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.09090909090909091, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6, 1.0, 1.0, 0.16, 0.0, 1.0, 1.0, 0.3333333333333333, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.7272727272727273]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-4673", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-1545", "mrqa_searchqa-validation-3826"], "SR": 0.40625, "CSR": 0.5825, "EFR": 1.0, "Overall": 0.79125}, {"timecode": 25, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.884765625, "KG": 0.48046875, "before_eval_results": {"predictions": ["The programme is listed in Guinness World Records as the longest-running science fiction series of all time", "Thomas Savery", "Vicodin", "Eastern crops", "22,000 years ago", "separatist campaign", "Eleven", "269,000", "The three men entered the E.G. Buehrle Collection", "38 feet", "Eintracht Frankfurt", "150", "a pool of blood beneath his head.", "Russian bombers", "41", "Los Alamitos Joint Forces Training Base", "Wally", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "the shoreline of the city of Quebradillas.", "Russian air force", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "\"several pieces of aircraft equipment were at fault or had broken down.\"", "ensuring that all prescription drugs on the market are FDA approved", "Japanese businessmen", "Tom Baer", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "\"iKini\"", "Brian Mabry", "iTunes, which completely changed the business of music,", "Sunday", "60 euros -- $89 --", "American Civil Liberties Union", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "some truly mind-blowing structures", "first name", "he was one of 10 gunmen who attacked several targets in Mumbai", "2006", "San Diego", "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "Bergdahl, 23, was captured June 30 from Paktika province in southeastern Afghanistan,", "\"BRB,\" \"tweet,\"", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "number one, the Haitian police, number two, the U.N. forces", "heart", "Hyderabad", "between the Mediterranean Sea to the north and the Red Sea", "to stay, abide", "Las Vegas", "Jackson Pollock", "lyrical", "Mississippi", "October 4, 1970", "King Duncan", "Brasstown Bald", "a battleship", "a stride"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4995043276972625}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.07407407407407408, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.1111111111111111, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-1982", "mrqa_searchqa-validation-11832"], "SR": 0.421875, "CSR": 0.5763221153846154, "EFR": 0.9459459459459459, "Overall": 0.7360942372661123}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "Krishna Rajaram,", "25", "Booches Billiard Hall,", "finance", "Ross Perot", "Hong Kong's Victoria Harbor", "2002", "six prostitutes and a runaway involved in the drug trade.", "the legitimacy of that race.", "think about saving the rainforests", "three", "Monday", "Scarlett Keeling", "two years,", "Since 1980, the 84-year-old Mugabe has been the country's only ruler.", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "July for A Country Christmas,", "Akshay Kumar", "Graham's wife", "\"against people who independent of their race, religion, ethnicity, social condition etc. accepted money and put themselves at the service of the army in an area that is the object of military operations.\"", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "James Newell Osterberg", "the death of Prince George's County police Cpl. Richard Findley,", "Phil Spector", "Kim Il Sung", "1994", "numerous suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "dependable Camry", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "in his 60s,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.", "the repeal of the military's \"don't ask, don't tell\" policy", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "6 - 6", "Matt Monro", "William Edward \"Jack\" Frost,", "the innermost digit of the forelimb", "1925", "25 million", "Peoria, Illinois", "Honolulu", "water near the bottom, as deep as 477 m but typically 75-150 m;", "King Lear", "Ottoman Empire"], "metric_results": {"EM": 0.625, "QA-F1": 0.665027854090354}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-5856", "mrqa_searchqa-validation-11586"], "SR": 0.625, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.747265625}, {"timecode": 27, "before_eval_results": {"predictions": ["in the early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "alongside", "bank robber John Dillinger,", "the second missing person", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "Brazil's response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "Lashkar-e-Tayyiba", "$1.5 million", "2006", "Rev. Alberto Cutie", "Angels", "eight Indian army troopers, including one officer, and 17 militants,", "there's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "in a Starbucks", "BADBUL", "98", "2008", "near the Somali coast", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Pakistan's combustible Swat Valley,", "South Dakota State Penitentiary", "Iran", "November 26", "people have chosen their rides based on what their cars say about them.", "in July", "Zoe's Ark", "Four Americans", "Josef Fritzl,", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "Scardia", "fractured pelvis and sacrum", "Wednesday", "abduction of minors", "gun", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn,", "U.S. Vice President Dick Cheney", "19 June 2018", "Flag Day in 1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "apteka", "Vermont's largest city", "beta blockers"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7371181067777163}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.3636363636363636, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705885, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4094", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-436", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-14535"], "SR": 0.59375, "CSR": 0.5786830357142857, "EFR": 1.0, "Overall": 0.7473772321428571}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author", "ownership of private industries", "1981", "forgery and flying without a valid license,", "remark made by his former caddy,", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force.", "North Korea's announcement has triggered international consternation.", "Intel has systematically given PC makers and stores rebates to keep computers with AMD chips off the shelves.", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "13", "the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations -- embraces this technology the same way the public has", "Anil Kapoor.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "The Rosie Show", "Form Design Center.", "collaborating with the Colombian government,", "Christianity and Judaism,", "the Dalai Lama's", "Russia", "8 p.m. local time Thursday", "Passers-by", "For weeks,", "executive director of the Americas Division of Human Rights Watch,", "750", "300", "Matthew Fisher", "The Ski Train", "Boys And Girls alone", "Ozzy Osbourne", "AbdulMutallab,", "some U.S. senators", "inconclusive", "5:20 p.m. at Terminal C", "environmental and political events.", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600", "a million", "Sen. Arlen Specter", "Deutschneudorf,", "bill that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "a deceased organ donor,", "bragging about his sex life", "vertebral column ( spine )", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the South Bank", "Douglas Hofstadter", "The Dark Tower series", "American", "Marmee", "Castle Rock", "anchovy"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7322756745820604}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.6, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.125, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9743589743589743, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_hotpotqa-validation-5376", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.609375, "CSR": 0.5797413793103448, "EFR": 0.96, "Overall": 0.739588900862069}, {"timecode": 29, "before_eval_results": {"predictions": ["downward pressure on wages", "poison", "438,000", "Marty Ingels", "coaxial", "Pakistan A", "Everbank Field", "7 members", "the German Campaign of 1813", "Arabella Churchill,", "1965", "Paris", "fifth", "Culiac\u00e1n, Sinaloa", "seven", "Province of Syracuse", "1963", "coca wine", "puzzle", "Knoxville, Tennessee", "Washington, D.C.", "Oryzomyini", "Tom Kartsotis", "2017", "Wayman Tisdale", "Mexico", "Srinagar", "Northern Ireland", "late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell", "23 March 1991", "Gregg Harper", "Shohola Falls", "small forward", "ARY Films", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey", "Floridians", "Virginia", "NBA Slam Dunk Contest", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "Somerset County, Pennsylvania", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "Richard Parker", "the southernmost tip of the South American mainland", "Charlotteton Heston", "allergic reaction", "Peter Townsend,", "3,000 kilometers (1,900 miles)", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist", "Russia", "shrimp", "Australia"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6897674374236874}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.25, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585"], "SR": 0.609375, "CSR": 0.5807291666666667, "EFR": 1.0, "Overall": 0.7477864583333333}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "effects of deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a cooperative where farmers pool their resources in certain areas of activity", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations.", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Werner Nowitzki", "the Cecil B. DeMille Award", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Hl\u00edn", "86,112", "Celtic", "Ouse and Foss", "the United States and Canada", "British comedian", "Apatosaurus", "1885", "American", "Frank Thomas' Big Hurt", "\"Gliding Dance of the Maidens\"", "Margarine Unie", "Winecoff Hotel fire", "mentalfloss.com", "The Seduction of Hillary Rodham", "2005", "Lambic", "Tom Clancy's The Division", "Argentina", "Larry Alphonso Johnson Jr.", "Mike Mills", "veto power", "Joseph E. Grosberg", "Chelsea Lately", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "New York", "discus thrower", "Villa Park", "2005", "228 people", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6977258158508158}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6399999999999999, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4446", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-672", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.5625, "CSR": 0.580141129032258, "EFR": 1.0, "Overall": 0.7476688508064516}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "Silent Snow, Secret Snow", "Log Ride", "Senator Tom Harkin", "a Van Morrison song", "Nassau", "a gemstone formed by the nacreous inner shell", "HIV", "Thomas Beekman", "a network of rail lines", "Rigoletto", "aardwolf", "Beijing", "Roger Bannister", "Jordin Tootoo", "Death Valley", "Yves Saint Laurent", "antlers", "Fortinbras", "the War of 1812", "Anna Mary Robertson", "a Sailor Moon", "Nevilles Superette", "Harry Bosch", "a bear", "a tornado", "Gilson Lavis", "a hamster", "negative electrode", "Milton Berle", "George Herbert Walker Bush", "Patrice Lumumba", "lunar module", "a Spanish conquistador", "Dan Marino", "Mars", "a clownfish", "E = mc2", "Guru Pitka", "Las Vegas", "millet", "a butterfly", "a Connecticut Yankee", "a chimpanzee", "Baja California", "a catastrophe", "Yitzhak Rabin", "Saul", "Gettysburg", "Jack Gleeson", "Plank", "Buddhism", "Jean Bernadotte", "Portugal", "John Mayall, Cyril Davies, Long John Baldry and Alexis Korner", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\"", "knocking the World Cup off the front pages for the first time in days.", "12.3 million people worldwide"], "metric_results": {"EM": 0.5, "QA-F1": 0.5318080357142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16966", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.5, "CSR": 0.57763671875, "EFR": 1.0, "Overall": 0.74716796875}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "62", "Herbert Henry Asquith", "40", "the Central African Republic", "Shania Twain", "Sheffield Wednesday", "glucagon", "The New York Yankees", "rapid eye movement", "green, red, white", "Ann Dunham", "his cousin, Saddam Hussein", "French", "Jim Branning (John Bardon)", "Ohio", "Francis Matthews", "photographic", "magnetite", "Noah", "London", "New Years Day", "Sarah Ferguson", "Mercury", "The wattage of an electrical component", "Jack Douglas", "Subway", "Madagascar", "Swansea City", "a former hunting lodge", "Rio de Janeiro", "his faith that \"something will turn up\"", "aged 75 or older", "Jennifer Lopez", "1664", "Eurythmics", "Fred Perry", "Downton Abbey", "Martina Hingis", "(Mac)Ahern", "Cyclopes", "The Woodentops", "Michael Miles", "Sheryl Crow", "Gulliver's Travels", "Pomona", "Milan", "Mike Skinner", "the Great Appalachian Valley", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "The Campbell Soup Company", "Kirkcudbright", "the Houston-based company Kellogg Brown and Root,", "the banned substance cortisone.", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "Juno", "a typeface", "lungs"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5695913461538462}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.15384615384615383, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-7689", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-6656", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.484375, "CSR": 0.5748106060606061, "EFR": 0.9696969696969697, "Overall": 0.7405421401515151}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "hay fever", "jim edward stanley", "Ast\u00e9rix", "Brighton", "Belfast", "wind", "fire insurance", "Robin Hood Men in Tights", "West Point", "Andy Warhol", "Spain", "jimmy", "rykjavik", "the solar system", "potatoes", "Moldova", "Mitsubishi A6M Zero", "the Dartford Warblers", "georgia stirling", "Estimate", "clare", "clon", "Pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "Luzon", "beaver", "Mel Blanc", "a dog", "Moffitt", "Ellen Morgan", "phil Woolas", "5000 meters", "racing", "casein", "Newfoundland and Labrador", "crow", "Yellowstone National Park", "St. Francis Xavier", "luzon", "Hugh Laurie", "Buddhism", "Guy Berryman", "Ohio", "Port Melbourne", "\u00c6thelred I", "Scarface", "forgery and flying without a valid license,", "Group D,", "Liza Murphy", "Spock", "Kazakhstan", "France"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6255952380952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-11382"], "SR": 0.578125, "CSR": 0.5749080882352942, "EFR": 1.0, "Overall": 0.7466222426470589}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business", "steppes steppe", "Bologna, Italy", "george Santayana", "opossum", "Alice Cooper", "angiotensin II", "trumpet", "Peter Kay.", "The Cry", "the Stockton & Darlington Railway", "appalachian mountain range", "MS Herald of Free Enterprise", "ballet", "philippine disasters", "philippines", "lizard", "Blackburn", "Frankie Laine,", "The Mystery of Edwin Drood", "pommel horse", "birds", "Dick Van Dyke", "egremont", "numb3rs", "Francisco de Goya", "phrixus", "p Basil Feldman,", "Canada", "ink", "pears soap", "Some Like It Hot", "Mull", "Ireland", "Mike Meyers", "a sea horse", "plutonium", "magma", "Passepartout", "welcome", "Sweden", "Denmark", "shrek", "26 miles", "Cleveland Brown", "heston Blumenthal", "One Direction", "paulin", "Saturn", "stringer", "Charles Lindbergh", "September 2001", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "stoneware", "Pittsburgh", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "tchaikovsky", "Howard Carter"], "metric_results": {"EM": 0.5625, "QA-F1": 0.649156746031746}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-78"], "SR": 0.5625, "CSR": 0.5745535714285714, "EFR": 1.0, "Overall": 0.7465513392857143}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "John Forster III", "Matlock", "American Civil War", "shoa", "cetaceans", "Arafura Sea", "armageddon", "the Euphrates River", "b Bavarian", "to make wrinkles in one's face", "Spain", "carousel", "bullfighting", "smith brady bunch", "tenor", "alpo", "flore", "Guys and Dolls", "Julian Fellowes", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "german", "armageddon", "L. Pasteur", "jean feldman", "rachmaninoff", "Finland", "stars", "Mille Miglia", "cave bug", "Bill feldman & his comets", "spain", "muriel Spark", "happy birthday to You", "seven", "opossum", "pickwick", "presliced bread", "Saga Noren", "raven", "jordan", "guinea", "jane rainford", "the Etruscan army", "ken Burns", "grosvenor hill", "Heather Stanning and Helen Glover", "Pyotr Ilich Tchaikovsky", "Mujib,", "Sagittarius", "Donna", "season four", "the atrioventricular node", "Yubin, Yeeun", "tomato", "2002", "problems with the way Britain implements European Union employment directives.", "the Civil Protection Authority.", "March 24,", "spain mountbatten", "equinox", "Pocahontas"], "metric_results": {"EM": 0.453125, "QA-F1": 0.546875}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_naturalquestions-validation-1091", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.453125, "CSR": 0.5711805555555556, "EFR": 0.9428571428571428, "Overall": 0.7344481646825397}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "kim", "city of acacias", "branson", "Gordon Ramsay", "Manchester City", "joan feldman", "sulfur dioxide", "Margot Betti", "Manchester Airport", "Portuguese", "travelocity", "The Avengers", "Richmondshire", "comets", "tom and Jerry", "a ghost", "canola", "Tina Turner", "Benjamin Barker", "duttlenheim", "Bolivia", "John Donne", "Uranus", "Rio Grande", "Percheron", "The Graduate", "pure Pearls", "ginger Rogers", "king James I", "One Foot in the Grave", "Bronx Mowgli", "Bob Anderson", "George Santayana", "Finger Tab", "Borrowdale", "Wee Jimmy Krankie and his father", "Tomas De Torquemada", "Daniel Barenboim", "Canada", "rum", "seattlepi.com", "ghee", "George III", "joan crawford", "Hyperbole", "newpatrick", "June", "David Graham", "Ceylon", "screwdrivers", "Kansas City Chiefs", "G minor", "A Christmas Story", "1974", "The Outsiders", "Amberley Village", "lack of a cause of death and the absence of any soft tissue", "President Obama", "kerstin and the rest of the family", "Cixi", "Brigham Young", "pearl", "chalk quarry"], "metric_results": {"EM": 0.5, "QA-F1": 0.5677455357142858}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-4108", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120"], "SR": 0.5, "CSR": 0.5692567567567568, "EFR": 0.9375, "Overall": 0.7329919763513514}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen )", "piped masonry", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist James Hutton", "N 17 \u00b0 26 \u2032 34 ''", "joy of living", "420", "John Prine and Roger Cook", "sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "London", "Los Angeles", "February 10, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "By 1770 BC", "Niveditha, Diwakar, Shruti", "two", "John C. Reilly", "cell nucleus", "Anakin", "Travis Tritt and Marty Stuart", "1976", "the Bee Gees", "Matt Czuchry", "Pradyumna", "1902", "Isle Vierge", "Psychomachia", "the New Jersey Devils", "two", "7.6 mm", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria", "the Canadian Rockies", "The Maginot Line", "France", "dumbo", "purple", "Charles Guiteau", "Gettysburg Address", "iTunes", "$273 million", "India", "Al Nisr Al Saudi", "Desperate", "Captain chaos", "Morelos", "Tuesday"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6364583965427156}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.5454545454545454, 0.3636363636363636, 1.0, 1.0, 1.0, 0.6363636363636364, 1.0, 1.0, 0.8, 0.4615384615384615, 0.0, 0.0, 0.0, 0.787878787878788, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07272727272727272, 0.25, 0.5454545454545454, 1.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-9735", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.46875, "CSR": 0.5666118421052632, "EFR": 0.9705882352941176, "Overall": 0.7390806404798761}, {"timecode": 38, "before_eval_results": {"predictions": ["Isaac Newton", "25 years after the release of their first record", "the United States", "Kim Basinger", "fall of 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in positions Arg15 - Ile16", "Charles Crozat Converse", "Lady Gaga", "the Chicago metropolitan area", "the President of the United States", "Domhnall Gleeson", "eusebeia", "horticulture", "Notts County", "nobiliary particle", "Stephen A. Douglas", "1984", "a loanword of the Visigothic word guma `` man ''", "Pakistan", "21 February", "Tagalog", "Bryan Cranston", "the thylakoid membranes", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "Franklin and Wake counties", "December 1922", "18 Divisional Round", "520", "stable, non-radioactive rubidium - 85", "between $10,000 and $30,000", "September 1980", "1931", "the `` Holy Club '' at the University of Oxford,", "Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "`` Southern Cause ''", "Randy", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "a divergent tectonic plate boundary", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "mrs Sir John Major", "roddy doddy dine", "Daniil Shafran", "TD Garden", "Venus", "starting a dialogue while maintaining sanctions,", "10 below", "General Motors'", "David McCullough", "Rendezvous with Rama", "CERN", "saudade"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6000065253374077}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.09523809523809525, 0.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.4, 0.0, 0.6666666666666666, 0.32, 1.0, 1.0, 0.1, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-5582", "mrqa_newsqa-validation-3486", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.53125, "CSR": 0.5657051282051282, "EFR": 0.9666666666666667, "Overall": 0.738114983974359}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale : Sammy's Adventures", "Jenny Slate", "ATP, generated by the root respiration", "Philippe Petit", "September 1980", "January 2004", "provinces along the Yangtze River and in provinces in the south", "Toby Keith", "development of electronic computers in the 1950s", "17 - year - old", "punk", "Set six months after Kratos killed his wife and child, he has been imprisoned by the three Furies for breaking his blood oath to Ares", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "( XXXX )", "the Berlin School of experimental psychology", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Skylar Astin", "Richard Crispin Armitage", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "either late 2018 or early 2019", "diffuse interstellar medium ( ISM ) of gas and dust", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "McKim Marriott", "irstjen Nielsen", "Charles Sherrington", "1886", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "September, 2016", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "1940", "Ariel Winter", "Mark Jackson", "Michael Buffer", "one faith, one baptism, one God and Father of all, who is over all and through all and in all", "on location", "the federal government", "New England", "Cody Fern", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "about 0.04 mg / L several times during a day", "Juan Manuel de Ayala", "Joseph Smith,", "doy", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew", "Pearl", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.5, "QA-F1": 0.6358941104166842}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4210526315789474, 0.967741935483871, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.17142857142857143, 1.0, 0.0, 0.0, 1.0, 0.11764705882352941, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302"], "SR": 0.5, "CSR": 0.5640625, "EFR": 0.96875, "Overall": 0.738203125}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "eight Indian army troopers, including one officer, and 17 militants,", "Joan Rivers", "education about rainforests", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "Zimbabwe President Robert Mugabe", "since 2004", "NATO", "Switzerland", "Monday", "second", "shows Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "\"To improve America's competitiveness, the senator from Illinois said he wants to spend $10 billion on childhood education, $150 billion over 10 years on developing alternative energy", "Clifford Harris,", "Ensenada, Mexico", "Robert Barnett,", "a class A traffic violation that can command a fine of $627,", "41", "Nick Adenhart", "a strict interpretation of the law,", "Derek Mears", "Nieb\u00fcll", "on 112 acres about 30 miles southwest of Nashville,", "Tuesday afternoon", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "on the headstones to show that a visitor had been to the grave.", "Ali Bongo", "Mandi Hamlin", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Derek Mears", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities.", "East Java", "St. Louis, Missouri.", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2019", "P.V. Sindhu", "Mexico", "Snickers candy bars", "monoceros", "capone", "Anaheim, California", "uncle", "Bergen", "embalming", "Fernando Vargas", "a graphical user", "The long - hair gene is recessive, making the long- hair variety rarer"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6902940316083699}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.37500000000000006, 0.08888888888888889, 1.0, 0.25, 1.0, 0.19999999999999998, 1.0, 1.0, 0.29629629629629634, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 0.3076923076923077, 0.4, 1.0, 1.0, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.484375, "CSR": 0.5621189024390244, "EFR": 1.0, "Overall": 0.7440644054878048}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s", "Stephen A. Douglas", "1997", "displacement", "the modern state system", "Megan Park", "The euro", "Kate Walsh", "September 14, 2008", "Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point", "increases the life of the pump, allows a smaller and lighter device to be used, and reduces electrical load", "international aid as one of the largest financial inflows to developing countries", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "In England, births were initially registered with churches, who maintained registers of births", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "collect menstrual flow", "pigs", "General George Washington", "Spanish", "Virgil Tibbs", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Jos Plateau", "Missouri River", "the right to vote", "frontal lobe", "10 June 1940", "at Tandi, in Lahaul", "Alberich", "ear", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "Kurdish Region of Iraq", "Denver, Colorado.", "a district advisory council, a neighborhood group that looks at local needs and passes on its assessments to the provincial government.", "CTU", "King Arthur", "Deal or No Deal", "Virgin America"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6815820621726951}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 1.0, 0.17391304347826086, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5205"], "SR": 0.59375, "CSR": 0.5628720238095238, "EFR": 0.8461538461538461, "Overall": 0.713445798992674}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "Water extinguishment", "in Middlesex County, Province of Massachusetts Bay", "chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "its population", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "to bring", "Hermann M\u00fcller", "Ceramic art", "Russia", "Covington, Kentucky", "New Mexico", "to condense the steam coming out of the cylinders or turbines", "December 15, 2017", "Paradise, Nevada", "L.K. Advani", "differential erosion", "Glenn Close", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "compasses", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Daya Jethalal Gada", "2,140 kilometres ( 1,330 mi )", "by producing an egg through parthenogenesis", "1926", "Durban, South Africa", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Alfheim", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "won two", "prostate cancer,", "wyvern", "Little Lord Fauntleroy", "a key", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6696790276368765}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5384615384615384, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 0.9, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.14814814814814814, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.9767441860465117, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.515625, "CSR": 0.5617732558139534, "EFR": 0.8709677419354839, "Overall": 0.7181888245498874}, {"timecode": 43, "before_eval_results": {"predictions": ["2003", "February 27, 2007", "a'pick yourself up and dust yourself off and keep going ', female - empowerment song", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "le Roi d'Irlande", "Miami Heat", "1981", "As late as the 1890s, building regulations in London did not require working - class housing to have indoor toilets", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Louis Leloir", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "rootlets", "Alex Ryan", "a habitat", "2018", "Advanced Systems Format ( ASF )", "100", "Toledo", "embryo", "the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "in Paradise, Nevada", "Alicia Vikander", "annually in late January or early February", "Ashoka", "the name of a work gang", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Bumblebee", "into the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "AMX - 50", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "2018", "Florida", "American comedy web television series", "in the southwestern part of the island", "considered to be unfair", "winter", "the North Atlantic Ocean", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\"", "\"We want to reset our relationship and so we will do it together.'\"", "Flint, Michigan", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5813419040636782}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7741935483870968, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.32, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9390", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977"], "SR": 0.453125, "CSR": 0.5593039772727273, "EFR": 0.9428571428571428, "Overall": 0.732072849025974}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "lightweight aluminum foil", "Laurel, Mississippi", "about the outdoors", "Indianola", "life insurance", "a British military on suspicion of being an American sympathizer in the American Revolutionary War.", "1992", "Cher", "Alabama", "Jim Harrison", "Montreal", "Tomorrowland", "fennec fox", "United States Army", "stop motion animation", "Jean Acker", "5,656", "Leucippus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "the Northrop F-15 Reporter", "Adelaide", "World Famous Gold & Silver Pawn Shop in Las Vegas", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony awards", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "OutKast", "Richard Street", "Zaire", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "French cuisine", "South America", "2006", "perjury and obstruction of justice", "Operation Neptune", "Mary Elizabeth Hartman", "over 9,000 employees", "John Nightingale", "potential of hydrogen", "Alamodome in San Antonio", "Carrie", "The Finger Tab", "Kent", "almost 9 million", "Bahrain", "2008", "'lying under oath'", "Moses", "Chapter 5", "Wilson Pickett"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6144039987789989}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.28571428571428575, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-993", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.484375, "CSR": 0.5576388888888889, "EFR": 1.0, "Overall": 0.7431684027777778}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "1858", "Australian", "1903", "interstate commerce", "Naomi Wallace", "(born 19 January 1980)", "Tufts College", "the People's Republic of China", "Azeroth", "Squam Lake", "Philip Livingston", "Tayeb Salih", "King James II of England", "God Save the King", "526", "Scotland", "A hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "Gesellschaft", "Mick Jackson", "Lalit", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "Sully", "Manhattan Project", "Pacific War", "Romantic", "Hugh Dowding", "AMC Entertainment Holdings, Inc.", "New York Islanders", "fennec", "1978", "a constructor in Formula One, Formula 2 and Formula 5000", "French", "Pacific Place", "the Female Socceroos", "is a song by American singer-songwriter Taylor Swift,", "\"SexyBack\"", "about 5320 km", "Francesco Maria Piave", "\"Super Hit\"", "Sacramento Kings", "Walldorf, Baden-W\u00fcrttemberg", "Fife", "Fyvie Castle", "Faisal Qureshi", "the British Army", "power directly or elect representatives from among themselves to form a governing body, such as a parliament", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Frances Ethel Gumm", "Switzerland", "Model A", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Maroon 5", "Saudi Arabia", "Geraldine A. Ferraro", "two"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6238162878787878}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-3592", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-8327"], "SR": 0.53125, "CSR": 0.5570652173913043, "EFR": 0.9333333333333333, "Overall": 0.7297203351449275}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican National Committee's website address is GOP.com", "1996", "5", "Greenland sharks", "The Word", "President Abraham Lincoln", "St Jude Thaddeus", "Anthoonij van Diemenslandt", "the death penalty", "xerophyte", "Jackie Robinson", "Staten Island", "Dian Fossey", "MI5", "Harrow", "creme anglaise", "onions", "pork", "curling", "Victoria Coren", "Gettysburg", "Chile", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "Barack Obama", "Canada's Liberal Party", "Bologna Song Lyrics - Daniel Bedingfield", "the Dominican Republic", "isle ring", "Stephen King", "Hinduism", "caryatid", "feet", "most of its land in North America", "Mary Poppins", "glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Sun", "(1939\u20131945)", "food that is permissible according to Islamic law", "beginning in 2016", "the courts", "2017", "Chief of Protocol", "Diamond White", "1944", "Daniel Nestor, from Canada,", "Jeddah, Saudi Arabia", "death", "Beatrix Potter", "Dan Eggen and Elizabeth Williamson", "Reader's Digest", "king"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6032118055555555}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-6488"], "SR": 0.546875, "CSR": 0.5568484042553192, "EFR": 1.0, "Overall": 0.7430103058510639}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "david Beckham", "florida", "Runic", "florida", "cricket", "geheimrat Dr. Max   Planck", "rotherham United", "heat transfer", "misery", "Styal", "stately", "blind beggar", "Mr Brainwash", "Leroy Burrell", "parlophone", "Wild Atlantic Way", "john Denver", "Ankh-Morpork", "noddy", "Lackawanna Six", "Brazil", "a Tree Swing", "muezzin", "a window", "a ship", "realist", "Apollo 11", "flit", "a \"Radio City\"", "oscar hurdle", "evita", "sperm whale", "a shrewd business man", "East Fife", "st Pancras International Station", "social environment", "presliced bread", "Dilbert", "a drunken fit", "nunc dimittis", "French", "medea", "eastern France", "cribbage", "w/e 5th Feb 2005", "Johannesburg", "France", "muffin man", "South Korea", "Prince James, Duke of York and of Albany", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Veracruz, Mexico", "carrier based in Texas.", "Robert Frost", "King Henry VIII", "Pillsbury", "Mitsubishi Lancer OZ Rally"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6414930555555556}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-4781", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.609375, "CSR": 0.5579427083333333, "EFR": 1.0, "Overall": 0.7432291666666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Route sixty-six", "sesame Street", "tomatoes", "cabbage", "Tasmania", "jimmy", "fleece", "Ash tree", "opossum", "New Zealand", "jug band", "60", "auguste", "1984", "frogs", "kara-Khitan", "1875", "tax collector", "pennies", "rodd Stewart", "Wars of the Roses", "bagram Collection Point", "jenkins", "Chrysler", "fur hat", "korky the cat", "arts", "states", "Brazil", "pei Tang", "biathlon", "nampa", "Charlie Chan", "Vienna", "white", "jaws", "Paul Rudd", "dwarf", "Scotland", "jerry", "Orson Welles", "hindu Wisdom", "menorah", "post-impressionist", "texas", "Super Bowl Sunday", "a quant pole", "Little Tommy Stout", "jonkins", "azalea", "Ireland", "Chuck Noland", "the Colony of Virginia", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park", "2010", "in a ceremony at the ancient Greek site of Olympia", "10 below", "100 to 150", "turtles", "the American Kennel Club", "Omaha", "George Jones"], "metric_results": {"EM": 0.453125, "QA-F1": 0.553125}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-263", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-726", "mrqa_naturalquestions-validation-4803", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.453125, "CSR": 0.5558035714285714, "EFR": 0.9714285714285714, "Overall": 0.7370870535714286}, {"timecode": 49, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.859375, "KG": 0.46484375, "before_eval_results": {"predictions": ["quin Ivy", "Iran", "alcohol", "frenchman", "dutch", "Daniel Boone", "Thames Street", "jennifer roosevelt", "satyrs", "crabs", "la boheme", "dutch", "wishbone", "garrick club", "Lackawanna 6", "steward of two Cities", "jubilee year", "the American Civil War", "dark", "jennifer cloris leachman", "Jimmy Robertson", "Florence", "saint Basil", "veruca salt", "shrewsbury", "australia", "South Africa", "bunch grasses", "guinea", "Churchill", "wars of the Roses", "chemnitz", "dutch", "trout", "ap\u00e9ro", "jennifer adan", "guinea", "American Folk Song", "hair loss", "sprint", "charlie darlings", "robin hood", "Chris Martin", "flinstone", "jennifer deed", "rugby", "honda", "steely Dan", "11", "tobacco", "heifer", "free floating", "Tom Selleck", "New Orleans", "comic book superhero", "Texas Tech University", "loughborough, Leicestershire, in the East Midlands of England", "Herman Cain", "the United States", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "George Babbitt", "Oklahoma", "vodka", "four"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4306687801932367}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.22222222222222224, 1.0, 0.0, 0.17391304347826086, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-5556", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.359375, "CSR": 0.551875, "EFR": 0.9512195121951219, "Overall": 0.7224939024390243}]}