{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2190, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "the Grand Annual Steeplechase at Warrnambool", "Paleoproterozoic", "the end", "1894", "French Rhin", "the Pacific", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "from \u00a315\u2013100,000", "the Purus Arch", "the infected corpses", "the United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "outrage from child protection and parental rights groups", "the Masovian Primeval Forest", "the days, weeks and months after it happened", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "an algorithm for multiplying two integers can be used to square an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "Mr Ratti's solvent and varnish business", "More than 1 million", "2011", "by the market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Buddhism", "Matthew 16:18", "the U.S. ship that was hijacked off Somalia's coast", "Wwanda", "the three-day festival has been canceled", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7355552433677434}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-7566", "mrqa_squad-validation-9243", "mrqa_squad-validation-7763", "mrqa_squad-validation-2732", "mrqa_squad-validation-4276", "mrqa_squad-validation-7728", "mrqa_squad-validation-2520", "mrqa_squad-validation-2035", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.640625, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50% of the population to die", "mountainous areas", "the coast of Denmark", "quantum mechanics", "On Tesla's 75th birthday in 1931", "Distinguished Service Medal", "30", "Virgin Media", "the destruction of Israel and the establishment of an Islamic state in Palestine", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "presidential representative democratic republic", "allows those tainted by sin to nevertheless make a truly free choice to accept or reject God's salvation in Christ", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "the compilation of a vast institutional compendium named Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two-page", "Arizona Cardinals", "1991", "Chaffee", "Isiah Bowman", "the poor", "100\u2013150", "Executive Vice President of Football Operations", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "In the coming decades, pharmacists are expected to become more integral within the health care system", "declare martial law and sent the state militia to maintain order", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Robbie Williams and Liam Gallagher", "Johnsonkip", "the company's factory in Waterford City, Ireland", "nitrogen", "Annemarie Moody", "Haraboard and the Agulhas current off the south and east coast of  southern Africa.", "six", "It always begins with the music, of course. The tune sticks with you long after the song is over; the sort of tune that makes it almost impossible to sit still.", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7576471079596079}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16216216216216214, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5067", "mrqa_squad-validation-1637", "mrqa_squad-validation-9600", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-9805", "mrqa_squad-validation-8159", "mrqa_squad-validation-7949", "mrqa_squad-validation-235", "mrqa_squad-validation-378", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.6875, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "housing projects", "Sakya", "monumental size", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "two of Triton's daughters set out on a journey through the depths of the oceans and seas.", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President", "2000", "oxygen", "increase local producer prices by 20\u201325%", "the Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "multiple revisions", "50 fund", "integer factorization problem", "necessity", "Isel", "The Huguenots adapted quickly and often married outside their immediate French communities", "Benazir Bhutto", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "potential energy stored in an H+ or hydrogen ion gradient", "the late 19th century", "the Channel Islands", "in no way", "Alberich", "9", "Emeril Lagasse", "Churchill Downs", "The port of Terneuzen", "travis", "ireland", "Colombia", "study insects and their relationship to humans, other organisms, and the environment", "the different glomeruli", "travis", "George Fox", "ireland", "Great Expectations", "24 hours a day and 7 days a week", "Sponsorship scandal", "\"Krabby Road\""], "metric_results": {"EM": 0.6875, "QA-F1": 0.7223958333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-9286", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-3069", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_hotpotqa-validation-3821"], "SR": 0.6875, "CSR": 0.7109375, "EFR": 0.95, "Overall": 0.83046875}, {"timecode": 4, "before_eval_results": {"predictions": ["in plants that contain them", "Parliament of Victoria", "Inigo Jones", "Fort Edward and Fort William Henry", "Science and Discovery", "the Army", "pedagogy", "the red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "Battle of Olustee", "at the port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover in the basin", "an wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related diseases", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "mike atherton", "22", "terror groups that they say were planning numerous suicide attacks", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "a bit more disposable income", "a Muslim background", "auction off one of the earliest versions of the Magna Carta later this year", "a unit of Time Warner", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "mike atherton", "Julissa Brisman", "one", "mike atherton", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "Dante Gabriel Rossetti"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6744743185735833}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.15999999999999998, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-5626", "mrqa_squad-validation-10247", "mrqa_squad-validation-7094", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.640625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a data network based on this voice-phone network", "roughly 500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "a way of reminding their countrymen of injustice", "June 1978", "Milton Latham", "1914", "the Philippines", "the Broncos", "the 1970s", "the conquered indigenous populations", "German Te Deum", "1795", "Bermuda 419", "air could be liquefied, and its components isolated, by compressing and cooling it", "the Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "a rudimentary immune system, in the form of enzymes that protect against bacteriophage infections", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "the Secretary of State and ambassadors are appointed by the President, with the advice and consent of the Senate", "It is typically found on laptops due to their keyboard size restrictions", "The term monkey's uncle, most notably seen in the idiom", "Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress", "the rate of angular rotation \u03c9, and the speed of the cannonball ( assumed constant ) v, the correct angle \u03b8 to aim so as to hit the target at the edge of the turntable", "Panning", "Justin Timberlake", "many countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "the peacekeeping force is 98,200 police, troops, and military experts", "unknown origin", "a yellow background instead of a white one", "Lowe's opened its first three stores in Canada on December 10, 2007, in Hamilton, Brampton and Brantford", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing ) and a storyline take place at a set time in the past ;", "Morgan Freeman reprising his role as detective Alex Cross", "David Gahan ( / \u0261\u0251\u02d0n / ; born David Callcott ; 9 May 1962 ) is an English singer - songwriter", "the Overlook Hotel in his 1977 bestseller The Shining and its 1980 film adaption of the same name, as well as the location for the 1997 miniseries", "a long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "garth", "Jaipur", "Jonas Olsson,", "\"torpedo boat destroyers\"", "garth"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6874312172498125}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.56, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.7200000000000001, 0.1111111111111111, 0.15384615384615385, 0.0, 0.14285714285714288, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 0.3636363636363636, 0.631578947368421, 0.08333333333333334, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-9908", "mrqa_squad-validation-3473", "mrqa_squad-validation-6450", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.5625, "CSR": 0.6744791666666667, "EFR": 0.9642857142857143, "Overall": 0.8193824404761905}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "always more expensive", "an antigen from a pathogen", "their disastrous financial situation", "a Serbian Orthodox priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "social and political action", "1936", "New Birth", "gold", "a deficit", "the Costiff collection of 178 Vivienne Westwood costumes", "reciprocating Diesel engines", "disease", "the \"TFIF\" block", "Confucian propriety and ancestor veneration", "\"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "members, conducting hearings into allegations of professional misconduct and taking appropriate disciplinary action and accrediting teacher education programs", "at the end of the season", "10", "Jonas", "murdering African-Americans", "will not support the Stop Online Piracy Act", "David Duchovny", "hot and humid and it rains almost every day of the year", "an animal tranquilizer", "10.1", "Sunday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "North Korea", "first five Potter films", "that you know what is important in life, and it's not your car.", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "a \"stressed and tired force\" made vulnerable by multiple deployments", "James Whitehouse", "we want to ensure we have all the capacity that may be needed over the course of the coming days", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "John Emburey", "Colgate University", "a personal healing in...", "a fat or fatty acid in which there is at least one double bond within the fatty acid chain", "New Testament"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6652348793384978}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.5, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.45454545454545453, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7114", "mrqa_squad-validation-5441", "mrqa_squad-validation-3370", "mrqa_squad-validation-6001", "mrqa_squad-validation-486", "mrqa_squad-validation-3390", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-1267", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.5625, "CSR": 0.6584821428571428, "EFR": 1.0, "Overall": 0.8292410714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "reduced wages", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "P = PSPACE", "July 1969", "when Hitler's secret police demanded to know if they were hiding a Jew in their house.", "a yellow chlorophyll precursor", "spontaneous", "the courts of member states", "gold", "a Time Lord", "Buckland Valley", "Scottish rivers", "\"Bricks for Warsaw\"", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "\u00a320,427", "21 October 1512", "James O. McKinsey", "\"Dance Your Ass Off.\"", "\"Freshman Year\" experience", "India", "Benazir Bhutto,", "at the Lindsey oil refinery in eastern England.", "April 24 through May 2.", "Krishna Rajaram", "early detection and helping other women cope with the disease.", "250,000", "Timothy Masters", "homicide by undetermined means", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jason Chaffetz", "United States Holocaust Memorial Museum, The American Academy of Diplomacy and the United States Institute of Peace.", "\"Dance\"", "military trials for some Guant Bay detainees.", "Matthew Fisher", "Cain", "9 a.m.", "the Frank case seemed to press every hot-button issue of the time: North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "seeking help", "Japanese officials", "a month of training", "\"Empire of the Sun,\"", "the Norman given name Robert", "for the Olympics.", "Matt Winer", "a 1946 Western film directed by John Ford and starring Henry Fonda as Wyatt Earp", "opposite R\u00fcgen island", "Mustique", "green"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6721983426601992}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 1.0, 0.9375, 1.0, 1.0, 0.5333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.3636363636363636, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.10256410256410256, 0.0, 0.6666666666666666, 1.0, 0.7058823529411764, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-3938", "mrqa_squad-validation-7587", "mrqa_squad-validation-1556", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_hotpotqa-validation-4367", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858"], "SR": 0.546875, "CSR": 0.64453125, "EFR": 0.9655172413793104, "Overall": 0.8050242456896552}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "that any French residents who chose to remain in the colony would be given freedom to continue worshiping in their Roman Catholic tradition, continued ownership of their property, and the right to remain undisturbed in their homes.", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe.", "Enric Miralles", "25-foot", "eight", "Tuesday", "Journey's End", "immediate", "Levi's Stadium", "decidedly Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "in variety of ways", "Aristotle and Archimedes", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington", "autonomy", "Islamic", "$12.9 million", "Fernando Gonzalez of Chile", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie business.", "finance", "terminal brain cancer", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "the Employee Free Choice act", "both were separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman -- to whom she was then married", "Animal Planet", "crashing his private plane into a Florida swamp.", "there were no radar outages and said it had not lost contact with any planes during the computer glitches.", "54 bodies", "early detection", "Diversity", "$250,000", "make sure water continues flow through the river channel and not spread out over land.", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Briton Carl Froch", "The Everglades", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans, Louisiana", "many investors paying huge sums for individual bulbs", "MIBs", "olympic games", "It is on the line for the game Wednesday night for this team and the St. Louis Blues"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6673832137647927}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.6538461538461539, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.21428571428571427, 1.0, 0.7368421052631579, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.07407407407407408, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-6300", "mrqa_squad-validation-10341", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.578125, "CSR": 0.6371527777777778, "EFR": 1.0, "Overall": 0.8185763888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "the General Sejm", "Derek Jacobi", "net force", "\"deed\", \"coo\", \"hoos\"", "50%", "very badly disposed towards the French, and are entirely devoted to the English.", "California", "CRISPR", "six", "about 300 km", "1962", "free radical", "Video On Demand", "issues related to the substance of the statement.", "the Edict of Fontainebleau", "15", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "Ronaldinho", "any kind of engagement with the Taliban -- either as part of NATO or bilaterally -- would have much worse long-term consequences.", "an average of 25", "a treadmill", "the couple's surrogate lost the pregnancy.", "environmental and political events.", "he fears a desperate country with a potential power vacuum that could lash out.", "at least two and a half hours.", "Elin Nordegren", "New York City", "6,000", "a medicine that contained the banned substance cortisone.", "President Clinton.", "purchasing the machine guns and silencers from an undercover Bureau of Alcohol, Tobacco, Firearms and Kanye West. Harris won two awards.", "Morgan Tsvangirai.", "\"The discovery of millions of extra ballots proves that President Robert Mugabe intends to rig next week's elections in his favor,", "future relations between the Middle East and Washington.", "a canyon", "South African ministers and the deputy president", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.", "posting a $1,725 bail,", "school", "strife", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "they did not know how many people were onboard.", "London", "a toothbrush", "an individual.", "William Tell", "Andr\u00e9 3000", "Groundhog Day", "Cleopatra,", "a fairground"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6414310515873016}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.742857142857143, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.07692307692307693, 0.0, 0.09523809523809523, 0.0, 0.25, 0.923076923076923, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-2429", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-11812"], "SR": 0.5625, "CSR": 0.6296875, "EFR": 0.9285714285714286, "Overall": 0.7791294642857143}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "the Cathedral of Saint John the Divine.", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever", "DC traction", "The Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "within the premises of the hospital.", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "the membrane of the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Lothar de Maizi\u00e8re", "33", "chairman and CEO", "Brazil", "July 18, 1994", "pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "heavy snow and ice", "Willem Dafoe", "\"Maude\"", "Phillip A. Myers.", "general astonishment", "two weeks after Black History Month", "58", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Isaac, and daughter, Rebecca.", "Lance Cpl. Maria Lauterbach", "Hyundai Steel", "Sharp-witted. Direct. In control. Loyal.", "Hu Jintao", "GospelToday", "second- and third-degree burns over about two-thirds of his body,", "October 3", "Adriano", "Larry Zeiger", "shock,", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Guerrero", "bankruptcies", "16 August 1975", "Bonnie Aarons", "one", "Kabinett", "Lionsgate.", "James Lofton", "is popularly known as becoming one with God or the Absolute,", "hair-like structures"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6218415775401069}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.47058823529411764, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7230", "mrqa_squad-validation-1299", "mrqa_squad-validation-3235", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.546875, "CSR": 0.6221590909090908, "EFR": 1.0, "Overall": 0.8110795454545454}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking", "The combination of hermaphroditism and early reproduction", "Victoria Department of Education", "seized", "Manned Spacecraft Center", "higher economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.", "Elway", "Philo of Byzantium", "36 acres", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "Euler's totient function", "a better relevant income", "Redwood City, California", "400 m", "Netherlands", "David Copperfield", "pink mice", "antelope", "nipples", "the Precambrian period", "a co-  operative", "Anastasia Dobromyslova", "Wikia", "9", "Space Jam 2", "Radish", "Robert Ludlum", "magical", "a golf term", "The Donington Grand Prix Collection", "Saturday Night Live", "Hebrew", "The London Underground Piccadilly Line", "Canada", "orangutan", "Manet", "Roald Dahl", "Wyoming", "2005", "1971", "DodgeDodge", "Benny Hill", "Venice", "a peplos", "Enrico Caruso", "Elizabeth Arden", "a collapsible support assembly", "Sir Hardy Amies", "Antigua and Barbuda", "England and Wales", "Kylie Minogue", "Cody Miller", "Bloomingdale Firehouse", "The Islamic republic's alleged efforts to acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "the Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Anzio", "Buddhism"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6208270010131712}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7234042553191489, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5142857142857142, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983", "mrqa_searchqa-validation-13120"], "SR": 0.5625, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.80859375}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "90-60's", "Panini", "Bills", "anti-colonial movements", "glacial", "protein A", "test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "if the EU does not comply with its basic constitutional rights and principles (particularly democracy, the rule of law and the social state principles)", "1788", "in 2006", "Roman Catholic", "Henry of Navarre", "John Wesley", "the Italian nationalisation law conflicted with the Treaty of Rome", "the Eternal Heaven", "largest Wind Turbine", "Simpson,", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "iceland", "Sir Hugo Drax", "Vladivostok", "Shirley Bassey", "iceland", "Camellia sinensis", "AFC Wimbledon", "Bob Monkhouse and Kenneth Connor", "Malaysia", "astronomy", "gin", "George Clooney", "Eric Coates", "John Dalton", "\"Ah, look at all the lonely people.\"", "Monopoly", "champagne", "a pluvial was an extended period of abundant rainfall lasting many thousands of years.", "the United States", "Brigit Forsyth", "london", "kanji", "a guy named Troilus and a girl named Cressida", "David Hogarth", "Kent", "london", "Vanguard", "white", "Switzerland", "gin", "Fr\u00e9d\u00e9ric Auguste Bartholdi", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Jason Voorhees", "German state and municipal budgets", "David", "\"The Screening Room\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.5784491298352186}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8387096774193548, 0.7272727272727273, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9126", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-5827", "mrqa_squad-validation-4590", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_naturalquestions-validation-594", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_newsqa-validation-3207", "mrqa_searchqa-validation-8450", "mrqa_newsqa-validation-3860"], "SR": 0.515625, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.8046875}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "Retaliating against the French Catholics, the Huguenots had their own militia", "after the end of the Mexican War", "61", "the quality of a country's institutions and high levels of education", "cilia", "friction", "Sky Digital", "2005", "the concept of force", "mustelids", "John Connally", "saffron", "HYMENAEUS", "Zeus", "albinism", "Straits of Tiran", "Brigit Forsyth", "call My Bluff", "March 10, 1997", "cuddly new pet", "the Battle of Austerlitz", "Velazquez", "Arthur Ashe", "lizards", "strong cold southwest wind", "table tennis", "medical treatise", "penhaligon", "Gandalf", "thaddeus sholto", "Jinnah International", "Monday", "Caracas", "rosary", "soap", "Cocktail", "avro Lancaster", "renoir", "Charlie Brooker", "herbal teas", "harrods", "2007", "cher", "Scarface", "pale yellow", "renoir", "bubba", "June 12, 2018", "Filipino", "London", "Lambic", "Nook", "Steven Green", "commas", "fortune", "principality", "Synchronicity"], "metric_results": {"EM": 0.546875, "QA-F1": 0.621577380952381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_squad-validation-2875", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-6628"], "SR": 0.546875, "CSR": 0.6049107142857143, "EFR": 1.0, "Overall": 0.8024553571428572}, {"timecode": 14, "before_eval_results": {"predictions": ["seven months", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep after it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "they were at least partly the product of a declining state of mind", "1898", "The Deadly Assassin and Mawdryn", "in outer space", "Cody Fern", "Nicklaus", "Jim Gaffigan", "Space is the Place", "2028", "1974", "332", "1936", "Authority", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "World Trade Center", "Kevin Spacey", "1 November", "78", "the main type of cell found in lymph", "Bangladesh", "the President", "minor key", "The Hustons", "Chandan Shetty", "sedimentary rock", "October 1, 2014", "the United States", "Claims adjuster", "neck", "Darlene Cates", "Atlanta, Georgia", "the homicidal thoughts of a troubled youth", "infection", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "the previous year's Palm Sunday celebrations", "business plan", "three", "flower", "sausages", "kew Gardens", "krushchev", "$500,000", "Alexandros Grigoropoulos,", "Cyrus McCormick", "a police badge", "BBC's central London offices", "a kidney transplant"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6359499007936508}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.6666666666666666, 0.25, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-2339", "mrqa_squad-validation-2523", "mrqa_squad-validation-7670", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_hotpotqa-validation-2116", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-121", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.53125, "CSR": 0.6, "EFR": 0.9666666666666667, "Overall": 0.7833333333333333}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "The Late Late Show with Stephen Colbert", "small renovations, such as addition of a room, or renovation of a bathroom", "the John Madejski Garden", "declared martial law and sent the state militia to maintain order", "Famous musicians", "ESPN Deportes", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "88%", "Necessity-based", "950 pesos ( approximately $ 18 )", "C key from left on a standard 88 - key piano keyboard", "Seattle, Washington", "Battle of Antietam and Lincoln's Emancipation Proclamation", "Dimitar Berbatov and Carlos Tevez", "In Time", "2nd century", "Glenn Close", "four times", "Agostino Bassi", "five seasons", "a beach in Malibu, California", "the church at Philippi", "the Dutch", "September 2017", "Professor Kantorek", "1546", "Jane Fonda", "Bhupendranath Dutt", "a warrior", "Dr. Lexie Grey ( Chyler Leigh )", "Majandra Delfino", "December 19, 1971", "Uruguay", "michael conan", "Matt Jones", "The National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "between 11000 and 9000 BC", "1970s", "the Director of National Intelligence", "D.D.A.", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "13,331 people", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Thespis", "Portugal. The Man", "burt Hammersmith", "Rachel Kelly Tucker", "Bohemia", "a garage beetle", "\"Myst3ry\"", "game designer", "the parliament within 15 days.", "the abduction of minors", "Nevada", "Chile", "Stage Stores, Inc.", "1881"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5896123407842158}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.9523809523809523, 0.3076923076923077, 0.6, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-7235", "mrqa_squad-validation-7141", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674", "mrqa_hotpotqa-validation-1852"], "SR": 0.421875, "CSR": 0.5888671875, "EFR": 0.9459459459459459, "Overall": 0.767406566722973}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "the New England Patriots", "Originally chairs were Rajendra K. Pachauri, elected in May 2002", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "the US", "six", "11", "hydrogen and helium", "Khitan Xiao Zhala", "November 1979", "Robert Lane and Benjamin Vail", "Germany for the Argentine Navy", "Francis the Talking Mule", "Helsinki, Finland", "Microsoft Office file formats", "SAVE", "Scandinavian Airlines", "1993 to 2001", "1951", "the NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "the Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award", "Jello Biafra drew on Nardwuar's face with a marker pen", "The uprising ended in defeat at the Battle of Culloden, effectively terminating the Jacobite cause", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements", "Julian Dana William McMahon", "Bervie Water", "The 7.63\u00d725mm Mauser (.30 Mauser Automatic) round", "the Academy Award for Best Animated Feature", "the Chengdu Aircraft Corporation (CAC) of China", "Delacorte Press", "Neighbourhoods are often social communities with considerable face-to-face interaction among members", "Secretariat", "Marcus Island", "a vehicle that uses hydrogen as its onboard fuel for motive power", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "\"Southern Living\" Reader's Choice Awards", "Thomas Harold Amer", "Johnson & Johnson", "Originally were a three-piece band, The Hustlers, The Warlocks, and American Blues", "Mahoning County", "Alticor", "Parlophone Records", "South Africa", "Surrey", "The Girl", "Charles Russell", "Boyd Gaming", "Anthony Davis of the New Orleans Pelicans", "1989, 1990, 1991, 1992, 1993, 1994, 1998, 2016, 2018", "Glenn Close", "silverfish", "Neighbours", "Ewan McGregor", "2011", "silverfish", "a enslaved African American who led", "expanding U.S. sanctions against Zimbabwe,", "Originally Brown-Waite,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5829971193022664}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3636363636363636, 0.35294117647058826, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-76", "mrqa_squad-validation-8509", "mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3521"], "SR": 0.453125, "CSR": 0.5808823529411764, "EFR": 0.9714285714285714, "Overall": 0.7761554621848739}, {"timecode": 17, "before_eval_results": {"predictions": ["force of gravity", "theology and philosophy", "ITV", "the University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter status", "1830", "nonfunctional pseudogenes", "the inner chloroplast membrane", "Charlie Harper", "Steveland Hardaway Morris", "beaver", "La Boh\u00e8me Giacomo Puccini", "formic acid", "Los Cristo de la Luz", "Zimbabwe", "Mr. Boddy", "Ted Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "massively multiplayer online games", "Mercury", "hound", "Socrates", "Fuller's", "Plimsoll", "Nick Hornby", "The Comedy of Errors", "Charles V", "England", "welchbrok", "weight plates", "big house", "Hadrian", "California", "human flea", "Los Angeles, London, Sydney, Switzerland", "Hamburg", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "Leicester City", "Prokofiev", "Nick Lachey", "Boy George", "Finland", "3000m", "Scotland", "Russia", "Travis Tritt", "It was a Confederate victory", "New Jewel Movement", "sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "Oshkosh", "\"Out of Sight.\"", "\"The World\"", "a special edition of a newspaper that is published when something important happens"], "metric_results": {"EM": 0.5, "QA-F1": 0.5813244047619048}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7089", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-2812", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.5, "CSR": 0.5763888888888888, "EFR": 0.9375, "Overall": 0.7569444444444444}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude and coastal location", "1622", "extremely high", "Manakintown", "northwest", "10 employees", "Middle Miocene", "new magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "a balance sheet", "July 2, 1776", "to prove disparate impact", "2010", "The Hustons", "Allison Janney", "chalkidice", "inability to comprehend and formulate language", "Jonathon Dutton", "Tyrion", "electron donors", "Meredith Quill", "( 1985 -- 1993 )", "775", "Solange Knowles", "Gupta Empire", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Coton", "( 55 -- 69 % )", "Zoe Badwi", "1995", "Identification of alternative plans / policies", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "aorta", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "An empty line", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "the Rime is not valued or used always or everywhere or by everyone in the same way or for the same reasons", "September 2017", "moral", "`` Rising Sun Blues ''", "Part 2", "Dumbo", "the failure of the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "jenkins", "Robert Langdon", "ABC1 and ABC2", "\"NBA 2K16\"", "Lord President of the Council and Lord Lieutenant of Ireland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6225144496204278}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.08695652173913042, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7213", "mrqa_squad-validation-3193", "mrqa_squad-validation-6937", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-4613"], "SR": 0.53125, "CSR": 0.5740131578947368, "EFR": 0.9666666666666667, "Overall": 0.7703399122807018}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law,", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music, also all classes, clergy and laity, men, women and children", "warming", "the mid-sixties", "270,000 tonnes", "Long troop deployments", "Joe Pantoliano", "Ronald Cummings, who disappeared in February, plans to file for divorce from the girl's stepmother, a key witness in the case, his attorneys told THN's \"Nancy Grace.\"", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "nearly three out of four Americans are scared about the way things are going in the country today.", "sovereignty over them", "Tuesday in Los Angeles. Cole's search for a new kidney ended this week when someone with a compatible organ died and their family asked that it be given to the singer, according to the organ procurement group that handled the donation.", "forgery and flying without a valid license", "Anil Kapoor", "55-year-old Hogan's three-decade career, during which he held multiple championship titles and, during his heyday in the 1980s, was easily the most popular wrestler in the world.", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "the new kid on the block in the modern art scene, a reputation that was cemented when the world's most prestigious modern art fair, Art Basel, arrived in the city in 2002.", "The Louvre", "snowstorm", "sports cars", "a lizard-like creature from New Zealand", "Moammar Gadhafi", "two Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18, the publicist said.", "stuntman: Joe Canutt", "Russia has also objected to the missile defense system agreement, signed between the United States and Poland and the Czech Republic in August, that places a ground-based ballistic defense facility in the two eastern European nations.", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone.", "\u00a320 million ($41.1 million) fortune", "Hoover Dam", "Kim Jong Il", "Manmohan Singh", "Michael Jackson", "give peace to my nation,\" Tuba Sahaab says, \"I will fight for it.\"", "40 militants and six Pakistan soldiers dead", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list, had been released.", "governor, to say that the federal government apparently has no voice in this crisis, when in some cases, they are the only answer, that pointed to a lot of the criticism, didn't it?", "the Southeast", "monochroms", "Nancy Sutley", "\"A Mother For All Seasons.\"", "tracheotomy", "back at work", "Georgia Aquarium announced late Monday.", "15 years on, no one would really know who Kurt Cobain was outside of a group of diehard fans,\" said Jeff Burlingame,", "Derek Hough", "John Adams", "Borsht (Borsch), a renowned Russian soup?", "Zager and Evans", "Bobby Hurley", "fourth term", "obscenity", "Cromwell", "Rovaniemi", "1937", "Emad Hashim"], "metric_results": {"EM": 0.375, "QA-F1": 0.48450543606793606}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.05405405405405406, 1.0, 1.0, 0.0, 0.1904761904761905, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.923076923076923, 0.6666666666666666, 0.7407407407407407, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_squad-validation-2400", "mrqa_squad-validation-3028", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-1958", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922"], "SR": 0.375, "CSR": 0.5640625, "EFR": 1.0, "Overall": 0.78203125}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "R\u0113nos", "1331", "Death wish Coffee", "L", "Cameroon,", "just after midday on a cold December Monday in South Korea's capital when news of Kim Jong Il's death filtered through.", "ballots", "a new fabric technique discovered during a life-changing trip to Angkor Wat that's sure to bring back sixties minimalism with a twist.", "\"no illegal substances\"", "Eikenberry sent private cables to Obama last week, urging the president not to rush to send more troops to Afghanistan", "1959", "Michael Schumacher", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "\"The woman involved -- Mandi Hamlin -- told reporters earlier Friday she was humiliated by last month's incident,", "the composer of \"Phantom of the Opera\" and \"Cats\"", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony, 2,", "Amanda Knox's aunt", "over 1,000 pounds", "development of a nuclear weapon", "a welcoming, bright blue-purple", "allegedly faking a doctor's note and was restricted from leaving his house in Tokyo,", "ceo Herbert Hainer", "Brett Cummins", "canceled the swimming privileges of a nearby day care center whose children are predominantly African-American.", "inmates", "Cameron-Ritchie,", "on the set at \"E! News\" on Tuesday.", "11-member Spanish flight crew and one Belgian", "jobs", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "The controversial technique that simulates drowning -- and which President Obama calls torture -- was used at least 83 times in August 2002 on suspected al Qaeda leader Abu Zubaydah,", "A group calling itself the \"Pelotones Omega\" distributed fliers indicating its members would fight against kidnapper, murderers and other criminals.", "Republicans", "get out of the game, and I wondered what will they do now?\"", "An undated photo of Alexandros Grigoropoulos,", "signed a power-sharing deal with the opposition party's breakaway faction,", "a 57-year old male", "\"A long-range missile strike or confrontation between the two countries at sea.\"", "Angola", "Gary Brooker", "\"outlaws\"", "\"The Hills Have Eyes II.\"", "refused to refer the case of Mohammed al-Qahtani to prosecutors", "at Sea World in San Antonio,", "a job he liked at the U.S. Holocaust Memorial Museum,", "about 50", "Ku Klux Klan", "1939", "Branford College", "Wigan", "a form of marriage in which one woman has two or more husbands at the same time.", "Malayalam", "August 17, 2017", "leather", "by the 1950s, scientists were able to do this to frogs; mice followed, in the '80s", "Hodel", "access to US courts", "Coldplay"], "metric_results": {"EM": 0.328125, "QA-F1": 0.4559760957866011}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9523809523809523, 0.33333333333333337, 0.25, 0.23076923076923078, 0.8, 1.0, 0.8571428571428571, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0851063829787234, 0.09999999999999999, 1.0, 0.1, 0.22222222222222224, 0.0, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 0.0, 0.8181818181818181, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3070", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.328125, "CSR": 0.5528273809523809, "EFR": 1.0, "Overall": 0.7764136904761905}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "the Treaties establishing the European Union", "\u201csplash\u201d", "Nicola Adams", "copper and zinc", "eagle with extended wings", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "chiton", "Madonna's", "Glasgow", "a satellite-based navigational system that can tell users exactly where they are on Earth.", "Australia", "gizzard", "Pearson PLC.", "Irish Setter Puppies", "the American Civil War,", "Loch Lomond", "rochner", "kata Tjuta", "medium-sized", "Tainan,", "Harrisburg", "Mustela erminea,", "glockenspiel", "Dr John Sentamu", "Baka hunter-gatherers", "Pongo", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "charlemagne", "Ewing Kauffman", "Russell Crowe", "President Warren G. Harding", "Grant Elliott,", "Puck", "Samuel Butler", "chamomile tea", "Ireland", "tarn", "SS Constitution", "Albert Square", "Newbury", "the Old Testament", "70 million people", "Target Corporation", "\"The Omega Man\"", "Michelle Rounds", "\"You build the house and I will put on the door and paint it,\"", "International NGO", "John Jackson Dickison", "organizations that support prisoners' rights and better conditions for inmates,", "talk show queen Oprah Winfrey.", "his mother."], "metric_results": {"EM": 0.5, "QA-F1": 0.5729166666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-4045", "mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_newsqa-validation-2971", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.5, "CSR": 0.5504261363636364, "EFR": 0.90625, "Overall": 0.7283380681818181}, {"timecode": 22, "before_eval_results": {"predictions": ["The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms.", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials, and because they are a matter of having sufficient funds at a specific time,", "Vicodin,", "jesuit", "jesuit hunters", "pearls", "Utah Territory", "Carrie Underwood", "liqueur", "he made his horse a consul, his palace a brothel, and his", "university of Michigan", "Langston Hughes", "pain tolerance", "jesus", "jesuit", "lariat", "CARE", "lST", "rhodesian ridgebacks", "jesuit", "arturo Toscanini", "economics", "Miracle in the Andes:", "london", "jesus", "discus", "thick", "basidiomycota", "1998", "jenkins", "president of the Democratic Republic of the Congo", "agricultural green & yellow", "a body, body part, or personal object", "terracotta", "jesuit jesus", "jesuit Giuliani", "masa", "40 seconds", "the Vikings", "CARE", "Bastille Day", "typhoid fever", "water near a coastline.", "capital of Bavaria", "jesuit", "jesuit", "international travel", "hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off", "jenkins", "the internal reproductive anatomy", "more than $1 billion worldwide", "risk factors for disease and targets for preventive healthcare", "the Big Bopper", "Tesco", "london and North Eastern Railway Class A4 4-6-2 Pacific steam locomotive", "the Triple Crown of Motorsport", "the Battelle Energy Alliance", "IT", "debris", "$10 billion", "Bailey, Colorado,"], "metric_results": {"EM": 0.234375, "QA-F1": 0.3103338411387947}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false], "QA-F1": [0.07407407407407407, 1.0, 0.9411764705882353, 1.0, 0.6842105263157895, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.28571428571428575, 0.14285714285714285, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6437", "mrqa_squad-validation-10104", "mrqa_squad-validation-6887", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16257", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-5091", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-16854", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-1118", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-1997"], "SR": 0.234375, "CSR": 0.5366847826086957, "EFR": 0.9795918367346939, "Overall": 0.7581383096716947}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "the Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "Atlantic", "domestic cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Basel, Switzerland", "the Argonauts", "prometheus", "Altamont Speedway Free Festival", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "mmorpgs", "Cyrenaica", "khaki", "magma", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "Barack Obama", "the Earth", "Nafea Faa Ipoipo", "a pentatope number", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "radar dish", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "cyclone", "Rangers", "charity", "Adidas", "the Hunting of the Snark", "Elizabeth Arden", "Buxton", "woe", "James Bond", "the square", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "President Pervez Musharraf", "Tennis Channel", "fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7229626225490196}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-4882", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-458"], "SR": 0.671875, "CSR": 0.5423177083333333, "EFR": 1.0, "Overall": 0.7711588541666666}, {"timecode": 24, "before_eval_results": {"predictions": ["limited coercion", "the chosen machine model", "20th Century Fox, Lionsgate, Paramount Pictures, Universal Studios and Walt Disney Studios", "1997", "a suite of network protocols created by Digital Equipment Corporation", "Christopher Savoie", "15", "the first home series defeat on Australia in almost 16 years", "Pyongyang and Seoul", "it was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver on February 14, 2002.", "11", "change course shortly before it crashed into the sea", "the oil rig", "Chaffetz", "money or other discreet aid for the effort if it could be made available,", "Sarah Brown", "it has not intercepted any Haitians attempting illegal crossings into U.S. waters", "environmental", "italy", "italy", "Saturday", "38", "70,000 or so", "italy", "\"E! News\"", "former Boca Juniors teammate and national coach Diego Maradona", "Steve Williams", "the fast-food chain", "crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "five female pastors", "2008", "Diego Maradona", "Dog patch Labs Europe", "The drama of the action in-and-around the golf course", "two", "Itawamba County School District", "the former Massachusetts governor", "the EU naval force", "Plymouth Rock", "Liza Murphy,", "The nomination of Sonia Sotomayor", "police", "former U.S. secretary of state", "At least 33 people", "five", "improve health and beauty.", "it is among a growing number of state governments going after them.", "it is primarily students, the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "Damon Bankston", "the wife, 39, Subasari Rajaram, and their two other sons, 12 and 7.", "Sunday,", "killing", "it is derived from the French Jeanette, which is cognate to the English Janet, itself a feminine form of the Hebrew Yohannan, `` God forgave / God gratified ''", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "northern irish", "radar", "art", "the NBA All-Star Game and All-NBA Team", "single-season touchdown reception", "it is one of the official languages of Paraguay (along with Spanish), where it is spoken by the majority of the rural population is monolingual", "freestyle", "the Nightingale", "Belief"], "metric_results": {"EM": 0.375, "QA-F1": 0.5278956286768787}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5555555555555556, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5454545454545454, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.4, 1.0, 1.0, 0.16666666666666669, 0.16666666666666663, 0.0, 0.14285714285714285, 0.25, 0.6666666666666666, 0.3076923076923077, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-610", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-1545", "mrqa_searchqa-validation-3826"], "SR": 0.375, "CSR": 0.535625, "EFR": 0.975, "Overall": 0.7553125}, {"timecode": 25, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.857421875, "KG": 0.47109375, "before_eval_results": {"predictions": ["the \"most successful\" science fiction series of all time", "Thomas Savery", "Vicodin,", "Eastern crops", "from 22,000 years ago onward, frozen subsoil and expanded alpine glaciers began to thaw and fall-winter snow covers melted in spring.", "violent separatist campaign", "Eleven", "269,000", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "38 feet", "Eintracht Frankfurt", "150", "a pool of blood beneath his head.", "Russian bombers", "41,", "Los Alamitos Joint Forces Training Base", "Wally", "137", "a Kurdish militant group in Turkey", "3-2", "autonomy.", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "the Russian air force,", "34", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "carbon neutral", "Amanda Knox's aunt", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "ensuring that all prescription drugs on the market are FDA approved,", "vitamin supplements", "Tom Baer.", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "Bikini Atoll,", "Brian Mabry", "changed the business of music,", "Sunday.", "60 euros -- $89", "Former detainees of Immigration and Customs Enforcement", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "some truly mind-blowing structures", "first name", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "2006", "the home of NFL's Chargers", "Six of the victims were women who were working as prostitutes", "The Taliban has threatened to kill Bergdahl if foreign troops continue targeting civilians in the name of search operations in Ghazni and Paktika provinces,", "Twitter", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "another American icon's", "maintain order and its tactics in doing so.", "heart", "Hyderabad", "Egypt, the only part of the country located in Asia", "it has spiritual meaning", "Las Vegas", "Jackson Pollock,", "Wye", "Kentwood, Louisiana", "January 19, 1943", "King Duncan", "Georgia", "the thimble", "a stride"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4794966901898984}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.25, 0.8, 0.0, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.29411764705882354, 0.0, 1.0, 1.0, 0.07407407407407408, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.8, 0.2105263157894737, 0.1111111111111111, 0.0, 0.4, 0.888888888888889, 1.0, 0.0, 0.0, 0.1818181818181818, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_squad-validation-9432", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-11832"], "SR": 0.390625, "CSR": 0.5300480769230769, "EFR": 0.9743589743589743, "Overall": 0.7079907852564102}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "Krishna Rajaram,", "25", "Booches Billiard Hall,", "finance", "the Brundell family in Deene Park, England,", "Hong Kong's Victoria Harbor", "2002", "Six", "the legitimacy of that race.", "his chest", "three", "Monday", "Scarlett Keeling", "two years,", "nearly 28 years", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "in July for A Country Christmas,", "Akshay Kumar", "\"We Found Love\"", "\"The FARC issued a statement dated February 11 saying the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation and had two broken bones in his neck,", "Phil Spector", "Kim Il Sung", "1994", "planned suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "dependable Camry", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "Mohammed Ali al-Moayad", "engage in operations to overthrow the government of Chile", "Miguel Cotto", "9 a.m.", "same-sex civil unions,", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "one", "Bongos", "\"A Touch of Frost\"", "the innermost digit of the forelimb; thumb", "1925", "over 20 million", "Peoria, Illinois", "Hawaii", "\"The eyes of these croaking critters usually bulge,", "Cordelia,", "Ottoman Empire"], "metric_results": {"EM": 0.5, "QA-F1": 0.5961476457570207}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.625, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.0, 0.0, 0.5, 0.33333333333333337, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-9563", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.5, "CSR": 0.5289351851851851, "EFR": 1.0, "Overall": 0.712896412037037}, {"timecode": 27, "before_eval_results": {"predictions": ["in the early 1990s", "leaf-shaped", "silver", "(1755)", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "John Dillinger,", "The remains of Cologne's archive building", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "a single mother with HIV in Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Lashkar-e-Tayyiba", "$1.5 million", "2006", "The Rev. Alberto Cutie", "Angels", "Indian Army", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "in a Starbucks this summer.", "\"BADBUL,\"", "112 people were thought to be aboard when the Hercules transport plane crashed about 6:30 a.m.", "1993", "on board the Columbus", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "between government soldiers and Taliban militants in the Swat Valley.", "in Iraq", "Iran", "last month's Mumbai terror attacks", "people have chosen their rides based on what their", "in July", "a French charity", "Four Americans", "\"It was a special moment where, walking, we were able to support her and called police,", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "within walking distance of the city center,", "broken pelvis", "Wednesday", "the abduction of minors.", "gun", "Aniston, Demi Moore and Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "Missouri", "largest city", "beta blockers"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6880559799677446}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.057142857142857134, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.3636363636363636, 0.4444444444444445, 1.0, 0.4, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.2222222222222222, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4094", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1450", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1496", "mrqa_naturalquestions-validation-6383", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-14535"], "SR": 0.578125, "CSR": 0.5306919642857143, "EFR": 1.0, "Overall": 0.713247767857143}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author", "informal rule", "1981,", "forgery and flying without a valid license,", "\"I wanted to shove it up that black a--.\"", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force.", "shoot down the satellite", "semiconductors", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "seven", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "misdemeanor assault charges", "5% of global greenhouse gas emissions,", "Anil Kapoor.", "kill members of the Zetas cartel from the state of Veracruz, Mexico,", "Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "Christianity and Judaism,", "the Dalai Lama's current \"middle way approach,\"", "Russia", "8 p.m. local time Thursday", "Passers-by", "one day,", "executive director of the Americas Division of Human Rights Watch,", "750", "300", "Matthew Fisher", "The Ski Train", "Big Brother.", "Ozzy Osbourne", "Umar Farouk AbdulMutallab,", "U.S. senators", "inconclusive", "5:20 p.m. at Terminal C", "environmental and political events", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600 people", "700,000", "Sen. Arlen Specter (D-Pennsylvania)", "Deutschneudorf,", "legislation that would let prisons jam cell-phone signals within their walls.", "OneLegacy,", "bragging about his sex life on television", "a vertebral column ( spine )", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the South Bank", "Douglas Hofstadter", "The Dark Tower", "American", "Marmee,", "Castle Rock", "why do pizzerias offer anchovies?"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6074426083427442}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.6, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.125, 1.0, 0.25, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-3839", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.484375, "CSR": 0.5290948275862069, "EFR": 1.0, "Overall": 0.7129283405172414}, {"timecode": 29, "before_eval_results": {"predictions": ["relatively stagnant wages for the working class", "poison by El Tem\u00fcr", "438,000", "Marty Ingels", "coaxial", "Pakistan A", "Ever Bank Field", "9th, 10th, 11th & 12th Chinese People's Political Consultative Conference", "the German Campaign of 1813", "John Churchill,", "1965", "Paris at Charles de Gaulle Airport", "fifth", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven children", "Syracuse", "1963", "coca wine", "puzzle video", "Knoxville, Tennessee", "Washington, D.C.", "Gal\u00e1pagos giant rat", "Tom Kartsotis,", "2017", "Wayman Lawrence Tisdale", "Mexico", "Srinagar", "Northern Ireland", "the late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell", "2002", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Digital Network", "Erinsborough", "Marine Corps", "Robert Allen Iger", "the Lost Battalion", "Seminole and Miccosukee tribes", "Virginia", "NBA Slam Dunk Contest", "$10\u201320 million", "Paul Kantner", "Kennedy Road", "a field in Somerset County, Pennsylvania,", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes", "IB Career - related Program for students aged 15 to 18", "Richard Parker", "the southernmost tip of the South American mainland", "Cecil B. De Mille", "allergic reaction", "Peter Townsend", "3,000 kilometers (1,900 miles)", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "Swiss art heist", "Russia", "peel and devein shrimp", "Australia"], "metric_results": {"EM": 0.4375, "QA-F1": 0.584672619047619}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true], "QA-F1": [0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.28571428571428575, 0.5, 1.0, 0.0, 0.0, 0.4444444444444444, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-1197", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-4935", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-6793"], "SR": 0.4375, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.7123177083333334}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian army", "London", "Dave Thomas", "Agricultural cooperative", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City Crime Family", "the Dallas Mavericks of the National Basketball Association", "the Cecil B. DeMille Award honoree", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "the United States and Canada", "British comedian", "Apatosaurus", "1885", "American", "Frank Thomas' Big Hurt", "\"Stranger in Paradise\"", "Margarine Unie", "Winecoff Hotel fire", "national and international media", "The Seduction of Hillary Rodham", "2005", "Lambic", "Ubisoft's The Division", "Argentina,", "Larry Alphonso Johnson Jr.", "Michael Stipe", "veto power", "Price Chopper", "\"Chelsea Lately\"", "Eurom\u00e9tropole de Strasbourg", "Saudi", "Wembley Stadium, London", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "New York", "the Rome Olympics", "Aston Villa F.C.", "2005", "228 people", "\"We're opening new chapters. And in fact, because some of this information is so new and it's so different from the way we use to think about the moon, there are chapters that are blank right now,\"", "Post Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6362830284552845}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6399999999999999, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.048780487804878044, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-1560", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-672", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.546875, "CSR": 0.5267137096774194, "EFR": 1.0, "Overall": 0.7124521169354839}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "Silent Snow, Secret Snow", "Log Ride", "Senator", "a Van Morrison song", "Nassau", "a gemstone", "HIV", "Martin Van Buren", "a network of rail lines", "Rigoletto", "aardwolf", "Beijing", "Roger Bannister", "Inuk", "Death Valley", "On the Runway", "because the reindeer", "Fortinbras", "the War of 1812", "Grandma Moses", "Naoko Takeuchi creation", "charne asada", "Dan Brown", "a bear", "Elijah", "Gilson Lavis", "Monty Python and the Holy Grail", "negative electrode", "Milton Berle", "George Herbert Walker Bush", "Congolese", "lunar module", "Pedro de Valdivia", "Dan Marino", "Mars", "Finding Nemo", "E = mc2", "Guru Pitka", "Las Vegas", "millet", "Butterflies", "Camelot", "orangutan", "Baja California", "Soothsayer", "Yitzhak Rabin", "David Spares Saul's", "Gettysburg", "Jack Gleeson", "Plank", "overland route hypothesis", "Carl Johan", "Portugal", "Graham Bond", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "knocking the World Cup off the front pages for the first time in days.", "12.3 million"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5747767857142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12657", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-2051", "mrqa_naturalquestions-validation-5808", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587"], "SR": 0.5625, "CSR": 0.52783203125, "EFR": 1.0, "Overall": 0.71267578125}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "62 acres", "Herbert Henry Asquith", "40", "Libya", "Shania Twain", "Sheffield Wednesday", "glucagon", "The New York Yankees", "rapid eye movement", "green, red, white", "Ann Dunham", "Saddam Hussein", "French", "Jim Branning", "Ohio", "Francis Matthews", "photographic", "iron", "Noah", "London", "New Years Day", "Sarah Ferguson", "Mercury", "watt", "del shyber", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "Pizarro", "his father,", "aged 75", "Jennifer Lopez", "1664", "Eurythmics", "Fred Perry", "Downton Abbey", "Martina Hingis", "Ahern, O\u2019Ahern, Hearne", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "a cheery optimist", "Pomona", "Italy", "The Streets", "Appalachian", "a black Ferrari", "a branch of mathematics", "bears", "Michaeloriarty", "June 1992", "24", "1994", "Campbell Soup Company", "Kirkcudbright", "the soldiers", "cortisone.", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan", "Brook Busey-Maurio", "Hermit", "a lung"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6274038461538461}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.15384615384615383, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5228", "mrqa_naturalquestions-validation-7976", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.546875, "CSR": 0.5284090909090908, "EFR": 1.0, "Overall": 0.7127911931818182}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "hay fever", "janeiro", "Ast\u00e9rix", "Brighton", "Belfast", "wind", "fire", "Robin Hood", "West Point", "Andy Warhol", "La Mancha", "Alex Murphy", "rio de janeiro", "the solar system", "potatoes", "Moldovan", "Mitsubishi A6M Zero Fighter", "the Dartford Warblers", "georgia", "st leger", "baroudeur", "clium", "The Beach Boys", "Madness", "Buxton", "discretion", "Yves Saint Laurent", "Rudyard Kipling", "Leeds", "Philippines", "beaver", "Mel Blanc", "a frog", "Moffitt", "Ellen DeGeneres", "rochdale", "5000 meters", "racing", "clump", "Newfoundland and Labrador", "crow", "Yellowstone", "St. Thomas the Apostle", "Manila", "Hugh Laurie", "Buddhism", "Jonny Buckland", "Ohio", "Port Melbourne", "Osbald", "Scarface", "forgery and flying without a valid license,", "roma", "Liza Murphy, 42,", "Spock", "Astana", "Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain and Switzerland"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5270833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-525", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-11382"], "SR": 0.484375, "CSR": 0.5271139705882353, "EFR": 1.0, "Overall": 0.7125321691176472}, {"timecode": 34, "before_eval_results": {"predictions": ["an attack against the forts Shirley", "business districts", "prairies", "spA", "George Santayana", "opossum", "Alice Cooper", "beta-blockers", "trumpet", "Marc Warren", "The Scream", "shildon", "appalachian mountain range", "Herald of Free Enterprise", "ballet", "The storm", "george i", "lizard", "Blackburn Lancashire", "Frankie Laine", "The Mystery of Edwin Drood", "pommel", "scarlet", "Dick Van Dyke", "Egremont", "Numb3rs", "Francisco de Goya", "Nephele", "spain", "Canada", "venom", "pears soap", "Some Like It Hot", "Mull", "ireland", "cock Tracy", "hippocampus", "DeLorean", "igneous", "Passepartout", "Thank you", "Iceland", "Spain", "shrek", "26.22", "Cleveland Brown", "Heston Blumenthal", "One Direction", "spain", "Jupiter", "spain", "Charles Lindbergh", "November 1999", "Baaghi", "Lead and lead dioxide", "boxer", "East Kn Boyle", "stoneware", "enka.", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "ballet", "Howard Carter"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5958333333333333}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10251", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-508", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-6449", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-78", "mrqa_searchqa-validation-4312"], "SR": 0.546875, "CSR": 0.5276785714285714, "EFR": 0.9655172413793104, "Overall": 0.7057485375615764}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "John Forster", "Matlock", "American Civil War", "jane", "saturn", "paul", "peter daedalus", "Tigris", "peter", "to make wrinkles in one's face", "Spain", "carousel", "bullfight", "Mike Brady,", "countertenor", "peter", "fidelio", "armageddon", "jane Fellowes", "Denmark", "jane", "The Last King of Scotland", "Ghana", "eddisbury", "gefen", "jane feldman", "marty feldmaninoff", "Finland", "massive stars", "Mille Miglia", "pronunciation", "Billaley", "50p", "Muriel Spark", "happy birthday", "seven", "opossum", "Pickwick", "presliced bread", "Saga Noren", "raven", "jane peter", "peter Creek Hummus", "jane nelsons", "Tarquin the Proud", "Ken Burns", "peter peter", "Heather Stanning and Helen Glover", "e. Tchaikovsky", "Mujib,", "saturn", "Eric", "season four", "sinoatrial node", "Lee Sunmi", "tomato", "vote in November 5, 2002", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "L'Aquila earthquake", "March 24,", "peter peter", "equinox", "p Pocahontas"], "metric_results": {"EM": 0.34375, "QA-F1": 0.431703629032258}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.12903225806451613, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-400", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-3566", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-5142", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_naturalquestions-validation-10727", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-1247", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-108"], "SR": 0.34375, "CSR": 0.5225694444444444, "EFR": 1.0, "Overall": 0.711623263888889}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "Kim", "city of a republic on the southeastern coast of Africa on the Mozambique Channel", "Branson,", "Gordon Ramsay", "luton Town", "julian crawford", "sulfur dioxide", "Margot Betti", "Manchester Airport", "Portuguese", "Travelocity", "Avengers", "stanley", "Batman", "julian", "julian crawford", "canola", "bartertown", "jonian crawford", "black eyed peas", "Bolivia", "John Donne", "Uranus", "Rio Grande", "clown", "The Graduate", "ethiopia", "julian crawford", "julian crawford", "One Foot in the Grave", "Bronx Mowgli", "julian Bristow", "George Santayana", "stalls", "sandermere", "crackerjack", "king Ferdinand and Queen Isabel", "julian crawford", "Canada", "rum and cola", "city of Seattle", "ghee", "George III", "comets", "hyperbole", "oldpatrick", "June", "David Graham", "Ceylon", "screwdrivers", "the Kansas City Chiefs", "G minor", "A Christmas Story", "1974", "\"The Outsiders\"", "Amberley Village", "if you don't have a cause of death,", "President Obama", "Elisabeth", "Pearl S. Buck", "Brigham Young", "pearl", "a former chalk quarry"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4841746794871794}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5196", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-4108", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120", "mrqa_hotpotqa-validation-1377"], "SR": 0.40625, "CSR": 0.5194256756756757, "EFR": 1.0, "Overall": 0.7109945101351351}, {"timecode": 37, "before_eval_results": {"predictions": ["not-for-profit United States computer networking consortium", "( pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen", "generally carved in decorative patterns", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "a perceived harmful event, attack, or threat to survival", "49 cents", "1876", "geologist James Hutton", "North America", "joy of living", "420", "John Prine and Roger Cook", "sovereignty over some or all of the current territory of the U.S. state", "1989", "Shawn", "Kiss", "January", "Los Angeles", "September 28, 2017", "Kelly Reno", "by government agencies to prevent organizations from abusing their tax - exempt status", "1770 BC", "Niveditha, Diwakar, Shruti", "two", "John C. Reilly", "DNA", "Anakin and Padm\u00e9 Amidala", "Travis Tritt and Marty Stuart", "1976", "Bee Gees", "Matt Czuchry", "Pradyumna", "England", "On the west", "seven heavenly virtues", "New Jersey Devils", "two", "4.0", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "because of known associations or hypothesized relationships", "Christian ( Josh Boswell )", "Canadian Rockies continental divide east to central Saskatchewan", "France", "pussia", "dumbo", "purple", "James A. Garfield", "Gettysburg Address", "iTunes, iTunes Radio, and iTunes Music", "$273 million", "India", "Jeddah, Saudi Arabia", "Desperate Housewives", "captain chaos", "morelos", "Tuesday"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5494150044575186}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.5454545454545454, 0.5714285714285714, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.7096774193548387, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-5082", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-5119", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.421875, "CSR": 0.516858552631579, "EFR": 0.9459459459459459, "Overall": 0.699670274715505}, {"timecode": 38, "before_eval_results": {"predictions": ["James Watt", "25 years after the release of their first record", "the United States", "Kim Basinger", "fall of 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in positions Arg15 - Ile16", "Charles Crozat Converse", "Lady Gaga", "the Chicago metropolitan area", "the commissioners or board members", "Domhnall Gleeson", "eusebeia", "Pastoral farming", "Notts County", "a nobiliary particle indicating a noble patrilineality or as a simple preposition that approximately means of or from in the case of commoners", "Stephen A. Douglas", "1984", "`` man ''", "Pakistan", "23 February", "either Tagalog or English", "Bryan Cranston", "thylakoid membrane", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "Franklin and Wake counties in the U.S. state of North Carolina", "January 1923", "The Minneapolis Miracle", "520", "maximum energy of 687 keV", "between $10,000 and $30,000", "September 1980", "1931", "University of Oxford", "Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "The statesmen", "Donna's", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "a divergent tectonic plate boundary", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "yellow", "Margaret Thatcher", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "\"It's a really dumb thing to say,\"", "10 below in Chicago, Illlinois.", "General Motors'", "David McCullough", "Rendezvous with Rama", "CERN", "saudade"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6121894058626796}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.09523809523809525, 0.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4347826086956522, 1.0, 0.0, 0.0, 0.0, 0.32, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4685", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.546875, "CSR": 0.5176282051282051, "EFR": 0.9655172413793104, "Overall": 0.7037384643015031}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale", "Jenny Slate", "active absorption is carried out in this process", "Philippe Petit", "September 1980", "January 2004", "southwest and along the Yangtze", "Toby Keith", "development of electronic computers in the 1950s", "17 - year - old", "punk rock", "March 12, 2013", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "XXXX", "Gestalt", "53", "lying between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Rachael Harris", "Richard Crispin Armitage", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "either late 2018 or early 2019", "the interstellar medium form clouds, or diffuse nebulae", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "Frederik Barth", "John F. Kelly", "Charles Sherrington", "1886", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "related to the Common Germanic word guma ( Old English gome ) / gomo ( High Old German gomes ) related to Latin homo `` man ''", "1960s", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "as a defense against rain rather than sun", "Since 1940", "Ariel Winter as Sara Lavrof", "Mark Jackson", "Michael Buffer", "there is one body and one Spirit just as you were called to the one hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all", "on location", "the United States of America", "1958", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "about 0.04 mg / L several times during a day", "Juan Manuel de Ayala", "the Prophet Joseph Smith, Jr.", "funny Folks", "1909", "Sarah Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew in Jaipur", "\"Me And Bobby McGee\"", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6451556899641577}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.7777777777777778, 0.967741935483871, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.4, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-4219", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1616", "mrqa_searchqa-validation-10341"], "SR": 0.46875, "CSR": 0.51640625, "EFR": 1.0, "Overall": 0.710390625}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "the Indian army and separatist militants in Indian-administered Kashmir", "Joan Rivers", "early detection and helping other women cope with the disease", "glamour and hedonism", "2-0 lead.", "15,000", "58 people", "Michael Schumacher", "can use brain signals to control computers", "numerous suicide attacks,", "the U.S. and Britain.", "2004", "NATO", "Switzerland", "Monday", "second time since the 1990s", "shows Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "\"one is impossible to turn back the tide of globalization.\"", "Clifford Harris,", "Oaxaca, Mexico", "Robert Barnett", "$627", "41", "Adenhart", "a strict interpretation of the law,", "Derek Mears", "Sylt", "rural Tennessee.", "Tuesday", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "Section 60.", "Ali Bongo", "Mandi Hamlin", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Two pages", "A Brazilian supreme court judge", "Derek Mears", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "SSM Cardinal Glennon Children's Medical Center in St. Louis, Missouri.", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "1994", "P.V. Sindhu", "Mexico", "Snickers candy bars", "monoceros", "capone", "Anaheim, California", "uncle Juan Nepomuceno Guerra", "Bergen", "embalming", "the lands in question were not only the first discovered but.... explorers thought of the new lands visited by them while they were still under", "a graphical user interface", "a two - layer coat which is close and dense with a thick undercoat"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6715330686092965}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 0.375, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.37500000000000006, 0.07142857142857142, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.29629629629629634, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.5714285714285715, 1.0, 0.0, 0.08695652173913043, 1.0, 0.4, 1.0, 1.0, 0.9411764705882353, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.5, 0.12500000000000003]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.46875, "CSR": 0.5152439024390244, "EFR": 1.0, "Overall": 0.7101581554878049}, {"timecode": 41, "before_eval_results": {"predictions": ["the Battle of Sainte-Foy", "during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1997", "displacement", "the modern state system", "Megan Park", "the currency used by the majority of European Union's member states, with all but two bound to adopt it", "Kate Walsh", "September 14, 2008", "Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "in early evenings to call ( in spring and summer ) and hunt for food", "semi solid", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "A remittance", "Akshay Kumar", "Shirley Partridge", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Malloy as Pierre, Phillipa Soo as Anatole, Amber Gray as H\u00e9l\u00e8ne", "collect menstrual flow", "pigs", "General George Washington", "Spanish", "Howard Ellsworth Rollins Jr", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node", "four", "Jack Nicklaus", "Norman Greenbaum", "Alan Menken", "16", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Intertropical Convergence Zone ( ITCZ )", "Missouri River", "all Americans the right to be served in facilities which are open to the public -- hotels, restaurants, theaters, retail stores, and similar establishments", "frontal lobe", "10 June 1940", "Tandi", "alekskacz", "ear", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "Kurdistan Region of Iraq", "through almost 30 tunnels, including the 6.2-mile Moffat Tunnel, which passes underneath the Continental Divide.", "the Sadr City and Adhamiya districts of Baghdad City,\"", "California", "King Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6745941195597525}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.08, 0.0, 0.7000000000000001, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8235294117647058, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.17391304347826084, 0.2666666666666667, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5481", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_triviaqa-validation-6700", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518"], "SR": 0.578125, "CSR": 0.5167410714285714, "EFR": 0.9259259259259259, "Overall": 0.6956427744708995}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "The vaporization of water also absorbs heat ; it thereby cools the smoke, air, walls, and objects that could act as further fuel", "in Middlesex County, Province of Massachusetts Bay", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "the composition and powers of the Senate", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "victory", "Gustav Bauer", "Ceramic art", "the Soviet Union", "Covington, Kentucky", "New Mexico", "to condense the steam coming out of the cylinders or turbines", "December 15, 2017", "between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "L.K. Advani", "differential erosion", "Glenn Close", "in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "the government - owned corporation of Puerto Rico", "by two easily observed features", "Norman Greenbaum", "the notion that an English p Larson may'have his nose up in the air ', upturned like the chicken's rear end", "till 2016 a total of 118 elements have been discovered and confirmed", "a circular arc centered at the vertex of the angle is drawn, e.g. with a pair of compasses", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Disha Vakani", "2,140 kilometres ( 1,330 mi )", "by producing an egg through parthenogenesis", "1926", "East Asia", "starting in 1560s", "Frankie Muniz", "R&B singer Lou Rawls", "between 1765 and 1783", "a bridge (Bifrost)", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "a process to ensure that auto owners comply with recalls.", "won two awards.", "prostate cancer,", "wyvern", "Little Lord Fauntleroy", "a rabbit with a fob", "yellow"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6532125333136363}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5384615384615384, 0.2222222222222222, 1.0, 1.0, 0.1818181818181818, 1.0, 0.7272727272727272, 1.0, 0.9, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6857142857142856, 0.14814814814814814, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.08333333333333333, 1.0, 0.7058823529411765, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9411764705882353, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.484375, "CSR": 0.5159883720930232, "EFR": 0.9090909090909091, "Overall": 0.6921252312367865}, {"timecode": 43, "before_eval_results": {"predictions": ["2003", "2007", "a'pick yourself up and dust yourself off and keep going ', female - empowerment song", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "le Roi d'Irlande", "Miami Heat", "1981", "After World War I", "mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Dan Wolff", "Paul Signac", "Don King", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "balsam", "Alex Ryan", "a habitat", "2018", "Windows Media Player 11", "100", "Toledo", "embryo", "During the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Alicia Vikander", "annually in late January or early February", "Ashoka", "the compartments known as Relieving Chambers", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Bumblebee", "the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "armored fighting vehicle", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011, it premiered on 19 August 2011", "2017", "Florida", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "the southwestern part of the island", "to be unfair", "wintertime", "Pangaea", "Newcastle", "Western Australia", "V\u00e1clav Havel", "Mary Bon auto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\"", "\" reset\"", "$60 billion", "Acheson", "Bob Kerrey", "chimpanzee", "Forrest Gump"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5828293733859691}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.5, 0.7741935483870968, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.9387755102040816, 0.5714285714285715, 0.4444444444444444, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.9189189189189189, 0.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-8027", "mrqa_triviaqa-validation-939", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-10340"], "SR": 0.40625, "CSR": 0.5134943181818181, "EFR": 1.0, "Overall": 0.7098082386363637}, {"timecode": 44, "before_eval_results": {"predictions": ["fixed annual carriage fees of \u00a330m", "lightweight aluminum foil with a narrow strip of paper protruding from the top", "Mississippi", "his writings about the outdoors, especially mountain-climbing", "Indianola, Mississippi", "Ritu Nanda Insurance Services (RNIS)", "the British military", "1992", "the Goddess of Pop", "Appalachians", "James Harrison", "Toronto", "the Tomorrowland section of the Magic Kingdom theme park at Walt Disney World Resort", "fennec fox", "the United States Army", "stop motion animation", "Jean Acker", "5,656", "Leucippus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich", "Maria Brink", "Manitowoc County, Wisconsin", "the Northrop P-61 Black widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "United States ambassador to Ghana", "Marvin Hamlisch", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Hillsborough County", "OutKast", "Richard Street", "Mobutu Sese Seko", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS Church)", "Pakistan", "Shohola Falls", "bringing French cuisine to the American public with her debut cookbook, \"Mastering the Art of French Cooking", "South America", "in 2006", "four months in jail", "the Battle of Normandy", "Mary Elizabeth Hartman", "over 9,000", "James Bolam", "potential of hydrogen", "San Antonio, Texas", "(Dean) King", "The Finger Tab", "Kent", "almost 9 million", "Bahrain", "2008", "small", "Moses", "a chapter, scene, or section of Their Eyes", "rhythm and blues and soul singer Wilson Pickett"], "metric_results": {"EM": 0.453125, "QA-F1": 0.569612809065934}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.2857142857142857, 0.3076923076923077, 0.6666666666666666, 0.2857142857142857, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.33333333333333337, 1.0, 1.0, 0.125, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2837", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-1020", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.453125, "CSR": 0.5121527777777778, "EFR": 1.0, "Overall": 0.7095399305555555}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "early 20th century", "Australian", "1903", "interstate commerce", "Naomi Wallace", "McLaren-Honda", "Tufts University", "Macau", "Azeroth", "Squam Lake", "The Livingston family of New York", "Tayeb Salih", "King James II of England", "the second line", "526", "Scotland", "\"rock and roll\"", "GmbH", "Mick Jackson", "Jatin\u2013Lalit", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "Chesley Sullenberger", "nuclear detonation detection systems", "the Asia-Pacific War", "Romantic", "Hugh Caswall Tremenheere Dowding", "AMC Entertainment Holdings, Inc.", "the New York Islanders", "fennec fox", "1978", "the Surtees Racing Organisation team that competed as a constructor in Formula One, Formula 2 and Formula 5000", "Canadian", "Pacific Place", "the Matildas", "\"Bad Blood\"", "Rudebox", "about 5320 km", "Francesco Maria Piave", "\"Super Hit\"", "Sacramento Kings", "Walldorf", "Fife", "Fyvie Castle", "Faisal Qureshi", "the British Army", "the citizens exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament", "Boletus edulis", "Schleiden and Schawnn", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "his pregnant wife", "Saudi Arabia", "Bob Turner", "two"], "metric_results": {"EM": 0.5, "QA-F1": 0.569828869047619}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.08333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-4995", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.5, "CSR": 0.5118885869565217, "EFR": 1.0, "Overall": 0.7094870923913044}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "Guiana Highlands", "The King and I", "Republican National Committee's website address is GOP.com", "1996", "5", "Greenland sharks", "The Word", "Abraham Lincoln", "St Jude Thaddeus", "Van Diemenslandt", "the Death Penalty", "xerophyte", "Jackie Robinson", "Manhattan", "Dian Fossey", "MI5", "Harrow", "creme anglaise", "onions", "pork", "curling", "Victoria Coren Mitchell", "Gettysburg", "Chile", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "Barack Obama", "Canada's Liberal Party", "mortadella", "Cuba", "David Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "Manila", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Daily Herald", "(1939\u20131945)", "food that is permissible according to Islamic law", "2016", "the courts", "2017 season", "Chief of Protocol", "Diamond White", "1944", "Daniel Nestor, from Canada,", "Jeddah, Saudi Arabia", "death of a pregnant soldier", "Beatrix Potter", "Dan Eggen and Elizabeth Williamson", "Reader's Digest", "Solomon"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6511284722222221}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5875", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3458", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262"], "SR": 0.5625, "CSR": 0.5129654255319149, "EFR": 1.0, "Overall": 0.709702460106383}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "alex turner", "the last battle on Scottish soil took place on a nearby moor at floden", "Runic", "florida", "florida", "alex Planck", "rotherham", "conduction", "misery", "Styal", "olukord", "a Blind Beggar", "alexy", "floroy Burrell", "parlophone", "Wild Atlantic Way", "canada", "alex", "noddy", "lackawanna", "canada", "oscar", "muezzin", "window", "keel", "alex", "a sub-orbital test flight of the Saturn IB and Command and Service Module", "flit", "alex florida", "alex hart", "evita", "albino", "alex", "east fife", "st Pancras International Station", "social environment", "sliced bread", "dilbert", "aristotelian Tragedy", "nunc dimittis", "French", "medea", "hk", "cribbage", "alex penny Lane", "Johannesburg", "France", "muffin", "South Korea", "Prince James, Duke of York", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Galveston, Texas,", "carrier based in Texas.", "Robert Frost", "King Edward III", "hk", "1995"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5182291666666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-7698", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.46875, "CSR": 0.5120442708333333, "EFR": 0.9117647058823529, "Overall": 0.6918711703431373}, {"timecode": 48, "before_eval_results": {"predictions": ["jimmy farfrae", "sesame Street", "marcella Hazan,", "cabbage", "eddie", "jimmy magoo", "fleece", "ash", "placentals", "new Zealand", "eagle", "60", "eaglefinger", "1983", "fish", "mongols", "1875", "collector", "penny", "rother Stewart", "jimmy of anjou", "bagram collection Point", "maggie Gilkeson", "chrysler", "fur hat", "mary farfrae", "classical Theology", "the United States", "spain", "spain", "biathlon", "nampa", "jimmy Chan", "Vienna, Austria", "white", "jaws", "jimmy henderson", "rabbit", "spain", "jimmy leadbetter", "Charles Foster Kane", "nikola", "menorah", "Dutch", "texland", "pepperoni", "quant pole", "jimmy stout", "marmole king", "azalea", "ireland", "Chuck Noland", "Virginia", "Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park, Kenya", "2010", "Greece,", "10 below", "100 to 150 troops", "silver", "the American Kennel Club", "Omaha", "jedoublen"], "metric_results": {"EM": 0.296875, "QA-F1": 0.43072916666666666}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.3333333333333333, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-1594", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-726", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.296875, "CSR": 0.5076530612244898, "EFR": 0.9777777777777777, "Overall": 0.7041955428004536}, {"timecode": 49, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.77734375, "KG": 0.4421875, "before_eval_results": {"predictions": ["Percy Sledge", "Iran", "tobacco", "frenchman", "jennifer", "Daniel Boone", "Thames Street", "theodore roosevelt", "satyrs", "tuna", "la boh\u00e8me", "bicapitalized Micro Soft", "scapulae", "Garrick club", "jaber elbaneh", "master Humphrey", "Susan Bullock", "the American Civil War", "black", "jennifer farfrae", "jimmy tambling", "Florence", "saint Basil", "Violet Wonka", "severn", "australian", "South Africa", "grass", "Nicaragua", "Clement Attlee", "wars of roses", "chemnitz", "clangers", "trout", "ap\u00e9ritif", "kennedy Norton", "baltic", "arts", "hair loss", "sprint", "charlie darlings", "Robin Hood", "Chris Martin", "bilma flinstone", "george gently", "rugby", "honda", "scotland", "11", "tobacco", "bovine", "free floating", "Tom Selleck", "Chicago", "superhuman abilities", "Texas Tech University", "leicestershire", "Herman Cain", "Afghanistan", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration", "George Babbitt", "Oklahoma", "another vodka", "four"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4965945512820512}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-2670", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.4375, "CSR": 0.50625, "EFR": 1.0, "Overall": 0.686953125}]}