{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4280, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "the Pacific", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "the infected corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "in series 1", "chief electrician", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "the Masovian Primeval Forest", "in the days, weeks and months after it happened", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "an algorithm for multiplying two integers can be used to square an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "British Gas plc", "More than 1 million", "2011", "in the same way as prices for any other good", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Tibetan", "Matthew 16:18", "the U.S. ship that was hijacked off Somalia's coast.", "Wwanda", "revelry", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.734375, "QA-F1": 0.8144445831945832}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-2520", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.734375, "CSR": 0.78125, "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "the coast of Denmark", "quantum mechanics", "On Tesla's 75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel", "locomotion", "each six months", "Japanese", "the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two-page", "Arizona Cardinals", "1991", "Chaffee", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "more integral", "declare martial law", "a customs union", "Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ricky Martin", "Documents", "the company's factory in Waterford City, Ireland", "nitrogen", "Annemarie Moody", "Ocean currents", "six", "It always begins with the music", "musician", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.765625, "QA-F1": 0.8091044372294371}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1637", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-7949", "mrqa_squad-validation-235", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.765625, "CSR": 0.7760416666666666, "EFR": 1.0, "Overall": 0.8880208333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["the 1994 Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "prefabricated housing projects", "Sakya", "woodcuts", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "two of Triton's daughters", "Upper Lake", "northern China", "giving her brother Polynices a proper burial", "political figures", "Jean-Claude Juncker", "2000", "oxygen", "20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "ireland", "multiple revisions", "50 fund", "integer factorization", "economic inequality", "Isel", "first names and surnames", "Benazir Bhutto", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "ATP synthase", "late 19th century", "Channel Islands", "separate faith and reason", "Alberich", "charleston", "Emeril Lagasse", "Churchill Downs", "charleston", "charleston", "10:00, Wed, Dec 22, 2010", "ireland", "conducting", "the limbic system", "travis", "George Fox", "charleston", "charleston", "24 hours a day and 7 days a week", "the Liberal Party of Canada", "10, 2010"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6152281746031746}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-4293", "mrqa_squad-validation-8324", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-1189", "mrqa_squad-validation-7230", "mrqa_squad-validation-8906", "mrqa_squad-validation-2463", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-1581", "mrqa_hotpotqa-validation-437", "mrqa_hotpotqa-validation-3821"], "SR": 0.578125, "CSR": 0.7265625, "EFR": 0.9629629629629629, "Overall": 0.8447627314814814}, {"timecode": 4, "before_eval_results": {"predictions": ["in higher plants", "Parliament of Victoria", "Zaha Hadid", "Fort Edward", "Science and Discovery", "the Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "Battle of Olustee", "port city of Kaffa", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei's grandson", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related diseases", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "Sabina Guzzanti", "22", "seven alleged militants", "racist", "Brian Smith", "used luxury cars", "Muslim", "this will be the first time any version of the Magna Carta has ever gone up for auction", "Monday night", "15", "fighters", "Capt. Chesley \"Sully\" Sullenberger", "backbreaking labor", "FBI recordings of his phone calls", "Julissa Brisman", "one", "Nevaeh", "$1,500", "National Industrial Recovery Act", "tracey\u2019s younger son Travis", "Humberside Airport", "colombia"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6706308356676003}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.32, 0.0, 1.0, 1.0, 0.8571428571428571, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-7094", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-7960", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.640625, "CSR": 0.709375, "EFR": 1.0, "Overall": 0.8546875}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "Connectional Table", "Deformational", "a high-level marketing manager", "500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "they must go to jail as part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "Philippines", "Denver's Executive Vice President of Football Operations and General Manager", "1970s", "the spoils of the war", "German Te Deum", "1795", "Bermuda 419 turf", "air could be liquefied, and its components isolated, by compressing and cooling it", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "rudimentary", "1957", "mother-of-pearl", "Gene Barry", "The President of the United States negotiates treaties with foreign nations", "It is mainly for the purpose of changing display or audio settings quickly", "an Ohio newspaper", "Herbert Hoover", "radius R of the turntable", "Panning", "Justin Timberlake", "the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP", "military experts", "unknown origin", "speed limit '' omitted and an additional panel stating the type of hazard ahead", "Lowe's opened its first three stores in Canada", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman", "David Gahan", "The Stanley Hotel", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the last day before the long fast for the Lent period", "Jaipur", "Jonas Olsson,", "The anti-torpedo boat origin of this type of ship is retained in its name", "Newport Gwen Dragons"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6961876849732793}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.38888888888888895, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7096774193548386, 0.0, 1.0, 0.09090909090909091, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-376", "mrqa_squad-validation-9908", "mrqa_squad-validation-436", "mrqa_squad-validation-3473", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.59375, "CSR": 0.6901041666666667, "EFR": 0.9615384615384616, "Overall": 0.8258213141025641}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "a Serbian Orthodox priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "off-campus rental policies", "1936", "New Birth", "gold", "a deficit", "Vivienne Westwood", "reciprocating Diesel engines, and gas turbines", "disease", "\"TFIF\"", "Confucian propriety and ancestor veneration", "rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "a chain or screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "end of the season", "10 years in prison", "Jonas", "African-Americans", "will not support the Stop Online Piracy Act", "Chuck Bass", "hot and humid", "an animal tranquilizer", "1980", "Sunday", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "more than 170", "United States", "first five Potter films", "a Prius driver will be at the front of the line, self-righteously driving under the speed limit", "3 to 17", "one bomber.", "a \"stressed and tired force\" made vulnerable by multiple deployments", "James Whitehouse", "we want to ensure we have all the capacity that may be needed", "a series of monthly meals", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "Mike Gatting", "Colgate University", "a personal healing in", "a fat or fatty acid in which there is at least one double bond within the fatty acid chain", "New Testament"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6661209130781499}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.2105263157894737, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.45454545454545453, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7950", "mrqa_squad-validation-3370", "mrqa_squad-validation-6001", "mrqa_squad-validation-486", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.609375, "CSR": 0.6785714285714286, "EFR": 1.0, "Overall": 0.8392857142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "unequal", "July 1969", "when Hitler's secret police demanded to know if they were hiding a Jew in their house.", "a yellow chlorophyll precursor", "spontaneous", "the courts of member states", "gold", "Time Lord", "Buckland Valley near Bright", "Scottish rivers", "ricks for Warsaw", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "their \"Freshman Year\" experience", "India", "Zulfikar Ali Bhutto,", "at the country's third-largest oil refinery", "April 24 through May 2.", "Krishna Rajaram,", "early detection and helping other women cope with the disease", "250,000", "Timothy Masters", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jason Chaffetz", "United States Holocaust Memorial Museum, The American Academy of Diplomacy and the United States Institute of Peace.", "\"Dance Your Ass Off.\"", "a legitimate forum for prosecution, while bringing them in line with the rule of law", "Gary Brooker", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "seeking help", "Japanese officials", "condition", "\"Empire of the Sun,\"", "Norman given name Robert", "stronger", "Matthew Ward Winer", "Doc Holliday", "opposite R\u00fcgen island", "Mustique", "green"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6803729907245532}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.9375, 1.0, 1.0, 0.5333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.08888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-3938", "mrqa_squad-validation-7587", "mrqa_squad-validation-872", "mrqa_squad-validation-1556", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_triviaqa-validation-2858"], "SR": 0.59375, "CSR": 0.66796875, "EFR": 0.9615384615384616, "Overall": 0.8147536057692308}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers and French regular troops", "Roman Catholic", "archenemy", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday afternoon", "\"Journey's End\"", "immediate", "Levi's Stadium", "decidedly Wesleyan", "art posters", "Elbegdorj", "Chinggis Khaan", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington", "autonomy", "Islamic", "$12.9 million", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic", "finance", "terminal brain cancer", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice act", "Bianchi's death during childbirth", "Animal Planet", "crashing his private plane into a Florida swamp.", "The situation is pretty much resolved", "54 bodies", "early detection", "Diversity", "$250,000", "make sure water continues flow through the river channel and not spread out over land", "Nazi Germany", "March 27", "Kirchners", "directly involved in an Internet broadband deal with a Chinese firm", "The son of Gabon's former president", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Mikkel Kessler", "The Everglades,", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans Pelicans", "many investors paying huge sums for individual bulbs", "Independence Day", "eat too many chockies, these, & you may find yourself crook,", "Hockey"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7191517577208366}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.07692307692307691, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.07407407407407408, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-9839"], "SR": 0.640625, "CSR": 0.6649305555555556, "EFR": 1.0, "Overall": 0.8324652777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "Book of Discipline", "Katharina", "theology, William of Ockham, and Gabriel Biel", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "\"coo\", \"hoos\"", "50%", "Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English.", "San Diego", "CRISPR", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "issues related to the substance of the statement.", "Edict of Fontainebleau", "15", "\"The U.S. subcontributed out an assassination program against al Qaeda... in early 2006.\"", "Ronaldinho", "cooperating with Turkey in engaging with the Taliban", "25", "a treadmill", "the couple's surrogate lost the pregnancy.", "environmental and political events", "people are starving, aid is scarce, and the only operating factories serve the military.", "at least two and a half hours.", "Elin Nordegren", "Arabic, Russian and Mandarin", "6,000", "drug test after taking a medicine that contained the banned substance cortisone", "President Clinton.", "purchasing the machine guns and silencers", "MDC head Morgan Tsvangirai.", "expel companies under British rule.", "future relations between the Middle East and Washington.", "canyon in the path of the blaze", "Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school,", "strife in Somalia,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois,", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "they did not know how many people were onboard.", "London", "Abigail '' that he loved her", "immediate physical and social setting in which people live or in which something happens or develops.", "William Tell", "Andr\u00e9 3000", "Groundhog Day", "Cleopatra,", "a singer"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6244686284428131}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.26086956521739124, 1.0, 1.0, 0.0, 1.0, 0.060606060606060615, 1.0, 1.0, 0.25, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2077", "mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-10185", "mrqa_squad-validation-2429", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-11812"], "SR": 0.546875, "CSR": 0.653125, "EFR": 0.9655172413793104, "Overall": 0.8093211206896551}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley, New York,", "pseudorandom", "John Wesley", "Genghis Khan's", "water", "internal strife", "yellow fever outbreaks", "DC traction motor", "Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars,", "Lothar de Maizi\u00e8re", "33", "chairman and CEO", "Brazil", "July 18, 1994", "pelvis and sacrum -- the triangular bone within the pelvis.", "issued his first military orders as leader of North Korea", "light snow or flurries", "Willem Dafoe", "\"Maude\"", "Phillip A. Myers.", "Korea's capital", "two weeks after Black History Month", "58 people", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Lance Cpl. Maria Lauterbach", "Dangjin, South Korea", "\"novel that you would be embarrassed to buy\"?", "China", "GospelToday", "his injuries,", "11-month-old Lisa Irwin was reported missing,", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "will American go bankrupt?", "16 August 1975", "Bonnie Aarons", "one", "Kabinett - Wines", "Lionsgate.", "James Lofton,", "meditation", "Hair-like structures that help paramecium move around."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6163983585858586}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-3235", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.5625, "CSR": 0.6448863636363636, "EFR": 1.0, "Overall": 0.8224431818181819}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks,", "Elway", "Philo of Byzantium", "36", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "sum of divisors", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "pink mice", "antelope", "nipples", "the Precambrian period", "cooperative", "Anastasia Dobromyslova", "Gagapedia", "9", "Space Jam 2", "Radish", "Robert Ludlum", "giant", "geothermal", "the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew", "London Underground Piccadilly Line", "Wisconsin", "Malay", "Manet", "Wikia", "Wyoming", "2005", "1971", "DodgeDodge", "bawdy", "Rome", "petticoat", "Enrico Caruso", "Elizabeth Arden", "collapsible support assembly", "Sir Hardy Amies", "Antigua and Barbuda", "Wales and 21st most common in England", "Can't Get You Out of My Head", "Ray Looze", "Bloomingdale Firehouse", "Nazi Germany", "Golden Gate Yacht Club of San Francisco", "footwear", "Jamaica", "Buddhism"], "metric_results": {"EM": 0.5625, "QA-F1": 0.641868418040293}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.125, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6153846153846153, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983"], "SR": 0.5625, "CSR": 0.6380208333333333, "EFR": 1.0, "Overall": 0.8190104166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "90-60's", "Panini", "Bills", "anti-colonial movements", "the Rhine Valley", "A", "be suspicious of even the greatest thinkers and to test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "if the EU does not comply with its basic constitutional rights and principles (particularly democracy, the rule of law and the social state principles)", "1788", "2006", "Roman Catholic", "Protestantism", "John Wesley", "the nationalisation law was from 1962, and the treaty was in force from 1958,", "Eternal Heaven", "largest Wind Turbine", "Jessica Simpson", "Leonard Cheshire", "Val Doonican", "Virgil", "France", "T.S. Eliot", "iceland", "spy dragon", "Vladivostok", "Shirley Bassey", "telephone and high-speed data communications", "Camellia sinensis", "AFC Wimbledon", "Bob Monkhouse and Kenneth Connor", "Malaysia", "astronomy", "gin", "George Clooney", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "Cr\u00ef\u00bf\u00bdme de cassis", "rain", "United States", "Brigit Forsyth", "london", "kanji", "Hector's death and the destruction of Troilus and Cressida", "Thomas Edward Lawrence", "Kent", "Zola", "Vanguard", "white", "Switzerland", "soda water", "the people of the United States", "79", "Sky News", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "bremen", "David", "\"The Screening Room\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6492948639622027}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.8387096774193548, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-6655", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-3161", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-3503", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_newsqa-validation-3860"], "SR": 0.609375, "CSR": 0.6358173076923077, "EFR": 1.0, "Overall": 0.8179086538461539}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "Huguenots had their own militia", "after the end of the Mexican War", "61", "quality of a country's institutions", "cilia", "gravity", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron", "hymen", "Zeus", "albinism", "Straits of Tiran", "Brigit Forsyth", "Call My Bluff", "March 10, 1997", "cuddly pet", "the Battle of the Three Emperors", "Velazquez", "althea Gibson", "lizards", "strong cold southwest wind", "table tennis", "medical manual", "penhaligon", "Gandalf", "michael", "Jinnah International Airport", "Monday", "Caracas", "bridge", "soap", "liquor", "Avro Lancaster", "william ryder", "konnie Huq", "michael teas", "harrods", "2007", "ryder", "scarface", "pale yellow", "renoir", "bubba", "June 12, 2018", "Filipino", "London", "Lambic", "Nook", "Steven Green", "commas", "fortune", "bridge Street, Huntsville, AL.", "\"Every Breath You Take\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.5869791666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-4981", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-13371"], "SR": 0.53125, "CSR": 0.6283482142857143, "EFR": 1.0, "Overall": 0.8141741071428572}, {"timecode": 14, "before_eval_results": {"predictions": ["seven", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium", "Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep after it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "they were at least partly the product of a declining state of mind", "1898", "The Deadly Assassin and Mawdryn undead", "either small fission systems or radioactive decay for electricity or heat", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1997", "Authority", "junior enlisted sailor", "Spanish moss", "Chinese cooking", "Vienna", "between 2 World Trade center and 3 World Trade Center", "Kevin Spacey", "1 November", "78", "white blood cell", "International Border", "President", "G minor", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shetty", "Sedimentary rock", "October 1, 2014", "United States", "Claims adjuster", "The neck", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "body plan", "three", "annual plants", "boudin", "kew Gardens", "Nikita Khrushchev", "$500,000", "Alexandros Grigoropoulos,", "reaper", "SHIELD", "BBC building in Glasgow, Scotland", "\"Larry King Live\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.6953067765567766}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9523809523809523, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.2857142857142857, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-2339", "mrqa_squad-validation-2523", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-121", "mrqa_searchqa-validation-726", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.59375, "CSR": 0.6260416666666666, "EFR": 0.9615384615384616, "Overall": 0.7937900641025641}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "The Late Late Show", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "destroyed over 2,000 buildings", "Famous musicians", "CBS", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "88%", "Necessity-based", "950 pesos ( approximately $ 18 )", "the fourth C key from left on a standard 88 - key piano keyboard", "Seattle, Washington", "the Battle of Antietam", "The all - time record for lowest number of goals scored to be bestowed the award", "In Time", "the 2nd century", "Glenn Close", "four times", "The Italian Agostino Bassi", "five seasons", "a beach in Malibu, California", "the church at Philippi", "the Dutch", "September 2017", "Professor Kantorek", "1546", "Sam Waterston", "Bhupendranath Dutt", "a warrior", "Dr. Lexie Grey", "Majandra Delfino", "December 19, 1971", "Uruguay", "arthur conan", "Matt Jones", "The National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "domesticated sheep", "1970s", "Director of National Intelligence", "D. a.D.A.", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson", "Thespis", "Portugal", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "boxelder bug", "Code 02PrettyPretty", "musician", "the parliament within 15 days.", "abduction of minors", "Nevada", "Pablo Neruda", "Stage Stores,", "1881"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5962157634032634}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-7235", "mrqa_squad-validation-551", "mrqa_squad-validation-7141", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674", "mrqa_searchqa-validation-5103"], "SR": 0.453125, "CSR": 0.615234375, "EFR": 0.9142857142857143, "Overall": 0.7647600446428571}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "New England Patriots", "Rajendra K. Pachauri", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "the Jin", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland", "Microsoft Office file formats", "SAVE", "SAS AB", "1993 to 2001", "1951", "Southern Miss Golden Eagles", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "two decades", "BAFTA TV Award Best Actor winner", "Jello Biafra drew on Nardwuar's face with a marker pen", "the Battle of Culloden", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements", "Julian McMahon", "lepisma saccharina", "7.63\u00d725mm Mauser", "Best Animated Feature", "cAC FC-1 \"Xiaolong\"", "Delacorte Press", "Neighbourhoods", "Secretariat", "Wake Island", "Hydrogen", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "\"Southern Living\" Reader's Choice Awards", "Thomas Harold Amer", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Amway", "Parlophone Records", "kwaZulu-Natal", "Surrey", "The Girl", "Charles Russell", "Boyd Gaming", "lepisma saccharina", "1966, 1967, 1968, 1970", "Glenn Close", "Florence Welch", "Neighbours", "Ewan McGregor", "2011", "pippa passes", "a enslaved African American", "expanding U.S. sanctions against Zimbabwe", "Shelley Moore Capito"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5979369588744589}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9090909090909091, 0.3636363636363636, 0.25, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-76", "mrqa_squad-validation-8509", "mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-1092", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3521"], "SR": 0.484375, "CSR": 0.6075367647058824, "EFR": 1.0, "Overall": 0.8037683823529411}, {"timecode": 17, "before_eval_results": {"predictions": ["force of gravity", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m.", "Japanese", "charter", "1830", "nonfunctional pseudogenes", "inner chloroplast membrane", "Charlie Sheen", "Steveland Hardaway Morris", "lepisma saccharina", "Sc\u00e8nes de la Vie de Boh\u00e8me", "formic acid", "Castilla-La Mancha", "Zimbabwe", "Mr. Boddy", "Edward \"Ted\" Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "hound", "General Xenophon", "Fuller's", "Fresh Water Load Line", "Nick Hornby", "The Comedy of Errors", "Charles V", "England", "welch", "weight plates", "\"big house\"", "Hadrian", "US", "lepisma saccharina", "Moonee Ponds, a suburb in Melbourne, Victoria", "Hamburg", "avian Aqua Miser", "Tangled", "\"The French Connection\"", "CBS", "Leicester City", "Robert Cummings", "Jessica Simpson", "Culture Club", "Finland", "3000m race", "Angus, Scotland", "Japan", "Travis Tritt and Marty Stuart", "The Union's forces", "New Jewel Movement", "sub-Saharan Africa", "north-south", "Onjuna beach in Goa", "Lev Ivanov", "Oshkosh", "\"Papa's Got a Brand New Bag\"", "jeopardy/1870_Qs.txt", "\"The Sunday Thing\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5747395833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-2208", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.484375, "CSR": 0.6006944444444444, "EFR": 1.0, "Overall": 0.8003472222222222}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "extremely high", "Manakintown", "northwest", "fewer than 10 employees", "Middle Miocene", "new magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "2010", "The Hustons", "Anna Faris", "Isthmus of Corinth", "ability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Alison", "1985 -- 1993", "775 rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Swadlincote", "threshold 85 %", "Zoe Badwi", "1995", "Identification of alternative plans / policies", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "coronary arteries", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "An empty line", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "a violation of nature", "September 2017", "moral", "\" Rising Sun Blues ''", "Part 2", "dumbo", "the failure of the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "Mentor", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6615550658669265}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.08695652173913042, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3193", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_hotpotqa-validation-4735"], "SR": 0.59375, "CSR": 0.600328947368421, "EFR": 0.9615384615384616, "Overall": 0.7809337044534412}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law,", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "Long troop deployments", "Joe Pantoliano", "a Florida girl who disappeared in February,", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "nearly three out of four", "natural resources around the islands should be protected, and Britain must accept international resolutions labeling the Falklands a disputed area", "Tuesday in Los Angeles", "forgery and flying without a valid license", "Anil Kapoor", "19-year-old", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "glamour and hedonism", "ancient Egyptian antiquities", "snowstorm", "sports cars", "a lizard-like creature from New Zealand", "Mutassim", "Thursday and Friday", "\"Stunt double Terry Leonard performs a hazardous jump from horseback to a truck as Indiana Jones in \" Raiders of the Lost Ark.\"", "Russia", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "\u00a320 million ($41.1 million) fortune", "Kingman Regional Medical Center", "Kim Jong Il", "Manmohan Singh", "his health and about a comeback", "silent", "40 militants and six Pakistan soldiers dead", "Roger Federer", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list, had been released", "Lousiana", "the Southeast", "the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home", "Obama", "\"A Mother For All Seasons.\"", "a progressive neurological disease", "back at work", "necropsy or animal autopsy", "teenager", "Amber Riley and her partner Derek Hough", "John Adams", "cabbage", "Zager and Evans", "Bob Hurley", "his fourth term as At-large Chairman of the Board of Supervisors of Prince William County, Virginia", "obscenity", "Royalist cavalry", "Finland", "1937", "Al Shorta"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5199762009504656}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.4, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.923076923076923, 0.6666666666666666, 0.7407407407407407, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.23529411764705882, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-1958", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-2013", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-1085"], "SR": 0.421875, "CSR": 0.59140625, "EFR": 0.972972972972973, "Overall": 0.7821896114864866}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death wish Coffee", "L", "Cameroon", "just after midday on a cold December Monday in South Korea's capital", "ballots", "clothes that are consistent and accessible", "three empty vodka bottles,", "secretary of defense on China, Taiwan, Hong Kong and Mongolia,", "1959", "Felipe Massa", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "she was humiliated by last month's incident,", "the composer of \"Phantom of the Opera\" and \"Cats\" and one of Britain's richest men,", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony,", "Amanda Knox's", "well over 1,000 pounds", "Iran's development of a nuclear weapon is unacceptable.", "bright blue-purple", "using recreational drugs", "ceo Herbert Hainer", "Brett Cummins", "a nearby day care center whose children are predominantly African-American.", "inmates", "Elspeth Cameron-Ritchie", "\"E! News\"", "Three French journalists, a seven-member Spanish flight crew and one Belgian", "fuel economy and safety", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Mexico's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "get out of the game,", "An undated photo of Alexandros Grigoropoulos,", "signed a power-sharing deal with the opposition party's breakaway faction,", "a 57-year old male", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Matthew Fisher,", "the creation of an Islamic emirate in Gaza,", "the iconic boogeyman Jason Voorhees in the new \"Friday the 13th\" movie.", "determining which Guant detainees should be tried by a U.S. military commision,", "Sea World in San Antonio", "a job he liked at the U.S. Holocaust Memorial Museum,", "about 50", "Ku Klux Klan", "1939", "Yale University", "Bury", "stamens", "Malayalam", "August 17, 2017", "a jacket, gloves or a briefcase", "By the 1950s, scientists were able to do this to frogs;", "Hodel", "gives them access to US courts", "the British rock group Coldplay"], "metric_results": {"EM": 0.375, "QA-F1": 0.5016042242604742}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.9523809523809523, 0.6, 0.14285714285714288, 0.23076923076923078, 1.0, 0.8, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 0.0, 0.9189189189189189, 0.14814814814814814, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987"], "SR": 0.375, "CSR": 0.5811011904761905, "EFR": 1.0, "Overall": 0.7905505952380952}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "\u201c Splash\u201d", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "shoulders", "Madonna's", "Glasgow", "satellite-based navigational system", "Australia", "gastronomy", "Pearson PLC", "Irish Setter", "American Civil War", "Loch Lomond", "Jeschner", "Queensland", "medium-sized", "Taiwan (or Republic of China)", "Harrisburg", "Mustela erminea,", "percussion", "Dr John Sentamu", "CameroonCameroon", "pongo", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "emperor,", "hometown", "Russell Crowe", "Phil Roberts", "NZ Herald News", "Robin Goodfellow", "Samuel Butler", "chamomile", "England", "tarn", "SS United States", "Albert Square", "Newbury", "the Old Testament", "70 million people", "Target Corporation", "Sister, Sister (1982 film)", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "\"international NGO\"", "John Jackson Dickison", "the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "talk show queen Oprah Winfrey.", "his mother."], "metric_results": {"EM": 0.5, "QA-F1": 0.5520833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-6408", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.5, "CSR": 0.5774147727272727, "EFR": 1.0, "Overall": 0.7887073863636364}, {"timecode": 22, "before_eval_results": {"predictions": ["flushing action of tears and urine", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin,", "jesuit", "Robert Peary", "pearls", "Utah Territory", "Carrie Underwood", "Drambuie", "he made his horse a consul, his palace a brothel, and his", "university of Michigan", "Langston Hughes", "Pain tolerance", "jesuit", "Tito Puente", "lasso", "UNFINISHED", "USS LST 325", "rhodesian ridgebacks", "David Robert Joseph Beckham,", "Arturo Toscanini", "chemistry", "Miracle in the Andes", "a proscenium arch", "Montenegro", "discus throw", "jedoublen", "basidiomycota", "james", "jenney Thorne-Smith", "Idi Amin", "jedoublen/jeopardy", "physical", "terracotta", "Plutarch", "Rudy Giuliani", "masa", "40 seconds", "the Vikings", "fairfield Street", "Bastille Day", "Typhoid Mary", "inlet formed by the partial submergence of an unglaciated river valley", "baviere-quebec.org", "Williamsburg", "Jul 13, 2010", "jesuit", "hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off", "John Knox", "the internal reproductive anatomy", "$652.4 million in North America and $1.528 billion in other countries", "Epidemiology", "the Big Bopper", "Tesco", "london and North Eastern Railway Class A4 4-6-2", "Bruce McLaren", "the Battelle Energy Alliance", "IT", "debris", "$10 billion", "Bailey, Colorado,"], "metric_results": {"EM": 0.375, "QA-F1": 0.4804304355774944}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false], "QA-F1": [0.2222222222222222, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.15384615384615385, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6437", "mrqa_squad-validation-10104", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-1118", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-1997"], "SR": 0.375, "CSR": 0.5686141304347826, "EFR": 1.0, "Overall": 0.7843070652173914}, {"timecode": 23, "before_eval_results": {"predictions": ["24 September 2007", "1493\u20131500", "the Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "the Gulf of Mexico", "cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Basel, Switzerland", "the Argo", "prometheus", "the Altamont", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "a multi-user dungeon", "Cyrenaica", "khaki", "a rock", "Miguel Indurain", "Diego Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "Barack Obama", "the Earth", "February 8, 2015", "the aliquot", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "radar dish", "Manchester City", "\"All I Want\"", "William Golding", "Sally Ride", "a cyclone", "Fife", "athletics", "Adidas", "the Hunting of the Snark", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "\"White\" and \"Black\"", "flour and water", "Ross Elliott", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "Tennis Channel", "fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6607602415966387}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7871", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-3049", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4882", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-458"], "SR": 0.578125, "CSR": 0.5690104166666667, "EFR": 1.0, "Overall": 0.7845052083333334}, {"timecode": 24, "before_eval_results": {"predictions": ["civil disobedients are constrained in their use of coercion by their conscientious aim to engage in moral dialogue,", "the chosen machine model", "Universal Studios and Walt Disney Studios", "1997", "a suite of network protocols created by Digital Equipment Corporation", "Christopher Savoie", "15", "nine-wicket win over the world's number one ranked Test nation in Melbourne on Tuesday.", "Pyongyang and Seoul", "committed adultery", "11", "change course", "Alwin Landry's supply vessel Damon Bankston", "Jason Chaffetz", "money or other discreet aid for the effort if it could be made available,", "Sarah Brown", "illegal crossings", "environmental", "Costa Rica", "Afghan security", "Saturday", "38,", "70,000 or so", "Climatecare", "\"E! News\"", "coach", "Steve Williams", "McDonald's", "is a slight girl of 11, living in a simple home in a suburb of Islamabad", "Pastor Paula White", "2008", "Diego Maradona", "Dog patch Labs", "club-themed", "two", "Itawamba County School District", "Mitt Romney", "EU naval force", "Plymouth Rock", "Liza Murphy", "civil rights leaders and prominent Democrats", "Goa", "former U.S. secretary of state", "33", "five", "get better skin, burn fat and boost her energy", "cell phones are valuable contraband, fetching a greater asking price from convicts than some shipments of illegal drugs.", "campus patrols are in reducing campus violence, the most powerful form of prevention is believing that students can help stop crime from happening.", "Alwin Landry's", "Karthik Rajaram", "Sunday's strike", "death and destruction", "an Irish feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "italy", "radar", "art", "the 16th season", "the NFL single-season touchdown reception record", "South America", "freestyle", "Florence Nightingale", "Belief"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5547126128622839}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [0.09999999999999999, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.2631578947368421, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-610", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_searchqa-validation-3826"], "SR": 0.46875, "CSR": 0.565, "EFR": 1.0, "Overall": 0.7825}, {"timecode": 25, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1085", "mrqa_hotpotqa-validation-1324", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2839", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5853", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-955", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3516", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7025", "mrqa_naturalquestions-validation-703", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9991", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-14", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2459", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3207", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-1273", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-1439", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-5103", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8325", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-9467", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9843", "mrqa_squad-validation-10033", "mrqa_squad-validation-10066", "mrqa_squad-validation-10139", "mrqa_squad-validation-1018", "mrqa_squad-validation-10406", "mrqa_squad-validation-1083", "mrqa_squad-validation-111", "mrqa_squad-validation-1174", "mrqa_squad-validation-1255", "mrqa_squad-validation-1268", "mrqa_squad-validation-1291", "mrqa_squad-validation-133", "mrqa_squad-validation-1454", "mrqa_squad-validation-1632", "mrqa_squad-validation-1637", "mrqa_squad-validation-164", "mrqa_squad-validation-164", "mrqa_squad-validation-1739", "mrqa_squad-validation-1763", "mrqa_squad-validation-1776", "mrqa_squad-validation-1817", "mrqa_squad-validation-1848", "mrqa_squad-validation-1893", "mrqa_squad-validation-2078", "mrqa_squad-validation-2087", "mrqa_squad-validation-2126", "mrqa_squad-validation-2137", "mrqa_squad-validation-2232", "mrqa_squad-validation-2239", "mrqa_squad-validation-2347", "mrqa_squad-validation-2400", "mrqa_squad-validation-2402", "mrqa_squad-validation-2448", "mrqa_squad-validation-2460", "mrqa_squad-validation-248", "mrqa_squad-validation-2520", "mrqa_squad-validation-2622", "mrqa_squad-validation-2643", "mrqa_squad-validation-2659", "mrqa_squad-validation-2731", "mrqa_squad-validation-2732", "mrqa_squad-validation-2844", "mrqa_squad-validation-2858", "mrqa_squad-validation-2910", "mrqa_squad-validation-2948", "mrqa_squad-validation-2948", "mrqa_squad-validation-2995", "mrqa_squad-validation-3043", "mrqa_squad-validation-3085", "mrqa_squad-validation-3180", "mrqa_squad-validation-3259", "mrqa_squad-validation-3280", "mrqa_squad-validation-3349", "mrqa_squad-validation-3370", "mrqa_squad-validation-3390", "mrqa_squad-validation-3418", "mrqa_squad-validation-3518", "mrqa_squad-validation-356", "mrqa_squad-validation-3567", "mrqa_squad-validation-3632", "mrqa_squad-validation-366", "mrqa_squad-validation-3667", "mrqa_squad-validation-3679", "mrqa_squad-validation-3711", "mrqa_squad-validation-378", "mrqa_squad-validation-3790", "mrqa_squad-validation-3889", "mrqa_squad-validation-3909", "mrqa_squad-validation-392", "mrqa_squad-validation-3957", "mrqa_squad-validation-3959", "mrqa_squad-validation-3967", "mrqa_squad-validation-4058", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4116", "mrqa_squad-validation-4128", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4276", "mrqa_squad-validation-4289", "mrqa_squad-validation-4328", "mrqa_squad-validation-436", "mrqa_squad-validation-4607", "mrqa_squad-validation-4673", "mrqa_squad-validation-4691", "mrqa_squad-validation-470", "mrqa_squad-validation-4708", "mrqa_squad-validation-4760", "mrqa_squad-validation-4772", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4834", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-492", "mrqa_squad-validation-4927", "mrqa_squad-validation-4986", "mrqa_squad-validation-5034", "mrqa_squad-validation-5100", "mrqa_squad-validation-516", "mrqa_squad-validation-516", "mrqa_squad-validation-5161", "mrqa_squad-validation-5256", "mrqa_squad-validation-5418", "mrqa_squad-validation-5436", "mrqa_squad-validation-5485", "mrqa_squad-validation-551", "mrqa_squad-validation-565", "mrqa_squad-validation-5702", "mrqa_squad-validation-5762", "mrqa_squad-validation-5874", "mrqa_squad-validation-5929", "mrqa_squad-validation-5936", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6029", "mrqa_squad-validation-6035", "mrqa_squad-validation-6043", "mrqa_squad-validation-6129", "mrqa_squad-validation-6300", "mrqa_squad-validation-6332", "mrqa_squad-validation-639", "mrqa_squad-validation-6437", "mrqa_squad-validation-6450", "mrqa_squad-validation-6463", "mrqa_squad-validation-6592", "mrqa_squad-validation-6637", "mrqa_squad-validation-6949", "mrqa_squad-validation-7089", "mrqa_squad-validation-7110", "mrqa_squad-validation-7126", "mrqa_squad-validation-7201", "mrqa_squad-validation-7230", "mrqa_squad-validation-7261", "mrqa_squad-validation-7333", "mrqa_squad-validation-7351", "mrqa_squad-validation-736", "mrqa_squad-validation-7364", "mrqa_squad-validation-7488", "mrqa_squad-validation-7527", "mrqa_squad-validation-7599", "mrqa_squad-validation-7656", "mrqa_squad-validation-7698", "mrqa_squad-validation-7717", "mrqa_squad-validation-7722", "mrqa_squad-validation-7728", "mrqa_squad-validation-7763", "mrqa_squad-validation-7805", "mrqa_squad-validation-7837", "mrqa_squad-validation-7897", "mrqa_squad-validation-7950", "mrqa_squad-validation-7951", "mrqa_squad-validation-7960", "mrqa_squad-validation-7972", "mrqa_squad-validation-800", "mrqa_squad-validation-8014", "mrqa_squad-validation-8109", "mrqa_squad-validation-811", "mrqa_squad-validation-8125", "mrqa_squad-validation-8182", "mrqa_squad-validation-823", "mrqa_squad-validation-8324", "mrqa_squad-validation-8440", "mrqa_squad-validation-8509", "mrqa_squad-validation-859", "mrqa_squad-validation-864", "mrqa_squad-validation-8806", "mrqa_squad-validation-882", "mrqa_squad-validation-8906", "mrqa_squad-validation-893", "mrqa_squad-validation-9008", "mrqa_squad-validation-9063", "mrqa_squad-validation-9162", "mrqa_squad-validation-9194", "mrqa_squad-validation-9254", "mrqa_squad-validation-9318", "mrqa_squad-validation-9364", "mrqa_squad-validation-9460", "mrqa_squad-validation-9486", "mrqa_squad-validation-9530", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9600", "mrqa_squad-validation-9623", "mrqa_squad-validation-9655", "mrqa_squad-validation-9896", "mrqa_squad-validation-9908", "mrqa_squad-validation-9990", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-1262", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-2022", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-300", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4202", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4386", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-6067", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7316", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-975", "mrqa_triviaqa-validation-977"], "OKR": 0.85546875, "KG": 0.453125, "before_eval_results": {"predictions": ["its 50th anniversary special", "Thomas Savery", "Vicodin,", "new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago", "violent separatist campaign", "Eleven people died and 36 were wounded", "269,000", "The Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso,", "38.8 feet", "Eintracht Frankfurt", "150", "a pool of blood beneath his head.", "Russian bombers", "41,", "Los Alamitos Joint Forces Training Base", "super-yacht designers Wally", "137", "a Kurdish militant group in Turkey", "3-2", "autonomy", "Quebradillas", "the Russian air force,", "34", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel", "greenhouse emissions.", "Amanda Knox's aunt", "a lightning strike was a possibility,", "no shortage of the drug while patients wait for an approved product to take its place", "nutritionists", "Tom Baer.", "militants", "these planning processes are urgently needed and have been a long time in coming.", "heavy flannel or wool", "Brian Mabry", "iTunes,", "May 2000", "60 euros -- $89 --", "the American Civil Liberties Union", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "some truly mind-blowing structures", "my wife's name", "he and the other attackers were from Pakistan", "2006", "the home of NFL's Chargers", "six prostitutes and a runaway involved in the drug trade.", "Bergdahl, 23, was captured June 30 from Paktika province in southeastern Afghanistan,", "Twitter", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "physical security and policing", "heart", "Hyderabad", "Asia", "to stay, abide", "Las Vegas", "Jackson Pollock", "Lyrical", "Mississippi", "January 19, 1943", "King Duncan", "Georgia", "mmorpg", "Jeopardy"], "metric_results": {"EM": 0.375, "QA-F1": 0.5031023630044665}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.923076923076923, 1.0, 0.8, 0.25, 0.0, 0.35294117647058826, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.35714285714285715, 0.4444444444444445, 1.0, 0.0, 0.9032258064516129, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.1111111111111111, 0.0, 0.6666666666666666, 0.10526315789473685, 1.0, 0.0, 0.0, 0.2608695652173913, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-11832", "mrqa_searchqa-validation-9476"], "SR": 0.375, "CSR": 0.5576923076923077, "EFR": 1.0, "Overall": 0.7185697115384615}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "reckless", "pro-democracy activists clashed Friday with Egyptian security forces", "Krishna Rajaram,", "at least 25 dead", "Shakespeare's Pizza,", "finance", "Ross Perot", "in Hong Kong's Victoria Harbor", "2002", "six prostitutes and a runaway involved in the drug trade.", "legitimacy of that race.", "carbon footprint.", "three", "Monday", "Scarlett Keeling", "two years,", "Since 1980, the 84-year-old Mugabe has been the country's only ruler.", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "in July for A Country Christmas,", "The poster boy of Indian action films", "Alan Graham", "the guerrillas detained and \"executed\" eight people", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation and had two broken bones in his neck,", "Phil Spector", "Kim Jong Il's", "1994", "numerous suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "\"People of Palestine\"", "dependable Camry know what's important in life,", "raping her in a Milledgeville, Georgia,", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "Sheik Mohammed Ali al-Moayad", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "by 9 a.m.", "same-sex civil unions,", "fallen comrades lost in the heat of battle.", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "one", "Matt Monro", "Jack Frost", "the innermost digit of the forelimb; thumb", "1974", "over 20 million", "Peoria, Illinois", "Honolulu", "KID-FRIendly 4- LETTER", "Course Hero", "Ottoman Empire"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5647569444444445}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1875", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-5856", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.46875, "CSR": 0.5543981481481481, "EFR": 1.0, "Overall": 0.7179108796296296}, {"timecode": 27, "before_eval_results": {"predictions": ["in the early 1990s", "leaf-shaped", "silver", "(1755)", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "bank robber John Dillinger,", "the second missing person", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "a government-run health facility that provides her with free drug treatment.", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "$1.5 million", "2006", "Rev. Alberto Cutie", "Los Angeles Angels", "Indian army troopers, including one officer, and 17 militants,", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "in a Starbucks", "\"BADBUL,\"", "98 people,", "2008", "Gulf of Aden,", "Paul Ryan (R-WI)", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "between government soldiers and Taliban militants in the Swat Valley.", "in Iraq", "Iran", "November 26", "a Prius driver will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "in July", "Chad", "Four Americans", "The 19-year-old woman", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death", "Scardia", "fractured pelvis and sacrum", "Wednesday", "abduction of minors.", "gun", "Aniston, Demi Moore and Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "Flag Day in 1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "wheat", "Norwood, Massachusetts", "Manchester, England", "Soil", "Missouri", "Vermont's largest city", "beta blockers"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6907447167527814}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5714285714285714, 0.3636363636363636, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.06451612903225806, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4094", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-1922", "mrqa_triviaqa-validation-3389", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-1144", "mrqa_searchqa-validation-14535"], "SR": 0.578125, "CSR": 0.5552455357142857, "EFR": 1.0, "Overall": 0.7180803571428571}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author of the Fifth Assessment Report", "policy of major powers, or simply, general-purpose aggressiveness", "1981", "forgery and flying without a valid license,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force.", "shoot down the satellite", "European Commission", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "13.", "the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Anil Kapoor", "eradication of the Zetas cartel from the state of Veracruz,", "The Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "Christianity and Judaism", "the Dalai Lama's", "Russia", "around 8 p.m. local time Thursday", "Passers-by", "in the first place.", "executive director of the Americas Division of Human Rights Watch,", "750", "Six people were killed and at least 300", "Matthew Fisher", "The Ski Train", "Big Brother.", "Ozzy Osbourne", "AbdulMutallab,", "some U.S. senators", "inconclusive", "5:20 p.m.", "an American ship captain held hostage by Somali pirates", "$250,000", "100% of its byproducts", "School-age girls", "5 percent", "700,000", "Sen. Arlen Specter", "Deutschneudorf,", "would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "a deceased organ donor,", "bragging about his sex life on television", "a vertebral column ( spine )", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the orange yacht Britannia", "Douglas Hofstadter", "\"The Dark Tower\"", "American", "Marmee takes care of her 4 girls while her husband is away serving as an army chaplain in the Civil War", "Castle Rock", "a tomato, mozzarella, anchovy and oil"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6715765881017239}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.125, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.972972972972973, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-3839", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.59375, "CSR": 0.556573275862069, "EFR": 0.9615384615384616, "Overall": 0.7106535974801061}, {"timecode": 29, "before_eval_results": {"predictions": ["downward pressure on wages.", "El Tem\u00fcr,", "438,000", "Martin Ingerman", "coaxial", "Pakistan A", "Ever Bank Field.", "14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "the German Campaign of 1813", "Arabella Churchill,", "1965", "Charles de Gaulle Airport", "club based in Bayonne.", "Culiac\u00e1n, Sinaloa,", "seven", "Syracuse", "1986", "non-alcoholic", "puzzle video game", "Knoxville, Tennessee", "Washington, D.C.", "Gal\u00e1pagos Islands", "Tom Kartsotis,", "2017", "Wayman Tisdale", "Mexico,", "Kolkata", "Northern Ireland", "late 19th and early 20th centuries", "political thriller novel", "22,500", "the Harpe brothers", "Eric Liddell,", "2002", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Films", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey", "Florida", "Kentucky", "NBA Slam Dunk Contest.", "$10\u201320 million", "January 28, 2016", "Kennedy Road,", "Somerset County, Pennsylvania,", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes,", "IB Diploma Program and the IB Career - related Program for students aged 11 to 14", "Richard Parker", "South American mainland", "Charlotte Heston", "allergic reaction", "King Edward VIII,", "3,000 kilometers (1,900 miles),", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist", "Russia,", "shrimp", "Australia"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6694549374236874}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.28571428571428575, 0.25, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585"], "SR": 0.5625, "CSR": 0.5567708333333333, "EFR": 1.0, "Overall": 0.7183854166666667}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a farmers' co-op", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations", "ten years of probation", "In Pursuit", "Bolton", "The Frost Report", "Kansas City Crime Family", "Dirk Werner Nowitzki", "the Cecil B. DeMille Award honoree", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "Springfield, Massachusetts", "British comedian", "\"Apatosaurus\"", "1993", "American", "Frank Edward Thomas Jr.", "\"Gliding Dance of the Maidens\"", "Margarine Unie", "the 1946 Winecoff Hotel fire", "New York City", "The Seduction of Hillary Rodham", "2005", "Lambic", "Massive Entertainment", "Argentina", "Larry Alphonso Johnson Jr.", "Michael Edward \" Mike\" Mills", "veto power", "Joseph E. Grosberg", "\"Chelsea Lately\"", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "along the Californian coast at The Inn at Newport Ranch", "NY", "the U.S Olympic Trials", "Aston Villa Football Club", "2005", "228", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "PTSD", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6659320887445888}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6399999999999999, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-13695", "mrqa_searchqa-validation-6975"], "SR": 0.546875, "CSR": 0.5564516129032258, "EFR": 1.0, "Overall": 0.7183215725806452}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno native", "79", "Iceland", "Wyoming", "a huge terrestrial globe", "free fall ride", "Iowa", "a Van Morrison song", "Nassau", "a mollusks", "Dr. Robert Gallo", "Thomas Beekman", "a network of rail lines", "Rigoletto", "aardwolf", "Dadu", "Sir Roger Gilbert Bannister", "New Jersey Devils", "San Jose, California, United States", "Yves Saint Laurent", "reindeer", "Fortinbras", "a schooner & 1 sloop", "Grandma Moses", "Sailor Moon", "georgia state", "Dan Brown", "a bear", "a whirlwind", "Gilson Lavis", "elderberries", "negative electrode", "Milton Berle", "George Herbert Walker Bush", "Patrice Lumumba", "lunar module", "Chile", "Dan Marino", "Mars", "clownfish", "E = mc2", "Guru Pitka", "Las Vegas", "millet", "Butterflies", "heavy drinking", "orangutan", "Baja California", "soothsayer", "Yitzhak Rabin", "Saul", "Gettysburg National Military Park", "Jack Gleeson", "Plank", "Buddhism", "Jean Bernadotte", "Portugal", "Graham Bond", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "multiple nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "knocking the World Cup off the front pages", "12.3 million"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5505208333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4708", "mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-1768", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-11913", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-13033", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587"], "SR": 0.515625, "CSR": 0.55517578125, "EFR": 1.0, "Overall": 0.71806640625}, {"timecode": 32, "before_eval_results": {"predictions": ["Second World War", "a quarter square kilometer", "(MP) Henry Addington", "40", "Libya, on the south by the Central African Republic,", "Shania Twain", "Hillsborough", "glucagon", "The New York Yankees", "REM sleep", "green, red, white and black", "Ann Dunham", "(A\u1e25mad \u1e24asan al-Bakr)", "French", "Jim Branning", "Ohio", "Francis Matthews", "photographic", "hematite", "Noah", "Spain", "New", "Sarah Ferguson", "Mercury", "watt", "Peter Butterworth", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "Rio de Janeiro", "long-suffering wife, Emma,", "aged 75", "Jennifer Lopez", "1664", "Annie Lennox", "Fred Perry", "Downton Abbey", "Martina Hingis", "painter", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "a gulliver", "Pomona", "Italy", "The Streets", "the Great Appalachian Valley", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "Campbell's", "Kirkcudbright", "the soldiers", "cortisone.", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "Diablo Cody", "Helvetica", "pulmonary"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6031850961538461}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.25, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4343", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-16567"], "SR": 0.53125, "CSR": 0.5544507575757576, "EFR": 1.0, "Overall": 0.7179214015151516}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "stanford", "insulin", "chicken", "Hudson Bay", "florida", "hay fever", "stanley", "Ast\u00e9rix", "london", "Belfast", "wind", "fire insurance", "a Holy Grail", "West Point", "Andy Warhol", "london", "stanley", "stanley", "the solar system", "potatoes", "Moldova", "mitsubishi A6M Zero Fighter", "wednesday", "jesus", "st leger", "baroudeur", "clare", "pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "Philippines", "beaver", "Mel Blanc", "stanley", "wednesday", "stanese", "stanley", "5000 meters", "racing", "calcium phosphate", "Newfoundland", "crow", "Yellowstone", "St. Francis Xavier", "luzon", "1st earl shales", "Buddhism", "jonny Buckland", "Ohio", "Port Melbourne", "\u00c6thelred I", "Scarface", "forgery and flying without a valid license", "Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0", "Sophia Stellatos", "Spock", "Kazakhstan", "london"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4822916666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-3524", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_triviaqa-validation-2642", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-525", "mrqa_searchqa-validation-11382"], "SR": 0.453125, "CSR": 0.5514705882352942, "EFR": 1.0, "Overall": 0.7173253676470589}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business districts", "steppes steppe", "Bologna, Italy", "george Santayana", "opossum", "Alice Cooper", "angiotensin II", "trumpet", "Marc Warren", "The Cry", "shildon", "appalachian mountain range", "Herald of Free Enterprise", "ballet", "Titanic", "george", "lizard", "blackburn Lancashire", "george laine", "The Mystery of Edwin Drood", "pommel", "cardinal", "Dick Van Dyke", "Egremont", "numb3rs", "george velazquez", "phrixus", "george roos", "Canada", "venom", "pears soap", "Some Like It Hot", "iona", "ireland", "Mike Meyers", "sea horse", "a DeLorean", "magma", "jules verne", "welcome", "spain", "laurie", "george Andrews", "26 miles", "Cleveland Brown", "heston Blumenthal", "One Direction", "Captain Flint", "Uranus", "george Christie", "george addis", "November 1999", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "stoneware", "Pittsburgh", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "ballet", "Howard Carter"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5390625}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-508", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-78", "mrqa_searchqa-validation-4312"], "SR": 0.484375, "CSR": 0.5495535714285714, "EFR": 1.0, "Overall": 0.7169419642857143}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Leonardo da Vinci", "matlock", "American Civil War", "shoa", "cetaceans", "Arafura Sea", "a statue", "Euphrates", "czech republic", "to make wrinkles in one's face,", "spain", "Carousel", "bullfighting", "jane jane", "Countertenor", "inks", "fidelio", "Guys and Dolls", "Julian Fellowes", "Denmark", "Another Day in paradise", "The Last King of Scotland", "ghanians", "pembrokeshire", "G. Ramon", "jane fonda", "rachmaninoff", "Finland", "stars", "Mille Miglia", "inks", "charleston and fats Domino", "50p", "Muriel Spark", "happy birthday", "seven", "inks", "Pickwick Papers", "presliced bread", "Saga noren", "raven", "jean", "can", "nelsons", "Etruscans", "Ken Burns", "jane park", "Helen Glover", "E. T. A. Hoffmann", "Sheikh Mujib", "inanimate object", "Donna", "season four", "sinoatrial node", "Yubin, Yeeun", "tomato", "2002", "workers who lose out to British labor to claim they were being discriminated against on the basis of nationality.", "L'Aquila earthquake", "March 24", "Prince Philip, Duke", "September", "Pocahontas"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5564285714285715}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228"], "SR": 0.484375, "CSR": 0.5477430555555556, "EFR": 1.0, "Overall": 0.716579861111111}, {"timecode": 36, "before_eval_results": {"predictions": ["the Iranian Islamic Revolution", "Kim", "city of acacias", "branson,", "Gordon Ramsay", "denis law", "Robert f. Kennedy", "sulfur dioxide and nitrogen oxides", "maurice Frank", "Manchester Airport", "Portuguese", "Travelocity", "Avengers", "knaresborough", "comets", "Paul Simon and Garfunkel", "disciples", "canola", "joan crawford", "joanna Lumley", "duttlenheim", "Bolivia", "John Donne", "Uranus", "Rio Grande", "clown", "maurice man", "30th anniversary", "joan Fontaine", "James I", "One Foot in the Grave", "Bronx Mowgli", "maurice crawford", "George Santayana", "archery", "borowdale", "louis crawford and his father", "joan de torquemada", "joan maurice du Pr\u00e9", "Canada", "rum", "Lake Union", "ghee", "King George III", "comets", "a statement or situation where the meaning is contradicted by the appearance or presentation of the idea", "oldpatrick", "June", "joan crawford", "Ceylon", "screwdrivers", "the Kansas City Chiefs", "minor key", "A Christmas Story", "1974", "The Outsiders", "Amberley Village", "lack of a cause of death and the absence of any soft tissue", "Canadian Prime Minister Stephen Harper,", "those who were locked in a basement,", "Pearl S. Buck", "Brigham Young", "Pearl", "chalk quarry"], "metric_results": {"EM": 0.390625, "QA-F1": 0.46979166666666666}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9574", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4108", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120"], "SR": 0.390625, "CSR": 0.5434966216216216, "EFR": 1.0, "Overall": 0.7157305743243243}, {"timecode": 37, "before_eval_results": {"predictions": ["United States computer networking consortium", "neutral", "aisles", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "a perceived harmful event, attack, or threat to survival", "49 cents", "1876", "geologist James Hutton", "14.69278", "joy of living", "420", "John Prine and Roger Cook", "sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "London, England", "Los Angeles", "February 10, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "1770 BC", "Niveditha, Diwakar, Shruti", "two", "John C. Reilly", "in organelles", "Anakin and Mace Windu", "Travis Tritt and Marty Stuart", "1976", "Bee Gees", "Matt Czuchry", "Pradyumna", "1902", "Lands End", "one of the seven heavenly virtues", "New Jersey Devils", "two", "4 in ( 10 cm )", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria", "Canadian Rockies", "Maginot Line", "silesia", "dumbo", "purple", "James A. Garfield", "Gettysburg Address", "iTunes", "$273 million", "India", "Jeddah, Saudi Arabia", "Desperate Housewives", "Cannonball Run", "morelos", "Tuesday"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6253618286906331}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.09523809523809523, 0.0, 0.0, 0.0, 0.787878787878788, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.8, 0.5, 0.6666666666666666, 0.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07272727272727272, 0.0, 0.5454545454545454, 1.0, 0.6666666666666666, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4860", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-3854", "mrqa_searchqa-validation-2335"], "SR": 0.484375, "CSR": 0.5419407894736843, "EFR": 0.9393939393939394, "Overall": 0.7032981957735247}, {"timecode": 38, "before_eval_results": {"predictions": ["James Watt", "roughly five hundred experts across the world", "the United States", "Kim Basinger", "fall of 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in the pancreas", "Charles Crozat Converse", "Lady Gaga", "the Chicago metropolitan area", "the President of the United States", "Domhnall Gleeson", "eusebeia", "horticulture", "Notts County", "f\u0254n", "Stephen A. Douglas", "1984", "man", "Pakistan", "23 February", "Tagalog or English", "Bryan Cranston", "thylakoid membranes", "low self - esteem", "Felix Baumgartner", "Franklin and Wake counties", "late 1922", "the final play of the 2017 / 18 Divisional Round game against the New Orleans Saints", "520", "stable, non-radioactive rubidium - 85", "between $10,000 and $30,000", "R2E Micral CCMC", "1931", "the University of Oxford", "Queenstown ( now Cobh ) in Ireland", "Gladys Knight & the Pips", "1959", "Southern Cause", "Randy", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "Joseph Stalin", "in the intermembrane space", "a divergent tectonic plate boundary", "North Dakota", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "yellow", "Sir John Major", "Roddy doddy doyle", "Daniil Shafran", "TD Garden", "Venus", "his foreign policy approach, particularly as it relates to human rights around the globe.", "10 below in Chicago, Illlinois", "General Motors'", "David McCullough", "a science fiction novel", "CERN", "london"], "metric_results": {"EM": 0.5, "QA-F1": 0.570767244397759}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1, 1.0, 0.11764705882352941, 0.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.32, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-5582", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.5, "CSR": 0.5408653846153846, "EFR": 0.96875, "Overall": 0.7089543269230769}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale", "Jenny Slate", "Active absorption", "Philippe Petit", "September 1980", "January 2004", "provinces along the Yangtze River and in provinces in the south", "Toby Keith", "development of electronic computers in the 1950s", "17 - year - old", "alternative rock", "Set six months after Kratos killed his wife and child,", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "A 30 - something man ( XXXX )", "Gestalt psychology", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "John DiMaggio", "Richard Crispin Armitage", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "either late 2018 or early 2019", "a diffuse interstellar medium ( ISM ) of gas and dust", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "Frederik Barth", "John F. Kelly", "Charles Sherrington", "1886", "small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "related to the Common Germanic word guma", "1960s", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "a defense against rain rather than sun", "1940", "Ariel Winter", "Mark Jackson", "Michael Buffer", "a baptism, one God and Father of all, who is over all and through all and in all", "on location", "the federal government", "1958", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "4.5", "alcatraz", "Joseph Smith,", "puck", "1909", "Lawn Dogs", "460", "Princess Diana", "Mikkel Kessler", "curfew", "\"Me and Bobby McGee\"", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.5, "QA-F1": 0.6277170400253557}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4210526315789474, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.0, 0.6666666666666666, 0.47058823529411764, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.18750000000000003, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-10341"], "SR": 0.5, "CSR": 0.53984375, "EFR": 0.96875, "Overall": 0.70875}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "eight Indian army troopers, including one officer, and 17 militants", "Rivers", "\"You're The One That I Want\"", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "\"Z Zimbabwe cannot be British, it cannot be American. Yes, it is African,\"", "since 2004", "NATO", "Switzerland", "Monday", "500", "\"Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "\"it is impossible to turn back the tide of globalization.\"", "Clifford Harris,", "Oaxacan countryside of southern Mexico", "Robert Barnett", "$627", "41,", "Adenhart", "a strict interpretation of the law,", "Derek Mears", "Sylt", "rural Tennessee", "Tuesday afternoon", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "to show that a visitor had been to the grave.", "The son of Gabon's former president", "The Transportation Security Administration", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Derek Mears", "Operation Pipeline Express", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "St. Louis, Missouri", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls", "2018 and 2019", "P.V. Sindhu", "location filming was in Cuernavaca, Durango, and Tepoztl\u00e1n", "snickers", "Monodon monoceros", "capone", "Lake Buena Vista, Florida", "a Mexican crime lord, bootlegger, businessman and smuggler", "Bergen", "embalming", "capagena", "a graphical user", "the American Kennel Club"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6605725744944031}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 0.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 0.14285714285714288, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.3870967741935484, 0.07142857142857142, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 0.3076923076923077, 0.4, 1.0, 0.3333333333333333, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.46875, "CSR": 0.538109756097561, "EFR": 1.0, "Overall": 0.7146532012195121}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "displacement", "layered systems of sovereignty", "Megan Park", "euro", "Kate", "September 14, 2008", "American country music artist Trace Adkins", "Mars Hill", "1648", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "semi solid", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "international aid as one of the largest financial inflows to developing countries", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "Initially registered with churches, who maintained registers of births", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Malloy as Pierre, Phillipa Soo asasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne", "to collect menstrual flow", "pigs", "General George Washington", "Spanish", "Howard Ellsworth Rollins Jr", "an integral membrane protein that builds up a proton gradient across a biological membrane", "through the right atrium to the atrioventricular node", "four", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Intertropical Convergence Zone ( ITCZ )", "Missouri River", "the right to be served in facilities which are open to the public", "frontal lobe", "10 June 1940", "Tandi", "alberich", "ear", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "the project, which is designed to promote private sector investment in a variety of gas-related industries, on September 21.", "Denver", "\"Mr. Farley was a member of our embedded Provincial Reconstruction Team for the Sadr City and Adhamiya districts of Baghdad City,\"", "Chloe O'Brian", "King Arthur", "Noel Edmonds", "Virgin America"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6356263667747801}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.5555555555555556, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.7000000000000001, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.8235294117647058, 0.25, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5205"], "SR": 0.484375, "CSR": 0.5368303571428572, "EFR": 0.9393939393939394, "Overall": 0.7022761093073593}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "the population, serving staggered terms of six years", "Zeus", "During Hanna's recovery masquerade celebration", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "victory", "Gustav Bauer", "ceramics", "the Soviet Union", "Covington, Kentucky", "New Mexico", "to condense the steam coming out of the cylinders or turbines", "December 15, 2017", "located on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue, just west of Interstate 15", "L.K. Advani", "differential erosion", "Glenn Close", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "the notion that an English p Larson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "compasses", "Charlotte Thornton", "the Northeast Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Disha Vakani", "2,140 kilometres ( 1,330 mi )", "by producing an egg through parthenogenesis", "1926", "East Asia", "31 December 1947", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Norse god of the ocean, and husband of Ran.", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "The show went on without the self-proclaimed \"King of the South,\"", "prostate cancer,", "wyvern", "Lord Fauntleroy", "A rabbit with a fob", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6468508886835664}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.15384615384615383, 1.0, 0.7272727272727272, 1.0, 0.9, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6976744186046512, 0.14814814814814814, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.9767441860465117, 1.0, 0.7058823529411765, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.515625, "CSR": 0.5363372093023255, "EFR": 0.9032258064516129, "Overall": 0.6949438531507877}, {"timecode": 43, "before_eval_results": {"predictions": ["2003", "February 27, 2007", "\" pick yourself up and dust yourself off and keep going ', female - empowerment song", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "le Roi d'Irlande", "Miami Heat", "1982", "After World War I", "in the mid - to late 1920s", "Maximilien Robespierre", "Juan Francisco Ochoa", "Brutus", "de Toulouse", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "balsam", "Alex Ryan", "a habitat", "2018", "Windows Media Player 11", "100 members", "Toledo", "no embryo", "During the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "in Paradise, Nevada", "Alicia Vikander", "in late January or early February", "Ashoka", "the compartments were intended to safeguard the King's Chamber from the possibility of a roof collapsing under the weight of stone above the Chamber", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority", "Bumblebee", "the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "AMX - 30", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011", "2008", "Florida", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "in the southwestern part of the island", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "wintertime", "Pangaea", "Newcastle Brown Ale", "Western Australia", "V\u00e1clav Havel,", "Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\"", "\"peregruzka\"", "America's education, infrastructure, energy and health care systems.", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5959112162770843}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 0.4705882352941177, 1.0, 0.0909090909090909, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.9189189189189189, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-8027", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977"], "SR": 0.484375, "CSR": 0.53515625, "EFR": 1.0, "Overall": 0.7140625}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "lightweight aluminum foil", "in Laurel, Mississippi", "about the outdoors, especially mountain-climbing", "Indianola", "Escorts Limited", "daniel Hale Williams Preparatory School of Medicine", "1964", "Cher", "Alabama", "Jim Harrison", "Toronto", "Tomorrowland", "fennec fox", "United States Army", "seasonal television specials", "Jean Acker", "4,530", "leucippus", "Caesars Entertainment Corporation", "Annie Leibovitz", "Reinhard Heydrich", "Karl Kraus", "Steve Howey", "Maria Brink", "Manitowoc County, Wisconsin", "Northrop P-61 Black widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Nationalism", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "Andr\u00e9 3000", "Richard Street", "Zaire", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "a pioneering New Zealand food writer", "South America", "2006", "perjury and obstruction of justice", "Operation Overlord", "Selina D'Arcy", "over 9,000", "John Nightingale", "potential of hydrogen", "Alamodome in San Antonio, Texas", "Carrie", "a tab", "Kent", "almost 9 million", "Kenya", "2008", "small", "Moses", "Chapter 5", "Wilson Pickett"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6202323717948719}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.8, 0.8, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.53125, "CSR": 0.5350694444444444, "EFR": 1.0, "Overall": 0.7140451388888889}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "1858", "Australian", "September 1903", "interstate commerce", "Naomi Wallace", "Jenson button", "Tufts College", "People's Republic of China", "Azeroth", "Squam Lake", "The Livingston family of New York", "Tayeb Salih", "King James II of England", "God Save the Queen", "203", "Scotland", "AC/DC", "GmbH", "Mick Jackson", "Lalit", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "Chesley Burnett \"Sully\" Sullenberger III", "Los Alamos National Laboratory", "the Asia-Pacific War", "Romantic", "1st Baron Dowding", "AMC Entertainment Holdings, Inc.", "New York Islanders", "fennec", "1978", "six different constructors taking the first six positions", "Canadian", "Pacific Place", "ASEAN Football Federation", "is a song by American rapper Kendrick Lamar", "Rudebox", "about 5320 km", "Gianni Schicchi", "Chief Minister of Tamil Nadu", "Sacramento Kings", "Walldorf", "Fife", "Fyvie", "Faisal Qureshi", "the British Army", "elections", "Boletus edulis", "Robert Remak", "JackScanlon", "D.J.", "Frances Ethel Gumm", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "a singer-songwriter, multi-instrumentalist, and actor", "Saudi Arabia", "Sabo", "two"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5552455357142857}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.4, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5048", "mrqa_triviaqa-validation-6575", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.46875, "CSR": 0.5336277173913043, "EFR": 1.0, "Overall": 0.7137567934782608}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican National Committee's website address is GOP.com", "1996", "5", "Greenland shark", "The Word", "President Abraham Lincoln's", "St Jude Thaddeus", "Van Diemenslandt", "the death penalty", "xerophyte", "Jack Roosevelt Robinson", "Manhattan", "Dian Fossey (Sigourney Weaver)", "MI5", "Harrow", "creme anglaise", "lemon juice, parsley, salt, pepper, and drawn butter", "pork", "curling", "Victoria Coren", "Gettysburg", "Chile\u2019s", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Venus", "Venus", "President Barack Obama", "Canada's Liberal Party", "F-O-R-G-T-E-N", "Castro and the Cuban Revolution", "David Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "Florida", "Mary Poppins", "Alex Turner", "Fairfax Moresby", "Connecticut", "Quentin Blake", "whooping cough", "Daily Herald", "numerous", "\"permissible.\"", "2016", "the courts", "2017", "Chief of Protocol", "Diamond White", "1944", "umpire Jake Garner", "near Garacad, Somalia", "death", "Flopsy & Mopsy", "George Stephanopoulos", "Reader's Digest Association", "(GNT)"], "metric_results": {"EM": 0.484375, "QA-F1": 0.59765625}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3458", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-4157", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-14604", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.484375, "CSR": 0.5325797872340425, "EFR": 1.0, "Overall": 0.7135472074468086}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "dian capllo", "Culloden", "Runic", "canada", "tennis", "dian kirchhoff", "rotherham United", "conduction", "Misery", "Styal", "stately", "Scotch Jack' Dickson", "Brainwash", "loy Burrell", "parlophone", "Wild Atlantic Way", "john Denver", "canada", "hilitos", "lackawanna", "chile", "a domino", "muezzin", "window", "the keel", "madame", "Apollo 11", "Cellophane", "Nikola Tesla", "Nicky Henderson", "is a song from the 1976 musical evita,", "albino sperm whale", "roddy", "east fife", "st Pancras International Station", "social environment", "pre sliced bread", "dilbert", "Aristotelian Tragedy", "nunc dimittis", "French", "medea", "Burgundy", "cribbage", "barlow", "Johannesburg", "london", "The Muffin Man", "korea", "Prince James, Duke of York and of Albany", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "the Greenbriar Boys", "Pansexuality", "Tony Ducks", "1754", "drugs", "Veracruz, Mexico", "carrier based in Texas.", "Robert frost", "King Henry VIII", "barack", "Mitsubishi Lancer OZ Rally"], "metric_results": {"EM": 0.546875, "QA-F1": 0.617906746031746}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-4571", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.546875, "CSR": 0.5328776041666667, "EFR": 1.0, "Overall": 0.7136067708333333}, {"timecode": 48, "before_eval_results": {"predictions": ["Route sixty-six", "sesame street", "onions", "cabbage", "south Wales", "jimmy magoo", "fleece", "Ash tree", "opossum", "New Zealand", "jug band", "60", "goldfinger", "1984", "pike", "mongols", "1875", "tax collector", "marmoleonic penny", "marmole", "jimmy of anjou", "bagram Collection Point", "maggie Gilkeson", "Chrysler", "ushanka", "noddy", "Philosophy (B.Phil.), or Bachelor of Sacred Theology (Baccalaureatus)", "france", "south Africa", "spain", "biathlon", "mercury", "charlie Chan", "france", "white", "jaws", "france noddy", "rabbit", "france", "jodhpurs", "Orson Welles", "ummarized", "menorah", "Dutch", "france", "Super Bowl Sunday", "quant pole", "Little Tommy Stout", "marmole king", "azalea", "france", "Chuck Noland", "Virginia", "in Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park", "2010", "ancient Greek site of Olympia", "10 below", "Nearly all of Britain's troops in Iraq", "turtle", "the American Kennel Club", "Omaha", "George Glenn Jones"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4874158902691511}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-2158", "mrqa_naturalquestions-validation-6564", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.390625, "CSR": 0.5299744897959184, "EFR": 1.0, "Overall": 0.7130261479591837}, {"timecode": 49, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2824", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-610", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7763", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6978", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.783203125, "KG": 0.40390625, "before_eval_results": {"predictions": ["Percy sledge", "shanghai", "tobacco", "france", "jockey", "daniel Boone", "Thames Street", "theodore roosevelt", "satyrs", "crab", "lee boheme", "Microsoft", "wishbone", "garrick club", "jaber elbaneh", "master Humphrey\u2019s Clock", "jennifer bullock", "American Civil War", "\"black\"", "jennifer gove", "jimmy Robertson", "Florence", "tsar Ivan IV", "veruca salt", "river severn", "australia", "South Africa", "plants that have proteins like those in pollen", "Nicaragua", "jennifer attlee", "war of roses", "leipzig and Dresden", "playoff years,", "trout", "an ap\u00e9ritif", "kenn Norton", "northern america", "state of texas", "hair loss", "sprint", "charlie Drake", "robin hood", "Chris Martin", "jennifer Flintstone", "jennifer jones", "rugby", "honda", "deacon blue", "11", "tobacco", "dharm", "free floating", "Tom Selleck", "New Orleans", "a pinball machine designed by Steve Ritchie and manufactured by Stern Pinball", "Texas Tech University", "Loughborough Technical Institute", "Herman Cain", "Afghanistan", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "George Babbitt", "state of Oklahoma", "Ladies Who Lunch", "four"], "metric_results": {"EM": 0.390625, "QA-F1": 0.47159455128205124}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6153846153846153, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2309", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-1810", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-12598", "mrqa_searchqa-validation-3615"], "SR": 0.390625, "CSR": 0.5271874999999999, "EFR": 0.9743589743589743, "Overall": 0.6728874198717948}]}